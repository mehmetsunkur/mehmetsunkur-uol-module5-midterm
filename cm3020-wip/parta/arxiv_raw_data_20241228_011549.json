[
  {
    "title": "Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems",
    "authors": [
      "Fernando Jia",
      "Jade Zheng",
      "Florence Li"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry.",
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18601v1",
    "entry_id": "http://arxiv.org/abs/2412.18601v1"
  },
  {
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Sora-like video generation models have achieved remarkable progress with a\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\nvideo generation models predominantly focus on single-prompt, struggling to\ngenerate coherent scenes with multiple sequential prompts that better reflect\nreal-world dynamic scenarios. While some pioneering works have explored\nmulti-prompt video generation, they face significant challenges including\nstrict training data requirements, weak prompt following, and unnatural\ntransitions. To address these problems, we propose DiTCtrl, a training-free\nmulti-prompt video generation method under MM-DiT architectures for the first\ntime. Our key idea is to take the multi-prompt video generation task as\ntemporal video editing with smooth transitions. To achieve this goal, we first\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\ndiffusion models, enabling mask-guided precise semantic control across\ndifferent prompts with attention sharing for multi-prompt video generation.\nBased on our careful design, the video generated by DiTCtrl achieves smooth\ntransitions and consistent object motion given multiple sequential prompts\nwithout additional training. Besides, we also present MPVBench, a new benchmark\nspecially designed for multi-prompt video generation to evaluate the\nperformance of multi-prompt generation. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance without additional training.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18597v1",
    "entry_id": "http://arxiv.org/abs/2412.18597v1"
  },
  {
    "title": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs",
    "authors": [
      "OpenMind",
      "Shaohong Zhong",
      "Adam Zhou",
      "Boyuan Chen",
      "Homin Luo",
      "Jan Liphardt"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans.",
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18588v1",
    "entry_id": "http://arxiv.org/abs/2412.18588v1"
  },
  {
    "title": "How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation",
    "authors": [
      "Dewu Zheng",
      "Yanlin Wang",
      "Ensheng Shi",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.",
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18573v1",
    "entry_id": "http://arxiv.org/abs/2412.18573v1"
  },
  {
    "title": "Token-Budget-Aware LLM Reasoning",
    "authors": [
      "Tingxu Han",
      "Chunrong Fang",
      "Shiyu Zhao",
      "Shiqing Ma",
      "Zhenyu Chen",
      "Zhenting Wang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18547v1",
    "entry_id": "http://arxiv.org/abs/2412.18547v1"
  },
  {
    "title": "Advancing Deformable Medical Image Registration with Multi-axis Cross-covariance Attention",
    "authors": [
      "Mingyuan Meng",
      "Michael Fulham",
      "Lei Bi",
      "Jinman Kim"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Deformable image registration is a fundamental requirement for medical image\nanalysis. Recently, transformers have been widely used in deep learning-based\nregistration methods for their ability to capture long-range dependency via\nself-attention (SA). However, the high computation and memory loads of SA\n(growing quadratically with the spatial resolution) hinder transformers from\nprocessing subtle textural information in high-resolution image features, e.g.,\nat the full and half image resolutions. This limits deformable registration as\nthe high-resolution textural information is crucial for finding precise\npixel-wise correspondence between subtle anatomical structures.\nCross-covariance Attention (XCA), as a \"transposed\" version of SA that operates\nacross feature channels, has complexity growing linearly with the spatial\nresolution, providing the feasibility of capturing long-range dependency among\nhigh-resolution image features. However, existing XCA-based transformers merely\ncapture coarse global long-range dependency, which are unsuitable for\ndeformable image registration relying primarily on fine-grained local\ncorrespondence. In this study, we propose to improve existing deep\nlearning-based registration methods by embedding a new XCA mechanism. To this\nend, we design an XCA-based transformer block optimized for deformable medical\nimage registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general\nnetwork block that can be embedded into various registration network\narchitectures. It can capture both global and local long-range dependency among\nhigh-resolution image features by applying regional and dilated XCA in parallel\nvia a multi-axis design. Extensive experiments on two well-benchmarked\ninter-/intra-patient registration tasks with seven public medical datasets\ndemonstrate that our MAXCA block enables state-of-the-art registration\nperformance.",
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18545v1",
    "entry_id": "http://arxiv.org/abs/2412.18545v1"
  },
  {
    "title": "Consistency Checks for Language Model Forecasters",
    "authors": [
      "Daniel Paleka",
      "Abhimanyu Pallavi Sudhir",
      "Alejandro Alvarez",
      "Vineeth Bhat",
      "Adam Shen",
      "Evan Wang",
      "Florian Tram\u00e8r"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18544v1",
    "entry_id": "http://arxiv.org/abs/2412.18544v1"
  },
  {
    "title": "Characterizations of Language Generation With Breadth",
    "authors": [
      "Alkis Kalavasis",
      "Anay Mehrotra",
      "Grigoris Velegkas"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "We study language generation in the limit, introduced by Kleinberg and\nMullainathan [KM24], building on classical works of Gold [Gol67] and Angluin\n[Ang79]. [KM24] proposed an algorithm that generates strings from any countable\nlanguage collection in the limit. While their algorithm eventually outputs\nstrings from the target language $K$, it sacrifices breadth, i.e., the ability\nto generate all strings in $K$. A key open question in [KM24] is whether this\ntrade-off between consistency and breadth is inherrent.\n  Recent works proposed different notions of consistent generation with\nbreadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced three\ndefinitions: generation with exact breadth, approximate breadth, and\nunambiguous generation. Concurrently and independently, Charikar and Pabbaraju\n[CP24a] proposed exhaustive generation. Both works examined when generation\nwith these notions of breadth is possible.\n  Building on [CP24a, KVM24], we fully characterize language generation for\nthese notions and their natural combinations. For exact breadth, we provide an\nunconditional lower bound, removing a technical condition from [KVM24] and\nextending the result of [CP24a] that holds for specific collections of\nlanguages. We show that generation with exact breadth is characterized by\nAngluin's condition for identification. We further introduce a weaker version\nof Angluin's condition that tightly characterizes both approximate breadth and\nexhaustive generation, proving their equivalence. Additionally, we show that\nunambiguous generation is also characterized by Angluin's condition as a\nspecial case of a broader result. Finally, we strengthen [KVM24] by giving\nunconditional lower bounds for stable generators, showing that Angluin's\ncondition characterizes the previous breadth notions for stable generators.\nThis shows a separation between stable and unstable generation with approximate\nbreadth.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18530v1",
    "entry_id": "http://arxiv.org/abs/2412.18530v1"
  },
  {
    "title": "Joint Adaptive OFDM and Reinforcement Learning Design for Autonomous Vehicles: Leveraging Age of Updates",
    "authors": [
      "Mamady Delamou",
      "Ahmed Naeem",
      "Huseyin Arslan",
      "El Mehdi Amhoud"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Millimeter wave (mmWave)-based orthogonal frequency-division multiplexing\n(OFDM) stands out as a suitable alternative for high-resolution sensing and\nhigh-speed data transmission. To meet communication and sensing requirements,\nmany works propose a static configuration where the wave's hyperparameters such\nas the number of symbols in a frame and the number of frames in a communication\nslot are already predefined. However, two facts oblige us to redefine the\nproblem, (1) the environment is often dynamic and uncertain, and (2) mmWave is\nseverely impacted by wireless environments. A striking example where this\nchallenge is very prominent is autonomous vehicle (AV). Such a system leverages\nintegrated sensing and communication (ISAC) using mmWave to manage data\ntransmission and the dynamism of the environment. In this work, we consider an\nautonomous vehicle network where an AV utilizes its queue state information\n(QSI) and channel state information (CSI) in conjunction with reinforcement\nlearning techniques to manage communication and sensing. This enables the AV to\nachieve two primary objectives: establishing a stable communication link with\nother AVs and accurately estimating the velocities of surrounding objects with\nhigh resolution. The communication performance is therefore evaluated based on\nthe queue state, the effective data rate, and the discarded packets rate. In\ncontrast, the effectiveness of the sensing is assessed using the velocity\nresolution. In addition, we exploit adaptive OFDM techniques for dynamic\nmodulation, and we suggest a reward function that leverages the age of updates\nto handle the communication buffer and improve sensing. The system is validated\nusing advantage actor-critic (A2C) and proximal policy optimization (PPO).\nFurthermore, we compare our solution with the existing design and demonstrate\nits superior performance by computer simulations.",
    "doi": null,
    "primary_category": "eess.SP",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18500v1",
    "entry_id": "http://arxiv.org/abs/2412.18500v1"
  },
  {
    "title": "How \"Real\" is Your Real-Time Simultaneous Speech-to-Text Translation System?",
    "authors": [
      "Sara Papi",
      "Peter Polak",
      "Ond\u0159ej Bojar",
      "Dominik Mach\u00e1\u010dek"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Simultaneous speech-to-text translation (SimulST) translates source-language\nspeech into target-language text concurrently with the speaker's speech,\nensuring low latency for better user comprehension. Despite its intended\napplication to unbounded speech, most research has focused on human\npre-segmented speech, simplifying the task and overlooking significant\nchallenges. This narrow focus, coupled with widespread terminological\ninconsistencies, is limiting the applicability of research outcomes to\nreal-world applications, ultimately hindering progress in the field. Our\nextensive literature review of 110 papers not only reveals these critical\nissues in current research but also serves as the foundation for our key\ncontributions. We 1) define the steps and core components of a SimulST system,\nproposing a standardized terminology and taxonomy; 2) conduct a thorough\nanalysis of community trends, and 3) offer concrete recommendations and future\ndirections to bridge the gaps in existing literature, from evaluation\nframeworks to system architectures, for advancing the field towards more\nrealistic and effective SimulST solutions.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18495v1",
    "entry_id": "http://arxiv.org/abs/2412.18495v1"
  },
  {
    "title": "An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving",
    "authors": [
      "Gnaneswar Villuri",
      "Alex Doboli"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This report characterized the suitability of existing datasets for devising\nnew Machine Learning models, decision making methods, and analysis algorithms\nto improve Collaborative Problem Solving and then enumerated requirements for\nfuture datasets to be devised. Problem solving was assumed to be performed in\nteams of about three, four members, which talked to each other. A dataset\nconsists of the speech recordings of such teams. The characterization\nmethodology was based on metrics that capture cognitive, social, and emotional\nactivities and situations. The report presented the analysis of a large group\nof datasets developed for Spoken Language Understanding, a research area with\nsome similarity to Collaborative Problem Solving.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18489v1",
    "entry_id": "http://arxiv.org/abs/2412.18489v1"
  },
  {
    "title": "MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation",
    "authors": [
      "Tengfei He",
      "Xiao Zhou"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Social segregation in cities, spanning racial, residential, and income\ndimensions, is becoming more diverse and severe. As urban spaces and social\nrelations grow more complex, residents in metropolitan areas experience varying\nlevels of social segregation. If left unaddressed, this could lead to increased\ncrime rates, heightened social tensions, and other serious issues. Effectively\nquantifying and analyzing the structures within urban spaces and resident\ninteractions is crucial for addressing segregation. Previous studies have\nmainly focused on surface-level indicators of urban segregation, lacking\ncomprehensive analyses of urban structure and mobility. This limitation fails\nto capture the full complexity of segregation. To address this gap, we propose\na framework named Motif-Enhanced Graph Prototype Learning (MotifGPL),which\nconsists of three key modules: prototype-based graph structure extraction,\nmotif distribution discovery, and urban graph structure reconstruction.\nSpecifically, we use graph structure prototype learning to extract key\nprototypes from both the urban spatial graph and the origin-destination graph,\nincorporating key urban attributes such as points of interest, street view\nimages, and flow indices. To enhance interpretability, the motif distribution\ndiscovery module matches each prototype with similar motifs, representing\nsimpler graph structures reflecting local patterns. Finally, we use the motif\ndistribution results to guide the reconstruction of the two graphs. This model\nenables a detailed exploration of urban spatial structures and resident\nmobility patterns, helping identify and analyze motif patterns that influence\nurban segregation, guiding the reconstruction of urban graph structures.\nExperimental results demonstrate that MotifGPL effectively reveals the key\nmotifs affecting urban social segregation and offer robust guidance for\nmitigating this issue.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18464v1",
    "entry_id": "http://arxiv.org/abs/2412.18464v1"
  },
  {
    "title": "GeFL: Model-Agnostic Federated Learning with Generative Models",
    "authors": [
      "Honggu Kang",
      "Seohyeon Cha",
      "Joonhyuk Kang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Federated learning (FL) is a promising paradigm in distributed learning while\npreserving the privacy of users. However, the increasing size of recent models\nmakes it unaffordable for a few users to encompass the model. It leads the\nusers to adopt heterogeneous models based on their diverse computing\ncapabilities and network bandwidth. Correspondingly, FL with heterogeneous\nmodels should be addressed, given that FL typically involves training a single\nglobal model. In this paper, we propose Generative Model-Aided Federated\nLearning (GeFL), incorporating a generative model that aggregates global\nknowledge across users of heterogeneous models. Our experiments on various\nclassification tasks demonstrate notable performance improvements of GeFL\ncompared to baselines, as well as limitations in terms of privacy and\nscalability. To tackle these concerns, we introduce a novel framework, GeFL-F.\nIt trains target networks aided by feature-generative models. We empirically\ndemonstrate the consistent performance gains of GeFL-F, while demonstrating\nbetter privacy preservation and robustness to a large number of clients. Codes\nare available at [1].",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18460v1",
    "entry_id": "http://arxiv.org/abs/2412.18460v1"
  },
  {
    "title": "Multi-Agent Norm Perception and Induction in Distributed Healthcare",
    "authors": [
      "Chao Li",
      "Olga Petruchik",
      "Elizaveta Grishanina",
      "Sergey Kovalchuk"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This paper presents a Multi-Agent Norm Perception and Induction Learning\nModel aimed at facilitating the integration of autonomous agent systems into\ndistributed healthcare environments through dynamic interaction processes. The\nnature of the medical norm system and its sharing channels necessitates\ndistinct approaches for Multi-Agent Systems to learn two types of norms.\nBuilding on this foundation, the model enables agents to simultaneously learn\ndescriptive norms, which capture collective tendencies, and prescriptive norms,\nwhich dictate ideal behaviors. Through parameterized mixed probability density\nmodels and practice-enhanced Markov games, the multi-agent system perceives\ndescriptive norms in dynamic interactions and captures emergent prescriptive\nnorms. We conducted experiments using a dataset from a neurological medical\ncenter spanning from 2016 to 2020.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.MA",
      "I.2.11; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18454v1",
    "entry_id": "http://arxiv.org/abs/2412.18454v1"
  },
  {
    "title": "SoK: On the Offensive Potential of AI",
    "authors": [
      "Saskia Laura Schr\u00f6er",
      "Giovanni Apruzzese",
      "Soheil Human",
      "Pavel Laskov",
      "Hyrum S. Anderson",
      "Edward W. N. Bernroider",
      "Aurore Fass",
      "Ben Nassi",
      "Vera Rimmer",
      "Fabio Roli",
      "Samer Salam",
      "Ashley Shen",
      "Ali Sunyaev",
      "Tim Wadwha-Brown",
      "Isabel Wagner",
      "Gang Wang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laymen -- all of which being valuable sources of information\non offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome.",
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18442v1",
    "entry_id": "http://arxiv.org/abs/2412.18442v1"
  },
  {
    "title": "Gaussian entropic optimal transport: Schr\u00f6dinger bridges and the Sinkhorn algorithm",
    "authors": [
      "O. Deniz Akyildiz",
      "Pierre Del Moral",
      "Joaqu\u00edn Miguez"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Entropic optimal transport problems are regularized versions of optimal\ntransport problems. These models play an increasingly important role in machine\nlearning and generative modelling. For finite spaces, these problems are\ncommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting\nprocedure). However, in more general settings the Sinkhorn iterations are based\non nonlinear conditional/conjugate transformations and exact finite-dimensional\nsolutions cannot be computed. This article presents a finite-dimensional\nrecursive formulation of the iterative proportional fitting procedure for\ngeneral Gaussian multivariate models. As expected, this recursive formulation\nis closely related to the celebrated Kalman filter and related Riccati matrix\ndifference equations, and it yields algorithms that can be implemented in\npractical settings without further approximations. We extend this filtering\nmethodology to develop a refined and self-contained convergence analysis of\nGaussian Sinkhorn algorithms, including closed form expressions of entropic\ntransport maps and Schr\\\"odinger bridges.",
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18432v1",
    "entry_id": "http://arxiv.org/abs/2412.18432v1"
  },
  {
    "title": "GeAR: Graph-enhanced Agent for Retrieval-augmented Generation",
    "authors": [
      "Zhili Shen",
      "Chenxin Diao",
      "Pavlos Vougiouklis",
      "Pascual Merita",
      "Shriram Piramanayagam",
      "Damien Graux",
      "Dandan Tu",
      "Zeren Jiang",
      "Ruofei Lai",
      "Yang Ren",
      "Jeff Z. Pan"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Retrieval-augmented generation systems rely on effective document retrieval\ncapabilities. By design, conventional sparse or dense retrievers face\nchallenges in multi-hop retrieval scenarios. In this paper, we present GeAR,\nwhich advances RAG performance through two key innovations: (i) graph\nexpansion, which enhances any conventional base retriever, such as BM25, and\n(ii) an agent framework that incorporates graph expansion. Our evaluation\ndemonstrates GeAR's superior retrieval performance on three multi-hop question\nanswering datasets. Additionally, our system achieves state-of-the-art results\nwith improvements exceeding 10% on the challenging MuSiQue dataset, while\nrequiring fewer tokens and iterations compared to other multi-step retrieval\nsystems.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18431v1",
    "entry_id": "http://arxiv.org/abs/2412.18431v1"
  },
  {
    "title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent",
    "authors": [
      "Farhad Nooralahzadeh",
      "Yi Zhang",
      "Jonathan Furst",
      "Kurt Stockinger"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "International enterprises, organizations, or hospitals collect large amounts\nof multi-modal data stored in databases, text documents, images, and videos.\nWhile there has been recent progress in the separate fields of multi-modal data\nexploration as well as in database systems that automatically translate natural\nlanguage questions to database query languages, the research challenge of\nquerying database systems combined with other unstructured modalities such as\nimages in natural language is widely unexplored.\n  In this paper, we propose XMODE - a system that enables explainable,\nmulti-modal data exploration in natural language. Our approach is based on the\nfollowing research contributions: (1) Our system is inspired by a real-world\nuse case that enables users to explore multi-modal information systems. (2)\nXMODE leverages a LLM-based agentic AI framework to decompose a natural\nlanguage question into subtasks such as text-to-SQL generation and image\nanalysis. (3) Experimental results on multi-modal datasets over relational data\nand images demonstrate that our system outperforms state-of-the-art multi-modal\nexploration systems, excelling not only in accuracy but also in various\nperformance metrics such as query latency, API costs, planning efficiency, and\nexplanation quality, thanks to the more effective utilization of the reasoning\ncapabilities of LLMs.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18428v1",
    "entry_id": "http://arxiv.org/abs/2412.18428v1"
  },
  {
    "title": "GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent",
    "authors": [
      "Kangjia Zhao",
      "Jiahui Song",
      "Leigang Sha",
      "Haozhan Shen",
      "Zhi Chen",
      "Tiancheng Zhao",
      "Xiubo Liang",
      "Jianwei Yin"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Nowadays, research on GUI agents is a hot topic in the AI community. However,\ncurrent research focuses on GUI task automation, limiting the scope of\napplications in various GUI scenarios. In this paper, we propose a formalized\nand comprehensive environment to evaluate the entire process of automated GUI\nTesting (GTArena), offering a fair, standardized environment for consistent\noperation of diverse multimodal large language models. We divide the testing\nprocess into three key subtasks: test intention generation, test task\nexecution, and GUI defect detection, and construct a benchmark dataset based on\nthese to conduct a comprehensive evaluation. It evaluates the performance of\ndifferent models using three data types: real mobile applications, mobile\napplications with artificially injected defects, and synthetic data, thoroughly\nassessing their capabilities in this relevant task. Additionally, we propose a\nmethod that helps researchers explore the correlation between the performance\nof multimodal language large models in specific scenarios and their general\ncapabilities in standard benchmark tests. Experimental results indicate that\neven the most advanced models struggle to perform well across all sub-tasks of\nautomated GUI Testing, highlighting a significant gap between the current\ncapabilities of Autonomous GUI Testing and its practical, real-world\napplicability. This gap provides guidance for the future direction of GUI Agent\ndevelopment. Our code is available at\nhttps://github.com/ZJU-ACES-ISE/ChatUITest.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18426v1",
    "entry_id": "http://arxiv.org/abs/2412.18426v1"
  },
  {
    "title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating",
    "authors": [
      "Chao Deng",
      "Jiale Yuan",
      "Pi Bu",
      "Peijie Wang",
      "Zhong-Zhi Li",
      "Jian Xu",
      "Xiao-Hui Li",
      "Yuan Gao",
      "Jun Song",
      "Bo Zheng",
      "Cheng-Lin Liu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18424v1",
    "entry_id": "http://arxiv.org/abs/2412.18424v1"
  },
  {
    "title": "Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models",
    "authors": [
      "Zihan Zhou",
      "Ziyi Zeng",
      "Wenhao Jiang",
      "Yihui Zhu",
      "Jiaxin Mao",
      "Yonggui Yuan",
      "Min Xia",
      "Shubin Zhao",
      "Mengyu Yao",
      "Yunqian Chen"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "As social changes accelerate, the incidence of psychosomatic disorders has\nsignificantly increased, becoming a major challenge in global health issues.\nThis necessitates an innovative knowledge system and analytical methods to aid\nin diagnosis and treatment. Here, we establish the ontology model and entity\ntypes, using the BERT model and LoRA-tuned LLM for named entity recognition,\nconstructing the knowledge graph with 9668 triples. Next, by analyzing the\nnetwork distances between disease, symptom, and drug modules, it was found that\ncloser network distances among diseases can predict greater similarities in\ntheir clinical manifestations, treatment approaches, and psychological\nmechanisms, and closer distances between symptoms indicate that they are more\nlikely to co-occur. Lastly, by comparing the proximity d and proximity z score,\nit was shown that symptom-disease pairs in primary diagnostic relationships\nhave a stronger association and are of higher referential value than those in\ndiagnostic relationships. The research results revealed the potential\nconnections between diseases, co-occurring symptoms, and similarities in\ntreatment strategies, providing new perspectives for the diagnosis and\ntreatment of psychosomatic disorders and valuable information for future mental\nhealth research and practice.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18419v1",
    "entry_id": "http://arxiv.org/abs/2412.18419v1"
  },
  {
    "title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
    "authors": [
      "Avinash Anand",
      "Kritarth Prasad",
      "Chhavi Kirtani",
      "Ashwin R Nair",
      "Manvendra Kumar Nema",
      "Raj Jaiswal",
      "Rajiv Ratn Shah"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large Language Models (LLMs) excel in linguistic tasks but struggle with\nmathematical reasoning, particularly in non English languages like Hindi. This\nresearch aims to enhance the mathematical reasoning skills of smaller, resource\nefficient open-source LLMs in both Hindi and English. We evaluate models like\nOpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B,\nGemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods,\nand supervised fine-tuning. Our approach incorporates curriculum learning,\nprogressively training models on increasingly difficult problems, a novel\nDecomposition Strategy to simplify complex arithmetic operations, and a\nStructured Solution Design that divides solutions into phases. Our experiments\nresult in notable performance enhancements. WizardMath 7B exceeds Gemini's\naccuracy on English datasets by +6% and matches Gemini's performance on Hindi\ndatasets. Adopting a bilingual approach that combines English and Hindi samples\nachieves results comparable to individual language models, demonstrating the\ncapability to learn mathematical reasoning in both languages. This research\nhighlights the potential for improving mathematical reasoning in open-source\nLLMs.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18415v1",
    "entry_id": "http://arxiv.org/abs/2412.18415v1"
  },
  {
    "title": "Exploring Flexible Scenario Generation in Godot Simulator",
    "authors": [
      "Daniel Peraltai",
      "Xin Qin"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Cyber-physical systems (CPS) combine cyber and physical components engineered\nto make decisions and interact within dynamic environments. Ensuring the safety\nof CPS is of great importance, requiring extensive testing across diverse and\ncomplex scenarios. To generate as many testing scenarios as possible, previous\nefforts have focused on describing scenarios using formal languages to generate\nscenes. In this paper, we introduce an alternative approach: reconstructing\nscenes inside the open-source game engine, Godot. We have developed a pipeline\nthat enables the reconstruction of testing scenes directly from provided images\nof scenarios. These reconstructed scenes can then be deployed within simulated\nenvironments to assess a CPS. This approach offers a scalable and flexible\nsolution for testing CPS in realistic environments.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18408v1",
    "entry_id": "http://arxiv.org/abs/2412.18408v1"
  },
  {
    "title": "A Statistical Framework for Ranking LLM-Based Chatbots",
    "authors": [
      "Siavash Ameli",
      "Siyuan Zhuang",
      "Ion Stoica",
      "Michael W. Mahoney"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses.",
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18407v1",
    "entry_id": "http://arxiv.org/abs/2412.18407v1"
  },
  {
    "title": "TPAoI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking",
    "authors": [
      "Haosheng He",
      "Jianpeng Qi",
      "Chao Liu",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "In compute-first networking, maintaining fresh and accurate status\ninformation at the network edge is crucial for effective access to remote\nservices. This process typically involves three phases: Status updating, user\naccessing, and user requesting. However, current studies on status\neffectiveness, such as Age of Information at Query (QAoI), do not\ncomprehensively cover all these phases. Therefore, this paper introduces a\nnovel metric, TPAoI, aimed at optimizing update decisions by measuring the\nfreshness of service status. The stochastic nature of edge environments,\ncharacterized by unpredictable communication delays in updating, requesting,\nand user access times, poses a significant challenge when modeling. To address\nthis, we model the problem as a Markov Decision Process (MDP) and employ a\nDueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive\nexperiments demonstrate that the proposed TPAoI metric effectively minimizes\nAoI, ensuring timely and reliable service updates in dynamic edge environments.\nResults indicate that TPAoI reduces AoI by an average of 47\\% compared to QAoI\nmetrics and decreases update frequency by an average of 48\\% relative to\nconventional AoI metrics, showing significant improvement.",
    "doi": null,
    "primary_category": "cs.NI",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18391v1",
    "entry_id": "http://arxiv.org/abs/2412.18391v1"
  },
  {
    "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction",
    "authors": [
      "Wu Xiaoping",
      "Hu Jie",
      "Wei Xiaoming"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach\nfor high-fidelity image synthesis, operating diffusion processes on continuous\nVAE latent, which significantly differ from the text generation methods\nemployed by Large Language Models (LLMs). In this paper, we introduce a novel\ngenerative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which\nenhances the diffusion process through a recurrent token prediction mechanism,\nthereby pioneering the field of Discrete Diffusion. By progressively\nintroducing Gaussian noise into the latent representations of images and\nencoding them into vector-quantized tokens in a recurrent manner, RDPM\nfacilitates a unique diffusion process on discrete-value domains. This process\niteratively predicts the token codes for subsequent timesteps, transforming the\ninitial standard Gaussian noise into the source data distribution, aligning\nwith GPT-style models in terms of the loss function. RDPM demonstrates superior\nperformance while benefiting from the speed advantage of requiring only a few\ninference steps. This model not only leverages the diffusion process to ensure\nhigh-quality generation but also converts continuous signals into a series of\nhigh-fidelity discrete tokens, thereby maintaining a unified optimization\nstrategy with other discrete tokens, such as text. We anticipate that this work\nwill contribute to the development of a unified model for multimodal\ngeneration, specifically by integrating continuous signal domains such as\nimages, videos, and audio with text. We will release the code and model weights\nto the open-source community.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18390v1",
    "entry_id": "http://arxiv.org/abs/2412.18390v1"
  },
  {
    "title": "Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model",
    "authors": [
      "Tenghui Li",
      "Guoxu Zhou",
      "Xuyang Zhao",
      "Qibin Zhao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The scaling capability has been widely validated with respect to the number\nof parameters and the size of training data. One important question that is\nunexplored is that does scaling capability also exists similarly with respect\nto the number of vision tokens? This study fills the gap by investigating the\nrelationship between the number of vision tokens and the performance of\nvision-language models. Our theoretical analysis and empirical evaluations\nreveal that the model exhibits weak scaling capabilities on the length \\(N_l\\),\nwith performance approximately \\(S(N_l) \\approx (c/N_l)^{\\alpha}\\), where \\(c,\n\\alpha\\) are hyperparameters. Interestingly, this scaling behavior remains\nlargely unaffected by the inclusion or exclusion of the user's question in the\ninput. Furthermore, fusing the user's question with the vision token can\nenhance model performance when the question is relevant to the task. To address\nthe computational challenges associated with large-scale vision tokens, we\npropose a novel architecture that efficiently reduces the token count while\nintegrating user question tokens into the representation. Our findings may\noffer insights for developing more efficient and effective vision-language\nmodels under specific task constraints.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18387v1",
    "entry_id": "http://arxiv.org/abs/2412.18387v1"
  },
  {
    "title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots",
    "authors": [
      "Shani Goren",
      "Oren Kalinsky",
      "Tomer Stav",
      "Yuri Rapoport",
      "Yaron Fairstein",
      "Ram Yazdy",
      "Nachshon Cohen",
      "Alexander Libov",
      "Guy Kushilevitz"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18377v1",
    "entry_id": "http://arxiv.org/abs/2412.18377v1"
  },
  {
    "title": "A Many Objective Problem Where Crossover is Provably Indispensable",
    "authors": [
      "Andre Opris"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This paper addresses theory in evolutionary multiobjective optimisation (EMO)\nand focuses on the role of crossover operators in many-objective optimisation.\nThe advantages of using crossover are hardly understood and rigorous runtime\nanalyses with crossover are lagging far behind its use in practice,\nspecifically in the case of more than two objectives. We present a\nmany-objective problem class together with a theoretical runtime analysis of\nthe widely used NSGA-III to demonstrate that crossover can yield an exponential\nspeedup on the runtime. In particular, this algorithm can find the Pareto set\nin expected polynomial time when using crossover while without crossover it\nrequires exponential time to even find a single Pareto-optimal point. To our\nknowledge, this is the first rigorous runtime analysis in many-objective\noptimisation demonstrating an exponential performance gap when using crossover\nfor more than two objectives.",
    "doi": null,
    "primary_category": "cs.NE",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18375v1",
    "entry_id": "http://arxiv.org/abs/2412.18375v1"
  },
  {
    "title": "Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors",
    "authors": [
      "Jinhyeok Choi",
      "Heehyeon Kim",
      "Joyce Jiyoung Whang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Graph neural networks (GNNs) have emerged as an effective tool for fraud\ndetection, identifying fraudulent users, and uncovering malicious behaviors.\nHowever, attacks against GNN-based fraud detectors and their risks have rarely\nbeen studied, thereby leaving potential threats unaddressed. Recent findings\nsuggest that frauds are increasingly organized as gangs or groups. In this\nwork, we design attack scenarios where fraud gangs aim to make their fraud\nnodes misclassified as benign by camouflaging their illicit activities in\ncollusion. Based on these scenarios, we study adversarial attacks against\nGNN-based fraud detectors by simulating attacks of fraud gangs in three\nreal-world fraud cases: spam reviews, fake news, and medical insurance frauds.\nWe define these attacks as multi-target graph injection attacks and propose\nMonTi, a transformer-based Multi-target one-Time graph injection attack model.\nMonTi simultaneously generates attributes and edges of all attack nodes with a\ntransformer encoder, capturing interdependencies between attributes and edges\nmore effectively than most existing graph injection attack methods that\ngenerate these elements sequentially. Additionally, MonTi adaptively allocates\nthe degree budget for each attack node to explore diverse injection structures\ninvolving target, candidate, and attack nodes, unlike existing methods that fix\nthe degree budget across all attack nodes. Experiments show that MonTi\noutperforms the state-of-the-art graph injection attack methods on five\nreal-world graphs.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18370v1",
    "entry_id": "http://arxiv.org/abs/2412.18370v1"
  },
  {
    "title": "Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges",
    "authors": [
      "Meixia He",
      "Peican Zhu",
      "Keke Tang",
      "Yangming Guo"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Recent studies have shown that Hypergraph Neural Networks (HGNNs) are\nvulnerable to adversarial attacks. Existing approaches focus on hypergraph\nmodification attacks guided by gradients, overlooking node spanning in the\nhypergraph and the group identity of hyperedges, thereby resulting in limited\nattack performance and detectable attacks. In this manuscript, we present a\nnovel framework, i.e., Hypergraph Attacks via Injecting Homogeneous Nodes into\nElite Hyperedges (IE-Attack), to tackle these challenges. Initially, utilizing\nthe node spanning in the hypergraph, we propose the elite hyperedges sampler to\nidentify hyperedges to be injected. Subsequently, a node generator utilizing\nKernel Density Estimation (KDE) is proposed to generate the homogeneous node\nwith the group identity of hyperedges. Finally, by injecting the homogeneous\nnode into elite hyperedges, IE-Attack improves the attack performance and\nenhances the imperceptibility of attacks. Extensive experiments are conducted\non five authentic datasets to validate the effectiveness of IE-Attack and the\ncorresponding superiority to state-of-the-art methods.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18365v1",
    "entry_id": "http://arxiv.org/abs/2412.18365v1"
  },
  {
    "title": "Point-DeepONet: A Deep Operator Network Integrating PointNet for Nonlinear Analysis of Non-Parametric 3D Geometries and Load Conditions",
    "authors": [
      "Jangseop Park",
      "Namwoo Kang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Nonlinear structural analyses in engineering often require extensive finite\nelement simulations, limiting their applicability in design optimization,\nuncertainty quantification, and real-time control. Conventional deep learning\nsurrogates, such as convolutional neural networks (CNNs), physics-informed\nneural networks (PINNs), and fourier neural operators (FNOs), face challenges\nwith complex non-parametric three-dimensional (3D) geometries, directionally\nvarying loads, and high-fidelity predictions on unstructured meshes. This work\npresents Point-DeepONet, an operator-learning-based surrogate that integrates\nPointNet into the DeepONet framework. By directly processing non-parametric\npoint clouds and incorporating signed distance functions (SDF) for geometric\ncontext, Point-DeepONet accurately predicts three-dimensional displacement and\nvon Mises stress fields without mesh parameterization or retraining. Trained\nusing only about 5,000 nodes (2.5% of the original 200,000-node mesh),\nPoint-DeepONet can still predict the entire mesh at high fidelity, achieving a\ncoefficient of determination reaching 0.987 for displacement and 0.923 for von\nMises stress under a horizontal load case. Compared to nonlinear finite element\nanalyses that require about 19.32 minutes per case, Point-DeepONet provides\npredictions in mere seconds-approximately 400 times faster-while maintaining\nexcellent scalability and accuracy with increasing dataset sizes. These\nfindings highlight the potential of Point-DeepONet to enable rapid,\nhigh-fidelity structural analyses, ultimately supporting more effective design\nexploration and informed decision-making in complex engineering workflows.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18362v1",
    "entry_id": "http://arxiv.org/abs/2412.18362v1"
  },
  {
    "title": "Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor",
    "authors": [
      "Hao Yu",
      "Xin Yang",
      "Le Zhang",
      "Hanlin Gu",
      "Tianrui Li",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Federated continual learning (FCL) allows each client to continually update\nits knowledge from task streams, enhancing the applicability of federated\nlearning in real-world scenarios. However, FCL needs to address not only\nspatial data heterogeneity between clients but also temporal data heterogeneity\nbetween tasks. In this paper, empirical experiments demonstrate that such\ninput-level heterogeneity significantly affects the model's internal parameters\nand outputs, leading to severe spatial-temporal catastrophic forgetting of\nlocal and previous knowledge. To this end, we propose Federated Tail Anchor\n(FedTA) to mix trainable Tail Anchor with the frozen output features to adjust\ntheir position in the feature space, thereby overcoming parameter-forgetting\nand output-forgetting. Moreover, three novel components are also included in\nFedTA: Input Enhancement for improving the performance of pre-trained models on\ndownstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous\nlocal knowledge on the server side; and Best Global Prototype Selection for\nfinding the best anchor point for each class in the feature space. Extensive\nexperiments demonstrate that FedTA not only outperforms existing FCL methods\nbut also effectively preserves the relative positions of features, remaining\nunaffected by spatial and temporal changes.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18355v1",
    "entry_id": "http://arxiv.org/abs/2412.18355v1"
  },
  {
    "title": "The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence",
    "authors": [
      "Viviane Clay",
      "Niels Leadholm",
      "Jeff Hawkins"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Artificial intelligence has advanced rapidly in the last decade, driven\nprimarily by progress in the scale of deep-learning systems. Despite these\nadvances, the creation of intelligent systems that can operate effectively in\ndiverse, real-world environments remains a significant challenge. In this white\npaper, we outline the Thousand Brains Project, an ongoing research effort to\ndevelop an alternative, complementary form of AI, derived from the operating\nprinciples of the neocortex. We present an early version of a thousand-brains\nsystem, a sensorimotor agent that is uniquely suited to quickly learn a wide\nrange of tasks and eventually implement any capabilities the human neocortex\nhas. Core to its design is the use of a repeating computational unit, the\nlearning module, modeled on the cortical columns found in mammalian brains.\nEach learning module operates as a semi-independent unit that can model entire\nobjects, represents information through spatially structured reference frames,\nand both estimates and is able to effect movement in the world. Learning is a\nquick, associative process, similar to Hebbian learning in the brain, and\nleverages inductive biases around the spatial structure of the world to enable\nrapid and continual learning. Multiple learning modules can interact with one\nanother both hierarchically and non-hierarchically via a \"cortical messaging\nprotocol\" (CMP), creating more abstract representations and supporting\nmultimodal integration. We outline the key principles motivating the design of\nthousand-brains systems and provide details about the implementation of Monty,\nour first instantiation of such a system. Code can be found at\nhttps://github.com/thousandbrainsproject/tbp.monty, along with more detailed\ndocumentation at https://thousandbrainsproject.readme.io/.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18354v1",
    "entry_id": "http://arxiv.org/abs/2412.18354v1"
  },
  {
    "title": "Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering",
    "authors": [
      "Zhongjian Hu",
      "Peng Yang",
      "Bing Li",
      "Zhenqi Wang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large Language Models (LLMs) have achieved impressive results in\nknowledge-based Visual Question Answering (VQA). However existing methods still\nhave challenges: the inability to use external tools autonomously, and the\ninability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able\nto give a direct answer to a familiar question, whereas they tend to use tools\nsuch as search engines when they encounter an unfamiliar question. In addition,\nhumans also tend to collaborate and discuss with others to get better answers.\nInspired by this, we propose the multi-agent voting framework. We design three\nLLM-based agents that simulate different levels of staff in a team, and assign\nthe available tools according to the levels. Each agent provides the\ncorresponding answer, and finally all the answers provided by the agents are\nvoted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18351v1",
    "entry_id": "http://arxiv.org/abs/2412.18351v1"
  },
  {
    "title": "The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment",
    "authors": [
      "Xinyi Zhang",
      "Chenshuo Sun",
      "Renyu Zhang",
      "Khim-Yong Goh"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "AI-generated content (AIGC), such as advertisement copy, product\ndescriptions, and social media posts, is becoming ubiquitous in business\npractices. However, the value of AI-generated metadata, such as titles, remains\nunclear on user-generated content (UGC) platforms. To address this gap, we\nconducted a large-scale field experiment on a leading short-video platform in\nAsia to provide about 1 million users access to AI-generated titles for their\nuploaded videos. Our findings show that the provision of AI-generated titles\nsignificantly boosted content consumption, increasing valid watches by 1.6% and\nwatch duration by 0.9%. When producers adopted these titles, these increases\njumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely\nattributed to the use of this generative AI (GAI) tool increasing the\nlikelihood of videos having a title by 41.4%. The effect was more pronounced\nfor groups more affected by metadata sparsity. Mechanism analysis revealed that\nAI-generated metadata improved user-video matching accuracy in the platform's\nrecommender system. Interestingly, for a video for which the producer would\nhave posted a title anyway, adopting the AI-generated title decreased its\nviewership on average, implying that AI-generated titles may be of lower\nquality than human-generated ones. However, when producers chose to co-create\nwith GAI and significantly revised the AI-generated titles, the videos\noutperformed their counterparts with either fully AI-generated or\nhuman-generated titles, showcasing the benefits of human-AI co-creation. This\nstudy highlights the value of AI-generated metadata and human-AI metadata\nco-creation in enhancing user-content matching and content consumption for UGC\nplatforms.",
    "doi": null,
    "primary_category": "econ.GN",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.HC",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18337v1",
    "entry_id": "http://arxiv.org/abs/2412.18337v1"
  },
  {
    "title": "FloNa: Floor Plan Guided Embodied Visual Navigation",
    "authors": [
      "Jiaxin Li",
      "Weiqi Huang",
      "Zan Wang",
      "Wei Liang",
      "Huijun Di",
      "Feng Liu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Humans naturally rely on floor plans to navigate in unfamiliar environments,\nas they are readily available, reliable, and provide rich geometrical guidance.\nHowever, existing visual navigation settings overlook this valuable prior\nknowledge, leading to limited efficiency and accuracy. To eliminate this gap,\nwe introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the\nfirst attempt to incorporate floor plan into embodied visual navigation. While\nthe floor plan offers significant advantages, two key challenges emerge: (1)\nhandling the spatial inconsistency between the floor plan and the actual scene\nlayout for collision-free navigation, and (2) aligning observed images with the\nfloor plan sketch despite their distinct modalities. To address these\nchallenges, we propose FloDiff, a novel diffusion policy framework\nincorporating a localization module to facilitate alignment between the current\nobservation and the floor plan. We further collect $20k$ navigation episodes\nacross $117$ scenes in the iGibson simulator to support the training and\nevaluation. Extensive experiments demonstrate the effectiveness and efficiency\nof our framework in unfamiliar scenes using floor plan knowledge. Project\nwebsite: https://gauleejx.github.io/flona/.",
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18335v1",
    "entry_id": "http://arxiv.org/abs/2412.18335v1"
  },
  {
    "title": "Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning",
    "authors": [
      "Safa Ben Atitallah",
      "Chaima Ben Rabah",
      "Maha Driss",
      "Wadii Boulila",
      "Anis Koubaa"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Graph Mamba, a powerful graph embedding technique, has emerged as a\ncornerstone in various domains, including bioinformatics, social networks, and\nrecommendation systems. This survey represents the first comprehensive study\ndevoted to Graph Mamba, to address the critical gaps in understanding its\napplications, challenges, and future potential. We start by offering a detailed\nexplanation of the original Graph Mamba architecture, highlighting its key\ncomponents and underlying mechanisms. Subsequently, we explore the most recent\nmodifications and enhancements proposed to improve its performance and\napplicability. To demonstrate the versatility of Graph Mamba, we examine its\napplications across diverse domains. A comparative analysis of Graph Mamba and\nits variants is conducted to shed light on their unique characteristics and\npotential use cases. Furthermore, we identify potential areas where Graph Mamba\ncan be applied in the future, highlighting its potential to revolutionize data\nanalysis in these fields. Finally, we address the current limitations and open\nresearch questions associated with Graph Mamba. By acknowledging these\nchallenges, we aim to stimulate further research and development in this\npromising area. This survey serves as a valuable resource for both newcomers\nand experienced researchers seeking to understand and leverage the power of\nGraph Mamba.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18322v1",
    "entry_id": "http://arxiv.org/abs/2412.18322v1"
  },
  {
    "title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search",
    "authors": [
      "Huanjin Yao",
      "Jiaxing Huang",
      "Wenhao Wu",
      "Jingyi Zhang",
      "Yibo Wang",
      "Shunyu Liu",
      "Yingjie Wang",
      "Yuxin Song",
      "Haocheng Feng",
      "Li Shen",
      "Dacheng Tao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "In this work, we aim to develop an MLLM that understands and solves questions\nby learning to create each intermediate step of the reasoning involved till the\nfinal answer. To this end, we propose Collective Monte Carlo Tree Search\n(CoMCTS), a new learning-to-reason method for MLLMs, which introduces the\nconcept of collective learning into ``tree search'' for effective and efficient\nreasoning-path searching and learning. The core idea of CoMCTS is to leverage\ncollective knowledge from multiple models to collaboratively conjecture, search\nand identify effective reasoning paths toward correct answers via four\niterative operations including Expansion, Simulation and Error Positioning,\nBackpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a\nmultimodal dataset with a tree of rich, explicit and well-defined reasoning\nnodes for each question. With Mulberry-260k, we perform collective SFT to train\nour model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and\nReflection capabilities. Extensive experiments demonstrate the superiority of\nour proposed methods on various benchmarks. Code will be available at\nhttps://github.com/HJYao00/Mulberry",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18319v1",
    "entry_id": "http://arxiv.org/abs/2412.18319v1"
  },
  {
    "title": "Data-Driven Self-Supervised Graph Representation Learning",
    "authors": [
      "Ahmed E. Samy",
      "Zekarias T. Kefatoa",
      "Sarunas Girdzijauskasa"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Self-supervised graph representation learning (SSGRL) is a representation\nlearning paradigm used to reduce or avoid manual labeling. An essential part of\nSSGRL is graph data augmentation. Existing methods usually rely on heuristics\ncommonly identified through trial and error and are effective only within some\napplication domains. Also, it is not clear why one heuristic is better than\nanother. Moreover, recent studies have argued against some techniques (e.g.,\ndropout: that can change the properties of molecular graphs or destroy relevant\nsignals for graph-based document classification tasks).\n  In this study, we propose a novel data-driven SSGRL approach that\nautomatically learns a suitable graph augmentation from the signal encoded in\nthe graph (i.e., the nodes' predictive feature and topological information). We\npropose two complementary approaches that produce learnable feature and\ntopological augmentations. The former learns multi-view augmentation of node\nfeatures, and the latter learns a high-order view of the topology. Moreover,\nthe augmentations are jointly learned with the representation. Our approach is\ngeneral that it can be applied to homogeneous and heterogeneous graphs. We\nperform extensive experiments on node classification (using nine homogeneous\nand heterogeneous datasets) and graph property prediction (using another eight\ndatasets). The results show that the proposed method matches or outperforms the\nSOTA SSGRL baselines and performs similarly to semi-supervised methods. The\nanonymised source code is available at https://github.com/AhmedESamy/dsgrl/",
    "doi": "10.3233/FAIA230325",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18316v1",
    "entry_id": "http://arxiv.org/abs/2412.18316v1"
  },
  {
    "title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models",
    "authors": [
      "Jiaxin Guo",
      "Daimeng Wei",
      "Yuanchang Luo",
      "Shimin Tao",
      "Hengchao Shang",
      "Zongyao Li",
      "Shaojun Li",
      "Jinlong Yang",
      "Zhanglin Wu",
      "Zhiqiang Rao",
      "Hao Yang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18299v1",
    "entry_id": "http://arxiv.org/abs/2412.18299v1"
  },
  {
    "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
    "authors": [
      "Xi Ding",
      "Lei Wang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Video anomaly detection (VAD) has witnessed significant advancements through\nthe integration of large language models (LLMs) and vision-language models\n(VLMs), addressing critical challenges such as interpretability, temporal\nreasoning, and generalization in dynamic, open-world scenarios. This paper\npresents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024,\nfocusing on four key aspects: (i) enhancing interpretability through semantic\ninsights and textual explanations, making visual anomalies more understandable;\n(ii) capturing intricate temporal relationships to detect and localize dynamic\nanomalies across video frames; (iii) enabling few-shot and zero-shot detection\nto minimize reliance on large, annotated datasets; and (iv) addressing\nopen-world and class-agnostic anomalies by using semantic understanding and\nmotion features for spatiotemporal coherence. We highlight their potential to\nredefine the landscape of VAD. Additionally, we explore the synergy between\nvisual and textual modalities offered by LLMs and VLMs, highlighting their\ncombined strengths and proposing future directions to fully exploit the\npotential in enhancing video anomaly detection.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18298v1",
    "entry_id": "http://arxiv.org/abs/2412.18298v1"
  },
  {
    "title": "Learning to Play Against Unknown Opponents",
    "authors": [
      "Eshwar Ram Arunachaleswaran",
      "Natalie Collina",
      "Jon Schneider"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "We consider the problem of a learning agent who has to repeatedly play a\ngeneral sum game against a strategic opponent who acts to maximize their own\npayoff by optimally responding against the learner's algorithm. The learning\nagent knows their own payoff function, but is uncertain about the payoff of\ntheir opponent (knowing only that it is drawn from some distribution\n$\\mathcal{D}$). What learning algorithm should the agent run in order to\nmaximize their own total utility?\n  We demonstrate how to construct an $\\varepsilon$-optimal learning algorithm\n(obtaining average utility within $\\varepsilon$ of the optimal utility) for\nthis problem in time polynomial in the size of the input and $1/\\varepsilon$\nwhen either the size of the game or the support of $\\mathcal{D}$ is constant.\nWhen the learning algorithm is further constrained to be a no-regret algorithm,\nwe demonstrate how to efficiently construct an optimal learning algorithm\n(asymptotically achieving the optimal utility) in polynomial time, independent\nof any other assumptions. Both results make use of recently developed machinery\nthat converts the analysis of learning algorithms to the study of the class of\ncorresponding geometric objects known as menus.",
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18297v1",
    "entry_id": "http://arxiv.org/abs/2412.18297v1"
  },
  {
    "title": "Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies",
    "authors": [
      "Qi Liu",
      "Wanjing Ma"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Data corruption, including missing and noisy data, poses significant\nchallenges in real-world machine learning. This study investigates the effects\nof data corruption on model performance and explores strategies to mitigate\nthese effects through two experimental setups: supervised learning with NLP\ntasks (NLP-SL) and deep reinforcement learning for traffic signal optimization\n(Signal-RL). We analyze the relationship between data corruption levels and\nmodel performance, evaluate the effectiveness of data imputation methods, and\nassess the utility of enlarging datasets to address data corruption.\n  Our results show that model performance under data corruption follows a\ndiminishing return curve, modeled by the exponential function. Missing data,\nwhile detrimental, is less harmful than noisy data, which causes severe\nperformance degradation and training instability, particularly in sequential\ndecision-making tasks like Signal-RL. Imputation strategies involve a\ntrade-off: they recover missing information but may introduce noise. Their\neffectiveness depends on imputation accuracy and corruption ratio. We identify\ndistinct regions in the imputation advantage heatmap, including an \"imputation\nadvantageous corner\" and an \"imputation disadvantageous edge\" and classify\ntasks as \"noise-sensitive\" or \"noise-insensitive\" based on their decision\nboundaries.\n  Furthermore, we find that increasing dataset size mitigates but cannot fully\novercome the effects of data corruption. The marginal utility of additional\ndata diminishes as corruption increases. An empirical rule emerges:\napproximately 30% of the data is critical for determining performance, while\nthe remaining 70% has minimal impact.\n  These findings provide actionable insights into data preprocessing,\nimputation strategies, and data collection practices, guiding the development\nof robust machine learning systems in noisy environments.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18296v1",
    "entry_id": "http://arxiv.org/abs/2412.18296v1"
  },
  {
    "title": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases",
    "authors": [
      "Christian Di Maio",
      "Cristian Cosci",
      "Marco Maggini",
      "Valentina Poggioni",
      "Stefano Melacci"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18295v1",
    "entry_id": "http://arxiv.org/abs/2412.18295v1"
  },
  {
    "title": "MinsStudio: A Streamlined Package for Minecraft AI Agent Development",
    "authors": [
      "Shaofei Cai",
      "Zhancun Mu",
      "Kaichen He",
      "Bowei Zhang",
      "Xinyue Zheng",
      "Anji Liu",
      "Yitao Liang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Minecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps://github.com/CraftJarvis/MineStudio.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18293v1",
    "entry_id": "http://arxiv.org/abs/2412.18293v1"
  },
  {
    "title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation",
    "authors": [
      "Junyi Lu",
      "Xiaojia Li",
      "Zihan Hua",
      "Lei Yu",
      "Shiqi Cheng",
      "Li Yang",
      "Fengjun Zhang",
      "Chun Zuo"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Code review is a vital but demanding aspect of software development,\ngenerating significant interest in automating review comments. Traditional\nevaluation methods for these comments, primarily based on text similarity, face\ntwo major challenges: inconsistent reliability of human-authored comments in\nopen-source projects and the weak correlation of text similarity with\nobjectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of\ncriteria informed by prior research and developer interviews. We then similarly\nrevisit the evaluation of existing methodologies. Our evaluation framework,\nDeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a\ncomprehensive reassessment of current techniques based on the criteria set.\nBesides, we also introduce an innovative and efficient baseline, LLM-Reviewer,\nleveraging the few-shot learning capabilities of LLMs for a target-oriented\ncomparison.\n  Our research highlights the limitations of text similarity metrics, finding\nthat less than 10% of benchmark comments are high quality for automation. In\ncontrast, DeepCRCEval effectively distinguishes between high and low-quality\ncomments, proving to be a more reliable evaluation mechanism. Incorporating LLM\nevaluators into DeepCRCEval significantly boosts efficiency, reducing time and\ncost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates\nsignificant potential of focusing task real targets in comment generation.",
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18291v1",
    "entry_id": "http://arxiv.org/abs/2412.18291v1"
  },
  {
    "title": "Towards understanding how attention mechanism works in deep learning",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Attention mechanism has been extensively integrated within mainstream neural\nnetwork architectures, such as Transformers and graph attention networks. Yet,\nits underlying working principles remain somewhat elusive. What is its essence?\nAre there any connections between it and traditional machine learning\nalgorithms? In this study, we inspect the process of computing similarity using\nclassic metrics and vector space properties in manifold learning, clustering,\nand supervised learning. We identify the key characteristics of similarity\ncomputation and information propagation in these methods and demonstrate that\nthe self-attention mechanism in deep learning adheres to the same principles\nbut operates more flexibly and adaptively. We decompose the self-attention\nmechanism into a learnable pseudo-metric function and an information\npropagation process based on similarity computation. We prove that the\nself-attention mechanism converges to a drift-diffusion process through\ncontinuous modeling provided the pseudo-metric is a transformation of a metric\nand certain reasonable assumptions hold. This equation could be transformed\ninto a heat equation under a new metric. In addition, we give a first-order\nanalysis of attention mechanism with a general pseudo-metric function. This\nstudy aids in understanding the effects and principle of attention mechanism\nthrough physical intuition. Finally, we propose a modified attention mechanism\ncalled metric-attention by leveraging the concept of metric learning to\nfacilitate the ability to learn desired metrics more effectively. Experimental\nresults demonstrate that it outperforms self-attention regarding training\nefficiency, accuracy, and robustness.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML",
      "68T07",
      "I.2.6; I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18288v1",
    "entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "title": "Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation",
    "authors": [
      "Sheng Xiang",
      "Mingzhi Zhu",
      "Dawei Cheng",
      "Enxia Li",
      "Ruihui Zhao",
      "Yi Ouyang",
      "Ling Chen",
      "Yefeng Zheng"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Credit card fraud incurs a considerable cost for both cardholders and issuing\nbanks. Contemporary methods apply machine learning-based classifiers to detect\nfraudulent behavior from labeled transaction records. But labeled data are\nusually a small proportion of billions of real transactions due to expensive\nlabeling costs, which implies that they do not well exploit many natural\nfeatures from unlabeled data. Therefore, we propose a semi-supervised graph\nneural network for fraud detection. Specifically, we leverage transaction\nrecords to construct a temporal transaction graph, which is composed of\ntemporal transactions (nodes) and interactions (edges) among them. Then we pass\nmessages among the nodes through a Gated Temporal Attention Network (GTAN) to\nlearn the transaction representation. We further model the fraud patterns\nthrough risk propagation among transactions. The extensive experiments are\nconducted on a real-world transaction dataset and two publicly available fraud\ndetection datasets. The result shows that our proposed method, namely GTAN,\noutperforms other state-of-the-art baselines on three fraud detection datasets.\nSemi-supervised experiments demonstrate the excellent fraud detection\nperformance of our model with only a tiny proportion of labeled data.",
    "doi": "10.1609/aaai.v37i12.26702",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18287v1",
    "entry_id": "http://arxiv.org/abs/2412.18287v1"
  },
  {
    "title": "GDM4MMIMO: Generative Diffusion Models for Massive MIMO Communications",
    "authors": [
      "Zhenzhou Jin",
      "Li You",
      "Huibin Zhou",
      "Yuanshuo Wang",
      "Xiaofeng Liu",
      "Xinrui Gong",
      "Xiqi Gao",
      "Derrick Wing Kwan Ng",
      "Xiang-Gen Xia"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Massive multiple-input multiple-output (MIMO) offers significant advantages\nin spectral and energy efficiencies, positioning it as a cornerstone technology\nof fifth-generation (5G) wireless communication systems and a promising\nsolution for the burgeoning data demands anticipated in sixth-generation (6G)\nnetworks. In recent years, with the continuous advancement of artificial\nintelligence (AI), a multitude of task-oriented generative foundation models\n(GFMs) have emerged, achieving remarkable performance in various fields such as\ncomputer vision (CV), natural language processing (NLP), and autonomous\ndriving. As a pioneering force, these models are driving the paradigm shift in\nAI towards generative AI (GenAI). Among them, the generative diffusion model\n(GDM), as one of state-of-the-art families of generative models, demonstrates\nan exceptional capability to learn implicit prior knowledge and robust\ngeneralization capabilities, thereby enhancing its versatility and\neffectiveness across diverse applications. In this paper, we delve into the\npotential applications of GDM in massive MIMO communications. Specifically, we\nfirst provide an overview of massive MIMO communication, the framework of GFMs,\nand the working mechanism of GDM. Following this, we discuss recent research\nadvancements in the field and present a case study of near-field channel\nestimation based on GDM, demonstrating its promising potential for facilitating\nefficient ultra-dimensional channel statement information (CSI) acquisition in\nthe context of massive MIMO communications. Finally, we highlight several\npressing challenges in future mobile communications and identify promising\nresearch directions surrounding GDM.",
    "doi": null,
    "primary_category": "cs.IT",
    "categories": [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18281v1",
    "entry_id": "http://arxiv.org/abs/2412.18281v1"
  },
  {
    "title": "Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization",
    "authors": [
      "Jiacai Liu",
      "Chaojie Wang",
      "Chris Yuhao Liu",
      "Liang Zeng",
      "Rui Yan",
      "Yiwen Sun",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18279v1",
    "entry_id": "http://arxiv.org/abs/2412.18279v1"
  },
  {
    "title": "GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge",
    "authors": [
      "Shammur Absar Chowdhury",
      "Hind Almerekhi",
      "Mucahid Kutlu",
      "Kaan Efe Keles",
      "Fatema Ahmad",
      "Tasnim Mohiuddin",
      "George Mikros",
      "Firoj Alam"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This paper presents a comprehensive overview of the first edition of the\nAcademic Essay Authenticity Challenge, organized as part of the GenAI Content\nDetection shared tasks collocated with COLING 2025. This challenge focuses on\ndetecting machine-generated vs. human-authored essays for academic purposes.\nThe task is defined as follows: \"Given an essay, identify whether it is\ngenerated by a machine or authored by a human.'' The challenge involves two\nlanguages: English and Arabic. During the evaluation phase, 25 teams submitted\nsystems for English and 21 teams for Arabic, reflecting substantial interest in\nthe task. Finally, seven teams submitted system description papers. The\nmajority of submissions utilized fine-tuned transformer-based models, with one\nteam employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This\npaper outlines the task formulation, details the dataset construction process,\nand explains the evaluation framework. Additionally, we present a summary of\nthe approaches adopted by participating teams. Nearly all submitted systems\noutperformed the n-gram-based baseline, with the top-performing systems\nachieving F1 scores exceeding 0.98 for both languages, indicating significant\nprogress in the detection of machine-generated text.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18274v1",
    "entry_id": "http://arxiv.org/abs/2412.18274v1"
  },
  {
    "title": "Sampling Bag of Views for Open-Vocabulary Object Detection",
    "authors": [
      "Hojun Choi",
      "Junsuk Choe",
      "Hyunjung Shim"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Existing open-vocabulary object detection (OVD) develops methods for testing\nunseen categories by aligning object region embeddings with corresponding VLM\nfeatures. A recent study leverages the idea that VLMs implicitly learn\ncompositional structures of semantic concepts within the image. Instead of\nusing an individual region embedding, it utilizes a bag of region embeddings as\na new representation to incorporate compositional structures into the OVD task.\nHowever, this approach often fails to capture the contextual concepts of each\nregion, leading to noisy compositional structures. This results in only\nmarginal performance improvements and reduced efficiency. To address this, we\npropose a novel concept-based alignment method that samples a more powerful and\nefficient compositional structure. Our approach groups contextually related\n``concepts'' into a bag and adjusts the scale of concepts within the bag for\nmore effective embedding alignment. Combined with Faster R-CNN, our method\nachieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel\ncategories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our\nmethod reduces CLIP computation in FLOPs by 80.3% compared to previous\nresearch, significantly enhancing efficiency. Experimental results demonstrate\nthat the proposed method outperforms previous state-of-the-art models on the\nOVD datasets.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18273v1",
    "entry_id": "http://arxiv.org/abs/2412.18273v1"
  },
  {
    "title": "Annotating References to Mythological Entities in French Literature",
    "authors": [
      "Thierry Poibeau"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "In this paper, we explore the relevance of large language models (LLMs) for\nannotating references to Roman and Greek mythological entities in modern and\ncontemporary French literature. We present an annotation scheme and demonstrate\nthat recent LLMs can be directly applied to follow this scheme effectively,\nalthough not without occasionally making significant analytical errors.\nAdditionally, we show that LLMs (and, more specifically, ChatGPT) are capable\nof offering interpretative insights into the use of mythological references by\nliterary authors. However, we also find that LLMs struggle to accurately\nidentify relevant passages in novels (when used as an information retrieval\nengine), often hallucinating and generating fabricated examples-an issue that\nraises significant ethical concerns. Nonetheless, when used carefully, LLMs\nremain valuable tools for performing annotations with high accuracy, especially\nfor tasks that would be difficult to annotate comprehensively on a large scale\nthrough manual methods alone.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18270v1",
    "entry_id": "http://arxiv.org/abs/2412.18270v1"
  },
  {
    "title": "Free the Design Space of Equivariant Graph Neural Networks: High-Rank Irreducible Cartesian Tensor Decomposition and Bases of Equivariant Spaces",
    "authors": [
      "Shihao Shao",
      "Yikang Li",
      "Zhouchen Lin",
      "Qinghua Cui"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Irreducible Cartesian tensors (ICTs) play a crucial role in the design of\nequivariant graph neural networks, as well as in theoretical chemistry and\nchemical physics. Meanwhile, the design space of available linear operations on\ntensors that preserve symmetry presents a significant challenge. The ICT\ndecomposition and a basis of this equivariant space are difficult to obtain for\nhigh-order tensors. After decades of research, we recently achieve an explicit\nICT decomposition for $n=5$ \\citep{bonvicini2024irreducible} with factorial\ntime/space complexity. This work, for the first time, obtains decomposition\nmatrices for ICTs up to rank $n=9$ with reduced and affordable complexity, by\nconstructing what we call path matrices. The path matrices are obtained via\nperforming chain-like contraction with Clebsch-Gordan matrices following the\nparentage scheme. We prove and leverage that the concatenation of path matrices\nis an orthonormal change-of-basis matrix between the Cartesian tensor product\nspace and the spherical direct sum spaces. Furthermore, we identify a complete\northogonal basis for the equivariant space, rather than a spanning set\n\\citep{pearce2023brauer}, through this path matrices technique. We further\nextend our result to the arbitrary tensor product and direct sum spaces,\nenabling free design between different spaces while keeping symmetry. The\nPython code is available in the appendix where the $n=6,\\dots,9$ ICT\ndecomposition matrices are obtained in <0.1s, 0.5s, 1s, 3s, 11s, and 4m32s,\nrespectively.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math-ph",
      "math.MP",
      "physics.chem-ph",
      "physics.comp-ph",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18263v1",
    "entry_id": "http://arxiv.org/abs/2412.18263v1"
  },
  {
    "title": "Robust Semi-Supervised Learning in Open Environments",
    "authors": [
      "Lan-Zhe Guo",
      "Lin-Han Jia",
      "Jie-Jing Shao",
      "Yu-Feng Li"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Semi-supervised learning (SSL) aims to improve performance by exploiting\nunlabeled data when labels are scarce. Conventional SSL studies typically\nassume close environments where important factors (e.g., label, feature,\ndistribution) between labeled and unlabeled data are consistent. However, more\npractical tasks involve open environments where important factors between\nlabeled and unlabeled data are inconsistent. It has been reported that\nexploiting inconsistent unlabeled data causes severe performance degradation,\neven worse than the simple supervised learning baseline. Manually verifying the\nquality of unlabeled data is not desirable, therefore, it is important to study\nrobust SSL with inconsistent unlabeled data in open environments. This paper\nbriefly introduces some advances in this line of research, focusing on\ntechniques concerning label, feature, and data distribution inconsistency in\nSSL, and presents the evaluation benchmarks. Open research problems are also\ndiscussed for reference purposes.",
    "doi": "10.1007/s11704-024-40646-w",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18256v1",
    "entry_id": "http://arxiv.org/abs/2412.18256v1"
  },
  {
    "title": "Detection and Forecasting of Parkinson Disease Progression from Speech Signal Features Using MultiLayer Perceptron and LSTM",
    "authors": [
      "Majid Ali",
      "Hina Shakir",
      "Asia Samreen",
      "Sohaib Ahmed"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Accurate diagnosis of Parkinson disease, especially in its early stages, can\nbe a challenging task. The application of machine learning techniques helps\nimprove the diagnostic accuracy of Parkinson disease detection but only few\nstudies have presented work towards the prediction of disease progression. In\nthis research work, Long Short Term Memory LSTM was trained using the\ndiagnostic features on Parkinson patients speech signals, to predict the\ndisease progression while a Multilayer Perceptron MLP was trained on the same\ndiagnostic features to detect the disease. Diagnostic features selected using\ntwo well-known feature selection methods named Relief-F and Sequential Forward\nSelection and applied on LSTM and MLP have shown to accurately predict the\ndisease progression as stage 2 and 3 and its existence respectively.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18248v1",
    "entry_id": "http://arxiv.org/abs/2412.18248v1"
  },
  {
    "title": "Fr\u00e9chet regression for multi-label feature selection with implicit regularization",
    "authors": [
      "Dou El Kefel Mansouri",
      "Seif-Eddine Benkabou",
      "Khalid Benabdeslem"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Fr\\'echet regression extends linear regression to model complex responses\n  in metric spaces, making it particularly relevant for multi-label regression,\n  where each instance can have multiple associated labels. However, variable\n  selection within this framework remains underexplored. In this paper, we pro\npose a novel variable selection method that employs implicit regularization\n  instead of traditional explicit regularization approaches, which can\nintroduce\n  bias. Our method effectively captures nonlinear interactions between predic\ntors and responses while promoting model sparsity. We provide theoretical\n  results demonstrating selection consistency and illustrate the performance of\n  our approach through numerical examples",
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18247v1",
    "entry_id": "http://arxiv.org/abs/2412.18247v1"
  },
  {
    "title": "An Automatic Graph Construction Framework based on Large Language Models for Recommendation",
    "authors": [
      "Rong Shan",
      "Jianghao Lin",
      "Chenxu Zhu",
      "Bo Chen",
      "Menghui Zhu",
      "Kangning Zhang",
      "Jieming Zhu",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.",
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18241v1",
    "entry_id": "http://arxiv.org/abs/2412.18241v1"
  },
  {
    "title": "OMG-HD: A High-Resolution AI Weather Model for End-to-End Forecasts from Observations",
    "authors": [
      "Pengcheng Zhao",
      "Jiang Bian",
      "Zekun Ni",
      "Weixin Jin",
      "Jonathan Weyn",
      "Zuliang Fang",
      "Siqi Xiang",
      "Haiyu Dong",
      "Bin Zhang",
      "Hongyu Sun",
      "Kit Thambiratnam",
      "Qi Zhang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "In recent years, Artificial Intelligence Weather Prediction (AIWP) models\nhave achieved performance comparable to, or even surpassing, traditional\nNumerical Weather Prediction (NWP) models by leveraging reanalysis data.\nHowever, a less-explored approach involves training AIWP models directly on\nobservational data, enhancing computational efficiency and improving forecast\naccuracy by reducing the uncertainties introduced through data assimilation\nprocesses. In this study, we propose OMG-HD, a novel AI-based regional\nhigh-resolution weather forecasting model designed to make predictions directly\nfrom observational data sources, including surface stations, radar, and\nsatellite, thereby removing the need for operational data assimilation. Our\nevaluation shows that OMG-HD outperforms both the European Centre for\nMedium-Range Weather Forecasts (ECMWF)'s high-resolution operational\nforecasting system, IFS-HRES, and the High-Resolution Rapid Refresh (HRRR)\nmodel at lead times of up to 12 hours across the contiguous United States\n(CONUS) region. We achieve up to a 13% improvement on RMSE for 2-meter\ntemperature, 17% on 10-meter wind speed, 48% on 2-meter specific humidity, and\n32% on surface pressure compared to HRRR. Our method shows that it is possible\nto use AI-driven approaches for rapid weather predictions without relying on\nNWP-derived weather fields as model input. This is a promising step towards\nusing observational data directly to make operational forecasts with AIWP\nmodels.",
    "doi": null,
    "primary_category": "physics.ao-ph",
    "categories": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18239v1",
    "entry_id": "http://arxiv.org/abs/2412.18239v1"
  },
  {
    "title": "Sch\u00f6dinger Bridge Type Diffusion Models as an Extension of Variational Autoencoders",
    "authors": [
      "Kentaro Kaba",
      "Reo Shimizu",
      "Masayuki Ohzeki",
      "Yuki Sughiyama"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Generative diffusion models use time-forward and backward stochastic\ndifferential equations to connect the data and prior distributions. While\nconventional diffusion models (e.g., score-based models) only learn the\nbackward process, more flexible frameworks have been proposed to also learn the\nforward process by employing the Schr\\\"odinger bridge (SB). However, due to the\ncomplexity of the mathematical structure behind SB-type models, we can not\neasily give an intuitive understanding of their objective function. In this\nwork, we propose a unified framework to construct diffusion models by\nreinterpreting the SB-type models as an extension of variational autoencoders.\nIn this context, the data processing inequality plays a crucial role. As a\nresult, we find that the objective function consists of the prior loss and\ndrift matching parts.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18237v1",
    "entry_id": "http://arxiv.org/abs/2412.18237v1"
  },
  {
    "title": "Expand VSR Benchmark for VLLM to Expertize in Spatial Rules",
    "authors": [
      "Peijin Xie",
      "Lin Sun",
      "Bingquan Liu",
      "Dexin Wang",
      "Xiangzheng Zhang",
      "Chengjie Sun",
      "Jiajia Zhang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Distinguishing spatial relations is a basic part of human cognition which\nrequires fine-grained perception on cross-instance. Although benchmarks like\nMME, MMBench and SEED comprehensively have evaluated various capabilities which\nalready include visual spatial reasoning(VSR). There is still a lack of\nsufficient quantity and quality evaluation and optimization datasets for Vision\nLarge Language Models(VLLMs) specifically targeting visual positional\nreasoning. To handle this, we first diagnosed current VLLMs with the VSR\ndataset and proposed a unified test set. We found current VLLMs to exhibit a\ncontradiction of over-sensitivity to language instructions and\nunder-sensitivity to visual positional information. By expanding the original\nbenchmark from two aspects of tunning data and model structure, we mitigated\nthis phenomenon. To our knowledge, we expanded spatially positioned image data\ncontrollably using diffusion models for the first time and integrated original\nvisual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and\nDINO). After conducting combination experiments on scaling data and models, we\nobtained a VLLM VSR Expert(VSRE) that not only generalizes better to different\ninstructions but also accurately distinguishes differences in visual positional\ninformation. VSRE achieved over a 27\\% increase in accuracy on the VSR test\nset. It becomes a performant VLLM on the position reasoning of both the VSR\ndataset and relevant subsets of other evaluation benchmarks. We open-sourced\nthe expanded model with data and Appendix at\n\\url{https://github.com/peijin360/vsre} and hope it will accelerate\nadvancements in VLLM on VSR learning.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18224v1",
    "entry_id": "http://arxiv.org/abs/2412.18224v1"
  },
  {
    "title": "Leveraging Convolutional Neural Network-Transformer Synergy for Predictive Modeling in Risk-Based Applications",
    "authors": [
      "Yuhan Wang",
      "Zhen Xu",
      "Yue Yao",
      "Jinsong Liu",
      "Jiating Lin"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "With the development of the financial industry, credit default prediction, as\nan important task in financial risk management, has received increasing\nattention. Traditional credit default prediction methods mostly rely on machine\nlearning models, such as decision trees and random forests, but these methods\nhave certain limitations in processing complex data and capturing potential\nrisk patterns. To this end, this paper proposes a deep learning model based on\nthe combination of convolutional neural networks (CNN) and Transformer for\ncredit user default prediction. The model combines the advantages of CNN in\nlocal feature extraction with the ability of Transformer in global dependency\nmodeling, effectively improving the accuracy and robustness of credit default\nprediction. Through experiments on public credit default datasets, the results\nshow that the CNN+Transformer model outperforms traditional machine learning\nmodels, such as random forests and XGBoost, in multiple evaluation indicators\nsuch as accuracy, AUC, and KS value, demonstrating its powerful ability in\ncomplex financial data modeling. Further experimental analysis shows that\nappropriate optimizer selection and learning rate adjustment play a vital role\nin improving model performance. In addition, the ablation experiment of the\nmodel verifies the advantages of the combination of CNN and Transformer and\nproves the complementarity of the two in credit default prediction. This study\nprovides a new idea for credit default prediction and provides strong support\nfor risk assessment and intelligent decision-making in the financial field.\nFuture research can further improve the prediction effect and generalization\nability by introducing more unstructured data and improving the model\narchitecture.",
    "doi": null,
    "primary_category": "q-fin.RM",
    "categories": [
      "q-fin.RM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18222v1",
    "entry_id": "http://arxiv.org/abs/2412.18222v1"
  },
  {
    "title": "On the Effectiveness of Adversarial Training on Malware Classifiers",
    "authors": [
      "Hamid Bostani",
      "Jacopo Cortellazzi",
      "Daniel Arp",
      "Fabio Pierazzi",
      "Veelasha Moonsamy",
      "Lorenzo Cavallaro"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Adversarial Training (AT) has been widely applied to harden learning-based\nclassifiers against adversarial evasive attacks. However, its effectiveness in\nidentifying and strengthening vulnerable areas of the model's decision space\nwhile maintaining high performance on clean data of malware classifiers remains\nan under-explored area. In this context, the robustness that AT achieves has\noften been assessed against unrealistic or weak adversarial attacks, which\nnegatively affect performance on clean data and are arguably no longer threats.\nPrevious work seems to suggest robustness is a task-dependent property of AT.\nWe instead argue it is a more complex problem that requires exploring AT and\nthe intertwined roles played by certain factors within data, feature\nrepresentations, classifiers, and robust optimization settings, as well as\nproper evaluation factors, such as the realism of evasion attacks, to gain a\ntrue sense of AT's effectiveness. In our paper, we address this gap by\nsystematically exploring the role such factors have in hardening malware\nclassifiers through AT. Contrary to recent prior work, a key observation of our\nresearch and extensive experiments confirm the hypotheses that all such factors\ninfluence the actual effectiveness of AT, as demonstrated by the varying\ndegrees of success from our empirical analysis. We identify five evaluation\npitfalls that affect state-of-the-art studies and summarize our insights in ten\ntakeaways to draw promising research directions toward better understanding the\nfactors' settings under which adversarial training works at best.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18218v1",
    "entry_id": "http://arxiv.org/abs/2412.18218v1"
  },
  {
    "title": "Accelerating AIGC Services with Latent Action Diffusion Scheduling in Edge Networks",
    "authors": [
      "Changfu Xu",
      "Jianxiong Guo",
      "Wanyu Lin",
      "Haodong Zou",
      "Wentao Fan",
      "Tian Wang",
      "Xiaowen Chu",
      "Jiannong Cao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Artificial Intelligence Generated Content (AIGC) has gained significant\npopularity for creating diverse content. Current AIGC models primarily focus on\ncontent quality within a centralized framework, resulting in a high service\ndelay and negative user experiences. However, not only does the workload of an\nAIGC task depend on the AIGC model's complexity rather than the amount of data,\nbut the large model and its multi-layer encoder structure also result in a huge\ndemand for computational and memory resources. These unique characteristics\npose new challenges in its modeling, deployment, and scheduling at edge\nnetworks. Thus, we model an offloading problem among edges for providing real\nAIGC services and propose LAD-TS, a novel Latent Action Diffusion-based Task\nScheduling method that orchestrates multiple edge servers for expedited AIGC\nservices. The LAD-TS generates a near-optimal offloading decision by leveraging\nthe diffusion model's conditional generation capability and the reinforcement\nlearning's environment interaction ability, thereby minimizing the service\ndelays under multiple resource constraints. Meanwhile, a latent action\ndiffusion strategy is designed to guide decision generation by utilizing\nhistorical action probability, enabling rapid achievement of near-optimal\ndecisions. Furthermore, we develop DEdgeAI, a prototype edge system with a\nrefined AIGC model deployment to implement and evaluate our LAD-TS method.\nDEdgeAI provides a real AIGC service for users, demonstrating up to 29.18%\nshorter service delays than the current five representative AIGC platforms. We\nrelease our open-source code at https://github.com/ChangfuXu/DEdgeAI/.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18212v1",
    "entry_id": "http://arxiv.org/abs/2412.18212v1"
  },
  {
    "title": "Sharper Error Bounds in Late Fusion Multi-view Clustering Using Eigenvalue Proportion",
    "authors": [
      "Liang Du",
      "Henghui Jiang",
      "Xiaodong Li",
      "Yiqing Guo",
      "Yan Chen",
      "Feijiang Li",
      "Peng Zhou",
      "Yuhua Qian"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Multi-view clustering (MVC) aims to integrate complementary information from\nmultiple views to enhance clustering performance. Late Fusion Multi-View\nClustering (LFMVC) has shown promise by synthesizing diverse clustering results\ninto a unified consensus. However, current LFMVC methods struggle with noisy\nand redundant partitions and often fail to capture high-order correlations\nacross views. To address these limitations, we present a novel theoretical\nframework for analyzing the generalization error bounds of multiple kernel\n$k$-means, leveraging local Rademacher complexity and principal eigenvalue\nproportions. Our analysis establishes a convergence rate of $\\mathcal{O}(1/n)$,\nsignificantly improving upon the existing rate in the order of\n$\\mathcal{O}(\\sqrt{k/n})$. Building on this insight, we propose a low-pass\ngraph filtering strategy within a multiple linear $k$-means framework to\nmitigate noise and redundancy, further refining the principal eigenvalue\nproportion and enhancing clustering accuracy. Experimental results on benchmark\ndatasets confirm that our approach outperforms state-of-the-art methods in\nclustering performance and robustness. The related codes is available at\nhttps://github.com/csliangdu/GMLKM .",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18207v1",
    "entry_id": "http://arxiv.org/abs/2412.18207v1"
  },
  {
    "title": "Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms",
    "authors": [
      "Zhuohuan Hu",
      "Richard Yu",
      "Zizhou Zhang",
      "Haoran Zheng",
      "Qianying Liu",
      "Yining Zhou"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This paper leverages machine learning algorithms to forecast and analyze\nfinancial time series. The process begins with a denoising autoencoder to\nfilter out random noise fluctuations from the main contract price data. Then,\none-dimensional convolution reduces the dimensionality of the filtered data and\nextracts key information. The filtered and dimensionality-reduced price data is\nfed into a GANs network, and its output serve as input of a fully connected\nnetwork. Through cross-validation, a model is trained to capture features that\nprecede large price fluctuations. The model predicts the likelihood and\ndirection of significant price changes in real-time price sequences, placing\ntrades at moments of high prediction accuracy. Empirical results demonstrate\nthat using autoencoders and convolution to filter and denoise financial data,\ncombined with GANs, achieves a certain level of predictive performance,\nvalidating the capabilities of machine learning algorithms to discover\nunderlying patterns in financial sequences. Keywords - CNN;GANs;\nCryptocurrency; Prediction.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18202v1",
    "entry_id": "http://arxiv.org/abs/2412.18202v1"
  },
  {
    "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks",
    "authors": [
      "Shiduo Zhang",
      "Zhe Xu",
      "Peiju Liu",
      "Xiaopeng Yu",
      "Yuan Li",
      "Qinghui Gao",
      "Zhaoye Fei",
      "Zhangyue Yin",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Xipeng Qiu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks.",
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18194v1",
    "entry_id": "http://arxiv.org/abs/2412.18194v1"
  },
  {
    "title": "An Analysis on Automated Metrics for Evaluating Japanese-English Chat Translation",
    "authors": [
      "Andre Rusli",
      "Makoto Shishido"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This paper analyses how traditional baseline metrics, such as BLEU and TER,\nand neural-based methods, such as BERTScore and COMET, score several NMT models\nperformance on chat translation and how these metrics perform when compared to\nhuman-annotated scores. The results show that for ranking NMT models in chat\ntranslations, all metrics seem consistent in deciding which model outperforms\nthe others. This implies that traditional baseline metrics, which are faster\nand simpler to use, can still be helpful. On the other hand, when it comes to\nbetter correlation with human judgment, neural-based metrics outperform\ntraditional metrics, with COMET achieving the highest correlation with the\nhuman-annotated score on a chat translation. However, we show that even the\nbest metric struggles when scoring English translations from sentences with\nanaphoric zero-pronoun in Japanese.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18190v1",
    "entry_id": "http://arxiv.org/abs/2412.18190v1"
  },
  {
    "title": "On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs",
    "authors": [
      "Andre Rusli",
      "Makoto Shishido"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "This research explores the applicability of cross-lingual transfer learning\nfrom English to Japanese and Indonesian using the XLM-R pre-trained model. The\nresults are compared with several previous works, either by models using a\nsimilar zero-shot approach or a fully-supervised approach, to provide an\noverview of the zero-shot transfer learning approach's capability using XLM-R\nin comparison with existing models. Our models achieve the best result in one\nJapanese dataset and comparable results in other datasets in Japanese and\nIndonesian languages without being trained using the target language.\nFurthermore, the results suggest that it is possible to train a multi-lingual\nmodel, instead of one model for each language, and achieve promising results.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18188v1",
    "entry_id": "http://arxiv.org/abs/2412.18188v1"
  },
  {
    "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization",
    "authors": [
      "Yucong Luo",
      "Mingyue Cheng",
      "Jie Ouyang",
      "Xiaoyu Tao",
      "Qi Liu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18185v1",
    "entry_id": "http://arxiv.org/abs/2412.18185v1"
  },
  {
    "title": "Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization",
    "authors": [
      "Sihao Liu",
      "Yibo Yang",
      "Xiaojie Li",
      "David A. Clifton",
      "Bernard Ghanem"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Online continual learning (OCL) seeks to learn new tasks from data streams\nthat appear only once, while retaining knowledge of previously learned tasks.\nMost existing methods rely on replay, focusing on enhancing memory retention\nthrough regularization or distillation. However, they often overlook the\nadaptability of the model, limiting the ability to learn generalizable and\ndiscriminative features incrementally from online training data. To address\nthis, we introduce a plug-and-play module, S6MOD, which can be integrated into\nmost existing methods and directly improve adaptability. Specifically, S6MOD\nintroduces an extra branch after the backbone, where a mixture of\ndiscretization selectively adjusts parameters in a selective state space model,\nenriching selective scan patterns such that the model can adaptively select the\nmost sensitive discretization method for current dynamics. We further design a\nclass-conditional routing algorithm for dynamic, uncertainty-based adjustment\nand implement a contrastive discretization loss to optimize it. Extensive\nexperiments combining our module with various models demonstrate that S6MOD\nsignificantly enhances model adaptability, leading to substantial performance\ngains and achieving the state-of-the-art results.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18177v1",
    "entry_id": "http://arxiv.org/abs/2412.18177v1"
  },
  {
    "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation",
    "authors": [
      "Yucong Luo",
      "Qitao Qin",
      "Hao Zhang",
      "Mingyue Cheng",
      "Ruiran Yan",
      "Kefan Wang",
      "Jie Ouyang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/.",
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18176v1",
    "entry_id": "http://arxiv.org/abs/2412.18176v1"
  },
  {
    "title": "INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent",
    "authors": [
      "Haohang Li",
      "Yupeng Cao",
      "Yangyang Yu",
      "Shashidhar Reddy Javaji",
      "Zhiyang Deng",
      "Yueru He",
      "Yuechen Jiang",
      "Zining Zhu",
      "Koduvayur Subbalakshmi",
      "Guojun Xiong",
      "Jimin Huang",
      "Lingfei Qian",
      "Xueqing Peng",
      "Qianqian Xie",
      "Jordan W. Suchow"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
    "doi": null,
    "primary_category": "cs.CE",
    "categories": [
      "cs.CE",
      "cs.AI",
      "q-fin.CP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18174v1",
    "entry_id": "http://arxiv.org/abs/2412.18174v1"
  },
  {
    "title": "KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management",
    "authors": [
      "Rongxin Cheng",
      "Yifan Peng",
      "Yuxin Lai",
      "Xingda Wei",
      "Rong Chen",
      "Haibo Chen"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art.",
    "doi": null,
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18169v1",
    "entry_id": "http://arxiv.org/abs/2412.18169v1"
  },
  {
    "title": "Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi",
    "authors": [
      "Rasika Ransing",
      "Mohammed Amaan Dhamaskar",
      "Ayush Rajpurohit",
      "Amey Dhoke",
      "Sanket Dalvi"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "India's vast linguistic diversity presents unique challenges and\nopportunities for technological advancement, especially in the realm of Natural\nLanguage Processing (NLP). While there has been significant progress in NLP\napplications for widely spoken languages, the regional languages of India, such\nas Marathi and Hindi, remain underserved. Research in the field of NLP for\nIndian regional languages is at a formative stage and holds immense\nsignificance. The paper aims to build a platform which enables the user to use\nvarious features like text anonymization, abstractive text summarization and\nspell checking in English, Hindi and Marathi language. The aim of these tools\nis to serve enterprise and consumer clients who predominantly use Indian\nRegional Languages.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18163v1",
    "entry_id": "http://arxiv.org/abs/2412.18163v1"
  },
  {
    "title": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities",
    "authors": [
      "Shray Mathur",
      "Noah van der Vleuten",
      "Kevin Yager",
      "Esther Tsai"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18161v1",
    "entry_id": "http://arxiv.org/abs/2412.18161v1"
  },
  {
    "title": "Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance",
    "authors": [
      "Yaoyun Zhang",
      "Xuenan Xu",
      "Mengyue Wu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The video-to-audio (V2A) generation task has drawn attention in the field of\nmultimedia due to the practicality in producing Foley sound. Semantic and\ntemporal conditions are fed to the generation model to indicate sound events\nand temporal occurrence. Recent studies on synthesizing immersive and\nsynchronized audio are faced with challenges on videos with moving visual\npresence. The temporal condition is not accurate enough while low-resolution\nsemantic condition exacerbates the problem. To tackle these challenges, we\npropose Smooth-Foley, a V2A generative model taking semantic guidance from the\ntextual label across the generation to enhance both semantic and temporal\nalignment in audio. Two adapters are trained to leverage pre-trained\ntext-to-audio generation models. A frame adapter integrates high-resolution\nframe-wise video features while a temporal adapter integrates temporal\nconditions obtained from similarities of visual frames and textual labels. The\nincorporation of semantic guidance from textual labels achieves precise\naudio-video alignment. We conduct extensive quantitative and qualitative\nexperiments. Results show that Smooth-Foley performs better than existing\nmodels on both continuous sound scenarios and general scenarios. With semantic\nguidance, the audio generated by Smooth-Foley exhibits higher quality and\nbetter adherence to physical laws.",
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18157v1",
    "entry_id": "http://arxiv.org/abs/2412.18157v1"
  },
  {
    "title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data",
    "authors": [
      "Cong Li",
      "Qingqing Long",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis.",
    "doi": null,
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18156v1",
    "entry_id": "http://arxiv.org/abs/2412.18156v1"
  },
  {
    "title": "GeneSUM: Large Language Model-based Gene Summary Extraction",
    "authors": [
      "Zhijian Chen",
      "Chuan Hu",
      "Min Wu",
      "Qingqing Long",
      "Xuezhi Wang",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Emerging topics in biomedical research are continuously expanding, providing\na wealth of information about genes and their function. This rapid\nproliferation of knowledge presents unprecedented opportunities for scientific\ndiscovery and formidable challenges for researchers striving to keep abreast of\nthe latest advancements. One significant challenge is navigating the vast\ncorpus of literature to extract vital gene-related information, a\ntime-consuming and cumbersome task. To enhance the efficiency of this process,\nit is crucial to address several key challenges: (1) the overwhelming volume of\nliterature, (2) the complexity of gene functions, and (3) the automated\nintegration and generation. In response, we propose GeneSUM, a two-stage\nautomated gene summary extractor utilizing a large language model (LLM). Our\napproach retrieves and eliminates redundancy of target gene literature and then\nfine-tunes the LLM to refine and streamline the summarization process. We\nconducted extensive experiments to validate the efficacy of our proposed\nframework. The results demonstrate that LLM significantly enhances the\nintegration of gene-specific information, allowing more efficient\ndecision-making in ongoing research.",
    "doi": null,
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18154v1",
    "entry_id": "http://arxiv.org/abs/2412.18154v1"
  },
  {
    "title": "EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation",
    "authors": [
      "Shuhao Han",
      "Haotian Fan",
      "Jiachen Fu",
      "Liang Li",
      "Tao Li",
      "Junhui Cui",
      "Yunqiu Wang",
      "Yang Tai",
      "Jingwei Sun",
      "Chunle Guo",
      "Chongyi Li"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Recently, Text-to-Image (T2I) generation models have achieved significant\nadvancements. Correspondingly, many automated metrics have emerged to evaluate\nthe image-text alignment capabilities of generative models. However, the\nperformance comparison among these automated metrics is limited by existing\nsmall datasets. Additionally, these datasets lack the capacity to assess the\nperformance of automated metrics at a fine-grained level. In this study, we\ncontribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with\nfine-grained human annotations for image-text alignment-related tasks. In the\nconstruction process, we employ various strategies such as balanced prompt\nsampling and data re-annotation to ensure the diversity and reliability of our\nbenchmark. This allows us to comprehensively evaluate the effectiveness of\nimage-text alignment metrics for T2I models. Meanwhile, we introduce two new\nmethods to evaluate the image-text alignment capabilities of T2I models:\nFGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to\nproduce fine-grained image-text alignment scores and PN-VQA which adopts a\nnovel positive-negative VQA manner in VQA models for zero-shot fine-grained\nevaluation. Both methods achieve impressive performance in image-text alignment\nevaluations. We also use our methods to rank current AIGC models, in which the\nresults can serve as a reference source for future study and promote the\ndevelopment of T2I generation. The data and code will be made publicly\navailable.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18150v1",
    "entry_id": "http://arxiv.org/abs/2412.18150v1"
  },
  {
    "title": "Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media",
    "authors": [
      "Zhen Sun",
      "Zongmin Zhang",
      "Xinyue Shen",
      "Ziyi Zhang",
      "Yule Liu",
      "Michael Backes",
      "Yang Zhang",
      "Xinlei He"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Social media platforms are experiencing a growing presence of AI-Generated\nTexts (AIGTs). However, the misuse of AIGTs could have profound implications\nfor public opinion, such as spreading misinformation and manipulating\nnarratives. Despite its importance, a systematic study to assess the prevalence\nof AIGTs on social media is still lacking. To address this gap, this paper aims\nto quantify, monitor, and analyze the AIGTs on online social media platforms.\nWe first collect a dataset (SM-D) with around 2.4M posts from 3 major social\nmedia platforms: Medium, Quora, and Reddit. Then, we construct a diverse\ndataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines\npopular open-source datasets and our AIGT datasets generated from social media\ntexts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors.\nWith this setup, we identify the best-performing detector (OSM-Det). We then\napply OSM-Det to SM-D to track AIGTs over time and observe different trends of\nAI Attribution Rate (AAR) across social media platforms from January 2022 to\nOctober 2024. Specifically, Medium and Quora exhibit marked increases in AAR,\nrising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast,\nReddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the\nsame period. Our further analysis indicates that AIGTs differ from\nhuman-written texts across several dimensions, including linguistic patterns,\ntopic distributions, engagement levels, and the follower distribution of\nauthors. We envision our analysis and findings on AIGTs in social media can\nshed light on future research in this domain.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18148v1",
    "entry_id": "http://arxiv.org/abs/2412.18148v1"
  },
  {
    "title": "Text-Aware Adapter for Few-Shot Keyword Spotting",
    "authors": [
      "Youngmoon Jung",
      "Jinyoung Lee",
      "Seungjin Lee",
      "Myunghun Jung",
      "Yong-Hyeok Lee",
      "Hoon-Young Cho"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Recent advances in flexible keyword spotting (KWS) with text enrollment allow\nusers to personalize keywords without uttering them during enrollment. However,\nthere is still room for improvement in target keyword performance. In this\nwork, we propose a novel few-shot transfer learning method, called text-aware\nadapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for\nspecific keywords with limited speech samples. To adapt the acoustic encoder,\nwe leverage a jointly pre-trained text encoder to generate a text embedding\nthat acts as a representative vector for the keyword. By fine-tuning only a\nsmall portion of the network while keeping the core components' weights intact,\nthe TA-adapter proves highly efficient for few-shot KWS, enabling a seamless\nreturn to the original pre-trained model. In our experiments, the TA-adapter\ndemonstrated significant performance improvements across 35 distinct keywords\nfrom the Google Speech Commands V2 dataset, with only a 0.14% increase in the\ntotal number of parameters.",
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18142v1",
    "entry_id": "http://arxiv.org/abs/2412.18142v1"
  },
  {
    "title": "An Instrumental Value for Data Production and its Application to Data Pricing",
    "authors": [
      "Rui Ai",
      "Boxiang Lyu",
      "Zhaoran Wang",
      "Zhuoran Yang",
      "Haifeng Xu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "How much value does a dataset or a data production process have to an agent\nwho wishes to use the data to assist decision-making? This is a fundamental\nquestion towards understanding the value of data as well as further pricing of\ndata. This paper develops an approach for capturing the instrumental value of\ndata production processes, which takes two key factors into account: (a) the\ncontext of the agent's decision-making problem; (b) prior data or information\nthe agent already possesses. We ''micro-found'' our valuation concepts by\nshowing how they connect to classic notions of information design and signals\nin information economics. When instantiated in the domain of Bayesian linear\nregression, our value naturally corresponds to information gain. Based on our\ndesigned data value, we then study a basic monopoly pricing setting with a\nbuyer looking to purchase from a seller some labeled data of a certain feature\ndirection in order to improve a Bayesian regression model. We show that when\nthe seller has the ability to fully customize any data request, she can extract\nthe first-best revenue (i.e., full surplus) from any population of buyers,\ni.e., achieving first-degree price discrimination. If the seller can only sell\ndata that are derived from an existing data pool, this limits her ability to\ncustomize, and achieving first-best revenue becomes generally impossible.\nHowever, we design a mechanism that achieves seller revenue at most $\\log\n(\\kappa)$ less than the first-best revenue, where $\\kappa$ is the condition\nnumber associated with the data matrix. A corollary of this result is that the\nseller can extract the first-best revenue in the multi-armed bandits special\ncase.",
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18140v1",
    "entry_id": "http://arxiv.org/abs/2412.18140v1"
  },
  {
    "title": "Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy",
    "authors": [
      "Qian Tao",
      "Xiyuan Wang",
      "Muhan Zhang",
      "Shuxian Hu",
      "Wenyuan Yu",
      "Jingren Zhou"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Graph neural networks (GNNs) have become a prevalent framework for graph\ntasks. Many recent studies have proposed the use of graph convolution methods\nover the numerous subgraphs of each graph, a concept known as subgraph graph\nneural networks (subgraph GNNs), to enhance GNNs' ability to distinguish\nnon-isomorphic graphs. To maximize the expressiveness, subgraph GNNs often\nrequire each subgraph to have equal size to the original graph. Despite their\nimpressive performance, subgraph GNNs face challenges due to the vast number\nand large size of subgraphs which lead to a surge in training data, resulting\nin both storage and computational inefficiencies. In response to this problem,\nthis paper introduces Ego-Nets-Fit-All (ENFA), a model that uniformly takes the\nsmaller ego nets as subgraphs, thereby providing greater storage and\ncomputational efficiency, while at the same time guarantees identical outputs\nto the original subgraph GNNs even taking the whole graph as subgraphs. The key\nis to identify and eliminate the redundant computation among subgraphs. For\nexample, a node $v_i$ may appear in multiple subgraphs but is far away from all\nof their centers (the unsymmetric part between subgraphs). Therefore, its first\nfew rounds of message passing within each subgraph can be computed once in the\noriginal graph instead of being computed multiple times within each subgraph.\nSuch strategy enables our ENFA to accelerate subgraph GNNs in an exact way,\nunlike previous sampling approaches that often lose the performance. Extensive\nexperiments across various datasets reveal that compared with the conventional\nsubgraph GNNs, ENFA can reduce storage space by 29.0% to 84.5% and improve\ntraining efficiency by up to 1.66x.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18125v1",
    "entry_id": "http://arxiv.org/abs/2412.18125v1"
  },
  {
    "title": "Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm",
    "authors": [
      "Xiaoyang Hu",
      "Richard L. Lewis"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Cognitive tasks originally developed for humans are now increasingly used to\nstudy language models. While applying these tasks is often straightforward,\ninterpreting their results can be challenging. In particular, when a model\nunderperforms, it's often unclear whether this results from a limitation in the\ncognitive ability being tested or a failure to understand the task itself. A\nrecent study argued that GPT 3.5's declining performance on 2-back and 3-back\ntasks reflects a working memory capacity limit similar to humans. By analyzing\na range of open-source language models of varying performance levels on these\ntasks, we show that the poor performance instead reflects a limitation in task\ncomprehension and task set maintenance. In addition, we push the best\nperforming model to higher n values and experiment with alternative prompting\nstrategies, before analyzing model attentions. Our larger aim is to contribute\nto the ongoing conversation around refining methodologies for the cognitive\nevaluation of language models.",
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18120v1",
    "entry_id": "http://arxiv.org/abs/2412.18120v1"
  },
  {
    "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
    "authors": [
      "Hao Wen",
      "Shizuo Tian",
      "Borislav Pavlov",
      "Wenjie Du",
      "Yixuan Li",
      "Ge Chang",
      "Shanhui Zhao",
      "Jiacheng Liu",
      "Yunxin Liu",
      "Ya-Qin Zhang",
      "Yuanchun Li"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18116v1",
    "entry_id": "http://arxiv.org/abs/2412.18116v1"
  },
  {
    "title": "AIGT: AI Generative Table Based on Prompt",
    "authors": [
      "Mingming Zhang",
      "Zhiqing Xiao",
      "Guoshan Lu",
      "Sai Wu",
      "Weiqiang Wang",
      "Xing Fu",
      "Can Yi",
      "Junbo Zhao"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Tabular data, which accounts for over 80% of enterprise data assets, is vital\nin various fields. With growing concerns about privacy protection and\ndata-sharing restrictions, generating high-quality synthetic tabular data has\nbecome essential. Recent advancements show that large language models (LLMs)\ncan effectively gener-ate realistic tabular data by leveraging semantic\ninformation and overcoming the challenges of high-dimensional data that arise\nfrom one-hot encoding. However, current methods do not fully utilize the rich\ninformation available in tables. To address this, we introduce AI Generative\nTable (AIGT) based on prompt enhancement, a novel approach that utilizes meta\ndata information, such as table descriptions and schemas, as prompts to\ngenerate ultra-high quality synthetic data. To overcome the token limit\nconstraints of LLMs, we propose long-token partitioning algorithms that enable\nAIGT to model tables of any scale. AIGT achieves state-of-the-art performance\non 14 out of 20 public datasets and two real industry datasets within the\nAlipay risk control system.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18111v1",
    "entry_id": "http://arxiv.org/abs/2412.18111v1"
  },
  {
    "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
    "authors": [
      "Gui Ling",
      "Ziyang Wang",
      "Yuliang Yan",
      "Qingwen Liu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large language models (LLMs) have garnered significant attention for their\nremarkable capabilities across various domains, whose vast parameter scales\npresent challenges for practical deployment. Structured pruning is an effective\nmethod to balance model performance with efficiency, but performance\nrestoration under computational resource constraints is a principal challenge\nin pruning LLMs. Therefore, we present a low-cost and fast structured pruning\nmethod for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We\npropose Batched Greedy Pruning for rapid and near-optimal pruning, which\nenhances the accuracy of head-wise pruning error estimation through grouped\nCholesky decomposition and improves the pruning efficiency of FFN via Dynamic\nGroup Size, thereby achieving approximate local optimal pruning results within\none hour. Besides, we explore the limitations of layer-wise pruning from the\nperspective of error accumulation and propose Incremental Pruning Ratio, a\nnon-uniform pruning strategy to reduce performance degradation. Experimental\nresults on the LLaMA benchmark show that SlimGPT outperforms other methods and\nachieves state-of-the-art results.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18110v1",
    "entry_id": "http://arxiv.org/abs/2412.18110v1"
  },
  {
    "title": "SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training",
    "authors": [
      "Jiaxing Yu",
      "Xinda Wu",
      "Yunfei Xu",
      "Tieyao Zhang",
      "Songruoyao Wu",
      "Le Ma",
      "Kejun Zhang"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Lyric-to-melody generation aims to automatically create melodies based on\ngiven lyrics, requiring the capture of complex and subtle correlations between\nthem. However, previous works usually suffer from two main challenges: 1)\nlyric-melody alignment modeling, which is often simplified to\none-syllable/word-to-one-note alignment, while others have the problem of low\nalignment accuracy; 2) lyric-melody harmony modeling, which usually relies\nheavily on intermediates or strict rules, limiting model's capabilities and\ngenerative diversity. In this paper, we propose SongGLM, a lyric-to-melody\ngeneration system that leverages 2D alignment encoding and multi-task\npre-training based on the General Language Model (GLM) to guarantee the\nalignment and harmony between lyrics and melodies. Specifically, 1) we\nintroduce a unified symbolic song representation for lyrics and melodies with\nword-level and phrase-level (2D) alignment encoding to capture the lyric-melody\nalignment; 2) we design a multi-task pre-training framework with hierarchical\nblank infilling objectives (n-gram, phrase, and long span), and incorporate\nlyric-melody relationships into the extraction of harmonized n-grams to ensure\nthe lyric-melody harmony. We also construct a large-scale lyric-melody paired\ndataset comprising over 200,000 English song pieces for pre-training and\nfine-tuning. The objective and subjective results indicate that SongGLM can\ngenerate melodies from lyrics with significant improvements in both alignment\nand harmony, outperforming all the previous baseline methods.",
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18107v1",
    "entry_id": "http://arxiv.org/abs/2412.18107v1"
  },
  {
    "title": "Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels",
    "authors": [
      "Mingcong Song",
      "Xinru Tang",
      "Fengfan Hou",
      "Jing Li",
      "Wei Wei",
      "Yipeng Ma",
      "Runqiu Xiao",
      "Hongjie Si",
      "Dingcheng Jiang",
      "Shouyi Yin",
      "Yang Hu",
      "Guoping Long"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18106v1",
    "entry_id": "http://arxiv.org/abs/2412.18106v1"
  },
  {
    "title": "EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent",
    "authors": [
      "Suyuan Wang",
      "Xueqian Yin",
      "Menghao Wang",
      "Ruofeng Guo",
      "Kai Nan"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "The rapid growth of scientific techniques and knowledge is reflected in the\nexponential increase in new patents filed annually. While these patents drive\ninnovation, they also present significant burden for researchers and engineers,\nespecially newcomers. To avoid the tedious work of navigating a vast and\ncomplex landscape to identify trends and breakthroughs, researchers urgently\nneed efficient tools to summarize, evaluate, and contextualize patents,\nrevealing their innovative contributions and underlying scientific\nprinciples.To address this need, we present EvoPat, a multi-LLM-based patent\nagent designed to assist users in analyzing patents through Retrieval-Augmented\nGeneration (RAG) and advanced search strategies. EvoPat leverages multiple\nLarge Language Models (LLMs), each performing specialized roles such as\nplanning, identifying innovations, and conducting comparative evaluations. The\nsystem integrates data from local databases, including patents, literature,\nproduct catalogous, and company repositories, and online searches to provide\nup-to-date insights. The ability to collect information not included in\noriginal database automatically is also implemented. Through extensive testing\nin the natural language processing (NLP) domain, we demonstrate that EvoPat\noutperforms GPT-4 in tasks such as patent summarization, comparative analysis,\nand technical evaluation. EvoPat represents a significant step toward creating\nAI-powered tools that empower researchers and engineers to efficiently navigate\nthe complexities of the patent landscape.",
    "doi": null,
    "primary_category": "cs.DL",
    "categories": [
      "cs.DL",
      "cs.AI",
      "I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18100v1",
    "entry_id": "http://arxiv.org/abs/2412.18100v1"
  },
  {
    "title": "An Attention-based Framework with Multistation Information for Earthquake Early Warnings",
    "authors": [
      "Yu-Ming Huang",
      "Kuan-Yu Chen",
      "Wen-Wei Lin",
      "Da-Yi Chen"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Earthquake early warning systems play crucial roles in reducing the risk of\nseismic disasters. Previously, the dominant modeling system was the\nsingle-station models. Such models digest signal data received at a given\nstation and predict earth-quake parameters, such as the p-phase arrival time,\nintensity, and magnitude at that location. Various methods have demonstrated\nadequate performance. However, most of these methods present the challenges of\nthe difficulty of speeding up the alarm time, providing early warning for\ndistant areas, and considering global information to enhance performance.\nRecently, deep learning has significantly impacted many fields, including\nseismology. Thus, this paper proposes a deep learning-based framework, called\nSENSE, for the intensity prediction task of earthquake early warning systems.\nTo explicitly consider global information from a regional or national\nperspective, the input to SENSE comprises statistics from a set of stations in\na given region or country. The SENSE model is designed to learn the\nrelationships among the set of input stations and the locality-specific\ncharacteristics of each station. Thus, SENSE is not only expected to provide\nmore reliable forecasts by considering multistation data but also has the\nability to provide early warnings to distant areas that have not yet received\nsignals. This study conducted extensive experiments on datasets from Taiwan and\nJapan. The results revealed that SENSE can deliver competitive or even better\nperformances compared with other state-of-the-art methods.",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18099v1",
    "entry_id": "http://arxiv.org/abs/2412.18099v1"
  },
  {
    "title": "LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting",
    "authors": [
      "Nan Yang",
      "Chong Wang",
      "Meihua Zhao",
      "Zimeng Zhao",
      "Huiling Zheng",
      "Bin Zhang",
      "Jianing Wang",
      "Xiaofeng Li"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Ocean forecasting is crucial for both scientific research and societal\nbenefits. Currently, the most accurate forecasting systems are global ocean\nforecasting systems (GOFSs), which represent the ocean state variables (OSVs)\nas discrete grids and solve partial differential equations (PDEs) governing the\ntransitions of oceanic state variables using numerical methods. However, GOFSs\nprocesses are computationally expensive and prone to cumulative errors.\nRecently, large artificial intelligence (AI)-based models significantly boosted\nforecasting speed and accuracy. Unfortunately, building a large AI ocean\nforecasting system that can be considered cross-spatiotemporal and air-sea\ncoupled forecasts remains a significant challenge. Here, we introduce LangYa, a\ncross-spatiotemporal and air-sea coupled ocean forecasting system. Results\ndemonstrate that the time embedding module in LangYa enables a single model to\nmake forecasts with lead times ranging from 1 to 7 days. The air-sea coupled\nmodule effectively simulates air-sea interactions. The ocean self-attention\nmodule improves network stability and accelerates convergence during training,\nand the adaptive thermocline loss function improves the accuracy of thermocline\nforecasting. Compared to existing numerical and AI-based ocean forecasting\nsystems, LangYa uses 27 years of global ocean data from the Global Ocean\nReanalysis and Simulation version 12 (GLORYS12) for training and achieves more\nreliable deterministic forecasting results for OSVs. LangYa forecasting system\nprovides global ocean researchers with access to a powerful software tool for\naccurate ocean forecasting and opens a new paradigm for ocean science.",
    "doi": null,
    "primary_category": "physics.ao-ph",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18097v1",
    "entry_id": "http://arxiv.org/abs/2412.18097v1"
  },
  {
    "title": "Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH) -- a Large Language Model Chatbot for Perioperative Medicine",
    "authors": [
      "Yu He Ke",
      "Liyuan Jin",
      "Kabilan Elangovan",
      "Bryan Wen Xi Ong",
      "Chin Yang Oh",
      "Jacqueline Sim",
      "Kenny Wei-Tsen Loh",
      "Chai Rick Soh",
      "Jonathan Ming Hua Cheng",
      "Aaron Kwang Yang Lee",
      "Daniel Shu Wei Ting",
      "Nan Liu",
      "Hairil Rizal Abdullah"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large Language Models (LLMs) are emerging as powerful tools in healthcare,\nparticularly for complex, domain-specific tasks. This study describes the\ndevelopment and evaluation of the PErioperative AI CHatbot (PEACH), a secure\nLLM-based system integrated with local perioperative guidelines to support\npreoperative clinical decision-making. PEACH was embedded with 35 institutional\nperioperative protocols in the secure Claude 3.5 Sonet LLM framework within\nPair Chat (developed by Singapore Government) and tested in a silent deployment\nwith real-world data. Accuracy, safety, and usability were assessed. Deviations\nand hallucinations were categorized based on potential harm, and user feedback\nwas evaluated using the Technology Acceptance Model (TAM). Updates were made\nafter the initial silent deployment to amend one protocol.\n  In 240 real-world clinical iterations, PEACH achieved a first-generation\naccuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across\nthree iterations. The updated PEACH demonstrated improved accuracy of 97.9%\n(235/240), with a statistically significant difference from the null hypothesis\nof 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and\ndeviations were observed (both 1/240 and 2/240, respectively). Clinicians\nreported that PEACH expedited decisions in 95% of cases, and inter-rater\nreliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among\nattendings.\n  PEACH is an accurate, adaptable tool that enhances consistency and efficiency\nin perioperative decision-making. Future research should explore its\nscalability across specialties and its impact on clinical outcomes.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18096v1",
    "entry_id": "http://arxiv.org/abs/2412.18096v1"
  },
  {
    "title": "BRIDGE: Bundle Recommendation via Instruction-Driven Generation",
    "authors": [
      "Tuan-Nghia Bui",
      "Huy-Son Nguyen",
      "Cam-Van Nguyen Thi",
      "Hoang-Quynh Le",
      "Duc-Trong Le"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Bundle recommendation aims to suggest a set of interconnected items to users.\nHowever, diverse interaction types and sparse interaction matrices often pose\nchallenges for previous approaches in accurately predicting user-bundle\nadoptions. Inspired by the distant supervision strategy and generative\nparadigm, we propose BRIDGE, a novel framework for bundle recommendation. It\nconsists of two main components namely the correlation-based item clustering\nand the pseudo bundle generation modules. Inspired by the distant supervision\napproach, the former is to generate more auxiliary information, e.g.,\ninstructive item clusters, for training without using external data. This\ninformation is subsequently aggregated with collaborative signals from user\nhistorical interactions to create pseudo `ideal' bundles. This capability\nallows BRIDGE to explore all aspects of bundles, rather than being limited to\nexisting real-world bundles. It effectively bridging the gap between user\nimagination and predefined bundles, hence improving the bundle recommendation\nperformance. Experimental results validate the superiority of our models over\nstate-of-the-art ranking-based methods across five benchmark datasets.",
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18092v1",
    "entry_id": "http://arxiv.org/abs/2412.18092v1"
  },
  {
    "title": "AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning",
    "authors": [
      "Lixian Jing",
      "Jianpeng Qi",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "As deep neural networks (DNNs) are increasingly deployed on edge devices,\noptimizing models for constrained computational resources is critical. Existing\nauto-pruning methods face challenges due to the diversity of DNN models,\nvarious operators (e.g., filters), and the difficulty in balancing pruning\ngranularity with model accuracy. To address these limitations, we introduce\nAutoSculpt, a pattern-based automated pruning framework designed to enhance\nefficiency and accuracy by leveraging graph learning and deep reinforcement\nlearning (DRL). AutoSculpt automatically identifies and prunes regular patterns\nwithin DNN architectures that can be recognized by existing inference engines,\nenabling runtime acceleration. Three key steps in AutoSculpt include: (1)\nConstructing DNNs as graphs to encode their topology and parameter\ndependencies, (2) embedding computationally efficient pruning patterns, and (3)\nutilizing DRL to iteratively refine auto-pruning strategies until the optimal\nbalance between compression and accuracy is achieved. Experimental results\ndemonstrate the effectiveness of AutoSculpt across various architectures,\nincluding ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning\nrates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming\nall baselines. The codes can be available at\nhttps://anonymous.4open.science/r/AutoSculpt-DDA0",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18091v1",
    "entry_id": "http://arxiv.org/abs/2412.18091v1"
  },
  {
    "title": "Multi-Point Positional Insertion Tuning for Small Object Detection",
    "authors": [
      "Kanoko Goto",
      "Takumi Karasawa",
      "Takumi Hirose",
      "Rei Kawakami",
      "Nakamasa Inoue"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Small object detection aims to localize and classify small objects within\nimages. With recent advances in large-scale vision-language pretraining,\nfinetuning pretrained object detection models has emerged as a promising\napproach. However, finetuning large models is computationally and memory\nexpensive. To address this issue, this paper introduces multi-point positional\ninsertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for\nsmall object detection. Specifically, MPI incorporates multiple positional\nembeddings into a frozen pretrained model, enabling the efficient detection of\nsmall objects by providing precise positional information to latent features.\nThrough experiments, we demonstrated the effectiveness of the proposed method\non the SODA-D dataset. MPI performed comparably to conventional PEFT methods,\nincluding CoOp and VPT, while significantly reducing the number of parameters\nthat need to be tuned.",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18090v1",
    "entry_id": "http://arxiv.org/abs/2412.18090v1"
  },
  {
    "title": "Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner",
    "authors": [
      "Aizierjiang Aiersilan"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Motion planning is a crucial component in autonomous driving.\nState-of-the-art motion planners are trained on meticulously curated datasets,\nwhich are not only expensive to annotate but also insufficient in capturing\nrarely seen critical scenarios. Failing to account for such scenarios poses a\nsignificant risk to motion planners and may lead to incidents during testing.\nAn intuitive solution is to manually compose such scenarios by programming and\nexecuting a simulator (e.g., CARLA). However, this approach incurs substantial\nhuman costs. Motivated by this, we propose an inexpensive method for generating\ndiverse critical traffic scenarios to train more robust motion planners. First,\nwe represent traffic scenarios as scripts, which are then used by the simulator\nto generate traffic scenarios. Next, we develop a method that accepts\nuser-specified text descriptions, which a Large Language Model (LLM) translates\ninto scripts using in-context learning. The output scripts are sent to the\nsimulator that produces the corresponding traffic scenarios. As our method can\ngenerate abundant safety-critical traffic scenarios, we use them as synthetic\ntraining data for motion planners. To demonstrate the value of generated\nscenarios, we train existing motion planners on our synthetic data, real-world\ndatasets, and a combination of both. Our experiments show that motion planners\ntrained with our data significantly outperform those trained solely on\nreal-world data, showing the usefulness of our synthetic data and the\neffectiveness of our data generation method. Our source code is available at\nhttps://ezharjan.github.io/AutoSceneGen.",
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18086v1",
    "entry_id": "http://arxiv.org/abs/2412.18086v1"
  },
  {
    "title": "Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models",
    "authors": [
      "Xuan Lin",
      "Long Chen",
      "Yile Wang",
      "Xiangxiang Zeng",
      "Philip S. Yu"
    ],
    "published": "2024-12-24",
    "year": 2024,
    "abstract": "Large language models (LLMs) are widely applied in various natural language\nprocessing tasks such as question answering and machine translation. However,\ndue to the lack of labeled data and the difficulty of manual annotation for\nbiochemical properties, the performance for molecule generation tasks is still\nlimited, especially for tasks involving multi-properties constraints. In this\nwork, we present a two-step framework PEIT (Property Enhanced Instruction\nTuning) to improve LLMs for molecular-related tasks. In the first step, we use\ntextual descriptions, SMILES, and biochemical properties as multimodal inputs\nto pre-train a model called PEIT-GEN, by aligning multi-modal representations\nto synthesize instruction data. In the second step, we fine-tune existing\nopen-source LLMs with the synthesized data, the resulting PEIT-LLM can handle\nmolecule captioning, text-based molecule generation, molecular property\nprediction, and our newly proposed multi-constraint molecule generation tasks.\nExperimental results show that our pre-trained PEIT-GEN outperforms MolT5 and\nBioT5 in molecule captioning, demonstrating modalities align well between\ntextual descriptions, structures, and biochemical properties. Furthermore,\nPEIT-LLM shows promising improvements in multi-task molecule generation,\nproving the scalability of the PEIT framework for various molecular tasks. We\nrelease the code, constructed instruction data, and model checkpoints in\nhttps://github.com/chenlong164/PEIT.",
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.18084v1",
    "entry_id": "http://arxiv.org/abs/2412.18084v1"
  }
]