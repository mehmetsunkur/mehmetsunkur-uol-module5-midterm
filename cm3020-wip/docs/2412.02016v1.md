## Explore Reinforced: Equilibrium Approximation with Reinforcement Learning

Ryan Yu 1 , Mateusz Nowak 2 , Qintong Xie 2 , Michelle Yilin Feng 1 and Peter Chin 2

- 1 - Boston University - Computer Science Department Boston, MA 02215 - United States
- 2 - Dartmouth College - Thayer School of Engineering Hanover, NH 03755 - United States

Abstract . Current approximate Coarse Correlated Equilibria (CCE) algorithms struggle with equilibrium approximation for games in large stochastic environments but are theoretically guaranteed to converge to a strong solution concept. In contrast, modern Reinforcement Learning (RL) algorithms provide faster training yet yield weaker solutions. We introduce Exp3-IXrl - a blend of RL and game-theoretic approach, separating the RL agent's action selection from the equilibrium computation while preserving the integrity of the learning process. We demonstrate that our algorithm expands the application of equilibrium approximation algorithms to new environments. Specifically, we show the improved performance in a complex and adversarial cybersecurity network environment - the Cyber Operations Research Gym - and in the classical multi-armed bandit settings.

## 1 Introduction

Reinforcement Learning (RL) is a goal-oriented machine learning paradigm that primarily focuses on how agents act in an environment to maximize cumulative reward. In game theory, a Nash Equilibrium describes a strategy in which no player can gain by unilaterally changing their strategy if the strategies of the others remain unchanged [1]. In most real-world scenarios, which consist of dynamic complex environments in multi-step situations, the equilibrium (or equilibria) is computationally intractable [2].

Game theory and RL both focus on decision-making within uncertain environments, with RL learning an optimal policy that maximizes some notion of a reward and game theory analyzing strategic interactions among players and predicting equilibrium outcomes of these interactions [3]. Combining these two disciplines has led to the development of agents that not only adapt to multifaceted, changing, or uncertain settings but also anticipate and strategically react to the actions of other agents, thereby enhancing the effectiveness of decision-making systems in various applications [4].

In this paper, we describe a new technique that adapts the powerful, computational feasibility of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm to approximate the coarse correlated equilibrium (CCE) [5]. We combine EXP3's high probability successor, EXP3-IX [6], with heuristics of the Local Best Response (LBR) algorithm [7] for sequential games and the power of reinforcement learning, to create this game-theoretic guide for reinforcement learning models.

## 2 Background

Asimultaneous-move task can be represented as a stochastic game. We define a basic, fully observable, N-player stochastic game as ( S , H , {A i } i ∈ N , T , {R i } i ∈ N , γ ) , where S is set of all states shared by all N players, H is the maximum number of time steps, A i is the action space for player i and A := A 1 ×··· × A N is a set of all valid actions, T : ( S × A ) →S is a transition function, R i : ( S × A ×S ) → R is a reward function for player i and γ ∈ [0 , 1] is a discount factor.

The notion of an equilibrium provides a strong learning objective in multiagent settings. The most popular equilibrium is the Nash Equilibrium (NE). While approximating NE is ideal, it was shown to be PPAD-complete even in 2p0s games [8]. Weaker, more computationally feasible, equilibrium forms can be approximated using no-regret learning. In this study, we approximate a coarse correlated equilibrium (CCE) [5]. A CCE, σ , is defined as:

∀ i, s ' i E s ∼ σ c i ( s ) ≤ E s ∼ σ c i ( s ' i , s -i )

where i represents a player, s ' i represents a strategy different from the recommended strategy, s , and c i represents the cost of following a strategy. Recently, the lower bound on number of iterations for the convergence of ϵ -CCE for a three-player extensive-form game was proven to be 2 log 1 -o (1) 2 ( | G | ) , where | G | is the size of the game [9], while in two-player games with no chance moves, a social-welfare maximizing extensive-form CCE can be computed in polynomial time [10].

Traditionally, regret R T at time step T and a probabilistic bound, which deals with the uncertainty introduced by learners in multi-armed bandits, called pseudo-regret ˆ R T is defined as:

R T = T ∑ t =1 ℓ t,I t -min i ∈ [ K ] T ∑ t =1 ℓ t,i , and ˆ R T = max i ∈ [ K ] E [ T ∑ t =1 ℓ t,I t -T ∑ t =1 ℓ t,i ] ,

where the player has an action space of size K , ℓ t,k represents the loss experienced at time step t for action i ∈ [ K ] , and I t defines a forecaster's choice [11, 6, 12]. No-regret learning measures the difference in loss compared to the best single action in hindsight. In this study, we utilize EXP-IX, demonstrating no-regret learning with a high probability [6].

## 3 Related Work

## 3.1 Modern reinforcement learning algorithms

Reinforcement Learning (RL) has seen significant advancements through algorithms like Deep Q-Networks (DQN) [13] and Policy Gradient methods [14], achieving remarkable results in Atari games and robotics. DQN introduced deep neural networks to approximate Q-values, enabling breakthroughs in complex environments. Policy gradient algorithms refine policies via gradient ascent on estimated returns but can suffer from instability and inefficiency due to highvariance gradients. TRPO [15] and PPO [16] address this by stabilizing updates with relative entropy constraints.

Multi-Agent Reinforcement Learning (MARL) is closely related to game theory and repeated games. Algorithms such as multi-agent deterministic policy gradient (MADDPG) [17] have been utilized to coordinate multiple agents in cooperative and competitive scenarios.

## 3.2 Exponential-weight algorithm for Exploration and Exploitation

Exp3 (Exponential-weight algorithm for Exploration and Exploitation) is an adversarial bandit algorithm designed for uncertain or adversarial environments. By balancing exploration and exploitation, Exp3 minimizes regret over time [5]. Exp3-IX refines the base Exp3 algorithm, by introducing a biased Implicit exploration toward better actions, reducing the regret's variance, and enhancing the algorithm's performance as uniform exploration has been shown to detrimentally impact the performance of learning algorithms, especially in environments with numerous suboptimal options [6]. Exp3 and Exp3-IX provide theoretical guarantees for convergence to no-regret convergence in non-stochastic multi-armed bandit problems.

Extending the exploration to more complex, multi-step, stochastic environments is the focus of this paper.

## 4 Algorithm

We propose to extend the Exp3-IX with the reinforcement learning action selection step (see Fig 1). Our Exp3-IXrl incorporates several extensions to ensure compatibility across types of RL methods and asymmetric, state-specific action spaces and to enable implementation in multi-step and stochastic environments.

Fig. 1: Algorithm overview. Exp3-IXrl is a blend of an RL and game-theoretic approach, separating the RL agent's action selection from equilibrium computation while preserving the integrity of the learning process.

<!-- image -->

The action selection of an RL agent during training serves as a non-intrusive enhancement, either at each timestep t or as an offline learning data, to leverage the exploration and convergence guarantees of an RL agent for concurrent CCE training. Exp3-IXrl extends the implicit exploration of the EXP3-IX algorithm, via the exploration-exploitation balances of RL algorithms, which might be more apt to handle more complex and stochastic game settings.

At each time step during learning, EXP3-IXrl will obtain an action from either the underlying RL algorithm or the CCE approximation based on the certainty threshold. The more a state is visited, the higher the certainty in the CCE approximation.

Previous methods that attempt to approximate a CCE using EXP3 algorithms require EXP3 to interact with the environment, which differs from our proposed method. Our algorithm implements EXP3-IXrl as a third-party observer until the certainty threshold is reached. In this way, we can leverage the underlying RL algorithm's strengths to accelerate training, and once the certainty measure has been reached, we can utilize the CCE policy. A natural question is whether we lose the theoretical convergence guarantees [18] by shifting EXP3-IXrl from an active agent to a third-party observer. We introduce a normalization factor to alleviate the third-party observer effect, addressing the possible algorithm's convergence issues.

## 5 Experiments and Results

## 5.1 Environments setting

We tested Exp3-IXrl within the Cyber Operations Research Gym (CybORG) [19] Cage Challenge 2 environment [20] (CC2), a complex and adversarial cybersecurity network, where the algorithm's objective is to minimize total network infection, represented through negative rewards.

Moreover, we test our algorithm in a stochastic and deterministic multi-armed bandit (MAB) with ten actions. For the stochastic environment, we set the rewards to a standard normal distribution centered around zero with a standard deviation of one and add a sampled random noise to the received reward when the action is chosen. As for the deterministic environment, we set the reward based on the action number and do not add additional noise during the action selection process.

## 5.2 Experimental procedure and metrics used

To compare our algorithms to previous approaches, we train each agent for 10000 in each environment and gather the cumulative reward over the next 30 timesteps. To ensure a fair comparison of our method, we average our results over 100 runs in each environment with an exact random seed used for both the baselines and our algorithm, limiting the influence of random seeds on our results.

Within the multi-armed bandit setting, we use classical RL algorithms as our baselines: ϵ -Greedy [21], UCB [22], and Gradient Bandit [23]. As for the CC2 environment, we use the CardiffUni agent, a hierarchical Proximal Policy Optimization (PPO) [16], which recently had its convergence guarantees proven [18] and won the Cage Challenge 2, as our baseline.

## 5.3 Results

For CC2, we achieve comparable performance with a certainty threshold of 2750 in just 10000 simulation episodes - a tenth of the training episodes of the previous winning challenge submission [19, 20] (see Fig. 2). In the multi-armed bandit scenario, we illustrate our algorithm's performance against classical reinforcement learning algorithms, used as baselines and teachers for the Exp3IX-rl. Except

Table 1: The average cumulative reward over 30 steps averaged over 100 runs. Our algorithm significantly outperforms its classical RL and CCE counterparts.

|                                         | Algorithm       | Algorithm             | Algorithm                                    | Algorithm                                   | Algorithm                                      |
|-----------------------------------------|-----------------|-----------------------|----------------------------------------------|---------------------------------------------|------------------------------------------------|
| Scenario                                | Exp3 [5]        | Exp3-IX [6]           | RL                                           | RL                                          | Exp3-IXrl                                      |
| Deterministic MAB [ ↑ ] Certainty: 2000 | 25.47 +/- 5.47  | 26.31 +/- 3.65        | Gradient Bandit [23] UCB [22] ϵ -Greedy [21] | 26.49 +/- 3.58 22.80 +/- 0.0 25.77 +/- 4.75 | 27.0 +/- 0.0 27.0 +/- 0.0 27.0 +/ -0.0         |
| Stochastic MAB [ ↑ ] Certainty: 2000    | 45.24 +/- 22.49 |                       | Gradient Bandit [23] UCB [22] ϵ              | 46.37 +/- 23.31 48.45 +/- 21.77             | 46.3 +/- 23.46                                 |
| CC2 [ ↑ ] Certainty: 2750               | N / A           | 46.85 +/- 22.97 N / A | -Greedy [21] PPO (CardiffUni) [16]           | 40.24 +/- 17.69 -2.94 +/- 1.41              | 48.57 +/- 21.72 44.08 +/- 17.69 -3.86 +/- 1.50 |

for the Gradient Bandit algorithm in the stochastic environment, our algorithm surpasses every baseline (see Table 1).

Fig. 2: Result of our agent in the CC2 environment with a varying certainty threshold. We achieve the performance of the PPO agent with a certainty threshold of around 2750 and with only 10000 steps, demonstrating faster convergence.

<!-- image -->

## 6 Conclusion and Future Work

The proposed Exp3-IXrl algorithm combines an RL agent as an explicit exploration bias during training with traditional coarse correlated equilibrium (CCE) approximation. It maintains the RL agent's autonomy while excelling in complex, stochastic environments, where current CCE-based implementations fail. Empirical results underscore the robustness and adaptability of Exp3-IXrl across diverse environments and policies, demonstrating enhanced learning depth.

Our findings also contribute to ongoing research about the necessity of exploration in CCE approximation, specifically the minimum certainty per actionobservation pair. Future research should investigate how certainty could adjust to environmental feedback, potentially refining the RL agent's policy and improving Exp3-IXrl's adaptability to evolving cooperative or adversarial contexts.

## References

- [1] Rogers Myerson. Game Theory: Analysis of Conflict . Harvard University Press, 1991.
- [2] Constantinos Daskalakis and Christos H Papadimitriou. Three-player games are hard. In Electronic colloquium on computational complexity , volume 139, pages 81-87. Citeseer, 2005.

| [3]       | Drew Fudenberg and Jean Tirole. Game theory . MIT press, 1991.                                                                                                                                                                                                                                                                                  |
|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [4]       | Garima Jain, Arun Kumar, and Shahid Ahmad Bhat. Recent developments of game theory and reinforcement learning approaches: A systematic review. IEEE Access , 12:9999-10011, 2024.                                                                                                                                                               |
| [5]       | Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing , 32(1):48-77, 2002.                                                                                                                                                                               |
| [6]       | Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015.                                                                        |
| [7]       | Viliam Lis'y and Michael Bowling. Eqilibrium approximation quality of current no-limit poker bots. In Workshops at the Thirty-First AAAI Conference on Artificial Intelligence , 2017.                                                                                                                                                          |
| [8]       | Noam Nisan, Eva Tardos, Tim Roughgarden, and Vijay Vazirani. Algorithmic Game Theory . Cambridge University Press, 2007.                                                                                                                                                                                                                        |
| [9]       | Binghui Peng and Aviad Rubinstein. The complexity of approximate (coarse) correlated equilibrium for incomplete information games. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory , volume 247 of Proceedings of Machine Learning Research , pages 4158-4184. PMLR, 30 Jun-03 Jul 2024. |
| [10]      | Gabriele Farina, Tommaso Bianchi, and Tuomas Sandholm. Coarse correlation in extensive-                                                                                                                                                                                                                                                         |
| [11]      | Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games . Cambridge                                                                                                                                                                                                                                                               |
| [12]      | Sébastien Bubeck and Cesa-Bianchi Nicolò. Regret Analysis of Stochastic and Nonstochastic                                                                                                                                                                                                                                                       |
| [13]      | Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G                                                                                                                                                                                                                                                             |
|           | Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature , 518(7540):529-533, 2015.                                                                                                                                                              |
| [14] [15] | Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems , 12, 1999. John Schulman. Trust region policy optimization. arXiv preprint arXiv:1502.05477 , 2015.                                  |
| [16]      | John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.                                                                                                                                                                                 |
| [17]      | Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems , 30, 2017.                                                                                                                      |
|           | Markus Holzleitner, Lukas Gruber, José Arjona-Medina, Johannes Brandstetter, and Sepp Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER                                                                                                                                                                                      |
| [18]      | Hochreiter. , pages 105-130. Springer Berlin Heidelberg, Berlin, Heidelberg, 2021.                                                                                                                                                                                                                                                              |
| [20]      | Mitchell Kiely, David Bowman, Maxwell Standen, and Christopher Moir. On autonomous agents in a cyber defence environment. ArXiv , abs/2309.07388, 2023.                                                                                                                                                                                         |
| [21]      | Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . A Bradford Book, 2018.                                                                                                                                                                                                                                           |
| [22]      | Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research , 3(Nov):397-422, 2002.                                                                                                                                                                                                       |
| [23]      | Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms , SODA '05, page 385-394, USA, 2005. Society for Industrial and Applied Mathematics.                     |