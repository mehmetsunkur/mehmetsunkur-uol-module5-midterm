## A quantum-classical reinforcement learning model to play Atari games

Dominik Freinberger 1 , Julian Lemmel 1 , Radu Grosu 1 , and Sofiene Jerbi 2

1 Institut für Technische Informatik, Technische Universität Wien, Austria 2 Dahlem Center for Complex Quantum Systems, Freie Universität Berlin, Germany

December 13, 2024

## Abstract

Recent advances in reinforcement learning have demonstrated the potential of quantum learning models based on parametrized quantum circuits as an alternative to deep learning models. On the one hand, these findings have shown the ultimate exponential speed-ups in learning that full-blown quantum models can offer in certain - artificially constructed - environments. On the other hand, they have demonstrated the ability of experimentally accessible PQCs to solve OpenAI Gym benchmarking tasks. However, it remains an open question whether these near-term QRL techniques can be successfully applied to more complex problems exhibiting high-dimensional observation spaces. In this work, we bridge this gap and present a hybrid model combining a PQC with classical feature encoding and post-processing layers that is capable of tackling Atari games. A classical model, subjected to architectural restrictions similar to those present in the hybrid model is constructed to serve as a reference. Our numerical investigation demonstrates that the proposed hybrid model is capable of solving the Pong environment and achieving scores comparable to the classical reference in Breakout. Furthermore, our findings shed light on important hyperparameter settings and design choices that impact the interplay of the quantum and classical components. This work contributes to the understanding of near-term quantum learning models and makes an important step towards their deployment in real-world RL scenarios.

## 1 Introduction

Reinforcement learning (RL) has had a profound impact on the machine learning (ML) landscape, thanks to the integration of deep neural networks (DNNs). Deep RL led to a series of advancements in mastering complex decision-making tasks in Atari games [1,2], Go [3,4], StarCraft II [5], and Dota 2 [6], where agents have surpassed human-level performance. However, despite its capabilities, RL remains one of the most computationally demanding areas within ML, and would substantially benefit from enhancements in computational efficiency.

In the pursuit of more efficient RL methods, quantum computing (QC) is a potential remedy. QC is a paradigm for computing in which information is processed based on the principles of quantum mechanics [7]. By harnessing quantum mechanical phenomena such as superposition and entanglement and using them as computational resources, QC tries to gain computational advantages in certain problem classes. Here, quantum machine learning (QML) based on variational quantum algorithms (VQAs) [8] is a promising application, compatible with current noisy intermediate-scale quantum (NISQ) devices [9]. VQAs optimize parametrized quantum circuits (PQCs), quantum circuits composed of quantum gates that can be tuned via classical hardware to achieve a desired computational outcome. Similarly to classical DNNs, the PQC acts as a function approximator that is trained via classical optimization techniques, in a hybrid quantum-classical manner. PQC-based QML models and hybrid approaches that leverage the strengths of both classical and quantum components have already demonstrated significant results in both supervised [10-15] and unsupervised [16-19] machine learning tasks.

Figure 1: The hybrid quantum-classical architecture . Three convolutional layers reduce the high-dimensional input and extract a low number of features which are further combined and reduced by a linear pre-processing layer to create a highly informative latent representation. The PQC encodes these latent features and outputs the expectation values of local PauliZ measurements. The output of the PQC is further post-processed by another fully connected layer with linear activation to match the Q-value magnitudes and action space dimension.

<!-- image -->

Research in quantum reinforcement learning (QRL) with PQCs has only recently started gaining traction [20-25]. Nonetheless, the early results have already made great strides in showing the potential of QRL methods based on PQCs. From a theory standpoint, they have shown the existence of RL tasks where PQC models could achieve exponential speed-ups in learning compared to all classical models, including deep neural networks [22, 23]. These environments remain however artificially constructed using cryptographic functions, and the PQC models used rely on large-scale, fault-tolerant implementations of Shor's algorithm. From a more practical standpoint, these works have also established that small-scale PQCs are capable of solving iconic benchmarking environments from OpenAI Gym [20-23], which have been successfully implemented on current quantum hardware [26, 27]. Despite these advances, research in this domain remains sparse, with only a few existing works [28-31] addressing the challenges of applying QRL agents to environments with high-dimensional observations, as present in many real-world applications. This leaves as an open question whether QRL agents can effectively learn and operate in these complex settings, and, if so, which model design choices are most crucial in achieving a good learning performance, and how these models compare to their classical counterparts.

## 1.1 Contributions

In this work, we propose a hybrid quantum-classical model and analyze its performance in RL environments with high-dimensional observation spaces. As a testbed, we consider the Atari 2600 game environments Pong and Breakout, often used as benchmarks for deep RL [32]. Our main contribution is to show that a hybrid quantum-classical model is capable of solving the Pong environment according to its specifications [33] and reaching a performance similar to a classical reference in the game Breakout. This is in stark contrast with the findings of [28], which reported that hybrid models are unable to learn in the Atari environments due to a lack of expressivity. Our second contribution is an in-depth analysis of the design choices that are responsible for an increase in performance. In particular, we find that rescaling rewards and adjusting the learning rate in the output layer can have a notable impact on learning performance, which we relate to the different nature of Q-function landscapes that hybrid

and classical models will learn. This research extends previous studies that first demonstrated the learning capabilities of quantum and hybrid models in an approximate Q-learning setting [20-22, 29] in simple benchmarking environments from the OpenAI Gym [34]. While our results do not show any kind of advantage of quantum-enhanced models over fully classical models, they nonetheless make an important step in this direction by showcasing how an interplay of classical feature extraction and quantum processing can be successfully combined to solve complex, high-dimensional RL problems.

## 1.2 Related work

The first efforts to use PQC-based models as function approximators in RL were made in [20], where quantum agents demonstrated learning capabilities in the simple benchmark environment Frozen Lake from the OpenAI Gym [34] using an approximate Q-learning approach. Similarly, [21] benchmarked PQCs as Q-function approximators in CartPole and Blackjack , proposing different encoding schemes and confirming that PQC models can perform comparably to DNNs. In [22], PQCs were successfully used as policy function approximators to solve CartPole , MountainCar , and Acrobot , highlighting data re-uploading [35,36] and trainable scaling weights on inputs and observables as key design choices. Concurrently, [23] adopted a similar architecture for Q-learning in FrozenLake and CartPole , stressing the importance of trainable weights on observables for matching the range of Q-values. In [22] and [23], the authors demonstrate a theoretical quantum advantage of quantum agents over classical learners in both policy- and value-based reinforcement learning settings, building on results on supervised learning based on the discrete logarithm problem [37], while the former also provides numerical evidence of an empirical advantage over DNNs in specifically crafted PQC-based environments.

Most of these studies deal with environments characterized by low-dimensional observations. However, [29] tackled a 20 × 20 dimensional maze environment, using a hybrid model with a classical DNN to preprocess inputs, demonstrating adaptability to higher-dimensional problems. Likewise, [30] introduced a tensor-network-based variational quantum circuit (TN-VQC) architecture for dimensionality reduction of high-dimensional inputs. A step closer to real-world problems is taken in [28] extending previous work [21] by proposing a hybrid model combining a classical convolutional network and quantum circuits. The authors study the performance of the hybrid architecture in the Atari games Pong and Breakout , where an observation consists of approximately 100,000 variables. Although the hybrid model achieves non-trivial results in CartPole , it fails to learn in the high-dimensional Atari game environments. In our work, we draw inspiration from several of these studies, particularly those that explore hybrid quantum-classical models. We overcome the shortcomings of [28] and demonstrate that hybrid quantum-classical agents can successfully learn in Pong and Breakout .

## 2 Quantum reinforcement learning

This section gives a concise introduction to quantum computing and its application in machine learning. It further introduces the hybrid quantum-classical model used as a Q-function approximator and its implementation in the approximate Q-learning framework.

## 2.1 Quantum computing

The basic unit of quantum information is a qubit, a two-level quantum system represented by a twodimensional complex Hilbert space H = C 2 . The two orthogonal states | 0 ⟩ = (1 , 0) T and | 1 ⟩ = (0 , 1) T form a basis of this space and are referred to as computational basis states 1 . A general state of a qubit is described by a unit vector | ψ ⟩ := a | 0 ⟩ + b | 1 ⟩ ∈ H , where a, b ∈ C satisfy | a | 2 + | b | 2 = 1 . This generalizes to n qubits, where the combined Hilbert space for an n -qubit system is H ⊗ n and a general state is described by a superposition of all 2 n possible combinations of its qubits' basis states. The states of qubits are manipulated through unitary operators U acting on H ⊗ n , called quantum gates. An important set of single-qubit gates are the Pauli gates denoted as X , Y and Z that give rise to the Pauli rotation gates R x , R y and R z , which are famous examples of parameterized gates:

Figure 2: A parameterized quantum circuit as a machine learning model. A feature vector x is encoded into the quantum system in its trivial state | 0 ⟩ ⊗ n via the repeated encoding unitaries U l ( x ) (red). Intermediate variational unitaries V l ( θ ) (blue) enable the training of the circuit. The output of the model f θ ( x ) is the expectation value ⟨ M ⟩ x , θ of a (or multiple) observables (e.g., a PauliZ observable on each qubit) measured at the end of the circuit.

<!-- image -->

R x ( θ ) = e -i θ 2 X , X = [ 0 1 1 0 ] R y ( θ ) = e -i θ 2 Y , Y = [ 0 -i i 0 ] R z ( θ ) = e -i θ 2 Z , Z = [ 1 0 0 -1 ] , (1)

with rotation angels θ ∈ R . An important 2 -qubit gate is the CZ := diag(1 , 1 , 1 , -1) or ControlledZ gate. It is a controlled gate that only affects the second qubit, or target qubit, if the first qubit, the control qubit, is in the | 1 ⟩ state and is used to introduce entanglement between multiple qubits.

Unlike a classical bit, the state of a qubit is accessible only through quantum measurement, performed using an observable represented by a Hermitian operator M = M † . The spectral decomposition of M = ∑ m mP m into eigenvalues m and corresponding orthogonal projections P m , defines the possible outcomes of the measurement. According to the Born rule, when a quantum state | ψ ⟩ is measured, the outcome m occurs with probability p ( m ) = ⟨ ψ | P m | ψ ⟩ = ⟨ P m ⟩ and the state is projected onto P m | ψ ⟩ / √ p ( m ) . The expectation value of a measurement is defined as:

E ( M ) = ∑ m p ( m ) m =: ⟨ M ⟩ . (2)

Considering the spectrum of the Pauli Z -matrix, the expectation value of a computational basis measurement of a single qubit, where Z is the observable acting on that qubit, falls within the interval [ -1 , 1] .

## 2.2 The hybrid quantum-classical model

Having introduced quantum computing we now move to its application in ML by defining the hybrid quantum-classical model as illustrated in Figure 1. Given an initial state | ψ ⟩ of an n -qubit quantum system, for example the trivial state | 0 ⟩ ⊗ n , a PQC applies the parameter-dependent unitary transformation U ( x , θ ) to its qubits, which is defined as

U ( x , θ ) = L ∏ l =1 V l ( θ ) U l ( x ) , (3)

where the U l ( x ) encode parts of the feature vector x ∈ R d into the quantum state using R x gates. The V l ( θ ) consist of R x , R y and R z gates that depend on adjustable parameters θ ∈ R k that are optimized during the training process by classical hardware. The repeated encoding of the feature vector x , referred to as data re-uploading [35], is known to enhance the expressivity of PQC-based machine learning models [36]. The state of the system resulting from the application of the circuit is U ( x , θ ) | 0 ⟩ ⊗ n = | ψ ( x , θ ) ⟩ . The expectation value of some measurement observable M on the quantum system prepared by the PQC is

⟨ M ⟩ x , θ = ⟨ ψ ( x , θ ) | M | ψ ( x , θ ) ⟩ =: f θ ( x ) . (4)

This defines a deterministic quantum machine learning model f θ ( x ) , which, given input x and parameters θ , produces a deterministic value ⟨ M ⟩ x , θ . While quantum models as defined in Equation 4 have the potential for processing various kinds of data, the direct encoding of high-dimensional inputs, such as images, into quantum circuits via single-qubit rotations is currently infeasible. The low qubit count of NISQ hardware, as well as the computational demands of simulating larger qubit systems, render it impossible to encode vast amounts of data into a quantum circuit. To address these limitations, we incorporate classical convolutional layers as trainable dimensionality reduction components. These classical layers pre-process the high-dimensional input, transforming it into fewer, highly informative features for the PQC. We define the hybrid model as:

Q hybrid = L w out ( f θ ( L w in ( ˜ x ))) , (5)

where Q hybrid represents the entire hybrid model. Here, L w in : R n in → R n q represents the classical pre-processing layers that map the raw unprocessed input data ˜ x to a format suitable for the quantum circuit. f θ : R n q → R n q is the core PQC as defined in Equation 4, and L w out : R n q → R n out is a classical post-processing layer that produces the final output. This hybrid approach leverages the strengths of both quantum and classical computing, allowing us to optimize the entire model through a combination of quantum gate parameters θ and classical neural network weights w in , w out .

## 2.3 Hybrid model architecture

In this section, we elaborate on the specific architecture of the hybrid model used in our experiments. Our hybrid model design is motivated by the limitations of encoding raw images directly into a quantum circuit. To address this, we apply classical convolutional layers, which are an effective dimensionality reduction technique in reinforcement learning, as demonstrated in the seminal work of Mnih et al. [1]. This approach allows us to extract fewer but richer features from high-dimensional observations before encoding them in the PQC. As depicted in Figure 1, three convolutional layers together with a fully connected layer serve the purpose of dimensionality reduction and feature extraction. Together, these layers take the role of L w in in the notation established in Equation 5. The number of neurons at the output of the fully connected layer coincides with the number of encoding gates in the PQC, as each neuron outputs a different latent feature. We refer to this layer as the pre-processing layer and explain in more detail why we use only linear activations in this layer in Appendix G. The amount of features we can encode in the PQC is constrained by the limited number of qubits and encoding gates that we can handle computationally in our classical simulations. This in turn restricts the number of output neurons we can accommodate at the pre-processing layer. However, no such restriction exists for classical models conventionally used in RL, where the convolutional layers are typically followed directly by a wide non-linear processing layer. To allow for a fair assessment of the hybrid model's performance when compared to a classical counterpart, we introduce an artificial bottleneck in our classical reference models. We do so by adding an equivalent pre-processing layer with the same number of neurons as in the hybrid model. The exact specifications of the convolutional layers of both the hybrid and classical reference model as well as the architecture of the classical model are detailed in Appendix B.

The design of a PQC remains an area of active research, and a standard architecture has not yet been established. In this work, we choose to build on previously successful approaches for QRL in lower-dimensional observation spaces [23]: The architecture consists of multiple layers, each composed of one variational unitary (consisting of single-qubit R x , R y and R z gates along with entangling CZ gates) followed by one feature encoding unitary (a set of single-qubit R x rotations). This sequence of variational and encoding unitaries is repeated across all layers with a final variational unitary at the end of the circuit. The general structure of the PQC is shown in Figure 2 and a more detailed description of the complete hybrid model is given in Appendix A. The layerwise encoding of features leverages data re-uploading, which has been shown to greatly increase model expressivity [35,36]. In our hybrid model, this technique takes on a unique form: instead of raw observations, the inputs to the PQC are trainable features produced by the preceding convolutional and fully connected layers. This adds to the model's flexibility and ability to learn complex functions. The output of the PQC are the expectation values of each qubit as obtained by local (i.e., qubit-wise) PauliZ measurements. Unlike prior work by Skolik et al. [23], where each observable together with a single trainable parameter is

assigned to a certain action in the environment, our model employs a more adaptive design. We use a fully connected linear post-processing layer, L w out , to combine and rescale the PQC outputs, while also making the model compatible with action spaces larger than the number of available qubits.

## 2.4 Quantum Q-learning

Having established the architecture of our hybrid quantum-classical model, we now turn to its application in the RL framework, specifically within the Q-learning paradigm, which serves as the foundation for training and evaluating our model. RL is a field of ML where an agent interacts with an environment by taking actions to maximize cumulative rewards [38]. At a given time step t the agent observes a state s t ∈ S and selects an action a t ∈ A based on a policy function π ( a | s ) and receives feedback in the form of numerical rewards r t ∈ R , with the goal of learning an optimal policy π ∗ through trial and error. The agent chooses actions so that to maximize the sum of discounted future rewards. In particular, it tries to maximize the expected discounted return G t := ∑ T k = t +1 γ k -t -1 r k , where the discount rate 0 < γ < 1 determines the relative importance of short-term versus long-term rewards. The expected return when taking action a while being in state s and following the policy π thereafter, is defined as the Q-function,

Q π ( s, a ) := E π [ G t | s t = s, a t = a ] . (6)

The optimal Q-function for a given state-value pair ( s, a ) is defined as Q ∗ ( s, a ) := max π Q π ( s, a ) . From the optimal Q-function Q ∗ ( s, a ) an optimal policy π ∗ can be derived: In state s , the agent should choose an action that maximizes the optimal Q-function, i.e

π ∗ ( a | s ) = arg max a Q ∗ ( s, a ) . (7)

This observation leads to the main objective of approximate Q-learning, where the goal is to learn an approximation Q ( s, a ; θ ) of the optimal Q-function Q ∗ ( s, a ) dependent on a set of parameters θ . A famous example is the deep Q-learning algorithm introduced in [1], where DNNs are used as Qfunction approximators, labelled deep Q-networks (DQNs). Quantum Q-learning [23] is a variation of deep Q-learning, where the DQN is replaced by a PQC or a hybrid model as introduced in Section 2.2. In this setting, the agent follows an ε -greedy policy, where ε decays during training. At each time step t , the agent observes state s t and, with a probability 1 -ε , selects action a t corresponding to the highest estimated Q-value. Otherwise, it selects a random action. This behaviour balances exploration of the state space and exploitation of the current policy. Upon transition to state s t +1 the agent obtains reward r t and the experience e t = ( s t , a t , r t , s t +1 ) is stored in a replay memory D . The replay memory D is initially populated with N experiences following a purely random policy, allowing the agent to sample a minibatch B of transitions randomly at each time step, thereby breaking temporal correlations. For each sampled minibatch, the Q-network is trained by minimizing the following loss function,

L ( θ ) = ( r t + γ max a ' ˆ Q ( s t +1 , a ' ; θ -) -Q ( s t , a t ; θ ) ) 2 , (8)

where the term in parenthesis is the temporal difference (TD) error. The ˆ Q ( s t +1 , a ' ; θ -) indicates the use of a target network. It is a copy of the online network Q ( s t , a t ; θ ) , but its parameters θ -are updated less frequently. This technique avoids undesirable feedback loops and correlations between the target and the estimated Q-values, stabilizing the learning process. The loss function in Equation 8 is minimized through gradient descent on the DQN weights, θ ← θ -α ∇ θ L ( θ ) , to refine the approximation of the Q-values. Every C steps, the weights of the target network ˆ Q are updated to match the weights of the online network Q . Gradient computation for quantum circuits on quantum hardware is non-trivial due to quantum state collapse upon measurement; Appendix I describes a method for obtaining gradients on real quantum computers. We refer to Appendix E for more details on the training procedure and give an overview of all hyperparameter settings investigated in Appendix F. Further, Appendix C provides more information on the Atari environments and necessary pre-processing steps. Algorithm 1 in Appendix D shows a pseudo-code implementation of quantum Q-learning with experience replay and a target network. The complete implementation is available in this GitHub repository.

Figure 3: Rewards obtained during training for the hybrid and classical baseline models. Shaded areas indicate the standard deviation of multiple runs. Left: The hybrid agent (blue) and the classical reference (grey) show differences in learning performance in Pong. In this environment, the hybrid agent appears to learn faster and more consistently across multiple runs. Right: Hybrid agent (blue) and classical reference (grey) in Breakout. Here, the hybrid model achieves a strong performance but shows a 41% gap compared to the reference model.

<!-- image -->

## 3 Model evaluation and analysis of design choices

In this section, our objectives are twofold: first, to demonstrate that our hybrid model can successfully learn in the Atari environments Pong and Breakout, and second, to analyze the impact of key design choices on its performance. We establish a baseline for both the quantum and classical models, showing that this baseline is sufficient to solve the Pong environment and achieves non-trivial performance in Breakout. Building on this, we systematically examine the effect of specific design choices - reward scaling and the latent feature space dimension - on the learning performance of both models.

## 3.1 Scores with baseline settings

The scores in this setting (see 'q. baseline' and 'c. baseline' in Table 1 in Appendix F) serve as a baseline to which we compare the performance of agents with different design choices. Figure 3 illustrates the rewards obtained by the agent time-averaged over 10 episodes in the game of Pong (left) and 250 episodes in Breakout (right). As described in Section F, multiple runs were conducted for each environment using different random seeds. The solid lines indicate the average over these runs and the shaded region highlights the standard deviation. The results demonstrate that the hybrid model achieves a strong performance in both environments. Notably, in the game of Pong, the hybrid agent matches the classical reference in terms of final performance, achieving a mean reward of 20. Moreover, the hybrid model requires fewer environment steps - between 400,000 and 600,000 - to solve Pong, compared to around 800,000 steps needed by the classical agent. This result, however, does not indicate quantum advantage; it is likely influenced by statistical variability due to the limited number of runs. For the hybrid model, 4 out of 5 runs lead to successful learning behaviour, consistently achieving scores above 20. In contrast, only 4 out of 11 runs with the classical reference model showed successful learning. Notably, one of the successful classical runs exhibited a significant delay in reaching the optimal policy, which contributed to a lower average performance. The remaining runs were excluded from the analysis because they failed to learn at all, indicated by constant rewards of -21. In Breakout, the hybrid agent achieves a mean reward of approximately 84 after around 2 million episodes, which corresponds to a performance gap of about 41% compared to the classical reference model. Despite this gap, the hybrid model demonstrates robust learning behaviour, closely following the trend of the classical agent. With additional design enhancements (see Figure 6), this gap can be significantly reduced to around 13%. This demonstrates that the hybrid model is capable of achieving competitive performance and effectively solving complex reinforcement learning tasks, with clear potential for further optimization.

Figure 4: Visualizing learned Q-values. We plot the output of the hybrid and classical baseline models as a function of two randomly chosen inputs to the pre-processing layer, in place of the output generated by the convolutional layers in Figure 1. Left: Close-up of the Q-value surface, where the range of input values reflects a typical range as observed during an episode of Breakout. Right: The Q-value surface over an expanded region of the input space.

<!-- image -->

## 3.2 Impact of reward scaling and post-processing layer learning rate

Having established the potential for the hybrid model to learn effective policies in both testing environments, we now turn our attention to a more detailed analysis of how the Q-value estimates of the trained models compare. To do so, we generate Q-value surfaces by plotting the three predicted Q-values as functions of two randomly selected inputs to the pre-processing layer, in place of the output of the convolutional layers (see Appendix H for more details). Based on our observations, we investigate the impact of reward scaling and the learning rate of the post-processing layer on overall learning performance. Figure 4 compares the Q-value surfaces of the hybrid and classical models. The left panels show surfaces for input values within a typical range observed during gameplay, while the right panels expand this range to better assess the surface shapes. The colours represent the Q-values corresponding to the three different actions.

A key observation is the difference in the shape of the Q-value surfaces. The hybrid model produces surfaces that are linear combinations of sinusoidal functions, reflecting the trigonometric nature of PQCs with rotational encoding, as analyzed in [36]. In contrast, the classical model generates approximately piecewise linear surfaces. This difference has significant implications: We suspect that the oscillatory behaviour of the hybrid model's predictions increases the likelihood of assigning high Q-values to suboptimal actions. This can be understood by examining the number of crossings in the Q-value surfaces, which are more frequent in the hybrid model. Trigonometric functions, by nature, have a higher tendency for crossings due to their oscillatory behaviour, and small changes in their parameters can lead to numerous overlaps. This effect is likely even more pronounced in the higher-dimensional feature space produced by the convolutional layers. Our proposed solution to address this phenomenon is to upscale the rewards issued by the environment, thereby promoting greater separation between Q-values associated with optimal and suboptimal actions. This approach aims to provide more room for the oscillatory behaviour while reducing the chances of undesirable overlaps. While further investigation is required to fully understand this phenomenon, our results indicate that upscaling rewards benefits the hybrid model, leading to improved performance.

Closely related to the magnitude of the target Q-values is the learning rate of the post-processing layer following the PQC in the hybrid model, as it controls how quickly the model can adapt to certain target values. As discussed in Section F Table 1, different combinations of reward scaling factors and

Figure 5: Analysis of the impact of reward rescaling in Breakout. Left: The hybrid baseline model (blue) as well as a hybrid model trained with 10x scaled rewards and a final layer learning rate of 2.5e-2 (setting 1c) and a hybrid model trained with 100x reward scaling and a final layer learning rate of 2.5e-1 (setting 1f). Right: The classical reference model (blue) and a classic reference trained with 10x reward scaling and final layer learning rate of 2.5e-2 (setting 1a) and 100x reward scaling and final layer learning rate of 2.5e-1 (setting 1b). While the hybrid model benefits from the modifications, the performance of the classical model deteriorates.

<!-- image -->

learning rates of the pre-processing layer are tested. The best results are presented in Figure 5 (left) and compared to the hybrid baseline in blue. It is evident that the scaled rewards combined with a higher learning rate in the final layer of the hybrid model lead to significantly higher rewards (setting 1c and 1f). For both cases, the average reward clearly surpasses 100 and gets close to a score of 120 . Results from literature [23] support the observation that higher learning rates in the post-processing part are beneficial. We find that the classical model does not benefit from reward scaling or higher learning rates, likely due to its inherently fewer Q-value crossings, which reduce the need for greater separation. Figure 5 (right) demonstrates that applying the settings beneficial for the hybrid model (settings 1a and 1b) to the classical model results in lower scores compared to the classical baseline.

## 3.3 Influence of latent space dimension

A primary constraint on the performance of both the hybrid and classical model is the dimension of the latent space representation following the pre-processing layer. The number of latent features ultimately limits the information accessible by the PQC. In Figure 6, we demonstrate the impact of this latent space dimension on the hybrid model's performance and compare it to the classical reference. In configuration 2a, the PQC consists of 6 qubits (compared to 4 in the baseline model), each encoding 6 features per qubit (up from 4 in the baseline). This increases the latent space dimension from 16 to 36, more than doubling its size. Without additional modifications, this configuration achieves scores exceeding 100, a significant improvement over the baseline hybrid model. Building on the insights from Section 3.2, we next examine configurations 2b and 2c, which combine the expanded latent space with reward scaling and increased learning rates in the post-processing layer. Specifically, configuration 2b incorporates a reward scaling factor of 10 and a learning rate of 2.5e-2, while configuration 2c uses a scaling factor of 100 and a learning rate of 2.5e-1. These adjustments lead to further improvements, with configuration 2c achieving a mean reward of nearly 150 within 2.5 million training steps, marking the highest performance achieved by the hybrid model under the given constraints. For comparison, the blue graph on the right side of Figure 6 depicts the classical reference model with 36 features in the latent space. The improvement margin for this classical model over its baseline is similar to that of the hybrid model. It is important to note that excessively increasing the latent space dimension may negatively impact the hybrid model. Larger latent spaces require more qubits and deeper quantum circuits to encode all the features, increasing quantum circuit complexity. This added complexity can lead to trainability issues, such as the barren plateau phenomenon [39], where gradients become

Figure 6: Analysis of the impact of latent space dimension and comparison of bestperforming hybrid versus classical models in Breakout. Left: Scores achieved by the hybrid model when the latent feature space dimension is increased by using more encoding gates in the PQC and expanding from 4 to 6 qubits in setting 2a. Settings 2b and 2c combine these relaxed constraints with reward scaling and higher learning rates in the post-processing. Right: The classical reference with increased latent space dimension also achieves noticeably higher scores when the constraint imposed by the bottleneck layer is loosened. Nonetheless, the classical-quantum performance gap is reduced to 13% compared to the baseline results in Figure 3.

<!-- image -->

exponentially small in the number of qubits. Exploring this regime numerically is computationally demanding and beyond the resources available for this project.

## 4 Conclusion

In this work, we have studied the performance of hybrid quantum-classical agents in RL environments characterized by a high-dimensional observation space. Specifically, we proposed a hybrid quantumclassical model based on PQCs for Q-function approximation and assessed its learning performance in the Atari game environments Pong and Breakout using a quantum approximate Q-learning framework. Our results demonstrate that hybrid models can learn effectively in these high-dimensional environments. This contrasts earlier work [28] that reported no learning capabilities in Atari environments due to the limited expressivity of the proposed hybrid model. Our research emphasizes the critical role of certain design choices, in particular the dimension of the latent feature space generated by the classical pre-processing layer, which ultimately controls the amount of information encoded in the PQC. Additionally, we identified how the magnitude of Q-values affects the hybrid model's learning performance when the learning rate of the classical post-processing layer is appropriately adjusted. Our findings suggest that with proper tuning, hybrid agents can closely match the performance of classical agents subjected to the same constraints in the latent space, highlighting the importance of fair benchmarking. The results presented contribute to our understanding of the interplay between quantum and classical components in hybrid models. We believe that this work marks a significant step towards the application of hybrid quantum-classical models in real-world RL scenarios. The performance of these models under the influence of noise, as present in today's quantum hardware, poses an interesting open question. Future research could explore the robustness of the proposed architecture in noisy simulators and, ultimately, on real quantum devices. Another interesting research direction would be the exploration of different learning tasks where hybrid models could outperform fully classical models. Indeed, Atari games are known to be classically efficiently solvable, which leaves little room for a quantum advantage. One could instead explore environments where quantum enhancements are expected, as in quantum chemistry [40] or combinatorial optimization [41]. This research direction would require a deeper analysis of the interplay of the classical and quantum components of the hybrid model, as well as finding more task-specific PQC designs.

## Acknowledgments

The computational results presented have been achieved in part using the Vienna Scientific Cluster (VSC). SJ thanks the BMWK (EniQma) for their support.

## References

- [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, 'Playing Atari with Deep Reinforcement Learning,' 2013, \_eprint: 1312.5602. [Online]. Available: https://arxiv.org/abs/1312.5602
- [2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, 'Human-level control through deep reinforcement learning,' Nature , vol. 518, no. 7540, pp. 529-533, Feb. 2015, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/nature14236
- [3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, 'Mastering the game of Go with deep neural networks and tree search,' Nature , vol. 529, no. 7587, pp. 484-489, Jan. 2016, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/nature16961
- [4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, 'Mastering the game of Go without human knowledge,' Nature , vol. 550, no. 7676, pp. 354-359, Oct. 2017, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/nature24270
- [5] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver, 'Grandmaster level in StarCraft II using multi-agent reinforcement learning,' Nature , vol. 575, no. 7782, pp. 350-354, Nov. 2019, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s41586-019-1724-z
- [6] OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. d. O. Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang, 'Dota 2 with Large Scale Deep Reinforcement Learning,' Dec. 2019, arXiv:1912.06680. [Online]. Available: http://arxiv.org/abs/1912.06680
- [7] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information: 10th Anniversary Edition . Cambridge University Press, 2010.
- [8] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. R. McClean, K. Mitarai, X. Yuan, L. Cincio, and P. J. Coles, 'Variational quantum algorithms,' Nature Reviews Physics , vol. 3, no. 9, pp. 625-644, Sep. 2021, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s42254-021-00348-9
- [9] J. Preskill, 'Quantum Computing in the NISQ era and beyond,' Quantum , vol. 2, p. 79, Aug. 2018, publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften. [Online]. Available: https://doi.org/10.22331/q-2018-08-06-79
- [10] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, 'Quantum circuit learning,' Phys. Rev. A , vol. 98, no. 3, p. 032309, Sep. 2018, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevA.98.032309

| [12] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, 'Supervised learning with quantum-enhanced feature spaces,' Nature , vol. 567, no. 7747, pp. 209-212, Mar. 2019, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s41586-019-0980-2                                                                                                                         |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [13] M. Schuld and N. Killoran, 'Quantum Machine Learning in Feature Hilbert Spaces,' Phys. Rev. Lett. , vol. 122, no. 4, p. 040504, Feb. 2019, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevLett.122.040504                                                                                                                                                                                        |
| [14] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, 'Circuit-centric quantum classifiers,' Phys. Rev. A , vol. 101, no. 3, p. 032308, Mar. 2020, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevA.101.032308                                                                                                                                                                                      |
| [15] A. Mari, T. R. Bromley, J. Izaac, M. Schuld, and N. Killoran, 'Transfer learning in hybrid classical-quantum neural networks,' Quantum , vol. 4, p. 340, Oct. 2020, publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften. [Online]. Available: https://doi.org/10.22331/q-2020-10-09-340                                                                                                                        |
| [16] M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy, and R. Melko, 'Quantum Boltzmann Machine,' Phys. Rev. X , vol. 8, no. 2, p. 021050, May 2018, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevX.8.021050                                                                                                                                                                                      |
| [17] S. Lloyd and C. Weedbrook, 'Quantum Generative Adversarial Learning,' Phys. Rev. Lett. , vol. 121, no. 4, p. 040502, Jul. 2018, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevLett.121.040502                                                                                                                                                                                                   |
| [18] S. Chakrabarti, H. Yiming, T. Li, S. Feizi, and X. Wu, 'Quantum Wasserstein Generative Adversarial Networks,' in Advances in Neural Information Processing Systems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox, and R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019. [Online]. Available: https://proceedings.neurips.cc/paper\_files/paper/ 2019/file/f35fd567065af297ae65b621e0a21ae9-Paper.pdf                   |
| [19] C. Zoufal, A. Lucchi, and S. Woerner, 'Quantum Generative Adversarial Networks for learning and loading random distributions,' npj Quantum Information , vol. 5, no. 1, pp. 1-9, Nov. 2019, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s41534-019-0223-2                                                                                                                                               |
| [20] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S. Goan, 'Variational Quantum Circuits for Deep Reinforcement Learning,' IEEE Access , vol. 8, pp. 141 007-141 024, 2020. [Online]. Available: https://doi.org/10.1109/ACCESS.2020.3010470                                                                                                                                                                                            |
| [21] O. Lockwood and M. Si, 'Reinforcement Learning with Quantum Variational Circuit,' Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment , vol. 16, no. 1, pp. 245-251, Oct. 2020. [Online]. Available: https: //ojs.aaai.org/index.php/AIIDE/article/view/7437                                                                                                                                         |
| [22] S. Jerbi, C. Gyurik, S. Marshall, H. Briegel, and V. Dunjko, 'Parametrized Quantum Policies for Reinforcement Learning,' in Advances in Neural Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, Eds., vol. 34. Curran Associates, Inc., 2021, pp. 28 362-28 375. [Online]. Available: https://proceedings.neurips.cc/paper\_files/ paper/2021/file/eec96a7f788e88184c0e713456026f3f-Paper.pdf |

- [24] S. Y.-C. Chen, 'Quantum Deep Recurrent Reinforcement Learning,' in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2023, pp. 1-5. [Online]. Available: https://doi.org/10.1109/ICASSP49357.2023.10096981
- [25] N. Meyer, C. Ufrecht, M. Periyasamy, D. D. Scherer, A. Plinge, and C. Mutschler, 'A survey on quantum reinforcement learning,' arXiv preprint arXiv:2211.03464 , 2022. [Online]. Available: http://arxiv.org/abs/2211.03464
- [26] J.-Y. Hsiao, Y. Du, W.-Y. Chiang, M.-H. Hsieh, and H.-S. Goan, 'Unentangled quantum reinforcement learning agents in the openai gym,' arXiv preprint arXiv:2203.14348 , 2022. [Online]. Available: http://arxiv.org/abs/2203.14348
- [27] N. Meyer, D. Scherer, A. Plinge, C. Mutschler, and M. Hartmann, 'Quantum policy gradient algorithm with optimized action decoding,' in International Conference on Machine Learning . PMLR, 2023, pp. 24592-24613. [Online]. Available: https://proceedings.mlr.press/ v202/meyer23a.html
- [28] O. Lockwood and M. Si, 'Playing Atari with Hybrid Quantum-Classical Reinforcement Learning,' in NeurIPS 2020 Workshop on Pre-registration in Machine Learning , ser. Proceedings of Machine Learning Research, L. Bertinetto, J. F. Henriques, S. Albanie, M. Paganini, and G. Varol, Eds., vol. 148. PMLR, Dec. 2021, pp. 285-301. [Online]. Available: https://proceedings.mlr.press/v148/lockwood21a.html
- [29] H.-Y. Chen, Y.-J. Chang, and C.-R. Chang, 'Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems,' Jul. 2023, arXiv:2304.10159 version: 2. [Online]. Available: http://arxiv.org/abs/2304.10159
- [30] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, H.-S. Goan, and Y.-J. Kao, 'Variational quantum reinforcement learning via evolutionary optimization,' Machine Learning: Science and Technology , vol. 3, no. 1, p. 015025, Feb. 2022, publisher: IOP Publishing. [Online]. Available: https://dx.doi.org/10.1088/2632-2153/ac4559
- [31] S. Y.-C. Chen, 'Efficient Quantum Recurrent Reinforcement Learning Via Quantum Reservoir Computing,' in ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2024, pp. 13 186-13 190. [Online]. Available: https://doi.org/10.1109/ICASSP48485.2024.10446089
- [32] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling, 'Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents,' Journal of Artificial Intelligence Research , vol. 61, pp. 523-562, 2018. [Online]. Available: https://dl.acm.org/doi/10.5555/3241691.3241702
- [33] 'AtariAge - Atari 2600 Manuals (HTML) - Video Olympics (Atari).' [Online]. Available: https://atariage.com/manual\_html\_page.php?SoftwareLabelID=587
- [34] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, 'OpenAI Gym,' Jun. 2016, arXiv:1606.01540. [Online]. Available: http: //arxiv.org/abs/1606.01540
- [35] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and J. I. Latorre, 'Data re-uploading for a universal quantum classifier,' Quantum , vol. 4, p. 226, Feb. 2020, publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften. [Online]. Available: https://doi.org/10.22331/q-2020-02-06-226
- [36] M. Schuld, R. Sweke, and J. J. Meyer, 'Effect of data encoding on the expressive power of variational quantum-machine-learning models,' Phys. Rev. A , vol. 103, no. 3, p. 032430, Mar. 2021, publisher: American Physical Society. [Online]. Available: https: //link.aps.org/doi/10.1103/PhysRevA.103.032430
- [37] Y. Liu, S. Arunachalam, and K. Temme, 'A rigorous and robust quantum speed-up in supervised machine learning,' Nature Physics , vol. 17, no. 9, pp. 1013-1017, Sep. 2021, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s41567-021-01287-z

- [38] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction , 2nd ed. The MIT Press, 2018. [Online]. Available: http://incompleteideas.net/book/the-book-2nd.html
- [39] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven, 'Barren plateaus in quantum neural network training landscapes,' Nature Communications , vol. 9, no. 1, p. 4812, Nov. 2018, publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s41467-018-07090-4
- [40] M. Ostaszewski, L. M. Trenkwalder, W. Masarczyk, E. Scerri, and V. Dunjko, 'Reinforcement learning for optimization of variational quantum circuit architectures,' Advances in Neural Information Processing Systems , vol. 34, pp. 18 182-18 194, 2021. [Online]. Available: https: //proceedings.neurips.cc/paper/2021/hash/9724412729185d53a2e3e7f889d9f057-Abstract.html
- [41] Y. J. Patel, S. Jerbi, T. Bäck, and V. Dunjko, 'Reinforcement learning assisted recursive qaoa,' EPJ Quantum Technology , vol. 11, no. 1, p. 6, 2024. [Online]. Available: https://doi.org/10.1140/epjqt/s40507-023-00214-w
- [42] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V. Isakov, P. Massey, R. Halavati, M. Y. Niu, A. Zlokapa, E. Peters, O. Lockwood, A. Skolik, S. Jerbi, V. Dunjko, M. Leib, M. Streif, D. V. Dollen, H. Chen, S. Cao, R. Wiersema, H.-Y. Huang, J. R. McClean, R. Babbush, S. Boixo, D. Bacon, A. K. Ho, H. Neven, and M. Mohseni, 'TensorFlow Quantum: A Software Framework for Quantum Machine Learning,' Aug. 2021, arXiv:2003.02989 version: 2. [Online]. Available: http://arxiv.org/abs/2003.02989
- [43] S. Guadarrama, A. Korattikara, O. Ramirez, P. Castro, E. Holly, S. Fishman, K. Wang, E. Gonina, N. Wu, E. Kokiopoulou, L. Sbaiz, J. Smith, G. Bartók, J. Berent, C. Harris, V. Vanhoucke, and E. Brevdo, 'TF-Agents: A library for reinforcement learning in tensorflow,' https://github.com/tensorflow/agents, 2018, [Online; accessed 25-June-2019]. [Online]. Available: https://github.com/tensorflow/agents
- [44] A. Mari, T. R. Bromley, and N. Killoran, 'Estimating the gradient and higher-order derivatives on quantum hardware,' Phys. Rev. A , vol. 103, no. 1, p. 012405, Jan. 2021, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevA.103.012405
- [45] A. Abbas, R. King, H.-Y. Huang, W. J. Huggins, R. Movassagh, D. Gilboa, and J. McClean, 'On quantum backpropagation, information reuse, and cheating measurement collapse,' in Advances in Neural Information Processing Systems , A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36. Curran Associates, Inc., 2023, pp. 44 792-44 819. [Online]. Available: https://proceedings.neurips.cc/paper\_files/paper/2023/file/ 8c3caae2f725c8e2a55ecd600563d172-Paper-Conference.pdf
- [46] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac, and N. Killoran, 'Evaluating analytic gradients on quantum hardware,' Phys. Rev. A , vol. 99, no. 3, p. 032331, Mar. 2019, publisher: American Physical Society. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevA.99.032331

## A Detailed architecture of the hybrid model

This section provides a detailed description of the hybrid quantum-classical model architecture complementing the overview in Section 2.3. The hybrid model architecture comprises three main components (see also Figure 1 in the main text): first, classical convolutional layers together with a fully connected layer for dimensionality reduction and feature extraction; second, a PQC for processing the latent features; and third, a fully connected output layer for gathering and re-scaling expectation values.

The configuration of the convolutional layers in the hybrid model is identical to the classical reference model (see Appendix B). Following the convolutional layers is a fully connected pre-processing layer. This layer contains n × l neurons with linear activation, where n is the number of qubits and l is the number of PQC layers. This layer reduces the 3136 values produced by the convolutional layers to just 16 or 36 latent features, depending on the setting (see Table 1). These features are directly encoded in the PQC.

```
| 0 ⟩ R x ( θ 0 ) R y ( θ 1 ) R z ( θ 2 ) · · R x ( x 0 ) R x ( θ 9 ) R y ( θ 10 ) R z ( θ 11 ) | 0 ⟩ R x ( θ 3 ) R y ( θ 4 ) R z ( θ 5 ) Z · R x ( x 1 ) R x ( θ 12 ) R y ( θ 13 ) R z ( θ 14 ) | 0 ⟩ R x ( θ 6 ) R y ( θ 7 ) R z ( θ 8 ) Z Z R x ( x 2 ) R x ( θ 15 ) R y ( θ 16 ) R z ( θ 17 )
```

Figure 7: Quantum circuit diagram of the PQC with 3 qubits and a single layer. A block of variational gates is followed by a set of entangling gates in a circular arrangement. The feature encoding block is highlighted by a dashed line. A second variational block comes after the encoding block. At the end of the circuit, local PauliZ measurements are conducted.

The PQC at the core of the hybrid model follows a layer-wise structure inspired by prior work in quantum reinforcement learning [23]. Each layer comprises:

- 1. A variational block with an R x -, R y - and R z -gate applied to each qubit in this order, each parameterized independently.
- 2. An entanglement block with CZ -gates arranged in a circular topology, connecting qubits 1 → 2 , 2 → 3 , 3 → 4 , ..., n -1 → n , 1 → n ( CZ -gates are symmetric).
- 3. An encoding block where an R x -gate is applied to each qubit, each encoding a distinct latent feature from the pre-processing layer.

This PQC structure is repeated l times, with a final variational block added at the end. The latent features are encoded layer-wise, where the first n features are encoded in the first encoding block, the second n features in the second encoding block and so forth. Consequently, the number of encoding gates in the circuit is n × l . The number of variational parameters in the PQC is (3 × n ) × ( l +1) . In contrast, the non-linear processing layer in the classical reference model contains ( n × l × 512) + 512 trainable weights. In our baseline hybrid model (see setting "q. baseline" in Table 1 in Appendix F) with a 4-qubit PQC and 4 layers, the PQC has only 60 trainable parameters, compared to 8704 parameters in the corresponding non-linear layer of the classical reference model. Figure 7 depicts the PQC structure with n = 3 and l = 1 . We initialize all variational parameters θ i in the PQC according to the following distribution: θ init i ∼ N (0 , σ 2 ) where σ = 0 . 01 · π .

The output of the PQC - expectation values from local PauliZ measurements - is passed to a classical post-processing layer. This layer consists of a fully connected linear layer where the number of neurons corresponds to the action space of the environment. This layer rescales the PQC's output to produce the final Q-values.

<!-- image -->

Figure 8: Comparison between the original DQN architecture proposed in [2] (left) and our adapted version with a bottleneck layer (right) . The original model consists of three convolutional layers outputting 3136 features, a fully connected (FC) ReLU layer composed of 512 neurons, and a final output layer with linear activation. The adapted model incorporates a bottleneck layer (with 16 or 36 neurons) with linear activation after the convolutional layers, limiting the latent space dimension and mirroring the constraints of the hybrid quantum-classical model described.

<!-- image -->

## B Classical reference model

Here we introduce the classical reference model used as a fair baseline for evaluating the performance of the hybrid quantum-classical model. The architecture is based on the deep Q-network (DQN) from [2], which we adopt without modification for the convolutional and fully connected layers. Specifically, the convolutional layers comprise three layers with the following parameters: The first layer consists of 32 filters of size 8 × 8 and a stride of 4, the second layer convolves 64 filters of size 4 × 4 and a stride of 2 and the third layer uses 64 filters of size 3 × 3 and a stride of 1. All convolutional layers use the non-linear rectifier activation function (ReLU). The convolutional layers are followed by a fully connected layer with 512 neurons and ReLU activation, and a final output layer with linear activation.

In this work, this original architecture has been modified to allow a more direct comparison with the hybrid quantum-classical model. One of the key limitations of the hybrid model is its restricted latent space dimension, owing to the limited number of features that can be encoded in the PQC - as discussed in Section 2.3. To account for this, a bottleneck layer is introduced in the classical model between the convolutional layers and the fully connected ReLU layer. This bottleneck layer is implemented as a layer with a reduced number of neurons and linear activation, matching the dimensionality of the pre-processing layer in the hybrid model. The linear activation function is chosen to minimize the introduction of non-linearity, preserving the behaviour of the original architecture while introducing constraints in the latent space dimension. Figure 8 compares the architecture proposed in [2] (left) to our adapted version with the bottleneck layer (right).

To assess the impact of the bottleneck layer, we compare the performance of the original model from [2] with our modified reference model. Figure 9 presents the rewards achieved by the classical model without the bottleneck constraint in both Pong and Breakout. The results demonstrate that the unconstrained model converges faster and more consistently in Pong and achieves significantly higher scores in Breakout. These findings highlight the performance limitations imposed by the bottleneck layer, which are also present in the hybrid model due to its architectural constraints.

## C Atari 2600 environments

The Atari 2600 game environments of Pong and Breakout, accessible via OpenAI Gym [34], were selected as testbeds for evaluating the hybrid quantum-classical framework. These environments are

Figure 9: Classical model without bottleneck layer. The left figure shows the rewards obtained in the game of Pong and the right figure shows the rewards obtained in Breakout. Without the bottleneck, the classical model converges faster and more consistently to the optimum in Pong and achieves significantly higher scores in Breakout.

<!-- image -->

10

10

well-established benchmarks in the field of reinforcement learning (RL) and have been extensively studied in classical settings. Both games meet the requirements for investigating the central research questions of this study, particularly due to their high-dimensional observation spaces represented by RGB images. Although Pong and Breakout share common elements in terms of their control structure, there are notable differences in their game dynamics. Breakout is generally considered a more challenging environment for RL agents to learn [2], making it a suitable platform for testing advanced configurations of the hybrid model.

Pong simulates a two-player tennis-like game where the objective is to outscore a computer-controlled opponent. The player controls a paddle, which can be moved vertically to hit a ball back and forth, while the opponent aims to do the same. Points are awarded when the opponent either misses the ball or hits it out of bounds. The agent receives a reward of 1.0 for each point scored, with no penalties for losing a point. The game concludes when either player reaches a score of 21. The action space of Pong is limited to three relevant actions: "NOOP" (no operation), "RIGHT" (move up), and "LEFT" (move down), while actions such as "FIRE," "RIGHTFIRE," and "LEFTFIRE" are excluded since the ball is automatically launched at the beginning of each episode. This simplified action space allows for more focused training on the core mechanics of the game.

Breakout shares similar paddle-and-ball dynamics with Pong but introduces additional complexity by requiring the agent to destroy a wall of bricks. The player must control a paddle to bounce the ball upwards, with the goal of breaking bricks located at the top of the screen. Each brick destroyed yields a reward, with the value of the reward depending on the row of the brick: lower rows yield fewer points, while higher rows provide higher rewards. The maximum achievable score in Breakout is 432 points, reflecting the total points available across six rows of bricks. Similar to Pong, the action space in Breakout is constrained to "NOOP," "RIGHT," and "LEFT." An automatic firing mechanism is implemented to circumvent the need for the agent to learn to start the game manually. This reduces the complexity of the initial training phase by removing the "FIRE" action, which is only necessary to begin the game.

## C.1 Pre-processing of the raw environment state

Before training the agent, several pre-processing steps are applied to the raw state representations of the game environments. These pre-processing steps, following the guidelines in [32], are crucial for standardizing the evaluation of RL algorithms on Atari 2600 games and are independent of the preprocessing conducted by the hybrid model. The raw observation space for both environments consists

## frame skipping

Figure 10: The pre-processing pipeline illustrated on a sequence of game frames. A rectangle represents a frame as produced by the game emulator. The frames with a cross are skipped and not shown to the agent. The last two frames of each sequence of four successive frames are combined using a frame pooling operation. Finally, the pooled frames are stacked to form a single new observation.

<!-- image -->

<!-- image -->

Figure 11: Breakout observation space before (left) and after (right) pre-processing applied. The four colours in the right image are for visualization purposes; each colour actually represents one of four distinct frames. The observation dimensions change from 210 × 160 × 3 for the raw RGB image to 84 × 84 × 4 for the cropped, grey-scaled and stacked observations.

<!-- image -->

of RGB images with a resolution of 210 × 160 × 3 , representing 210 pixels in height, 160 pixels in width, and three colour channels (red, green, and blue). The first step in the pre-processing pipeline involves converting the images to grayscale, reducing the number of colour channels from three to one. The images are then downsampled to a resolution of 84 × 84 pixels. To reduce the complexity of the RL problem, a technique known as frame skipping is applied, where only every fourth frame of the game is used for decision-making. This means that the agent selects an action every four frames, with rewards accumulated over the skipped frames. Each of these decision points corresponds to a step in the environment. Additionally, a frame pooling operation is applied to the last two frames of each four-frame sequence, combining them into a single frame. To introduce temporal information into the observation, four consecutive frames are stacked along the channel dimension, a process referred to as frame stacking . Figure 10 illustrates this pre-processing pipeline and Figure 11 shows an instance of an environment observation in Breakout before and after applying the pre-processing. The final observation for the agent is thus represented as an 84 × 84 × 4 tensor, where the four channels correspond to four consecutive frames rather than colour information. It is important to note that Atari 2600 environments are deterministic, meaning that each episode would evolve identically if restarted from the same seed. To mitigate the risk of the agent learning a fixed sequence of actions (open-loop control), stochasticity is introduced through ϵ -greedy policies and random sampling from experience replay buffers. However, the strategy of applying random no-op actions at the start of the game, as suggested by [2], is not used in this work, as it would lower the scores obtained by both hybrid and classical models, which is undesirable for the comparative analysis conducted in this study.

## D Quantum Q-learning algorithm

In Section 2.4, we introduced the quantum Q-learning algorithm based on the deep Q-learning algorithm. Algorithm 1 presents a pseudo-code implementation following the notation established above.

## Algorithm 1: Quantum Q-learning with experience replay and target network

```
1 Initialize replay memory D to capacity N 2 Initialize action-value function Q with random weights θ 3 Initialize target action-value function ˆ Q with weights θ -= θ 4 for episode = 1 to M do 5 Initialize sequence s 1 6 for t = 1 to T do 7 With probability ε select random action a t otherwise select a t = argmax a Q ( s t , a ; θ ) 8 Execute action a t in emulator, observe reward r t and next state s t +1 9 Store transition ( s t , a t , r t , s t +1 ) in D 10 Sample random minibatch of size | B | of transitions ( s j , a j , r j , s j +1 ) from D where j is from the index set B 11 for j ∈ B do 12 Set y j = { r j , for terminal s j +1 r j + γ max a ' ˆ Q ( s j +1 , a ' ; θ -) , for non-terminal s j +1 13 end 14 Compute the loss for the minibatch: L ( θ ) = 1 | B | ∑ j ∈ B ( y j -Q ( s j , a j ; θ )) 2 15 Perform a gradient descent step: θ ← θ -α ∇ θ L ( θ ) 16 Every C steps, reset θ -= θ 17 end 18 end
```

## E Training procedure and evaluation metrics

We evaluate the hybrid quantum-classical model within a reinforcement learning framework using quantum Q-learning with a target network and experience replay, following the approach of Mnih et al. [2]. Training starts with a random policy for 20,000 environment steps to populate the replay buffer. After accumulating 100,000 ( s t , a t , r t , s t +1 ) transitions, the replay buffer operates on a firstin, first-out (FIFO) basis with uniform sampling. After this initial warm-up phase, the predictions of the hybrid agent are used to derive a policy. The online model is trained every 4 steps, and the target network is updated every 8,000 steps. In the baseline setting (see 'q. baseline' in Table 1), we minimize the loss function (Equation 8) using the Adam optimizer with a learning rate of 2 . 5 × 10 -4 for both classical and quantum components. An ϵ -greedy policy is employed with ϵ linearly decaying from 1 to 0.01 over 250,000 steps. The discount factor γ is set to 0.99. Training spans 1 million steps in Pong and 2.5 million steps in Breakout. The main evaluation metric employed for assessing the agents in this study is the total undiscounted reward obtained in each episode. These rewards exhibit significant variance across different game episodes, due to the ϵ -greedy policy and random sampling from the replay memory. For evaluation purposes, the rewards are temporally averaged, where we take 10 episodes in Pong and 250 episodes in Breakout. Following the recommendations in [32], no evaluation runs where the agent fully exploits its learned policy (without any parameter updates) are conducted.

## F Numerical experiments and hyperparameter settings

This section provides a brief overview of the hyperparameters we investigate and the software frameworks used. Table 1 outlines the hyperparameter values tested and indicates the number of runs

conducted for each setting to ensure statistically reliable results and mitigate the effects of randomness. The learning rate of the post-processing layer is identified as critical for Q-learning, referring to results presented in [23], with settings 1a to 1f in Table 1 exploring various learning rates. Alongside adjusting the learning rate of the post-processing layer, we upscale rewards to mitigate the impact of the hybrid model's oscillatory Q-value behaviour and reduce undesirable overlaps (see Section 3.2). Additionally, as the number of latent features, determined by the number of neurons in the classical pre-processing layer, ultimately limits the information accessible to the PQC, we compare the baseline to settings with higher latent space dimensions (2a to 2c). The values of all hyperparameters are tested through an informal search conducted in Breakout. We refrain from a systematic grid search due to significant computational requirements. The settings in which the hybrid model performed best were also tested with the classical model for reference. All numerical experiments are conducted using the TensorFlow Quantum [42] module for noise-free quantum circuit simulations and TensorFlow Agents [43] for RL-related algorithms. The game environments, Pong and Breakout, are provided by OpenAI Gym [34].

Table 1: A summary of the numerical experiment settings. The first column provides labels to identify each hyperparameter setting. Here, 'quantum' or 'q.' refers to the hybrid model and 'classical' or 'c.' to the classical reference. 'Learning rate' is the learning rate of the classical post-processing layer. 'Reward scaling' indicates by which factor rewards are up-scaled and 'Latent features' resembles the latent space dimension in terms of the number of features. For the classical reference, only those 'Reward scaling' settings are chosen, that performed best for the hybrid model.

| Hyperparameter settings   | Hyperparameter settings   | Hyperparameter settings   | Hyperparameter settings   | Hyperparameter settings   |
|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| Setting                   | Learning rate             | Reward scaling            | Latent features           | Number of runs            |
| q. baseline               | 2.5e-4                    | -                         | 16                        | 5 (4 in Pong)             |
| quantum 1a                | 2.5e-3                    | -                         | 16                        | 3                         |
| quantum 1b                | 2.5e-3                    | 10x                       | 16                        | 3                         |
| quantum 1c                | 2.5e-2                    | 10x                       | 16                        | 3                         |
| quantum 1d                | 2.5e-2                    | 100x                      | 16                        | 3                         |
| quantum 1e                | 2.5e-1                    | 10x                       | 16                        | 3                         |
| quantum 1f                | 2.5e-1                    | 100x                      | 16                        | 3                         |
| quantum 2a                | 2.5e-4                    | -                         | 36                        | 5                         |
| quantum 2b                | 2.5e-2                    | 10x                       | 36                        | 5                         |
| quantum 2c                | 2.5e-1                    | 100x                      | 36                        | 5                         |
| c. baseline               | 2.5e-4                    | -                         | 16                        | 5 (4 in Pong)             |
| classical 1a              | 2.5e-2                    | 10x                       | 16                        | 5                         |
| classical 1b              | 2.5e-1                    | 100x                      | 16                        | 5                         |
| classical 2               | 2.5e-4                    | -                         | 36                        | 5                         |

## G Effect of rescaling latent features

A key design choice in the hybrid model architecture is the use of a linear activation function in the pre-processing layer preceding the PQC. This decision aims to preserve the original latent features and avoid introducing any transformations that might overshadow the learning process within the quantum circuit. However, this approach does not impose any constraint on the magnitude of the latent features produced by the pre-processing layer. Since the PQC encodes features as rotation angles, it is essential to ensure that the values of the latent features do not exceed a range of 2 π , as otherwise two vastly different features might be mapped to the same value due to the periodicity of the encoding scheme. One common strategy to enforce this constraint is to apply a scaling activation function, such as tanh · π , which restricts the output of the pre-processing layer to values within the desired range. Figure 12 (left) compares the performance of the baseline hybrid model introduced in Section 2.3 with the linear activation function to a hybrid model employing the tanh · π activation function. The results demonstrate that the baseline model achieves better performance, suggesting that rescaling latent features is not necessary. A possible explanation for the poor performance of the scaling activation function lies in the distortion introduced by the tanh · π function. As shown in Figure 12 (right), the

<!-- image -->

Feature value

Figure 12: Performance of the hybrid model depending on the pre-processing layer activation function. Left: Scores achieved by the baseline hybrid model (baseline) and a hybrid model with tanh · π activation function in the pre-processing layer ( tanh · π ) in Breakout. The tanh · π activation in the pre-processing layer leads to worse performance. Right: Comparison of the linear (identity) function (blue) and a tanh function scaled by a factor of π (red). Applying tanh · π to feature values in the shaded area ± (0 . 5 π, π ) , maps them to a very narrow region in the image of tanh · π , possibly resulting in information loss.Figure 13: Distributions of individual latent feature values during training. Left: The feature values of the hybrid model remain within the range of ± π . Right: The feature values in the classical reference model exceed this range, indicating that the classical pre-processing layers in the hybrid model adapt to the rotational encoding constraints of the PQC.

<!-- image -->

Figure 14: A truncated hybrid model without convolutional layers . This model is used to plot predicted Q-values as a function of two randomly chosen inputs to the pre-processing layer, in place of the output of the convolutional layers. All except for two inputs (blue arrows) to the truncated model are kept constant. The Q-value predictions are plotted as a function of these two inputs resulting in a Q-value surface plot.

<!-- image -->

feature values near ± π are mapped to a narrow range, potentially diminishing the influence of large latent features (in absolute terms) and thus reducing the amount of meaningful information encoded in the PQC. Further analysis of the latent feature distributions, as shown in Figure 13 (left), suggests that a linear activation function in the pre-processing layer is sufficient for maintaining the necessary range of values. The distribution of feature values, sampled every 10,000 environment steps during training, indicates that the majority of the values remain well within the range of ± 2 π , with most falling within ± π . This observation implies that the classical layers preceding the PQC, particularly the pre-processing layer, naturally learn to regulate the magnitudes of latent features during training, without requiring an explicit scaling activation function. In comparison, Figure 13 (right) shows the distribution of feature values in the classical reference model, which does not have the same rotational encoding constraints. As expected, the magnitudes of the latent features in the classical model exceed the range observed in the hybrid model. This further supports the hypothesis that the classical preprocessing components of the hybrid model adapt to respect the constraints imposed by the quantum encoding, without the need for additional feature rescaling.

## H Plotting Q-value surfaces

Here we explain how the Q-value surface plots presented in Section 3.2 are generated for both the hybrid and classical model. We analyze the predicted Q-values as a function of inputs to the preprocessing layer. The output vectors of the convolutional layers (after flattening) are logged for each step of a single episode of Breakout, resulting in a collection of 3136-dimensional vectors. From these, we compute an average output vector, providing a representative baseline for the convolutional network's outputs during the episode.

Next, a copy of the model is created, excluding the convolutional layers. Since the full 3136-dimensional input space is too large to visualize directly, we select two features (input nodes of the truncated model) at random to reduce the dimensionality to a more interpretable two-dimensional space (see Figure 14). This allows us to visualize the predicted Q-values as surface plots, providing an intuitive means to analyze how variations in selected features affect the model's output.

For the two selected features, multiple values are fed into the pre-processing layer, varying over a range observed during the episode. All other input features are fixed to their respective average values computed earlier. This approach generates three Q-value surfaces (corresponding to the three actions in Breakout) as functions of the two selected features.

## I Estimating gradients of quantum computations

As discussed in Section 2.4, training quantum machine learning (QML) models involves minimizing a cost or loss function L ( θ ) , which measures the discrepancy between computed expectation values f θ ( x ) and target values. Like in classical machine learning, this optimization task can be tackled with gradient-based methods like gradient descent and its variations in a hybrid quantum-classical manner. Once the gradient of the cost function with respect to the variational gate parameters ∇ θ L ( θ ) is known, the classical computer can update the parameters of the PQC in the direction of smaller cost by performing a gradient descent step. In classical machine learning, gradients are obtained via backpropagation , a form of automatic differentiation (autodiff), which uses intermediate values to efficiently calculate the gradient of a function with a runtime similar to the execution of the function itself. This dependency on intermediate information makes it difficult to apply backpropagation, or autodiff in general, to quantum computations, as the collapse of the quantum state upon measurement prevents the straight-forward reuse of information [44,45]. This fundamental difference requires alternative approaches for gradient computation in quantum machine learning, such as the parameter-shift rule [10, 44, 46].

̸

The parameter-shift rule exploits the trigonometric nature of PQCs as functions of their parameters. As a function of a single variational parameter θ i with all other parameters θ j ∈ θ , j = i , and the inputs x fixed, the quantum model as defined in Equation 4 is of the form

f θ i = a · sin( θ i + b ) + c, (9)

where the constants a , b and c are determined by the fixed part of the circuit. Functions of this type adhere to the parameter-shift rule, which allows the exact computation of the derivative with respect to the parameter θ i using two evaluations of the function with a shift in the parameter. Specifically, for a given parameter θ i in a quantum circuit, the gradient of the expectation value f θ i := ⟨M⟩ θ i can be evaluated as:

∂f θ i ∂θ i = f θ i + π/ 2 -f θ i -π/ 2 2 = a · cos( θ i + b ) . (10)

This applies to an expectation value as in Equation 4 - the deterministic quantum machine learning model f θ . It is important to note that Equation 10 is not a finite difference approximation but gives the exact value of the gradient - if the expectation values ⟨M⟩ θ were known exactly. In that sense, an estimation of the exact gradient can be computed via Equation 10 using 2 s circuit evaluations per parameter θ i , where s is the number of shots to estimate an expectation value. When simulating quantum computations on classical computers, the situation is however different: The gradients of simulated quantum computations can be evaluated without the need for the parameter-shift rule. Since the quantum state of the system is completely known during all times of the computation, classical automatic differentiation can be applied directly.