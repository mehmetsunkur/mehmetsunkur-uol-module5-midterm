IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
# Neural Rendering for Game Character Auto-Creation
Tianyang Shi , Zhengxia Zou , Zhenwei Shi , Member, IEEE, and Yi Yuan
Abstract—Many role-playing games feature character creation systems where players are allowed to edit the facial appearance of their in-game characters. This paper proposes a novel method to automatically create game characters based on a single face photo. We frame this “artistic creation” process under a self-supervised learning paradigm by leveraging the differentiable neural rendering. Considering the rendering process of a typical game engine is not differentiable, an “imitator” network is introduced to imitate the behavior of the engine so that the in-game characters can be smoothly optimized by gradient descent in an end-to-end fashion. Different from previous monocular 3D face reconstruction which focuses on generating 3D mesh-grid and ignores user interaction, our method produces ﬁne-grained facial parameters with a clear physical signiﬁcance where users can optionally ﬁne-tune their auto-created characters by manually adjusting those parameters. Experiments on multiple large-scale face datasets show that our method can generate highly robust and vivid game characters. Our method has been applied to two games and has now provided over 10 million times of online services.
Index Terms—Game character customization, role-playing games, neural rendering, deep learning
# Ç
# 1 INTRODUCTION
THE character customization system in many Role-Playing Games (RPGs) provides an interactive interface for play- ers to edit the facial appearance of the in-game characters with their preferences instead of using default templates. To improve the player’s immersion and interactivity, char- acter customization systems are becoming sophisticated - in many modern RPGs such as “Grand Theft Auto Online” (https://www.rockstargames.com/GTAOnline) and “Dark Souls III” (https://www.darksouls.jp), players are now allowed to precisely manipulate their characters on detailed parts, e.g., corner of the eyes, hairstyles, and even makeups. As a result, the character customization process turns out to be laborious and time-consuming for most players. To cre- ate an in-game character with a desired facial appearance (e.g., a pop star or the players themselves), most players need to spend hours manually adjusting hundreds of parameters, even after considerable practice.
In computer vision, efforts have been made in generating 3D faces based on a single input face photo, in which a rep- resentative group of methods are the 3D Morphable Model (3DMM) [1] and its variants [2], [3], [4]. However, these methods are difﬁcult to be applied to in-game environ- ments due to the style gap and the different infrastructure
between the two environments. In this paper, we propose a
novel method to automatically create in-game characters for players according to a single input face photo,1 as shown in Fig. 1. We frame this “artistic creation” process under a self-supervised learning paradigm by leveraging the differentiable neural rendering. Different from the 3DMM based methods which focus on generating 3D mesh-grid and ignore user interaction, our method produ- ces ﬁne-grained facial parameters with a clear physical sig- niﬁcance where users can optionally ﬁne-tune their auto- created characters by manually adjusting those parameters according to their needs. We refer to our methods as a “Face-to-Parameter” translation method. In our method, each facial parameter controls the attribute (e.g., the posi- tion, orientation, and scale) of an individual facial compo- nent. As the rendering process of a typical game engine is not differentiable, a generative network G is designed as an “imitator” to imitate the physical behavior of the game engine so that our model can be learned by gradient descent in an end-to-end fashion. By taking advantage of the differ- entiable rendering in our method, the character auto-crea- tion can be naturally formulated as a cross-domain facial similarity measurement problem between the face of a gen- erated character and a real one. The training of our frame- work neither requires any ground truth references nor any user interactions.
- e Tianyang Shi and Yi Yuan are with the Fuxi AI Lab, NetEase, Hangzhou, Zhejiang 310052, China. E-mail: {shitianyang, yuanyi}@corp.netease.com.
- e Zhengxia Zou is with the Department of Computational Medicine and Bio- informatics, University of Michigan, Ann Arbor, MI 48109 USA. E-mail: zzhengxi@umich.edu.
- e Zhenwei Shi is with the Image Processing Center, School of Astronautics, Bei- hang University, Beijing 100191, China. E-mail: shizhenwei@buaa.edu.cn.
Manuscript received 6 May 2020; revised 14 Aug. 2020; accepted 6 Sept. 2020. Date of publication 15 Sept. 2020; date of current version 3 Feb. 2022. (Corresponding author: Yi Yuan.)
Fig. 2a shows an overview of the proposed method. Our method consists of multiple components:
- – An imitator G. We design an imitator G to imitate the behavior of a game engine and make the rendering process differentiable, as shown in Fig. 2b. The G takes in a group of facial customization parameters
Recommended for acceptance by Y. A Sheikh.
Digital Object Identiﬁer no. 10.1109/TPAMI.2020.3024009
1. Preliminary versions of this work were published in ICCV 2019 [5] and AAAI 2020 [6].
0162-8828 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1489
1490
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
ZU» Eyer Left-Right ——©— 57 Up-down —— — © Front-Back ———*— 50 Angle —©—— 8 High-Low — = a Length —*— ‘Width ————@®—— 2 Thickness ——*— 51 Character customization panel Ly \ N Input vara Facial parameters Input photo Manual, N adjustment | N optional) zi Load parameters
Bone-driven 3D model
In-game rendered character
Fig. 1. We propose a novel method for game character auto-creation under a self-supervised learning framework by leveraging differential neural rendering. The proposed method converts a single input face photo to a large set of physically meaningful facial parameters. Users can further ﬁne-tune the parameters optionally according to their needs.
and is trained to produce the face images of the cor- responding in-game character. Fig. 2c shows two examples of the “rendering” output of our imitator and their in-game “ground truth”.
- – A translator T . We introduce a facial parameter trans- lator T , which aims to transform an input facial image to a set of in-game facial parameters. The gen- erated parameters can be either used for rendering 3D characters in the game environments or further manually ﬁne-tuned by players.
- – A facial descriptor F . We take advantage of the deep Convolutional Neural Networks (CNNs) and intro- duce a facial descriptor F which learns high-level facial representations in terms of both global facial identity and local details. The descriptor consists of two networks, a face recognition network Frecg, and a face segmentation network Fseg, where the former encodes an input face image to a set of pose-irrele- vant face embeddings and the latter learns position- sensitive face representations.
We formulate the training of our method as a multi-task regression process with multiple self-supervised loss func- tions. To measure the similarity between the input face and the generated one, we deﬁne two loss functions, an “identity loss” Lidt and a “facial content loss” Lctt, where the former one focuses on facial identity (pose-irrelevant) and the later one compute facial similarity base on pixel- wise representations. To improve the robustness and stabil- ity of the generation, we further introduce a “loopback loss” Lloop, which ensures the translator correctly interprets its own output [3]. By minimizing the distance between the created face and the real one, the input face photos can be effectively converted to vivid in-game characters. On basis of the above framework, we propose two generation modes for facial parameters, a “one-shot” generation mode and an “iterative” generation mode, where the former one gener- ates the facial parameters directly from the input image through the translator T in a single forward propagation while in the latter one we discard the translator and frame the generation as a parameter searching process at the input
Predicted facial parameters £ Facial Parameter Translator T Input: aligned face Outputs: In game character Imitation learning 3 Game Engine Facial parameters x (b) Self-supervised Facial Similarity Measurement y 1 — Vd Descriptor F L 5 (Maximizing the facial similarity Input: between the two inputs) aligned face ‘wong <2 a ı 1 | he t I se), by _— Gi 1 i Facial Generated Rendered by ” parameters X face image (o game engine ()
Fig. 2. An overview of our method. (a) Our model consists of multiple components: an imitator G, a facial parameter translator T , and a facial descrip- tor F . The G, as shown in (b), aims to imitate the behavior of a game engine by taking in a set of user-customized facial parameters x and producing a “rendered” facial image y. The T is trained to convert an input face photo to facial parameters which maximize the facial similarity between the input and the “rendering” result in the feature space produced by the F . In (c), we show two groups of faces generated by our imitator and their in-game ground truth.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
end of the renderer. We show in different aspects of their advantages in our experiment, such as high-quality facial generation, robustness, and speed. Our method has been applied to two role-playing games, a PC game “Justice” (in October 2018, https://n.163.com) and a mobile game “Heaven” (coming soon, https://tym.163.com/) and now has provided over 10 million times of online services.
Our contributions are summarized as follows:
- e We propose a novel method for game character auto- creation. To our best knowledge, we are the first to launch this feature in the gaming industry.
# TABLE 1
# A Comparison Between “Statistical Shape Basis (SSB)” and “Manually Deﬁned Shape Basis (MDSB)” on 3D Face Representation
SSB MDSB Semantics Flexibility ambiguous normal explicit high Texture style real real or game-style Ground truth Makeup 3D scans a few none many (in-game) Face model morphable face model bone-driven face model
# e
- Since the rendering process of mainstream game engines is not differentiable, we introduce an imitator by building a deep generative network to imitate the behavior of the engine. In this way, the gradient of the facial similarity can be smoothly back-propagated all-though the generating pipeline and the model can be optimized in an end-to-end fashion.
gradient back-propagation and thus makes the renderer non-differentiable. Differentiable Rendering (DR), which allows calculation of the derivative from the rendering out- put to the input 3D model, camera parameters, and even environment variables (e.g., light conditions), has drawn increasing attention in recent years. The key to the DR is to approximate the gradient of the rendering process by using a set of differentiable operators or structural units. The ﬁrst differentiable renderer, OpenDR [23], was proposed by M. Loper et al. in 2014. In their method, they use the ﬁrst-order Taylor expansion, i.e., a series of predeﬁned spatial ﬁlters to approximate the gradient of the rasterizer. Later, some other approaches based on interpolation approximation [24], [25], triangle barycentric interpolation [3], and Monte Carlo [26] were proposed to improve the ﬁdelity rendering result as well as the accuracy of the gradients. Since deep neural net- works provide naturally differentiable topology, a new research topic called “neural rendering” quickly emerged and the neural networks were introduced to the rendering tasks [27], [28], [29].
- e We introduce multiple loss functions under a self- supervised learning paradigm which proves to be effective for cross-domain facial similarity measure- ment. The loss functions can be jointly optimized by multi-task learning.
# 2 RELATED WORK
# 2.1 Monocular 3D Face Reconstruction
Recovering 3D information from a single 2D face image has long been a challenging but important task in computer vision. On one hand, it forms the foundation of a large group of real-world applications, such as game production, medical plastic surgery, facial augmented reality, and vir- tual reality, etc. On the other hand, it is a typical ill-posed problem where the difﬁculty lies not only in the missing of the stereoscopic information but also in a highly variable imaging environment such as illumination changes, occlu- sion, blurring, etc.
As the 3D face reconstruction can be essentially consid- ered as a parameter ﬁtting problem between pre-scanned 3D faces and input facial image, making renderer differen- tiable recently became the key to solve this problem. Thanks to the development of the differentiable rendering and self- supervised learning, neural networks are now able to achieve high-ﬁdelity rendering results even without using pre-scanned 3D faces [3], [19]. Despite the recent advances in this ﬁeld, most of the 3D face reconstruction methods are not naturally applicable to RPGs. The main reason behind this is that the face models in these methods are typically designed based on statistical (PCA) shape basis but the sta- tistical shape basis is not friendly for user interactions. As a comparison, the 3D faces in most RPGs are usually repre- sented by using manually deﬁned shape basis (e.g., bone- driven face models) in which the parameters have clear semantics. Table 1 shows a comparison between the two groups of representation methods.
A representative of early monocular 3D face reconstruc- tion is the 3D Morphable Model (3DMM), which was origi- nally proposed by Blanz et al. in 1999 [1]. In 3DMM and its recent variants [1], [7], [8], [9], [10], [11], [12], [13], a 3D mesh of a morphable face model is ﬁrst parameterized and then optimized to ﬁt the projection of the model to the 2D input face. In recent years, deep CNNs [14], [15] were introduced to the monocular 3D face reconstruction by tak- ing advantage of the high-level image representations [2], [16], [17], [18], [19], [20], [21]. These methods typically for- mulate the reconstruction as a standard regression problem between the 2D input and the morphable model. The regres- sion based paradigm allows the integration of auxiliary con- straints on their objectives such as the adversarial loss [22] and the loopback loss to achieve high-ﬁdelity reconstruction results [3], [4].
Beyond the above morphable face model and bone- driven face model, Gruber et al. recently proposed a new anatomical local face model for interactive sculpting [30]. In their method, the authors integrate anatomical knowledge into the 3D scanned face models to obtain a high degree of freedom for face editing. Note that since our proposed neu- ral renderer – the imitator, is not limited to the choice of the face model, our method can also be well applied on either of the morphable face model or the anatomical local face model. However, considering that the bone-driven face model is more frequently used in Massively Multi Player
# 2.2 Differentiable Rendering
Graphic rendering is a fundamental problem in computer graphics that converts 3D models into 2D images. Tradi- tional rendering pipelines used in 3D graphics consider the forward process only. These methods typically involve a discrete operation called rasterization, which prevents
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1491
1492
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
On-line Role-Playing Games (MMO-RPGs) than other types of face models, we only focus on the bone-driven face mod- els in this paper.
# 3 METHODOLOGY
Here we introduce each part of our method in details, includ- ing our imitator G, facial parameter translator T , our facial similarity measurement, and other implementation details.
R°! represents a c dimensional internal representation of our translator. We use a gating function to learn a group of attention weights a = o(W>6(W,v), where o(-) and 4(-) denote the sigmoid and ReLU activation function, respec- tively. W, and W> are the weights of two FC layers. The internal representation v is element-wisely re-scaled by the attention weights 0; = aj,v%,k =1,...¢.
# 3.3 Facial Similarity Measurement
# 3.1 Imitator
We train a convolutional neural network with eight trans- posed convolution layers as our imitator G to learn the behavior of a game engine so as to make the character cus- tomization system differentiable. We take the similar net- work conﬁguration of the DCGAN [31] for our imitator. Fig. 4 shows an illustration of our imitator.
Once we have a well-trained imitator G, the generation of the facial parameters ﬁnally becomes a face similarity mea- surement problem. The measurement is conducted under a self-supervised learning framework, i.e., to enforce the facial representation of the rendered image I0 to be similar to that of its input face photo I:
We frame the “rendering” as a standard pixel-wise regression problem, where we aim to minimize the differ- ence between the in-game rendered image and the gener- ated one in their raw pixel space. We train our imitator to minimize the following objective function:
Lala) = Eynal Gla) — Engine(x)||, }, (1)
I’ =G(T(D) KI (3)
As the input face photo and the rendered game character belong to different image domains, to effectively measure the facial similarity, we design three types of loss functions as measurements - a facial identity loss Lidt, a facial content loss Lctt, and a loopback loss Lloop, as shown in Fig. 3a. The ﬁnal loss function in our model can be written as the sum- mary of the above three losses:
where x is the facial parameters sampled from a uniform distribution. Given a group of input parameters x, GðxÞ and EngineðxÞ represent the outputs of our imitator and the game engine (ground truth), respectively. We use the pixel- wise l1 loss rather than l2 since the l1 encourages less blur- ring effect. In the training process, we randomly generate 20,000 faces as well as their corresponding facial customiza- tion parameters for each game by using the game engine. For simplicity, our imitator G only ﬁts the front view images of the facial model. An advantage of using neural networks for rendering is that it can handle complex lighting/shading models under a uniﬁed framework. This is a reason why we train a CNN as our differentiable renderer in our method.
# 3.2 Translator
In our facial parameter translator T , we train a neural net- work T 0 to map the facial embeddings to our in-game facial parameters x:
L(G,T, Frecgs Fseg) = Ai Lia + Aslen + A3L oops (4)
where A; > 0, i = 1,2,3 control the balance between differ- ent loss terms.
Facial Identity Loss. We use a popular face recognition net- work named LightCNN-29v2 [33] to conduct measurement of the global appearances of the two faces, as shown in Fig. 3b. We follow the idea of perceptual distance, which has been widely applied in a variety of tasks, e.g., image style trans- fer [34], super-resolution [35], [36], and feature visualiza- tion [37], and assume that for the different portraits of the same person, their features should have similar representa- tions. We use the backbone of the the LightCNN-29v2 to com- pute a 256-d face embedding and deﬁne the facial identity loss between two faces as the cosine distance on their embeddings:
# q
# ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 2 e1 les Lia = 1 — ef e2/ > 6)
x ¼ T ðIÞ ¼ T 0ðFrecgðIÞÞ; (2)
where e1 ¼ FrecgðIÞ, e2 ¼ FrecgðI0Þ are the face embeddings of an input face photo I and a rendered face image I0.
where I is an input face photo and frecg ¼ FrecgðIÞ is the facial embeddings produced by the face recognition net- work Frecg. Since the embeddings correspond to a global description of the facial identity while the facial parameters depict local details, to learn better correspondence of the two ﬁelds, we introduce an attention-based block as the core building block of our translator. We use three residual attention blocks and two Fully Connected (FC) layers on top of the facial embeddings to translate the embeddings to facial parameters. Fig. 3d shows the details of our translator.
Facial Content Loss. In addition to the facial identity loss, we also deﬁne a content loss by computing pixel-wise image distance based on the facial representations extracted from a pre-trained face semantic segmentation model Fseg, as shown in Fig 3c. The facial content loss provides constraints on the contour and displacement of different face compo- nents in two images regardless of different image domains. We build our face segmentation model based on Resnet- 50 [15]. To improve the position sensitivity of the facial semantic feature, we further use the segmentation results (class-wise probability maps) as the pixel-wise weights of the feature maps to construct the position-sensitive content loss function. We deﬁne the facial content loss as follows:
In each of our attention-based block, we compute the ele- ment-wise importance of the neurons and then performs feature re-calibration by multiplying the representations with them. We make a simple modiﬁcation of the squeeze and excitation block in SENet [32] to apply it to an FC layer (the global pooling layer thus is removed). Suppose v 2
Lew = \lor fi — o fel,
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
(6)
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
Face Recognition Network F,ecg (Light CNN-29) Face recognition network Frecg at Facial identity loss Brie UYU Face segmentation L (b) , network Ezeg ctt Facial 1 Face Segmentation Network Frog acial content loss (Resnet-50) ; Aligned Generated gerze input photo oy—G(x) Facial Parameter L map) Translator T loop weighting loopback loss Fusion (a) (c) Facial Parameter fi eS \ Translator T 1 Residual Attention Block i | ı Predicted facial 1 i parameters uJ © ı Facial ENE GR RR 2 eT ! embedding a cease Th 1 i Block-1 Block-2 Block-3 i FC FC ReLU ' FC FC SSS eee aa di (d)
Fig. 3. (a) To effectively measure the cross-domain similarity between a generated face and a real one, we design three loss functions: 1) a facial identity loss L;w, which computes the distance between two images on top of the facial embeddings produced by a pre-trained face recognition net- work Fyecg, as Shown in (b); 2) a facial content loss £,;;, which computes the pixel-wise similarity based on the facial semantic maps produced by a face segmentation network F.,,, as shown in (c); 3) a loopback loss £),.,, which ensures the facial parameter translator T correctly interprets its own output. (d) shows the architecture of our translator 7. The key to our method is a self-supervised learning framework where a “recursive consistency” is introduced to enforce the facial representation of the rendered image J’ to be similar with the input J: J! = G(T(J)) < I.
where k is the pixel location of the feature map. f1 ¼ FsegðIÞ, f2 ¼ FsegðI0Þ are the facial semantic features of the image I and I0. v1 and v2 are the class-wise probability maps of facial components.
a fully trained translator T ? : x? ¼ T ? ðIÞ. In the training phase, the G, Frecg and Fseg are ﬁrst trained separately. Then we ﬁxed their weights and train our translator T by minimiz- ing the facial similarity loss function (4)
Loopback Loss. Inspired by the unsupervised 3D face recon- struction method proposed by Genova et al. [3], we also introduce a “loopback loss” to further improve the robust- ness of our prediction. After we obtain the rendered face image I0, we further feed it into our translator T to produce a set of new parameters x0 ¼ T ðI0Þ and force the generated facial parameters before and after the loop unchanged, as shown in Fig 3d. The loopback loss can be deﬁned as follows:
T —argminp L(G,T, Frecg, seg) (8) =argming (A Liat + A2Lete + A3Lioop)-
A detailed optimization pipeline of the one-shot genera- tion is summarized as follows:
Lioop Na TI, 0
- Stage I (training). Train the imitator G, face recogni- tion network Frecg and the face segmentation net- work Fseg separately.
# e
- e Stage II (training). Fix G, F, translator 7. Yay Fey and train the
# 3.4 One-Shot Generation and Iterative Generation
We propose two types of methods for facial parameter gen- eration under our facial similarity measurement framework - 1) one-shot generation (our default method) and 2) itera- tive generation.
- e — Stage III (inference). Given an input photo I, predict the facial parameters by using the well-trained trans- lator T*: ~ = T’ (1).
We use the CelebA dataset [38] to train our translator T. We freeze all other networks (Frecg, F'seg, and T) when train- ing our translator, and set A, = 0.01, Ax = 1, and A3 = 1. We use the Adam optimizer [39] with the learning rate = 0-4 and max-iteration = 20 epochs. To improve the prediction on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
# 3.4.1 One-Shot Generation
In the one-shot generation mode, the facial parameters are directly generated from the input photo I by passing through
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1493
1494
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
512x512 6) Generated image
512x512 256x256 6) 64) vexize “9 (64) Architecture of our imitator G(x) 5x8 64x64 x 64) Gy (512) 6 conv] conv2 conv6 conv7 Facial parameters x Generated image
Fig. 4. The architecture of our imitator GðxÞ. We train the imitator to learn a mapping from a group of facial customization parameters x to a ren- dered facial image ^y produced by the game engine.
on side-view faces, we set 2 = 0 every 4 training steps. When the Az is set to 0, we update the 7 by sampling from the full CelebA training set, while when the 2 is set > 0, we update the T by sampling from a subset of the CelebA training set which only contains high-quality front-view faces.

Fig. 5. In our iterative generation mode, the game character auto-crea- tion can be considered as a searching process on the manifold of the imi- tator. We aim to find an optimal point y* = G(2*) that minimizes the distance between y and the reference face photo y, in their feature space.
# 3.4.2 Iterative Generation
In the iterative generation mode, we frame the facial param- eter generation as a parameter searching process. In this case, we remove the translator T and its loopback loss Lloop, and directly optimize on the facial parameters x from the very input-end of the renderer. We use the gradient descent to update the parameters x so that to minimize the facial similarity loss:
x =argmin (Milim + Aslen), s.t. @ € [0,1]. (9)
parameter space, we learn their low-dimensional represen- tation and making predictions in their orthogonal subspace. This can be seen as an integration of facial priors or an addi- tional regularization on the predicted parameters. We found this operation greatly improves the stability of the gener- ated characters.
We ﬁrst run our method on the faces from CelebA [38] (front-view only) to obtain a large set of facial parameters. We then learn a whitening projection P by performing sin- gular value decomposition on the parameter matrix. The dimension reduction can be expressed as follows:
To help understand, we can also express the above opti- mization process as a facial distance minimization process over the manifold S of the game characters:
=P" (x —m), an
y= argmin Lsy.yr), st. y= G(x), x € [0,1 (10)
where m is the mean of the facial parameters which is sub- tracted from the input. We ﬁnally recover the predicted parameters x? by the following inverse mapping:
where we aim to find an optimal face y* = G(a*) that mini- mizes the distance between y and the reference face photo yr. Fig. 5 shows an illustration of the searching process.
A detailed optimization pipeline of the iterative genera- tion is summarized as follows:
- Stage I (training). Train the imitator G, face recogni- tion network Frecg and the face segmentation net- work Fseg.
# e
# e
- Stage II (inference). Initialize the facial parameter x based on the “average face”. Fix G, Frecg, Fseg, and update x until reach the max-number of iterations:
x = (PP) 'P# +m, (12)
where ^x? is the prediction in the low-dimensional space.
Fig. 6a plots the reconstruction energy on seven groups of facial components with a different number of subspace dimensions. The curves suggest high redundancy of the original facial parameters. In Figs. 6b and 6c, we show the importance of the facial prior (dimension reduction) and how it affects the rendering results. We show the integration of facial priors helps generate more stable and more mean- ingful characters.
£-—-x-p os
- @x (m: learning rate). –
- — Project «; to [0, 1]: 2; — max(0, min(2;, 1)).
# 3.6 Implementation Details
In Stage II, we set the max-number of iteration to 50, the learning rate m to 10, and the decay rate to 20 percent per 5 iterations.
Here we provide some additional implementation details on our method. We use the well-known framework PyTorch [40] to implement our method.
Imitator. Our imitator consists of eight transposed convo- lution layers. In each layer, the convolution kernel size is set to 4 x 4 and the stride of each transposed convolution layer is set to 2 so that the size of the feature map is doubled after on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
# 3.5 Integrating Facial Priors
To further improve the robustness of our method, instead of predicting facial parameters directly in the original
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
Energy o Ky rrrrrerry 08 group 1: eyebrow eect group 2: eye — group 3: nose ad group 4: mouth — group 5: face 04 — group 6: jaw --- group 7: position 0.2 0 10 20 30 40 Number of dimensions (a) Reconstruction energy on number of principal components (b) Random character creation with facial priors (c) Random characters without facial priors
Fig. 6. Human faces are typically embedded in a low-dimensional subspace with facial priors. We investigate the importance of the dimension reduc- tion and how it affects the rendering results. (a) Reconstruction energy versus Number of principal components in the parameter space of CelebA dataset. We divide the facial parameters into seven groups and perform dimension reduction accordingly. (b)-(c) Randomly generated characters w/ and w/o dimension reduction.
TABLE 2 A Detailed Conﬁguration of Our Imitator G
TABLE 3 A Detailed Conﬁguration of Our Face Segmentation Model Fseg
Layer Component Configuration Output Size Conv_1 Deconv + BN + ReLU 512x4x4/1 4x4 Conv 2 Deconv + BN + ReLU 512x4x4/2 8x8 Conv 3 Deconv + BN + ReLU 512x4x4/2 16x16 Conv 4 Deconv + BN + ReLU 256x4x4 /2 32x32 Conv 5 Deconv + BN + ReLU 128x4x4 /2 64x64 Conv 6 Deconv + BN + ReLU 64x4x4 /2 128x128 Conv 7 Deconv + BN + ReLU 64x4x4 /2 256x256 Conv 8 Deconv 3x4x4/2 512x512
# Output Size
—
Layer Component Configuration Resolution Conv_1 Conv + BN + ReLU 64x7x7 /2 (1/2)x(1/2) MaxPool MaxPool 3x3/2 (1/4)x(1/4) Conv 2 3 x ResNet Block 64/2 (1/8)x(1/8) Conv 3 4 x ResNet Block 128/1 (1/8)x(1/8) Conv_4 6 x ResNet Block 256 / 1 (1/8)x(1/8) Conv_5 3 x ResNet Block 512/1 (1/8)x(1/8) Conv 6 Convolution 11Xx1x1/1 (1/8)x(1/8)
TABLE 4 A Detailed Conﬁguration of Our Facial Parameter Translator T
a convolution operation. The detailed configuration of our imitator G is listed in Table 2. Specifically,inacxwXxw/s . in pa of transposed convolution layer (denoted as “Deconv” in Table 2), c denotes the number of filters, w x w denotes the filter's size and s denotes the filter’s stride. A Batch-Normal- ization (“BN”) 1 rand a ReLU 1 1 are embedded in our N at on ( ) ayer a a c" U ayer are € edde ° imitator after every convolution layers, except for the out- put layer. In each game, we adopt three imitators to fit three models for adult male characters, adult female characters, and young girl characters, respectively. We use the SGD optimizer to train our imitator with batch_size = 16, and momentum = 0.9. The learning rate is set to 0.01 and the learning rate decay is set to 10 percent per 50 epochs. The training stops after 500 training epochs.
Component Facial recognition network Fully-Connected Residual-Attention Block Conﬁguration LightCNN-29v2 [33] (256, 512) (512, 512) Residual-Attention Block Residual-Attention Block Fully-Connected (512, 512) (512, 512) (512, nc þ nd)
# Layer
# Embedding
# FC_1
# o
# Res_Att_2
# Res_Att_3
# Res_Att_4
# FC_5
continuous and discrete facial parameters separately. We ﬁnally concatenate the outputs of two heads together and feed them to our imitator or the game engine for rendering. A detailed conﬁguration of our translator is shown in Table 4. In column “Conﬁguration”, (in_dim, out_dim)” represents the input and output dimension of its layer. “nc” and “nd” represent the dimensions of continuous and discrete facial parameters respectively. We follow the ResNet [15] and SENet [32], and set the layers in the residual attention blocks to “FC(512,1024) - FC(1024,512) - SE(512,16,512)”.
Face Segmentation Network. We use the Resnet-50 [15] as the backbone of our segmentation network. We remove its fully connected layers and adding a 1 x 1 convolution layer on its top. We also change the stride of the “Conv_3” and “Conv_4” from 2 to 1 to increase its output resolution from 1/32 to 1/8. The face segmentation network is first pre-trained on the ImageNet [41] and then fine-tuned on the Helen face semantic segmentation dataset [42] with pixel-wise cross-entropy loss. We use the same training configurations as our imitator, except that the learning rate is set to 0.001.
Facial Parameters. Here we use the character customization system of the game “Justice” as an example to show how the facial parameters are conﬁgured in our experiments. In “Justice”, there are 264 facial parameters for “male” characters and 310 for “female” characters. Among these parameters, 208 of them are in continuous values, which are listed in Table 5. In the column “Controllers”, the parameters (tx, ty, tz), (u, f, r), and (sx, sy, sz) correspond to the translation, rotation and scale changes of a facial bone on x, y and z axis respectively.
Facial Parameter Translator. Our translator consists of a facial recognition network, three residual attention blocks and two fully connected layers. We design two individual prediction head on top of the translator network to predict
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1495
1496
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
# TABLE 5
A Detailed Conﬁguration of the Facial Customization Parameters (Continuous Part) in the Game “Justice”
# Controllers (tar, ty, tz), (0, @ p), (Sar Sy, Sz) (te, ty, tz), (0, 8, pr (Sx, Sy, 52) (ta, ty, tz), (0, 8, pr (Sa Sy, 52) Component eyebrow-head eyebrow-body eyebrow-tail eye (ba, ty, tz), (0, b, p), (se, 87, $2) outside eyelid (ta, ty, tz), (0, &, p), (Sa, Sy, Sz) inside eyelid (ta, ty, tz), (0, ‘ P), (Sx, Sy, Sz) lower eyelid (ta, ty, tz), (0, &, p), (Sa, Sy, 52) inner eye corner (ta, ty, tz), (0, ¢, Pp), (Sx, Sy, Sz) outer eye corner (ta, ty, tz), (0, &, p), (Sa, Sy, Sz) nose body (tz, ty, tz), (0, , p), (8x, iy Sz) nose bridge (tz, ty, tz), (0, 8, P), (Sa, Sy, Sz) nose wing (tx, ty, tz), (0, b, p)r (Say Syr 82) nose tip (tz, ty, tz), (0, 8, P), (Sa, Sy, Sz) nose bottom (tz, ty, tz), (9, & P), (sx, Sy, 8z) (tz, ty, tz), (0, ¢, p), (8x, 89, Sz) (tz, ty, tz), (0, 4, Pİ (Sx, Sy, Sz) (ta, ty, tz), (0, 8, p), (Sx, Sy, Sz) (tz, ty, tz), (0, 4, Pİ (Sx, Sy, Sz) (ta, ty, tz), (0, 8, p), (Sx, Sy, Sz) (ta, ty, tz), (0, 8, p), (Sx, Sy, Sz) mouth middle upper lip outer upper lip middle lower lip outer lower lip mouth corner COOCDAUUAT!COARGCAW!/ADOAW| CODD OD| HHH forehead (tz, ty, tz), (0, &, p), (Sx, Sy, 52) glabellum (te, ty te), (8, 4, 9), (Say Sy, 52) cheekbone (tx, ty, tz), (0, 6, p), (8x, 87, 8z) risorius (tar ty te), (8, öp (sx si 32) cheek (ta, ty, tz), (0, 8, p), Ge $y, 37) jaw (tz, ty, tz), (0, , p) lower jaw (ta, ty, tz), (0, &, p) mandibular (ta, ty, tz), (0, &, p) outer jaw (ta, ty, tz), (8, 8,p)
ao 2 Vv
Aligned input
Ours (3D)
# 3DMM-CNN
Fig. 7. A visual comparison between our method and a well-known mon- ocular 3D face reconstruction method 3DMM-CNN [2].
devices (coming soon). The two games are mainly developed for East Asian gamers.
We train our model on a large-scale celebrity face attrib- utes dataset CelebA [38]. For quantitative evaluation, we test our method on multiple large scale face veriﬁcation datasets, including LFW [44], CFP_FF [45], CFP_FP [45], AgeDB [46], CALFW [47], CPLFW [48], and Vggfa- ce2_FP [49]. We also build an HD celebrity dataset with 50 high-resolution facial close-up photos, all along with the images in CelebA, to conduct subjective evaluations.
The “# c” represents the number of user-adjustable controllers in each group. For those strikethrough controllers, their move- ments are banned considering the symmetry of the human face. Besides, there are additional 102 discrete parameters for female (22 for hairstyle, 36 for eyebrow style, 19 for lipstick style, and 25 for lipstick color) and 56 discrete parameters for male (23 for hairstyles, 26 for eyebrow styles, and 7 for beard styles), which are not listed in Table 5.
In our method, we encode the discrete parameters as one-hot vectors so that the prediction for the two types of parameters can be performed under a uniﬁed framework. Since it is difﬁcult to directly optimize the one-hot vectors, we use the softmax function to smooth those binary values. P D0 The smoothing can be written as hðxk; bÞ ¼ ebxk = i¼1 ebxi; k ¼ 1; 2; . . . ; n, where D0 represents the dimension of the discrete parameters. b > 0 controls the degree of smooth- ness. We set a relatively large value on b, say, b ¼ 100, to speed up optimization in iterative mode (b ¼ 1 in one-shot mode).
Face Alignment. We perform face alignment by using the “dlib” library [43] before feeding the input face photo into our networks. We use the rendered “average face” as a ref- erence for the alignment.
# 4.1 Game Character Auto-Creation
Fig. 7 shows four input face photos and their auto-customi- zation results. We visually compares our method with a well-known monocular 3D face reconstruction method: 3DMM-CNN [2], where we can see the 3DMM-CNN only focuses on the facial outlines in its generated masks while ignores the modeling of the facial components. In Fig. 8 we give more examples of our auto-customization results. With the generated facial parameters, in-game 3D characters can be rendered by the game engine at multiple views. Figs. 9 and 10 shows some close-look, front-view customization results of our method. The generated characters share a high degree of similarity to the input photos where both of the “identity” and “expressions” are modeled although we do not make any manual adjustments on the facial parameters. Note that although the 3DMM-CNN was not initially designed for game character customization, here we still make a comparison with it in our experiment. This is because as far as we know, there are very few researches on the auto- matic creation of game characters and there is no open source code on this topic released yet. For more generated examples and comparison results, please refer to our supplementary material, which can be found on the Computer Society Digi- tal Library at http://doi.ieeecomputersociety.org/10.1109/ TPAMI.2020.3024009.
# 4 EXPERIMENTAL RESULTS AND ANALYSIS
# 4.2 Quantitative and Subjective Evaluation
We test our method on two role-playing games named “Justice” and “Heaven”, where the former one is a PC game launched in June 2018 with over 20 million registered users, and the latter one is a new mobile game on Android/IOS
We also evaluate the two different versions of our method (“iterative” and “one-shot”) quantitatively and subjectively on both their accuracy and speed.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
A ome ' VE
Input photo
Aligned input
Generated character
Oo Generated in-game character (side-view)
Generated in-game character (front-view)
Generated facial parameters
Fig. 8. Four examples of the auto-created game characters in the game “Justice” by using our method. All the example results in this ﬁgure are gener- ated by the iterative method.

Fig. 9. A close look at the game characters generated by our method in two games: “Justice” (ﬁrst two rows) and “Heaven” (last two rows). In each group of the image, the left one shows an input photo (after alignment) and the right one shows the generated faces. All the example results in this ﬁgure are generated by the iterative method and rendered by the game engines. Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1497
1498
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
Ne. «all
Fig. 10. A close look at the game characters generated by our method in two games: “Justice” (ﬁrst two rows) and “Heaven” (last two rows). In each group of the image, the left one shows an input photo (after alignment) and the right one shows the generated faces. All the example results in this ﬁgure are generated by the one-shot method and rendered by the imitators.
To make quantitative comparisons, we follow the 3DMM-CNN [2] and use the “face veriﬁcation accuracy” as our evaluation metric. We ﬁrst generate facial parame- ters for every pair of input faces in each face veriﬁcation datasets and then use the benchmarking toolkit “face. evoLVe” [50], [52], [53] to compute the veriﬁcation accu- racy based on the generated parameters. The intuition behind is that if the two face images belong to the same person, they should have similar facial parameters. Table 6 shows the veriﬁcation scores of different methods on seven face veriﬁcation datasets. A higher score sug- gests a better performance. The evaluation score of the
3DMM-CNN is reported by Tran et al. [2]. The right-side column of Table 6 shows the speed performance of differ- ent methods.
The subjective comparisons are conducted on two data- sets, an “HD front-view” dataset, which consists of 50 high- resolution facial close-up photos and an “in-the-wild” data- set where the face images are collected from the CelebA and are captured in an open environment with different poses, occlusions, and light conditions. We follow the subjective evaluation used by Wolf et al. [51] and invite 15 non-profes- sional volunteers to rank the results generated by different methods, in which the generated characters are in random
TABLE 6 Quantitative Performance Comparison of Different Methods on Their Accuracy and Speed
Datasets Method LFW CFP_FF CFP_FP AgeDB CALFW CPLFW Vegface2_FP Speed* 3DMM-CNN [2] 0.9235 - - - - - - ~ 10°Hz Ours (iterative) 0.6977 0.7060 0.5800 0.6013 0.6547 0.6042 0.6104 ~ 1Hz Ours (one-shot) 0.9402 0.9450 0.8236 0.8408 0.8463 0.7652 0.8190 ~ 10°Hz LightCNN-29v2** 0.9958 0.9940 0.9494 0.9597 0.9433 0.8857 0.9374 ~ 10°Hz
# Method
3DMM-CNN [2]
# Ours (iterative)
# Ours (one-shot)
* Inference time under GTX 1080Ti. The time cost for data exchange and face alignment are not considered.
** Here we use the performance of LightCNN-29v2 on input photos as a reference (upper-bound accuracy).
The accuracy is computed on seven face veriﬁcation datasets: LFW [44], CFP_FF [45], CFP_FP [45], AgeDB [46], CALFW [47], CPLFW [48], and Vggface2_FP [49].
We follow the evaluation benchmark “face.evoLVe” [50] to compute the accuracy in each of these datasets. Higher scores indicate better. Face embeddings are nor- malized by principal components analysis as applied in 3DMM-CNN for a fair comparison.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
TABLE 7 Subjective Evaluation: Selection Ratio [51] of Different Methods on Two Datasets
Method 3DMM-CNN [2] Ours (iterative) Ours (one-shot) In-the-wild 17.4% + 1.2% 33.9% + 1.2% 48.7% +1.1% HD-front view 1.7% £0.1% 70.0% + 1.1% 28.3% + 1.3%
# HD-front view
the-wild” dataset). However, when dealing with the HD front-view images, the “iterative” version of our method can better catch the facial details but at the same time has a lower speed. We believe the reason why the scores in Tables 6 and 8 are inconsistent is because that the criteria we used have different focuses. The quantitative indicator we used (face veriﬁcation accuracy) pays more attention to the consistency of the facial identity, while under subjective evaluation, peo- ple tend to pay more attention to the generated facial details.
A higher score indicates a higher user preference for the generated result.
order. We deﬁne the “selection ratio” of an output character as the percentage of volunteers who select the character in each group. Finally, the overall selection ratio is used to evaluate the quality of the results generated by each method. Due to a large number of images, we only ran- domly select 50 images from CelebA test set for a proxy evaluation. Table 8 shows the subjective evaluation results of different methods on the above two datasets.
Together from Tables 6 and 8 we can see that our method has higher accuracy than the 3DMM-CNN in terms of both quantitative and subjective evaluations. We also observe that the “one-shot” version of our method has a much faster speed than other approaches (1000x over the “iterative” ver- sion and 10x over the 3DMM-CNN) and also is more robust on the face images from the open environment (on the “in-
# 4.3 Robustness
We further test the robustness of our method on the faces with different poses, different light and blurring conditions. Fig. 11 shows some examples of the generation results.
Not limited to real photos, our method can also generate vivid game characters for some artistic portraits, including the sketch image and caricature. Although these images are either collected from an open environment or in totally dif- ferent styles, we still obtain high-quality results.
# 4.4 Controlled Experiment
Here we discuss the importance of each technical compo- nent of the proposed method and how they contribute to the result, including:
# TABLE 8
Ablations Studies on Different Technical Components of Our Method: 1) the Facial Content Loss Lctt, 2) the Facial Identity Loss Lidt, 3) the Loopback Loss Lloop, and 4) the Residual Attention block (ResAtt) in Our Translator
Ablations Datasets Lett Liat Lioop ResAtt LFW CFP_FF CFP_FP AgeDB CALFW CPLFW Vggface2 FP v x x v 0.7880 0.7930 0.6666 0.6868 0.6792 0.6252 0.6696 v v x v 0.8843 0.8777 0.7507 0.7917 0.7675 0.7032 0.7432 v v v x 0.8870 0.8901 0.7626 0.7875 0.7725 0.7042 0.7618 v v v v 0.9243 0.9200 0.7896 0.8152 0.8130 0.7400 0.7854 LightCNN-29v2* 0.9948 0.9939 0.9476 0.9537 0.9438 0.8872 0.9326
# Lctt
@
@
@
@
# LightCNN-29v2*
* Here we use the performance of LightCNN-29v2 on input photos as a reference (upper-bound accuracy). We show the integration of the above components yields consistent improvements in face veriﬁcation accuracy.

(a) First row: input photos with pose changes and blurring. Second row: generated characters.
(b) First row: input caricature and sketch images. Second row: generated characters.
Fig. 11. (a) We test our method on faces with different poses, different light and blurring conditions. (b) Not limited to real face photos, our method can also generate realistic game characters for artistic portraits although they have completely different styles with our training data. The results in (a) are generated by the one-shot method, and the results in (b) are generated by the iterative method. Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1499
1500
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
# TABLE 9
Subjective Evaluation Results of Two Technical Components of Our Method 1) Facial Identity Loss Lidt, and 2) Facial Content Loss Lctt
Ablations Identity loss Liat Content loss Ley Selection Ratio v x 13.47% + 0.38% x v 36.27% + 0.98% v v 50.26% + 0.40%
A higher selection ration indicates a higher user preference for the result.
- the facial identity loss (see Lidt in Eq. (5) and Fig. 3b);
- the facial content loss (see Lctt in Eq. (6) and Fig. 3c);
ak Aligned photo Ours (w/o L2) Ours (w/ Lz)
- the loopback loss (see Lloop in Eq. (7));
- the residual attention mechanism in our facial parameter translator, (see Fig. 3d).
Edge maps (w/o £2) Semantic map Edge maps (w/
Ablation studies are conducted to analyze the impor- tance of each of the above components. We use the same evaluation metric as we used in Table 6. We ﬁrst evaluate the baseline of our method, where we train our model only based on the facial content loss, then we gradually add other loss components. All evaluations are made based on the “one-shot” version of our method. To verify the effec- tiveness of the residual attention block in our translator, we remove all attention modules on top of the full implementa- tion of our method and simply apply a vanilla multi-layer perceptron which is used in Genova’s method [3]. Table 8 shows their performance on different face veriﬁcation data- sets. We observe the integration of the facial identity loss, loopback loss, and the residual attention mechanism in our translator brings consistent improvements in the accuracy. Particularly, the identity loss and the attention mechanism bring noticeable improvement to our baseline method.
We also studied the visual impact on characters with dif- ferent loss items. We follow the subjective evaluation method we used in Tabel 8 and analyze the user preference w/ or w/o the help of the facial identity loss Lidt and the facial content loss Lctt. Ablations are conducted on top of our full implementation. The statistics are shown in Table 9. We show that both loss items are essential for improving the visual quality of the result and the facial content loss contributes more to a higher visual preference.
Fig. 12. (Better viewed in color) A comparison of the generated faces w/ or w/o the help of the facial content loss Lctt. The ﬁrst column shows the aligned photo and its semantic map produced by our face segmentation network. The second and third columns show the generated faces w/ or w/o the help of facial content loss. Their edge maps are presented for a better visual comparison, where the yellow pixels correspond to the edge of the reference photo and the red pixels correspond to the gener- ated faces.
details than the one-shot method. On the face photos cap- tured in the wild, the one-shot method is more robust than the iterative method. To discuss the two methods more clearly, let’s consider a special case where we only have a single face image in our training set. In this case, the two methods will become almost equivalent – the only differ- ence is that the one-shot method optimizes parameters indi- rectly through the T and the iterative method optimizes directly on the parameters themselves. This means that if the T has a large enough capacity and if its loss surface is smooth, the two approaches should have similar perfor- mance. However, since the face parameters are nonlinear and their dimensions are usually highly coupled, we found that the T is usually difﬁcult to ﬁt all training images, partic- ularly on large-scale datasets (e.g., CelebA). In other words, it is always easy to “overﬁt” on a single training sample but hard on many, especially when the model capacity is lim- ited, and the parameter space is highly complex. Therefore, in the one-shot method, sometimes we can see the facial details are not recovered very well.
Fig. 12 shows a comparison of the generated faces w/ or w/o the help of the facial content loss. For a better view, the facial semantic maps and the edges of the facial components of the generated characters are presented. In the edge maps, the yellow pixels correspond to the edge of the reference photo and the red pixels correspond to the generated faces. We observe a better correspondence of the pixel location between the input photo and the generated face when we apply the facial content loss.
In real application scenarios, we would recommend the following as an automatic way to choose a better mode for arbitrary input. Without considering the speed, we recom- mend using the iterative method on HD front-view images and using the one-shot method on the faces captured in the wild. When speed is the priority, the one-shot method is clearly a better choice.
Limitation and Future Work. Limitation of our method is twofold. First, we found in our experiment that the iterative version of our method may fail on the faces with poses. The possible reason behind is that the iterative method tends to overﬁt on the 2D facial component layout and ignores the 3D structures. Fig. 13 shows a failure case of our iterative method. Our second limitation lies in our imitator. Since we use a sin- gle neural network to imitate the behavior of the renderer, it may be difﬁcult to deal with the face models with large
# 4.5 Discussion
One-Shot versus Iterative. Here we give a further discussion on the the difference and connection between the two methods (one-shot versus iterative). As we mentioned in Section 4.2, the two approaches have different properties in different application scenarios. We show that on HD front- view face photos, the iterative method can better catch the
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
£2)
SHI ET AL.: NEURAL RENDERING FOR GAME CHARACTER AUTO-CREATION
(a) (b) (©)
- [3] K. Genova, F. Cole, A. Maschinot, A. Sarna, D. Vlasic, and W. T. Freeman, “Unsupervised training for 3D morphable model regression,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8377–8386.
- [4] B. Gecer, S. Ploumpis, I. Kotsia, and S. Zafeiriou, “Ganﬁt: Genera- tive adversarial network ﬁtting for high ﬁdelity 3D face reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 1155–1164.
- [5] T. Shi, Y. Yuan, C. Fan, Z. Zou, Z. Shi, and Y. Liu, “Face-to-param- eter translation for game character auto-creation,” in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 161–170.
Fig. 13. A failure case of our iterative method. (a) shows an input face photo with pose and (b) shows an overﬁtted generation result of our iter- ative method. As we can see, although the 2D facial components are well aligned, the 3D structures are ignored. For a better comparison, in (c), we show another front-view face photo of the same celebrity shown in (a).
- [6] T. Shi, Z. Zou, Y. Yuan, and C. Fan, “Fast and robust face-to- parameter translation for game character auto-creation,” in Proc. 34th AAAI Conf. Artif. Intell., 2020, pp. 1733–1740.
- [7] V. Blanz and T. Vetter, “Face recognition based on ﬁtting a 3D morphable model,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 9, pp. 1063–1074, Sep. 2003.
deformations. Fortunately, the faces in RPGs are usually built with simple deformation basis. That is to say, our methods can be easily applied to most RPG environments. In other ﬁelds, such as animated movies where character faces are typ- ical with a more complex deformation space, we may need to design a more complex differentiable rendering model than what we used in our paper. This will be one of our future research direction (e.g., using differentiable mesh renderer or neural mesh renderer). Another of our future work is to ﬁnd a potential solution to unify the two approaches into a single pipeline and complement each other.
- [8] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter, “A 3D face model for pose and illumination invariant face recog- nition,” in Proc. 6th IEEE Int. Conf. Advanced Video Signal Based Sur- veillance, 2009, pp. 296–301.
- [9] O. Aldrian and W. A. Smith, “Inverse rendering of faces with a 3D morphable model,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 5, pp. 1080–1093, May 2013.
- [10] T. J. Cashman and A. W. Fitzgibbon, “What shape are dolphins? building 3D morphable models from 2D images,” IEEE Trans. Pat- tern Anal. Mach. Intell., vol. 35, no. 1, pp. 232–244, Jan. 2013.
- [11] J. Booth, E. Antonakos, S. Ploumpis, G. Trigeorgis, Y. Panagakis, and S. Zafeiriou, “3D face morphable models”in-the-wild”,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5464–5473.
- [12] W. Peng, Z. Feng, C. Xu, and Y. Su, “Parametric t-spline face morphable model for detailed ﬁtting in shape subspace,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5515–5523.
- [13] L. Tran and X. Liu, “On learning 3D face morphable model from in-the-wild images,” IEEE Trans. Pattern Anal. Mach. Intell., p. 1, 2019.
# 5 CONCLUSION
We propose a novel method to automatically create charac- ters in game environments based on a single input face photo. We frame the auto-creation under a self-supervised learning paradigm by leveraging the differentiable neural rendering, which bridges the gap between the deep learning and game graphics. We propose two generation modes based on this framework, namely, a one-shot generation mode and an iterative generation mode, and show in differ- ent aspects of their advantages, such as the generation qual- ity and speed. Comparison results and ablation analysis on seven face veriﬁcation benchmark datasets and a high-reso- lution celebrity dataset suggest the effectiveness of our method. Our method achieves a high degree of generation similarity and robustness between the input face photo and the rendered in-game character. Our method also proves to be robust to pose variants and image style changes.
- [14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ- cation with deep convolutional neural networks,” in Proc. Advan- ces Neural Inf. Process. Syst., 2012, pp. 1097–1105.
- [15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog- nit., 2016, pp. 770–778.
- [16] P. Dou, S. K. Shah, and I. A. Kakadiaris, “End-to-end 3D face reconstruction with deep neural networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1503–1512.
- [17] A. S. Jackson, A. Bulat, V. Argyriou, and G. Tzimiropoulos, “Large pose 3D face reconstruction from a single image via direct volu- metric CNN regression,” in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 1031–1039.
- [18] E. Richardson, M. Sela, R. Or-El, and R. Kimmel, “Learning detailed face reconstruction from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5553–5562.
- [19] A. Tewari et al., “MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction,” in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 3735–3744.
- [20] A. T. Tran, T. Hassner, I. Masi, E. Paz, Y. Nirkin, and G. Medioni, “Extreme 3D face reconstruction: Seeing through occlusions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3935–3944.
# ACKNOWLEDGMENTS
The authors would like to thank the development teams of the game “Justice” and “Heaven (Mobile)”. The authors would also like to thank them for their valuable help and their beautiful 3D character models. Tianyang Shi and Zhengxia Zou contributed equally to this work.
- [21] L. Tran and X. Liu, “Nonlinear 3D face morphable model,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7346–7355.
- [22] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Advances Neural Inf. Process. Syst., 2014, pp. 2672–2680. [Online]. Available: http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
- [23] M. M. Loper and M. J. Black, “OpenDR: An approximate differen- tiable renderer,” in Proc. Eur. Conf. Comput. Vis., 2014, pp. 154–169.
- [24] H. Kato, Y. Ushiku, and T. Harada, “Neural 3D mesh renderer,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3907–3916.
# REFERENCES
- [1] V. Blanz and T. Vetter, “A morphable model for the synthesis of 3D faces,” in Proc. 26th Annu. Conf. Comput. Graph. Interactive Techn., 1999, pp. 187–194.
- [2] A. T. Tran, T. Hassner, I. Masi, and G. Medioni, “Regressing robust and discriminative 3D morphable models with a very deep neural network,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1493–1502.
- [25] S. Liu, T. Li, W. Chen, and H. Li, “Soft rasterizer: A differentiable renderer for image-based 3D reasoning,” in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 7707–771.
- [26] T.-M. Li, M. Aittala, F. Durand, and J. Lehtinen, “Differentiable monte carlo ray tracing through edge sampling,” in Proc. SIG- GRAPH Asia Technical Papers, 2018, Art. no. 222.
- [27] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee, “Perspective trans- former nets: Learning single-view 3D object reconstruction with- out 3D supervision,” in Proc. Advances Neural Inf. Process. Syst., 2016, pp. 1696–1704.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
1501
1502
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 3, MARCH 2022
- [28] T. H. Nguyen-Phuoc, C. Li, S. Balaban, and Y. Yang, “Rendernet: A deep convolutional network for differentiable rendering from 3D shapes,” in Proc. Advances Neural Inf. Process. Syst., 2018, pp. 7891–7901.
- [29] S. A. Eslami et al., “Neural scene representation and rendering,” Science, vol. 360, no. 6394, pp. 1204–1210, 2018.
- [52] J. Zhao et al., “Multi-prototype networks for unconstrained set- based face recognition,” in Proc. 28th Int. Joint Conf. Artif. Intell., 2019, pp. 4397–4403.
- [53] J. Zhao et al., “Towards pose invariant face recognition in the wild,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2207–2216.
- [30] A. Gruber et al., “Interactive sculpting of digital faces using an anatomical modeling paradigm,” in Computer Graphics Forum, vol. 39, Hoboken, NJ, USA: Wiley, 2020.
- [31] A. Radford, L. Metz, and S. Chintala, “Unsupervised representa- tion learning with deep convolutional generative adversarial networks,” 2015, arXiv:1511.06434.
- [32] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7132–7141.
- [33] X. Wu, R. He, Z. Sun, and T. Tan, “A light CNN for deep face representation with noisy labels,” IEEE Trans. Inf. Forensics Secur., vol. 13, no. 11, pp. 2884–2896, Nov. 2018.
—
Tianyang Shi received the BS and MS degrees from the School of Astronautics, Beihang Univer- sity, in 2016 and 2019, respectively. He is cur- rently a researcher at Netease Fuxi AI Lab. His research interests include image processing, deep learning, and their application in games.
- [34] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using convolutional neural networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 2414–2423.
[35]
- J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real- time style transfer and super-resolution,” in Proc. Eur. Conf. Com- put. Vis., 2016, pp. 694–711.
- [36] C. Ledig et al., “Photo-realistic single image super-resolution using a generative adversarial network,” in Proc. IEEE Conf. Com- put. Vis. Pattern Recognit., 2017, pp. 4681–4690.
- [37] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, “Understanding neural networks through deep visualization,” 2015, arXiv:1506.06579.
= oe a >»
Zhengxia Zou received the BS and PhD degrees from the Image Processing Center, School of Astronautics, Beihang University, in 2013 and 2018. He is currently working with the Depart- ment of Computational Medicine and Bioinfor- matics, University of Michigan, Ann Arbor, as a postdoc research fellow. His research interests include computer vision, pattern recognition, and remote sensing image analysis. He serves as the PC member/reviewer for several top conferences and top journals, including the NeurIPS, CVPR,
- [38] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proc. Int. Conf. Comput. Vis., 2015, pp. 3730–3738.
- [39] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti- mization,” 2014, arXiv:1412.6980.
AAAI, TIP, SPM, TGRS, etc. He was selected as one of the 2017 best reviewers for the Infrared Physics and Technology. For more informa- tion, please visit http://www-personal.umich.edu/~zzhengxi/
- 40 A. Paszke et al., “PyTorch: An imperative style, high-performance deep learning library,” in Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, Eds., Curran Associates, Inc., 2019, pp. 8024-8035. [Online]. Available: http://papers.neurips.cc/ paper /9015-pytorch-an-imperative-style-high-performance-deep- learning-library.pdf
- [41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 248–255.
- [42] V. Le, J. Brandt, Z. Lin, L. Bourdev, and T. S. Huang, “Interactive facial feature localization,” in Proc. Eur. Conf. Comput. Vis., 2012, pp. 679–692.

Zhenwei Shi (Member, IEEE) received the PhD degree in mathematics from the Dalian University of Technology, Dalian, China, in 2005. He was a postdoctoral researcher with the Department of Automation, Tsinghua University, Beijing, China, from 2005 to 2007. He was visiting scholar with the Department of Electrical Engineering and Com- puter Science, Northwestern University, Evanston, IL., from 2013 to 2014. He is currently a professor and the dean of the Image Processing Center, School of Astronautics, Beihang University. His
- [43] D. E. King, “Dlib-ml: A machine learning toolkit,” J. Mach. Learn. Res., vol. 10, pp. 1755–1758, 2009.
- [44] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, “Labeled faces in the wild: A database forstudying face recognition in unconstrained environments,” Workshop Faces ‘Real-Life’ Images: Detection, Alignment, Recognit., 2008. [Online]. Available: https:// hal.inria.fr/inria-00321923
- [45] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs, “Frontal to proﬁle face veriﬁcation in the wild,” in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2016, pp. 1–9.
current research interests include remote sensing image processing and analysis, computer vision, pattern recognition, and machine learning. His serves as an associate editor for the Infrared Physics and Technology. He has authored or co-authored more than 100 scientiﬁc papers in refereed journals and proceedings, including the IEEE Transactions on Pattern Analysis and Machine Intelligence, the IEEE Transactions on Neural Net- works, the IEEE Transactions on Geoscience and Remote Sensing, the IEEE Geoscience and Remote Sensing Letters and the IEEE Conference on Computer Vision and Pattern Recognition. For more information, please visit http://levir.buaa.edu.cn/
- [46] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou, “AgeDB: The ﬁrst manually collected, in-the- wild age database,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog- nit. Workshops, 2017, pp. 51–59.
- [47] T. Zheng, W. Deng, and J. Hu, “Cross-age LFW: A database for studying cross-age face recognition in unconstrained environ- ments,” 2017, arXiv: 1708.08197.
- [48] T. Zheng and W. Deng, “Cross-pose LFW: A database for study- ing crosspose face recognition in unconstrained environments,” Beijing Univ. Posts and Telecommun., Rep. no. 18-01, Feb. 2018.
- [49] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “VGGFace2: A dataset for recognising faces across pose and age,” in Proc. 13th IEEE Int. Conf. Autom. Face Gesture Recognit., 2018, pp. 67–74.
a
Yi Yuan received the PhD degree in photogramme- try and remote sensing from Wuhan University, in 2017. He is currently a computer vision researcher at Netease Fuxi AI Lab. His research interests include face reconstruction, image generation, transfer learning, etc.
- [50] J. Zhao et al., “Look across elapse: Disentangled representation learning and photorealistic cross-age face synthesis for age-invari- ant face recognition,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 9251–9258.
" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.
- [51] L. Wolf, Y. Taigman, and A. Polyak, “Unsupervised creation of parameterized avatars,” in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 1530–1538.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:10:04 UTC from IEEE Xplore. Restrictions apply.
