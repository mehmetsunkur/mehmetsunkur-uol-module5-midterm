<!-- image -->

Contents lists available at ScienceDirect

## Neurocomputing

journal homepage: www.elsevier.com/locate/neucom

## A data-efficient deep learning approach for deployable multimodal social robots

## Heriberto Cuayáhuitl

School of Computer Science, University of Lincoln, Lincoln Centre for Autonomous Systems (L-CAS), Brayford Pool, Lincoln LN6 7TS, United Kingdom

## a r t i c l e i n f o

Article history:

Received

1

April 2018

Revised

2 August 2018

Accepted

11 September 2018

Available

online

24 April 2019

Keywords:

Deep reinforcement learning Deep supervised learning Interactive robots Multimodal perception and interaction Board games

## a b s t r a c t

The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games-and use the game of 'Noughts and Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play.

©2019 Elsevier B.V. All rights reserved.

## 1. Introduction

In a not so distant future, we will be able to buy purposeful and socially-aware humanoid robots that can be delivered home much like buying personal computers nowadays. While robots may come with a pre-defined or pre-trained set of skills-arguably and ideally-they should be able to self-adapt or self-extend for carrying out new useful tasks relevant to their individual user(s). A new task can be one that is entirely distinct from pre-defined skills or one that is similar but not the same. We will refer to both types as 'new tasks' and present two examples of new tasks in our case study below. Deploying robots with pre-built skills is still challenging assuming that they have to adapt to different spatial and social environments each time an adaptation of existing knowledge occurs. Deploying robots with the ability to acquire new skills has the potential to be even more challenging. This latter form of deployment is of great interest to AI because it requires advanced multimodal communication (via human-like verbal and non-verbal commands) in order to achieve a task or set of tasks successfully. A concrete scenario e.g. is as follows: Your new robot has arrived and you want to teach it to play a game (that is at least partially unknown to the robot) so it can play with you, your family and friends whenever you want. Having said that ... How can the

robot be equipped and/or trained to play such a game with a reduced amount of human intervention? What are the basic building blocks required to make that happen? This article makes a step towards answering some of these challenging questions and discusses future work towards purposeful and socially-aware humanoid robots.

As a case study, the multimodal game that we focus on is Noughts and Crosses also known as ' Tic-Tac-Toe'-with two variants. In both games players alternate turns, and each player is represented by either noughts or crosses.

- · Its standard version uses a 3 × 3 grid, where a game is won if and only if three noughts or crosses are in line or diagonal (a draw otherwise)-see Fig. 1 (left). One player adopts noughts and the other crosses, alternating turns until the game is over. In this game, the more expertise the players acquire, the more likely it is to end up in a draw. This has motivated the development of variants of the game with higher degrees of complexity.
- · A substantially more difficult variant, called Ultimate Noughts and Crosses 1 , uses 3 × 3 subgrids each of 3 × 3 squares (81 squares in total), where the goal is to win three subgrids (each of 3 × 3 squares) in line or diagonal. In this latter game, while the first game move is allowed to take any of the 81 squares, a subsequent game move is restricted to take a square in the

<!-- image -->

<!-- image -->

Fig. 1. A humanoid robot playing the game of noughts and crosses with two variants using multiple modalities and learnt behaviour.

<!-- image -->

Nougths & Crosses

<!-- image -->

Ultimate Nougths & Crosses

subgrid that mirrors the previous game move of the opponent player. For example, a player taking the middle right square of any subgrid would restrict the opponent to take a move anywhere in the middle right subgrid-as shown in Fig. 2 (a) and (b). Similarly, a player taking for example the bottom-right square of any subgrid would restrict the opponent to take a move anywhere in the bottom-right subgrid-as in Fig. 2 (b) and (c), and so on. There is one exception to these restrictions: a player can take an empty square anywhere in the entire board if and only if the target subgrid has been won/lost/draw already. This game is more advanced than its standard counterpart and strategically challenging-see Fig. 1 (right).

The challenging task for the robot is to successfully play either game against unknown humans and partially familiar physical environments. This article describes a machine intelligence approach for efficiently training such a deployable robot, and makes the following contributions:

- 1. We propose a deep learning-based approach for training a multimodal robot with low data requirements. This is demonstrated by a scenario with two variants of different com plexity and the following data requirements: a few hundred example images, a dozen example multimodal dialogues (see example in [1] -Appendix A), a few example physical demonstrations of handwriting, and automatically generated simulated games. Applying our approach to other games or tasks would require similar resources (though with further training examples depending on task complexity), plus a mechanism to let the robot know about valid actions and when a task has been achieved or not (e.g. game won/lost). The latter together with more refined manipulation or locomotion would require additional programming, which future work should try to automate.
- 2. We propose two novel learning algorithms, one for visual perception and the other one for policy learning. The former is useful for tracking the game state, i.e., what moves have been made so far by each player. Accurate recognition is important for playing the game so that the robot's view of the world is as accurate as possible, as opposed to a blurry view that would lead to unexpected or non-human-like behaviours. Deep learning can be used to provide the robot with game moves (in our case), and it can also provide the interaction agent with learnt internal representations. The latter algorithm for interac-

tion (policy learning) is important for training an autonomous robot that learns-in a scalable way-its competitive behaviour from trial and error. Our newly proposed algorithms win substantially more than the well-known DQN method [2] .

- 3. We carried out a near real world evaluation of our deep learning-based humanoid robot, who played the game of Noughts and Crosses against 130 different individuals in the wild. While most previous work has carried out evaluations using simulations only or controlled experiments in lab environments, we believe it is important and timely to show that newly developed approaches or algorithms work (to a large extent) in real or near-real world settings -out of lab conditions. Robots interacting in the wild have to be able to deal with unstructured interactions, partially known environments, unseen human behaviour, etc. This is a big challenge for multimodal robots, and this work reports a step in this direction.

## 2. Related work

So far the topic of deep learning-based conversational and/or multimodal social robots is in many respects unexplored. Some exceptions include the following. Noda et al. [3] train a humanoid robot to carry out the following object manipulation behaviours: ball lift, ball roll, bell right (left and right), ball roll on a plate, and ropeway. To train this multimodal robot three neural networks are used: first, a deep autoencoder is used for feature learning from audio signals in the form of spectrograms; second, a deep autoencoder is used for feature learning from 2D images; and third, a deep autoencoder is also used for multimodal feature learning from audio and visual features generated by the previous two autoencoders. The latter learnt features are given as input to a multiclass Support Vector Machine classifier in order to predict the object manipulation task to carry out. Focusing more on social skills, Qureshi et al. [4] train a humanoid robot with social skills whose goal is to choose one of four actions: wait, look towards human, wave hand, and handshake. The authors use the DQN method [2] and a two-stage approach. While the first stage collects grayscale and depth images from the environment, the second stage trains two Convolutional neural nets with fused features. The robot receives a reward of +1 for a successful handshake, -0.1 for an unsuccessful handshake, and 0 otherwise. Combining social and action learning, [1,5] train a robot to play games also using the DQN method and a variant of it. In this work a Convolutional neural net is used to predict game moves, and a fully-connected neural net is used to learn multimodal actions (18 in total) based on game rewards. Other previous works have addressed multimodal deep learning but in non-conversational settings [6-9] . From all these works it can be observed that learning agents use small sets of actions in single-task scenarios. Thus, humanoid social robots with more complex behaviours including larger sets of actions remain to be investigated.

There is a similarly limited amount of previous work on humanoid robots playing games against human opponents. Notable exceptions include [10] , where the DB humanoid robot learns to play air hockey using a Nearest Neighbour classifier; [11] , where the Nico humanoid torso robot plays the game of rock-paperscissors using a 'Wizzard of Oz' setting; [12] , where the Sky humanoid robot plays catch and juggling using inverse kinematics and induced parameters with least squares linear regression; [13] , where the Nao robot plays a quiz game, an arm imitation game, and a dance game using tabular reinforcement learning; [14] , where the Genie humanoid robot plays the poker game using a 'Wizard of Oz' setting; and [15] , where the NAO robot plays Checkers using a MinMax search tree. Most of these robots only exhibit non-verbal abilities and are either teleoperated or based on heuristic methods, which suggests that verbal abilities in autonomous

Robot game movel (b) Human game movel (c) Robot game move2 Human game move2

<!-- image -->

Fig. 2. Example robot and user game moves -robot's field of view from bottom to top.

trainable robots playing games are underdeveloped. Apart from [1,5] , we are not aware of any other previous work in humanoid robots playing social games against human opponents and trained with deep learning methods.

Previous work on multimodal robots trained to carry out specific tasks and that have been deployed in the wild are almost absent as pointed out by [16-18] -perhaps due to the complexity involved. Most previous multimodal trainable robots are either trained and tested in simulation, or trained (usually offline) and tested in controlled conditions and/or using recruited participants. For the sake of clarity, we refer to robots deployed in the wild as those robots interacting with non-recruited participants in a noncontrolled manner and with rather spontaneous, unrestricted and untimed interactions. The closest previous work is the Minerva robot [19] , which gave 620 tours to people through the exhibitions of a museum. Another related work is the Nao robot [20] , which gave route instructions to employees and visitors of a company building. Lessons learnt by these works include the application of probabilistic approaches to deal with uncertainty in the interaction, and challenges in starting and finishing conversational engagements with out-of-domain responses. Our work complements previous work by showcasing a robot that carries out a joint activity with people, namely playing multimodal games, in a spontaneous and uncontrolled setting.

In the remainder of the article we describe a deep learningbased approach for efficiently training a robot with the ability of behaving with reasonable performance in a near real world deployment. In particular, we measure the effectiveness of neuralbased game move interpretation and the effectiveness of Deep QNetworks (DQN) [2] for interactive social robots. Field trial results show that the proposed approach can induce reasonable and competitive behaviours, especially when they are not affected by unseen noisy conditions.

## 3. Proposed learning approach

Our proposed approach uses two deep learning tasks in cascade with low data requirements, which is useful to enable robots with new skills and where training data is either absent or scarce. While the first learning task predicts what is going on in the environment-game moves and who said what, the second learning task inherits such predictions in order to decide what to do or say next. Our approach is motivated by the fact that once a robot system is trained, it is expected to operate not only in known environments but also in partially-known environments. The latter may include unseen rooms, unseen furniture, and unseen human opponents, among others. This approach can be applied to other tasks beyond the case study in this article through the following methodology:

- 1. Collect a modest set of example images (e.g. a few hundred or as needed) and label them.
- 2. Train a deep supervised learner for visual perception to keep track of the environment dynamics as described in Section 3.1 .
- 3. Write a set of example dialogues (e.g. a dozen or as needed) as in the Appendix of [1] , which can be used for generating simulated interactions as in [21] .
- 4. Use the outputs of the previous two steps for training a deep reinforcement learner using simulations as described in Section 3.2 .
- 5. Collect a set of pre-recorded motor motions or train a component to carry out commands such as 'write cross or circle in a particular location'.
- 6. Test the robot using the previous resources, and iterate from step 1 if needed.
- 7. Deploy the robot subject to successful interactions in the previous step.

## 3.1. Visual-based perception of game moves

We use the 2D camera in the robot's mouth to recognise drawings on the game grid using Algorithm 1 . The robot extracts video frames, locates the game grid using colour-based detection of the largest contour-see Fig. 3 , and splits the game grid into a set of

## Algorithm 1 Game Move Recogniser.

- 1: Input: video stream from 2D camera, n × n =grid size (e.g. 3 × 3, 9 × 9), noise threshold τ , ρ =resolution of subimages, γ =pause between recognition events, C =statistical classifier, initialise L ( i ) t =labels at time t for grid square i
- 2: Output: set G of recognized game moves

## 3: repeat

- 4: F ← extract video frame from video stream
- 6: P ' ← get projected transformation from P
- 5: P ← extract page region from F
- 7: if d ist ( P ' t , P ' t 1 )+ d ist ( P ' t 1 , P ' t 2 ) > τ then
- 8: continue (detect and omit hands in

handwriting)

- ---

## 9: end if

- 10: G ← remove grid and convert image P ' to grayscale
- 12: L ( i ) t -2 ← L ( i ) t -1
- 11: G ' ← divide G into n × n images of ρ pixels
- 13: L ( i ) t -1 ← L ( i t

)

- 14: L ( i ) t ← predict labels for each image in G ' using C

16:

if

label

of

grid

square

i

- 15: i ∗ = arg max i 1 Z P r ( L ( i ) t -2 ) + P r ( L ( i ) t -1 ) + P r ( L ( i ) t )

∗

is

not

'nothing'

then

- 17: Update G with game move of grid square i ∗
- 18: break

19:

end

if

20:

sleep γ milliseconds

- 21: until end of game turn

Fig. 3. Raw input image in colour space BGR-Gray (left), largest contour with projected transformation (middle), and game space used for game move recognition (right). Our image pre-processing is based on OpenCV ( http://opencv.org/ ).

<!-- image -->

Fig. 4. Illustration of a game move before and after handwriting.

<!-- image -->

subimages (one for each grid square) in order to pass them to a probabilistic classifier for game move recognition.

Although a game move recogniser can be trained in an online fashion, our algorithm below assumes the existence of a supervised learner trained offline. Thus, we leave the topic of online training, during the course of the interaction, as future work. In our case we use a Convolutional Neural Network (CNN) [22] C trained from data set D = { ( x 1 , y 1 ) , . . . , ( x N , y N ) } , where x i are n × n matrices of pixels and y j are class labels. This classifier maps images to labels-in our case {'nought', 'cross', 'nothing'}. Our CNN uses the following architecture: input layer of 40 × 40 pixels, convolutional layer with 8 filters, ReLU, pooling layer of size 2 × 2 with stride 2, convolutional layer with 16 filters, ReLU, pooling layer of size 3 × 3 with stride 3, and the output layer used a linear Support Vector Machine with 3 labels. This CNN is used multiple timesonce per grid cell-to predict the state of each game grid, in each game turn, for detecting drawing events. Note that the larger the grid the larger the number of recognition events needed for predicting the state of the entire game grid. In addition, rather than using only the most recent video frame for game move recognition, we use a history of user and robot game moves (denoted as G ) to focus recognition on valid game moves, i.e., newly recognised moves from empty grid squares to non-empty grid squares. This process needs to be done in (near) real-time for exhibiting smooth human-robot interactions.

Algorithm 1 formalises the description above for game move recognition, which can be used at each game turn in a game. Lines 7-9 2 are particularly useful for ignoring video frames with human hands, which can be a source of misrecognitions-see Fig. 4 . In this way the game move recogniser is responsible for maintaining, as accurately as possible, the state of the game based on

a set of user and robot game moves 3 in the following format: [ who = usr ∧ what = draw ∧ where = i ∗ ] .

## 3.2. Learning to interact given multimodal inputs

The visual perceptions above plus words raised in the interaction by both conversants 4 are given as input to a reinforcement learning agent to induce its behaviour, where such multimodal perceptions are mapped to multimodal actions by maximizing a longterm reward signal. The goal of a reinforcement learner is to find an optimal policy denoted as π ∗ ( s ) = arg max a ∈ A Q ∗ ( s , a ) , where Q ∗ represents the maximum sum of rewards r t discounted by factor γ at each time step, after taking action a in state s [24,25] . While reinforcement learners take stochastic actions during training, they select the best actions a ∗ = π ∗ ( s ) at test time.

Our reinforcement learning agents approximate Q ∗ using a multilayer neural network as in [2] . The Q function is parameterised as Q ( s, a ; θ i ), where θ i are the parameters (weights) of the neural network at iteration i . Furthermore, training a deep reinforcement learner requires a dataset of experiences D = { e 1 , . . . e N } (also referred to as 'experience replay memory'), where every experience is described as a tuple e t = ( s t , a t , r t , s t + 1 ) . Inducing Q ∗ θ consists in iteratively applying Q-learning updates over minibatches of experience MB = { ( s , a , r , s ' ) ∼ U ( D ) } drawn uniformly at random from the full data set D . A learning update at iteration i is thus defined

## Algorithm 2 Competitive DQN Learning.

- 1: Initialise Deep Q-Networks with replay memory D , action-value function Q with random weights θ , and target action-value functions ˆ Q with weights ˆ θ = θ

## 2: repeat

- 3: s ← initial environment state in S

## 4: repeat

5:

6:

7:

ˆ

A

A

a

actions

with

min

(

r

(

s

,

a

)

<

0

∅

otherwise

action(s)

leading

to

win

(legally)

the

game

all

available

actions

in

state

s

otherwise

rand

max

a

a

∈

A

≤

- ∈ A \ A 8: Execute action a and observe reward r and next state s '

9:

Append

transition

(

s

,

a

,

r

,

s

10:

Sample

random

minibatch

(

s

11:

12:

13:

14:

15:

s

16:

until

s

is

a

goal

state

17:

until convergence

according to the following loss function

L i (θ i ) = E MB [ ( r + γ max a ' Q ( s ' , a ' ; θ i ) -Q ( s , a ; θ i )) 2 ] ,

where θ i are the parameters of the network at iteration i , and θ i are the target parameters of the network at iteration i . The latter are held fixed between individual updates. This process is known as Deep Q-Learning with Experience Replay [2] .

We extend the learning algorithm described in [1] by refining the action set at each time step so that agents gain access to lookahead information and can learn to make inferences over the effects of their actions. In this way, our agents anticipate the effects of their decision making better than during pure naive exploration/exploitaion. An agent may for example have the winning or loosing move at some point during the game together with other available actions. But this raises the question 'Why should agents learn what to do if they have the ability to infer that a game is about to be won or lost?' In the former case (win), it could omit all actions that are not winning moves-unless winning is not the objective, see line 6 in Algorithm 2 . In the latter case (lose), it could avoid all actions that lead to loosing the game-see line 5 in Algorithm 2 . This algorithm requires taking actions temporarily in the environment to observe the consequent rewards (with 1 look-ahead time step), and then undo such temporary actions to remain in the original environment state s -before looking for the worst negative actions and/or winning actions. The main changes in contrast to the original DQN algorithm require the identification of worst actions ˆ A as well as best actions (if any) so that decision making can be made based on actions in A not in ˆ A , also denoted as A \ ˆ A . Our agents thus select actions according to

π ∗ θ ( s ) = arg max A \ ˆ A Q ∗ ( s , a , θ) ,

where both s and a exhibit multimodal aspects. While states s include verbal and visual observations (i.e., words and game moves), actions a include verbalisations and motor commands-see videos in Section 4.2 .

y

j

=

Set

err

Gradient

descent

step

on

err

with

respect

to

θ

Reset

←

s

r

r

j

j

if

final

step

of

episode

+

=

ˆ

Q

=

(

γ

max

y

j

-

Q

(

s

a

∈

A

'

,

a

ˆ

Q

(

s

'

;

'

,

a

θ)

'

2

)

;

ˆ

θ)

otherwise

Q

every

C

steps

{

if

random

number

ˆ

ˆ

Q

(

s

'

,

a

'

;

ˆ

θ)

otherwise

'

)

to

D

j

,

a

j

{

/epsilon1

=

=

=

{

{

'

,

r

∀

j

a

,

s

## 4. Experimental setting

In contrast to previous studies that require millions of video frames for training visually-aware game-based policies [2] , we train our game move recogniser from a few hundred example images and our reinforcement learners using simulations due to the large amount of training examples required.

## 4.1. Characterisation of deep reinforcement learners

## 4.1.1. Multimodal states

Our environment states include 73 and 289 features (for N&C 3 × 3 and N&C 9 × 9, respectively) that describe the game moves, executed commands, and words raised in the interactions. While words derived from system responses are treated as binary variables (i.e., present or absent), words derived from user responses are treated as continuous variables. We treat game moves as binary features and future work can consider treating them as continuous variables to mirror the recognition confidence. In addition, we consider a robot that cannot distinguish the order of game moves versus a robot that can distinguish the sequence of game moves. The latter is addressed by features that describe when a game move happened, where an earlier move has a lower value than a later move. We calculate such values according to T emporalInfo = T imeStep | RobotGameMo v es | , which is a value between 0 and 1. Fig. 6 shows an example set of features and their values in a particular game, which can have different values in other games due to different sequences of game moves. Table 1 summarises the features given as inputs to our reinforcement learning agents 5 .

## 4.1.2. Multimodal actions

Multimodal actions include 18 or 90 dialogue acts (for N&C 3 × 3 and N&C 9 × 9, respectively), where grid square i = { topLef t , ... , bottomRight } or i = { a 1 , . . . , i 9 } . Rather than training agents with all actions in every environment state, the actions per state were automatically restricted in two ways. First, dialogue acts are derived from the most likely actions, Pr ( a | s ) > 0.001, with probabilities derived from a Naive Bayes classifier trained from example dialogues-see [1] . Second, all game moves were allowed from the subset of those not taken yet (to the robot's knowledge). Table 2 illustrates the set(s) of outputs of reinforcement learning agents.

## 4.1.3. State transitions

The features in every environment state are based on numerical vectors representing the last system and user responses, and game history. The language generator used template-based responses similar to those provided in Table 2 .

## 4.1.4. Rewards

The game-based rewards are as follows:

r ( s , a , s ' ) =        + 5 for winning or about to win the game + 1 for a draw or about to draw in the game -5 for a repeated (already taken) action -5 for loosing or about to loose the game 0 otherwise.

∈

'

j

)

from

D

A

)

Fig. 5. Illustration of our multimodal deep reinforcement learning agent.

<!-- image -->

## Table 1

Feature sets for the standard and ultimate noughts and crosses games corresponding to 9 + 9 + 7 + 39 + 9 = 73 and 81 + 81 + 7 + 39 + 81 = 289 features, respectively. While words and temporal information are continuous features [0 . . . 1] , the remaining ones are binary features [0,1]. For the sake of clarity, assuming that all features are binary, the sizes of state spaces would correspond to 2 73 and 2 289 , respectively-these sizes justify the use of a neural-based approach to scale up to such large state spaces.

Table 2

| Num.    | Feature                                   | Description                                                                          |
|---------|-------------------------------------------|--------------------------------------------------------------------------------------|
| 9 or 81 | [ who = rob ∧ what = draw ∧ where = i ∗ ] | Robot game moves, one for each grid square                                           |
| 9 or 81 | [ who = usr ∧ what = draw ∧ where = j ]   | Human game moves, one for each grid square                                           |
| 7       | [ who = usr ∧ what = command ]            | Robot commands, e.g. gestures                                                        |
| 39      | Words                                     | Presence or absence of uttered words with recognition confidence for human responses |
| 9 or 81 | Temporal Information                      | Game moves with time-based occurrence                                                |

Action set for the standard Noughts and Crosses (N&C) game, where squared brackets denote robot commands such as gestures. A similar set is used for the ultimate N&C game but with a larger set of moves.

| Dialogue Act                    | Multimodal verbalisation                                          |
|---------------------------------|-------------------------------------------------------------------|
| Salutation(greeting)            | 'Hello! [who = rob ∧ what = hello]'                               |
| Provide(name)                   | 'I am Pepper [who = rob ∧ what = please]'                         |
| Provide(feedback = win)         | 'Yes, I won! [who = rob ∧ what = happy]'                          |
| Provide(feedback = loose)       | 'No, I lost. [who = rob ∧ what = no]'                             |
| Provide(feedback = draw)        | 'It's a draw. [who = rob ∧ what = think]'                         |
| GameMove(gridloc = Middle)      | 'I take this one [who = rob ∧ what = draw ∧ where = middle]'      |
| GameMove(gridloc = UpperMiddle) | 'I take this one [who = rob ∧ what = draw ∧ where = uppermiddle]' |
| GameMove(gridloc = LowerMiddle) | 'I take this one [who = rob ∧ what = draw ∧ where = lowermiddle]' |
| GameMove(gridloc = MiddleRight) | 'I take this one [who = rob ∧ what = draw ∧ where = middleright]' |
| GameMove(gridloc = MiddleLeft)  | 'I take this one [who = rob ∧ what = draw ∧ where = middleleft]'  |
| GameMove(gridloc = UpperRight)  | 'I take this one [who = rob ∧ what = draw ∧ where = upperright]'  |
| GameMove(gridloc = LowerRight)  | 'I take this one [who = rob ∧ what = draw ∧ where = lowerright]'  |
| GameMove(gridloc = UpperLeft)   | 'I take this one [who = rob ∧ what = draw ∧ where = upperleft]'   |
| GameMove(gridloc = LowerLeft)   | 'I take this one [who = rob ∧ what = draw ∧ where = lowerleft]'   |
| Request(playGame)               | 'Would you like to play a game? [who = rob ∧ what = asr]'         |
| Request(userGameMove)           | 'your turn [who = rob ∧ what = read]'                             |
| Reply(playGame = yes)           | 'Nice. Let me start.'                                             |
| Salutation(closing)             | 'Good bye!'                                                       |

## 4.1.5. Model architectures

The neural networks consist of fully-connected multilayer neural nets with 5 layers organised as follows: 62 or 207 nodes in the input layer, 100 nodes in each hidden layer, and 18 or 90 nodes in the output layer. The hidden layers use ReLU (Rectified Linear Units) activation functions to normalise their weights. The same learning parameters are used for both games including experience replay size = 10,0 0 0, burning steps = 10 0 0, discount factor = 0.7, minimum epsilon = 0.005, batch size = 2, learning steps = 200 K, and maximum number of actions per dialogue = 100.

## 4.1.6. Simulated interactions

In our simulated dialogues (one per game) and for practical purposes, the behaviour of the simulated opponent is driven by semi-random user behaviour, i.e., from random but legal game

moves. While system actions are chosen by the learnt policies, system responses are sampled from templates (verbalisations seen in demonstration dialogues). In addition, while non-game user actions are sampled from observed interactions in the demonstration dialogues, game user actions are generated randomly from available legal actions in order to explore all possible game strategies.

## 4.2. Integrated system

The humanoid robot 'Pepper' 6 was equipped with the components below running concurrently, via multi-threading. This robot system illustrated in Fig. 5 uses the Naoqi API version 2.5, and has

Fig. 6. Illustration of input features describing temporal information, i.e. when game moves occur - the higher the value the later the game move occurred in a dialogue/game.

<!-- image -->

been fully implemented and tested. Example interactions can be seen in the following videos: https://www.youtube.com/watch?v= 8MqBdkfNl4c and https://www.youtube.com/watch?v=377tVIvd67I . While the former uses handwriting, the latter does not use it due to higher complexity (future work)-instead, the human opponent does the handwriting on behalf of the robot.

## 4.2.1. Interaction manager

The interaction manager, based on the publicly available tools SimpleDS 7 [26] and ConvNetJS 8 , can be seen as the robot's brain due to orchestrating modalities by continuously receiving visual and verbal perceptions from the environment, and deciding what to do next and when based on the learning agents described above. Most actions are multimodal; for example, action GameMo v e ( gridloc = a 1 ) can be unfolded as 'I take this one [ who = rob ∧ what = draw ∧ where = a 1] ', where the square brackets denote a physical action (drawing a circle or cross at the given location).

## 4.2.2. Speech recognition

This component activates the Nuance Speech Recogniser once the robot finishes speaking. Although the targeted games are mostly based on non-verbal interaction, speech recognition results

(words with confidence scores) are used as features in the state space of the deep reinforcement learning agents.

## 4.2.3. Game move recognition

This component receives video frames as input in order to output interpreted game moves as described in Section 3.1 . It gets active as soon as previous verbal and non-verbal commands are executed, and inactive as soon as a game move is recognised. In other words, our robot uses an automatic turn-taking mechanism based on recognised game moves. These vision-based perceptions are used as features in the state space of the deep reinforcement learners.

## 4.2.4. Speech synthesis

The verbalisations in English, translations of high-level actions from the interaction manager, used a template-based approach and the built-in Acapela speech synthesizer. They were synchronised with arm movements, where the next verbalisation waited until the previous verbalisation and arm movements completed their execution.

## 4.2.5. Arm movements and wheel-based locomotion

This component receives commands from the interaction manager for carrying out gestures. We used both built-in arm movements for non-game moves and pre-recorded arm movements

(a)

<!-- image -->

b Representative examples with noise

<!-- image -->

Fig. 7. Example images for training the game move recogniser.

from human demonstrations to indicate game moves. In addition and due to the robot's short arms, it used its omnidirectional wheels to move from an initial location (right in front of the game grid) to the left/right/front/back in order to reach a targeted grid cell in the game-with return to the initial location. While our robot used locomotion for the standard N&C game, it only indicated verbally its game moves in the case of the ultimate N&C game. The latter was due to higher complexity of motor commands and interaction efficiency without locomotion. This game setting required the human player to do the drawings on behalf of the robot.

## 5. Automatic evaluation

## 5.1. Deep supervised learner for character recognition

This classifier labels grayscale images into three classes (nought, cross, nothing) as described in Section 3.1 . The classifier used two sets of images: one set without noise (109 images as shown see Fig. 7 (a)), and the other set with noise (201 images from human

Table 3 Confusion matrix of test results in character recognition.

|         | Nought   | Cross   | Nothing   |
|---------|----------|---------|-----------|
| Nought  | 95.1%    | 0       | 0         |
| Cross   | 4.9%     | 100%    | 0         |
| Nothing | 0        | 0       | 100%      |

writings with partially included grid lines as shown in Fig. 7 (b)). First, the classification accuracy in the data set without noise was 99.9% according to a leave-one-out cross validation. Second, the classifier trained without the noisy data set obtained 74% of classification accuracy when tested in the noisy data set. Third, the classifier trained with the noisy data set obtained 98.4% of classification accuracy when tested in the non-noisy data set-see more details in Table 3 . This is an indication of accurate classification of human handwriting for the targeted game. At the same time though these results suggest that a vision-based clas-

Test results averaged over 30 0 0 games (N&C = Noughts and Crosses) using the baseline and proposed algorithms. 1 and 2 used 100 and 150 nodes per hidden layer, respectively.

Table 4

| Game ModelArch .   | Learning Algorithm              | Average Reward   |   Task Success |   Dialogue Length |
|--------------------|---------------------------------|------------------|----------------|-------------------|
| Standard N&C 1     | DQN-Original (Baseline 1) [2]   | 0.0658           |         0.8258 |             13.93 |
|                    | DQN-Variant (Baseline 2) [1]    | 0.5530           |         0.972  |             13.99 |
|                    | Proposed without Temporal Info. | 0.4710           |         0.9868 |             15.31 |
|                    | Proposed with Temporal Info.    | 0.6300           |         0.998  |             14.06 |
| Ultimate N&C 1     | DQN-Original (Baseline 1) [2]   | - 0.6900         |         0.7873 |             63.54 |
|                    | DQN-Variant (Baseline 2) [1]    | 0.0177           |         0.9074 |             64.82 |
|                    | Proposed without Temporal Info. | 0.0693           |         0.9377 |             63.62 |
|                    | Proposed with Temporal Info.    | 0.1440           |         0.9753 |             52.03 |
| Ultimate N&C 2     | DQN-Original (Baseline 1) [2]   | - 0.1120         |         0.729  |             65.49 |
|                    | DQN-Variant (Baseline 2) [1]    | - 0.0310         |         0.8663 |             70.57 |
|                    | Proposed without Temporal Info. | 0.0997           |         0.9473 |             59.69 |
|                    | Proposed with Temporal Info.    | 0.1640           |         0.9846 |             52.09 |

Fig. 8. Learning curves of DQN-based agents for playing Standard Noughts and Crosses.

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Proposed with Temporal Info.

<!-- image -->

sifier should be retrained in case substantially different images are observed.

## 5.2. Deep reinforcement learners for game playing

We compare our proposed algorithm described in Section 3.2 against two baselines [1,2] in the domain of Noughts and Crosses (N&C) with two variants. Figs. 8 and 9 show learning curves for the baseline agents (see top plots (a) and (b)), and agents using the proposed algorithm (see bottom plots (c) and (d)). All agents report results over 400 K learning steps or 20 K games-whatever occurred first. We use four metrics to measure system performance: average reward, learning time 9 , average task success [0 . . . 1] (win/draw rate), and avg. dialogue length (avg. number of actions per game). Results can be seen as the higher

the better in avg. reward and avg. task success, and the lower the better in training time and dialogue length.

Our results show that our proposed algorithm can train more successful agents than previous work. This is evidenced by higher task success in plots (c) and (d) vs. (a) and (b), and lower dialogue length in plots (c) and (d) vs. (a) and (b).

We tested the performance of the learnt policies over 30 0 0 games for each of the three agents per game and per architecture (100 vs. 150 nodes per hidden layer), obtaining the results shown in Table 4 . It can be noted that indeed the proposed learning algorithm performs better than the baseline algorithms, across games and model architectures. These results also suggest that there is room for hyperparameter optimisation in future work. Nonetheless, these results suggest that our proposed algorithm can be used for training agents with competitive behaviour in social games.

(b) DQN-Variant (baseline 2)

<!-- image -->

Fig. 10. Robot's training, test, and deployment environments.

<!-- image -->

<!-- image -->

DQN-Original (baseline 1)

<!-- image -->

<!-- image -->

(c Proposed without Temporal Info.

<!-- image -->

Proposed with Temporal Info.

Fig. 9. Learning curves of DQN-based agents for playing Ultimate Noughts and Crosses.Training and test environment

<!-- image -->

(b) Deployment environment.

## 6. Human evaluation

We trained and tested our robot system in an office environment, and deployed it in a partially-known environment (atrium of a University building)-see Fig. 10 . This evaluation ran for four nonconsecutive days where the robot played against 29, 64, 27 and 10 human opponents, respectively. The first three days involved only the Standard N&C game, and the latter involved both games (Standard N&C and Ultimate N&C). These human-robot games included primary and secondary school children, prospective university students, and parents-they were visitors to the building rather than traditional experiment participants (no questionnaires involved). While our best interaction policies (but without temporal information) were used over all games across days, the game move recogniser evolved due to improvements after each deployment day. The improvements consisted in better game grid detection and hand detection, which reduced misrecognised game moves as follows: 31% on day one, 25% on day two, 22%, and 10% on day four. In games without misrecognitions the robot ended up winning or in

a draw. Algorithm 1 describes our game move recogniser after such improvements, which has been used in both games (Standard and Ultimate Noughts and Crosses) and can be applied to other social games beyond those in this article. Algorithm 2 has also been used in both games and can be applied to other social games and domains beyond those in this article. The reason for this is because this algorithm is general enough, not only applicable to games, as long as there is a notion of success and failure (in our case win or loose) at the end of a dialogue, where the same reward function can be applied or extended.

Although the partially-known conditions exhibited in the deployment environment were challenging for our interactive multimodal robot (e.g., different light conditions, multiple visual backgrounds, two human opponents instead of one, among others), its human opponents continuously expressed to be impressed - presumably due to the fact that the robot was speaking, moving, writing, listening, and seeing-all of these autonomously, with near real-time execution, and in a coordinated way.

DQN-Variant (baseline 2)

<!-- image -->

<!-- image -->

<!-- image -->

Learring Steps (no. experiences)

## 7. Conclusion and future work

This article presents a deep learning-based approach for efficiently training deployable robots that can carry out joint multimodal activities together with humans that involve speaking, listening, gesturing, and learning. Although the proposed approach assumes no initial training data, it is bootstrapped with relatively small datasets, and it learns interactive behaviour from trial and error using simulations. This approach is demonstrated using the game of Noughts and Crosses (N&C) with two variants. Even when these two variants exhibit different degrees of complexity, the data requirements remained equivalent. In other words, the more complex task (Ultimate N&C) did not require more data than the simpler task (Standard N&C). Given the generality of the approach and proposed algorithms, they can also be applied to other tasks beyond N&C. An automatic evaluation shows that our deep supervised and reinforcement learners achieve high performance in both game move recognition and task success. In the latter, anticipating the effects of the decision making and temporal information proved essential for improved performance. Our experimental results with 130 human participants showed that when the visionbased perception works as expected, successful human-robot interactions can be obtained from the induced skills using the proposed data-efficient approach.

Example avenues for future related works are as follows.

- · Extending the robot's language skills for larger-scale language interpretation [27] and language generation [28] coupled with visual perception, multimodal interaction and motor commands remains to be investigated. In addition, the conversational behaviour of the robot can be framed within a chatbot approach [23] in order to deal with the out-of-domain responses pointed out by Bohus et al. [20] .
- · A comparison of the temporal information approach used in this article versus an approach based on recurrent neural networks (as in [29] ) remains to be investigated, identifying pros and cons of each approach.
- · Another interesting extension to this work is online training [30,31] , which can be investigated for improving the performance of both supervised and reinforcement learning by for example retraining them after each game. For this, it should be taken into account that the reinforcement learner requires longer training times than its supervised counterpart, where faster training algorithms can be investigated by combining ideas from [32-38] .
- · Another interesting extension is learning to write as in [39,40] . While handwriting was relatively straightforward for the standard Nought and Crosses game, handwriting for the ultimate Noughts and Crosses game was a challenge due to more fine grained motions of smaller characters in smaller grid squares on the game board. This extension requires visuomotor learning in order to achieve human-like handwriting.
- · Our policy learning algorithm with 1 step look-ahead information could be explored with multiple time steps, leading to combinations of deep reinforcement learning policies with MinMax [41] or Monte Carlo tree search methods [42] . But there is a trade-off because the more look-ahead information is used, the more computational expense is involved. The training times of our proposed algorithm vs. the original DQN method are equivalent, and the quality of policies are substantially different-ours much more competitive. In addition, our policy learning algorithm can be applied for optimising robot behaviour that goes beyond always winning. Instead, it could be used to train multimodal robots that keep players as happy as possible. This would require adding further multimodal actions (for varied behaviour rather than repetitive or monotonous be-
- viour across games) and also keeping track of emotion-based signals, which can be taken into the reward function for policy retraining.
- · The proposed algorithms can be applied to other social games (possibly more complex) and also other domains. A robot at home for example should not only be expected to be able to play games, but also to carry out other tasks-potentially trained with the same algorithms across tasks.
- · More real-world evaluations are needed across the field to truly and thoroughly assess the performance of human-robot interactions in the wild, out of the lab [16-18] . The robot system described in this article would not have been possible without the participation of unseen humans playing against the robot.

## Conflict of interest

None

## Acknowledgement

The robot used in this paper was donated by the Engineering and Physical Sciences Research Council ( EPSRC ), U.K. This work was carried out under ethical approval by the University of Lincoln with reference UID CoSREC396 .

## References

- [1] H. Cuayáhuitl , Deep reinforcement learning for conversational robots playing games, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2017 .
- [2] V.E.A. Mnih , Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529-533 .
- [3] K. Noda , H. Arie , Y. Suga , T. Ogata , Multimodal integration learning of robot behavior using deep neural networks, Robot. Auton. Syst. 62 (6) (2014) 721-736 .
- [4] A.H. Qureshi , Y. Nakamura , Y. Yoshikawa , H. Ishiguro , Robot gains social intelligence through multimodal deep reinforcement learning, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2016, pp. 745-751 .
- [5] H. Cuayáhuitl , G. Couly , C. Olalainty , Training an interactive humanoid robot using multimodal deep reinforcement learning, in: Proceedings of the NIPS Workshop on the Future of Interactive Learning Machines, 2016 .
- [6] S. Wermter , C. Weber , M. Elshaw , C. Panchev , H.R. Erwin , F. Pulvermüller , Towards multimodal neural robot learning, Robot. Auton. Syst. 47 (2-3) (2004) .
- [7] J. Ngiam , A. Khosla , M. Kim , J. Nam , H. Lee , A.Y. Ng , Multimodal deep learning, in: Proceedings of the International Conference on Machine Learning ICML, 2011 .
- [8] N. Srivastava , R. Salakhutdinov , Multimodal learning with deep Boltzmann machines, J. Mach. Learn. Res. 15 (1) (2014) .
- [9] S. Levine, C. Finn, T. Darrell, P. Abbeel, End-to-end training of deep visuomotor policies, CoRR (2015) arXiv: 1504.00702 .
- [10] D.C. Bentivegna, C.G. Atkeson, A. Ude, G. Cheng, Learning to act from observation and practice, Int. J. Hum. Robot. 01 (04) (2004) 585-611, doi: 10.1142/ S02198436040 0 0307 .
- [11] E. Short , J. Hart , M. Vu , B. Scassellati , No fair!!: an interaction with a cheating robot, in: Proceedings of the International Conference on Human-Robot Interaction (HRI), ACM, 2010, pp. 219-226 .
- [12] J. Kober , M. Glisson , M. Mistry , Playing catch and juggling with a humanoid robot, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2012 .
- [13] T.E.A. Belpaeme, Multimodal child-robot interaction: building social bonds, J. Hum.-Robot Interact. 1 (2013) 33-53, doi: 10.5898/JHRI.1.2.Belpaeme .
- [14] M. Kim , K. Suzuki , Comparative study of human behavior in card playing with a humanoid playmate, Int. J. Soc. Robot. 6 (1) (2014) 5-15 .
- [15] E.I. Barakova, M.D. Haas, W. Kuijpers, N. Irigoyen, A. Betancourt, Socially grounded game strategy enhances bonding and perceived smartness of a humanoid robot, Connect. Sci. 30 (1) (2018) 81-98, doi: 10.1080/09540091.2017. 1350938 .
- [16] M. Jung, P. Hinds, Robots in the wild: a time for more robust theories of human-robot interaction, ACM Trans. Hum.-Robot Interact. 7 (1) (2018) 2:12:5, doi: 10.1145/3208975 .
- [17] H. Cuayáhuitl , Robot learning from verbal interaction: a brief survey, in: Proceedings of the Fourth International Symposium on New Frontiers in HRI, 2015 .
- [18] H. Cuayáhuitl, K. Komatani, G. Skantze, Introduction for speech and language for interactive robots, Comput. Speech Lang. 34 (1) (2015) 83-86, doi: 10.1016/ j.csl.2015.05.006 .
- [19] S. Thrun , et al. , Probabilistic algorithms and the interactive museum tour-guide robot minerva, Int. J. Robot. Res. 19 (11) (20 0 0) 972-999 .

- [20] D. Bohus , C.W. Saw , E. Horvitz , Directions robot: In-the-wild experiences and lessons learned, in: Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '14, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2014, pp. 637-644 .
- [21] H. Cuayáhuitl , SimpleDS: A simple deep reinforcement learning dialogue system, in: Proceedings of the International Workshop on Spoken Dialogue Systems (IWSDS), 2016 .
- [22] Y. LeCun , L. Bottou , Y. Bengio , P. Haffner , Gradient-based learning applied to document recognition, in: Proceedings of the IEEE, 86, 1998, pp. 2278-2324 .
- [23] H. Cuayáhuitl , D. Lee , S. Ryu , S. Choi , I. Hwang , J. Kim , Deep reinforcement learning for chatbots using clustered actions and human-likeness rewards, To appear in proc. of the International Joint Conference on Neural Networks (IJCNN), 2019 .
- [24] R.S. Sutton , A.G. Barto , Reinforcement learning - an introduction, Adaptive computation and machine learning, 2nd ed., MIT Press, 2018 .
- [25] C. Szepesvári , Algorithms for Reinforcement Learning, Morgan and Claypool Publishers, 2010 .
- [26] H. Cuayáhuitl, SimpleDS: A Simple Deep Reinforcement Learning Dialogue System, Springer, Singapore, pp. 109-118, doi: 10.1007/9789811025853 \_ 8 .
- [27] L. Deng , Y. Liu , Deep Learning in Natural Language Processing, Springer Singapore, 2018 .
- [28] N. Dethlefs, Domain transfer for deep natural language generation from abstract meaning representations, IEEE Comp. Int. Mag. 12 (3) (2017) 18-28, doi: 10.1109/MCI.2017.2708558 .
- [29] A.H. Qureshi, Y. Nakamura, Y. Yoshikawa, H. Ishiguro, Show, attend and interact: perceivable human-robot social interaction through neural attention qnetwork, CoRR (2017) arXiv: 1702.08626 .
- [30] B. Settles , Active Learning, Morgan & Claypool Publishers, 2012 .
- [31] H. Cuayáhuitl , N. Dethlefs , Dialogue systems using online learning: beyond empirical methods, in: Proceedings of the NAACL-HLT Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, 2012 .
- [32] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A.D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V. Mnih, K. Kavukcuoglu, D. Silver, Massively parallel methods for deep reinforcement learning, CoRR (2015) arXiv: 1507.04296 .
- [33] V. Mnih, A.P. Badia, M. Mirza, A. Graves, T.P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, CoRR (2016) arXiv: 1602.01783 .
- [34] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized experience replay, CoRR (2015) arXiv: 1511.05952 .
- [35] F.S. He, Y. Liu, A.G. Schwing, J. Peng, Learning to play in a day: faster deep reinforcement learning by optimality tightening, CoRR (2016) arXiv: 1611.01606 .
- [36] T.D. Kulkarni , K. Narasimhan , A. Saeedi , J. Tenenbaum , Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation, in: Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2016 .
- [37] H. Cuayáhuitl , S. Yu , A. Williamson , J. Carse , Scaling up deep reinforcement learning for multi-domain dialogue systems, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2017 .
- [38] H. Cuayáhuitl , S. Yu , Deep reinforcement learning for multidomain dialogue systems using less weight updates, in: Proceedings of the INTERSPEECH, 2017 . [39] K. Gregor , I. Danihelka , A. Graves , D.J. Rezende , D. Wierstra , DRAW: a recurrent neural network for image generation, in: F.R. Bach, D.M. Blei (Eds.), Proceedings of the Thirty-second International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, JMLR Workshop and Conference Proceedings, 37, JMLR.org, 2015, pp. 1462-1471 .
- [40] D. Bullock, S. Grossberg, C. Mannes, A neural network model for cursive script production, Biol. Cybern. 70 (1) (1993) 15-28, doi: 10.10 07/BF0 0202562 .
- [41] D. Kalles , P. Kanellopoulos , A minimax tutor for learning to play a board game, in: Proceedings of the ECAI Workshop on Artificial Intelligence in Games (AIG@ECAI), 2008 .
- [42] D. Silver , A. Huang , C.J. Maddison , A. Guez , L. Sifre , G. van den Driessche , J. Schrittwieser , I. Antonoglou , V. Panneershelvam , M. Lanctot , S. Dieleman , D. Grewe , J. Nham , N. Kalchbrenner , I. Sutskever , T. Lillicrap , M. Leach , K. Kavukcuoglu , T. Graepel , D. Hassabis , Mastering the game of go with deep neural networks and tree search, Nature 529 (7587) (2016) 4 84-4 89 .

<!-- image -->

- Dr. Heriberto Cuayáhuitl is a Senior Lecturer in Computer Science in the College of Science at the University of Lincoln, and member of the Lincoln Centre for Autonomous Systems (L-CAS). He received a Ph.D. from the University of Edinburgh in 2009, and has an international research profile in academia and industry in the discipline of machine intelligence including (spoken) language processing, (deep) machine learning, (multimodal) dialogue systems and robotics. He has published over 70 research papers in these areas, has been led organiser of the international workshop on Machine Learning for Interactive Systems (MLIS), and guest editor of the journals ACM Transactions on Interactive Interactive Systems and Else-

vier Computer Speech and Language. His work in industry has been carried out at SpeechWorks International Inc. (now Nuance Communications Inc.), the German Research Center for Artificial Intelligence (DFKI), and Samsung Research.