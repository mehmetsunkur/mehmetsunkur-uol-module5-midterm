Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
LSEVIER
Contents lists available at ScienceDirect
# Entertainment Computing
journal homepage: www.elsevier.com/locate/entcom
Entertainment Computing
Playing first-person shooter games with machine learning techniques and methods using the VizDoom Game-AI research platform
T 
# Adil Khana,b,⁎
# , Muhammad Naeema, Muhammad Zubair Asgharc, Aziz Ud Dinb, Atif Khand
a Department of Computer Science, University of Peshawar, KP, Pakistan
- b Department of Computer Science, SZIC, University of Peshawar, KP, Pakistan
- c Institute of Computing and Information Technology, Gomal University, D. I. Khan, KP, Pakistan
- d Department of Computer Science, Islamia College, Peshawar, KP, Pakistan
# A R T I C L E I N F O
A B S T R A C T
Keywords: Artificial Intelligence Artificial Neural Network Autonomous Systems Computational Intelligence Intelligent agents Visual Deep Reinforcement Learning
# Machine Learning
Artificial Intelligence in the form of machine learning is employed in games to control non-human computer- players, agents or bots. However, most of these games such as Atari took place in 2D environments that were not fully observable to the agents. Currently, it is of extreme significance to employ such machine learning tech- niques and methods in 3D environments such as Doom. Therefore, In this paper, we train agents on the health gathering scenario of the classical first-person shooter game Doom by first presenting the Direct Future Prediction to train an agent that uses a simple architecture with no additional supervisory signals, then differ- entiate and compare the performance of the agents trained by using several different machine learning tech- niques, and the AI reinforcement learning platform ‘VizDoom’, a 3D partially observable environment, with interesting enhanced properties that makes agents to stand out from inbuilt AI agents and human players. We have continued to use computer games as a benchmark for the performance of AI as having been so successful in the past. We also compared the results of our findings to conclude the performance of the agents trained with different machine learning techniques. The agents performed well against both human players and inbuilt game agents.
- 1. Introduction and research motivation
In the last few decades, due to the progress in artificial intelligence, a revolution and sudden change has been observed in the technology both in hardware and software. This change is seeping and taking over in our lives up to a certain extent, affecting how we live, work and entertain ourselves such as employing domestic robots servants, healthcare uses, electronic trading, remote sensing, expert systems, traffic control systems, autonomously-powered self-driving vehicles, and from behavioural algorithms to suggestive searches, etc. In the same way, gaming is a widely recognized part of our cultural landscape and as old as our human ancestors. The earliest computers were very slow and the interaction with the user was limited to basic principles. In the early '1940 s, computers evolved, and programmers commenced to develop new virtual worlds and surprising ways of interaction between the user and the machine. But now due to advancements in technology such as GPU’s [1], TPU’s [2] and the revolution in deep neural networks [3] it has become possible for artificial intelligence to step-in in video
be more specific a huge amount of multidimensional data is required to be processed and executed [4]. In the recent past, machine learning techniques and methods were employed in Atari games for training agents, where later, the agents performed on 49 different Atari games with better and improved results. However, most of these Atari games took place in 2D environments that were fully observable to the agents [5]. Currently, it is of extreme significance to employ such machine learning techniques and methods in 3D environments such as Doom [6] a first-person shooter game shown in Fig. 1, StarCraft [7] a third-person shooter game based on real-time strategies, and sandbox open-world games such as Grand Theft Auto V [8] and Minecraft [9] because the research community in AI think and consider that computer video games are the best test-beds for testing different artificial intelligence techniques, methods, and algorithms before evaluating them in the real world. Thus, in this paper, state-of-the-art machine learning techniques that were before partially tested in 2D environments are now employed in a 3D environment known as Doom, to train, differentiate and com- pare agents performances, such as advantage actor-critic (A2C) [10],
games as well where massive graphical data in the form of frames, or to
advantage actor-critic long short-term memory (A2C-LSTM) [11],
⁎
* Corresponding author.
Corresponding author.
E-mail addresses: adil ©uop.edu.pk, Dradil@hit.edu.pk (A. Khan).
# E-mail addresses: adil@uop.edu.pk, Dradil@hit.edu.pk (A. Khan).
# https://doi.org/10.1016/j.entcom.2020.100357
https://doi.org/10.1016/j.entcom.2020.100357
Received 2 April 2019; Received in revised form 14 January 2020; Accepted 15 February 2020 Available online 19 February 2020
# 1875-9521/ © 2020 Published by Elsevier B.V.
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
HEALTH FrAG
Fig. 1. A sample screen from Doom showing the first-person perspective.
asynchronous advantage actor-critic (A3C) [12], Deep Q-network (DQN) [13], Deep recurrent Q-network (DRQN) [14], Double deep Q- network (DDQN) [15], C51-DDQN [16], Dueling deep Q-network (DDQN) [17], and Reinforce [18] whereafter applying them most of the agents are found useful and effective. In addition, this paper presents one of the 4 best techniques that performed well on the VizDoom Game- AI research platform [19]. It was suggested that making such research available is beneficial for the community researching on first-person shooter games which may set up a base for further research and im- provement.
# 1.1. The academic motivation of AI in games
Why use machine learning (AI) techniques and algorithms to re- search on games? because the future belongs to artificial intelligence in games, particularly machine learning has immense potential and role in games designing and development. The possibilities abound, however, the challenges are also innumerable. Without a doubt, game develop- ment will experience a proliferation of these machine learning con-
cepts, which is only a matter of time.
In addition, the more primary use of AI in games is to train games agents or bots in an intelligent way so that they could perform and act intelligently similar to human being’s. By achieving such objectives, it creates more fun, challenge, and understanding to human players as for as playing or interacting with games is experienced and concerned. While playing against skilful human players AI agents or bots need to understand what a player does and how a player feels during the play. To gauge and enhance AI agent’s vs human player’s in-game experience, machine learning scientists, practitioners, games researchers, and de- velopers use machine learning methods, such as reinforcement learning, supervised learning like support vector machines or neural networks to build and train the models to make them more effective and intelligent. Such advancements are particularly significant for the
OpenAI gym, Unity and VizDoom which is based on first-person shooter (FPS) game Doom used for visual deep reinforcement learning from raw screen pixels in 3D game environments. The speed of the learning agent greatly depends on the number of frames the agent is permitted to skip. In other words, the frame skipping rate influences the agent’s learning and final performance greatly particularly using deep Q-learning, ex- perience replay memory, and the VizDoom Game AI research platform. The agents can be trained and tested on several of Doom’s scenarios or maps in order to obtain good results and compare them with the ex- isting state-of-the-art research work on Doom-based AI agents. So far the experiments performed on Doom’s scenarios demonstrate that the profitable and optimal frame skipping rate falls in the range of 3 to 11 that provides the best balance between the learning speed and the final performance of the agent which exhibits human-like behaviour and outperforms an average human player and inbuilt game agents [20].
Moreover, there is a lot of existing research that relate to playing FPS games with visual deep reinforcement learning [21] such as the work introduced in [22], in which a method is presented to augment the models to exploit game feature information such as the presence of enemies or items. Similarly, another work described in [23] in which a competitive agent is proposed that is trained on the Doom’s basic sce- nario(s) in the same semi-realistic 3D environment VizDoom using convolutional deep learning with Q-learning [24] that considers only the screens raw pixels for exhibiting agent’s usefulness. The era of re- search using games changed when agents were trained using only the screen raw pixels.
Another more related work is proposed in [25] in which there is supposedly no reward signal (like there is in Atari games via the score). Instead, the authors used matching future state (measurements) pre- dictions as a replacement to a reward signal. However, Probably, it should be noted that there is a constant health reduction in e.g. the basic room (walking on the radioactive ground) kind of resemblance to typical reinforcement learning r = −1 reward for each step when
# progress and development of computer video games industry.
# 2. Research on Doom using the VizDoom Game-AI research
# platform
These days, game AI is one of the focused and active research areas in artificial intelligence as computer games are the best test-beds for testing theoretical ideas in AI before practically applying them in the real world. In this regard, many Game AI research platforms are fa- miliarized for research on computer video games such as DeepMind,
trying to reach a goal state quickly.
Sometimes Reinforcement Learning [26] environments with dis- crete actions are not getting it properly. They don't simulate human muscles easily. In other words, it’s not easy for human players to wiggle the joystick at 114 microseconds between left and right as muscles get tired soon. Moreover, the author's proposed network doesn't employ LSTMs [27] to memorize transactions due to their proposed approach as the second-best agent in the visual Doom AI competition used LSTM but its simple feedforward architecture was 50% efficient.
In addition, each year a visual doom AI competition is organized for
2
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
evaluating AI agents on two different tracks: limited death-match on a known map and a full death-match on an unknown map by using ma- chine learning techniques. In this paper, agents are trained, tested, and compared using different machine learning techniques and methods. The methodology and experimental works are presented in several sections as follows.
# 2.1. Basic objective
The purpose of the experiments is to train competent, well balanced and robust agents using machine learning techniques such as re- inforcement learning and supervised learning [28] that can adapt to learn, act in complex and dynamic 3D environments. Such agents ac- cept raw sensory input and core measurements to show that employed techniques are better at outperforming human players and other inbuilt game agents on the ‘health gathering scenario(s)’ of the VizDoom platform where only a few limited actions are allowed. Besides, the purpose also includes to prove, differentiate and compare several ma- chine learning techniques by training agents on the VizDoom health gathering scenario(s).
2.4. The environment used for the experiments
All the experiments are performed in Pycharm 2017.2 professional version using ViZDoom 1.1.5, Tensorflow 1.5.0 [31], Keras 1.2.2/2.0.5, OpenCV 3.3 [32], CMake 2.8+, GCC 4.6+, and Python 3.6 (64-bit) with Numpy on an Ubuntu Server 16.04.3 LTS Operating System with Intel® Core™ i7-7700 CPU @3. 60 GHz × 8 and NVIDIA GeForce GTX 1080/PCIe/SSE2 powerful GPU machine for processing the CNN’s. The agents are trained for thousand to millions of steps consisting of per- forming actions, observing transitions, and updating the networks. The hyperparameter settings for all the experiments listed in Table 2. are met and found after hundreds of runs. The parameters are kept tuned until the models were found converged accurately and properly to meet the desired or expected results. The discount factor is set to γ = 0.99 for almost all algorithms, the learning rate α varied from 0.001 to 0.0001, Experience replay buffer memory capacity is set from 50,000 elements to 60,000, the screen-buffer is set to 640, 480 and remain the same for almost all of the algorithms, the batch-size is set to 32 and 64 for some algorithms, initial decay varied from 0.9 to 1, the final decay is set from 0.001 to 0.0001 and frame-per-action to 4 and 5. The overall learning and testing process is measured by the number of hours it takes to complete on a set of powerful GPU machines.
# 2.2. VizDoom health gathering environment (scenario)
We demonstrate implementing DFP on the health gathering scenario shown in Fig. 2, provided by the VizDoom Game-AI research platform. The main objective of the agent is to survive as long as possible. However, at each time step, the agent’s health decreases, so in order to live longer, the agent has to locate and pick up the health packs scat- tered across different parts of the environment or map. At the same time, the agent also needs to avoid running into poison jars which will take away its health. There are no active opponents in this simple health gathering scenario.
# 2.3. Machine learning approaches
Several Artificial Intelligence approaches are used to train agents such as using policy optimization [29] value optimization [30], and DFP as shown in Fig. 3. In this paper, similar to traditional reinforce- ment learning, we suggest and compare training agents that learn through the response provided by interacting with the environment
- 3. Direct future prediction (DFP)
Direct future prediction (DFP) is a machine learning technique and it has one of the major ability and benefit to pursuing complex goals at test time. Normally, in reinforcement learning settings, learning is guided by a series of scalar reward signals [33], but in complex en- vironments, the scalar rewards can be sparse and delayed, which means that sometimes it is not easy to tell which action or sequence of actions are responsible for a particular positive reward that happens several time-steps later, this problem is known as credit assignment [34]. Be- sides rewards, if the environment provides some kind of rich and temporally dense multidimensional feedback, for example, measure- ments like kills, health, ammunition levels in a first-person shooter game, the agent can be programmed to learn to predict such rich and temporally dense measurements feedback instead [35]. It is possible for agents at inference time to observe the effects of different actions on such measurement streams and choose the action that maximizes an objective that can be expressed as a function of the predicted mea-
using machine learning techniques.
surements at time ‘t’ (i.e. mt), for example, if the scenario(s) of the first-

Fig. 2. The Health gathering scenario(s) used in the experiments.
3
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357

Fig. 3. Machine Learning Approaches.
person shooter game (FPS) Doom is considered, and if the predicted measurements vector is (Kills, Ammo-used, Health) and the objective is
to maximize the number of kills, then the objective can be set as,
U = f(m,) = g-m; = 1 x Kills — 0.5 x Ammo used + 0.5 x Health
where g = [1, −0.5, 0.5] is known as the goal vector. The −0.5 wt assigned to the Ammo-used measurement just informs the agent it’s not good to waste ammo. Such an approach has two major benefits that are
as follows,
(1)
to saying pick the action which will lead to an increase in expected kill
# counts.
Further, if the health-level drops below a particular threshold, a different objective to the agent can be assigned so that it could focus on
picking up health packs in order to improve health and avoid dying.
U = 0x kills + 0 x Heath + 1 x Health Packs
Under this goal vector [0, 0, 1], the agent concentrates obsessively to pick up health packs. Once the health level goes back to normal, the goal vector can be switched back to [1, 0, 0] so that the agent could
start killing again.
3.1. To stabilize and accelerate the training
Dealing supervised learning [36], with concrete labels such as multidimensional measurements attached to each input state for ex- ample pixels’ input, the agent is able to learn from a richer and denser signal than a single scalar reward stream can provide. Training per- formance can be greatly enhanced and stabilized as a result, just like in typical supervised learning tasks such as image classification.
The ability to pursue complex goals at inference time has great implications for reinforcement learning, so a truly intelligent agent needs to be able to adapt itself to different goals under different cir- cumstances [39]. However, these days most traditional reinforcement learning methods limit learning to only a single objective following the guidance of the scalar reward which is not the true way of learning for intelligent agents to behave. In fact, similar to human beings, re- inforcement learning agents need to possess the innate ability to switch
goals based on different circumstances [40].
# 3.2. Complex goals at inference time
It is one of the more interesting aspects of this approach. In tradi- tional reinforcement learning the objective is to maximize the expected future rewards, to be more specific, it implies that the agent only knows how to act based on the objective given. The agent cannot be simply instructed to behave differently (i.e. with another objective) in any
meaningful sense at inference time [37].
As in traditional reinforcement learning, responses are received in the form of scalar rewards. In the same way, in DFP responses are re- ceived in the form of measurements (m) which can be thought of as a multidimensional vector with each element capturing some aspects of
the game e.g. kills, ammunition, health, etc.,
Let [τ1…, τn] be a set of temporal offsets that the model has to learn to predict the differences between future and present measurements can
be formulated as follows:
In contrast, the presented supervised learning approach enables the agent to flexibly pursue different objectives (i.e. goals) or a combina- tion of multiple objectives at inference time. It can be achieved through the model under supervised learning settings that outputs the predic- tion of measurements. The objective can be basically expressed as a
# function of the predicted measurements.
[Myc — My Men — M] (4)
It is beneficial to use [1,2,4,8,16,32] as the set of temporal offsets. In practice, the model outputs a set of ‘f’, one for each action. At in- ference time, the agent simply picks the action that maximizes the
objective U that can be computed as follows:
In an FPS game, the environment provides at least three measure- ments for every time step (Kills, Health, Health Packs). A health pack is a box scattered around the environment that can be picked up by the agent to improve its health. Health Packs measures the number of health packs picked up by the agent, to be more specific, it is a rea- sonable objective to simply tell the agent to maximize the number of kills [38],
U= gf
Here ‘g’ denotes the goal vector that controls the behaviour of the agent, the scalar reward is used as the only measurement and set ‘g’ as a vector of discounted factors i.e. (1, γ, γ 2, ….…..) then the resulting objective function resembles the Q value, which is the sum of dis- counted future rewards. Therefore, sometimes in a sense, DQN is va-
U = 1x kills + 0 x Heath + 0 x Health Packs
The coefficients of the measurements i.e. [1, 0, 0] represent the goal vector (g). Then at each time step, the agent will pick the action i.e.
Turn Left, Turn Right, or Shoot that maximizes U, which is equivalent
(2)
guely viewed and considered as a special case of DFP.
The network model used in the implementation consists of three inputs modules i.e. a perception module S(s), a measurement module M (m) and a goal module G(g) as shown in Fig. 4. If ‘s’ is an image, then
the perception module ‘S’ is implemented as a convolutional neural
4
(3)
(4)
(5)
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Input Image m. ük rae Measurements(m) FCNN m) m E = = EXPECTATION > de) ep ACTION Ah x Ag e0 p Duplicate Prediction Target Action Taken f Normalize
Fig. 4. DFP Neural Network Architecture.
network. The measurement and goal modules are fully-connected net- works. The outputs of the three input modules are concatenated, forming the joint input representation (j) which is used for subsequent
# processing:
(6)
The model consists of two streams, the expectation stream E(j) and the Action Stream A(j). Usually using such two separate streams leads to better performance and this approach is based on the dueling ar-
chitecture introduced by Google Deep Mind [41].
Poison are derivative measurements from health. This is quite sur- prising and unreasonable that the performance of DFP deteriorates by 50% if just ‘health’ is used as the only measurement, even though health packs and poisons are derived from the change in health. Further, there is a beneficial effect by allowing the model to generate a richer set of predictions, similar to the way auxiliary tasks enhance the performance of deep learning vision classifier [42]. One of the best things about DFP is its capability to pursue different goals at inference time. For illus- trating, the trained model can be used and the goal vector ‘g’ can be altered from (1, 1, −1) to (0, 0, 1). Where the objective becomes as
While training, each transition produces a tuple (s, a, m) using the environment. where ‘s’ represents the state e.g. image pixels, ‘a’ is the action taken, and ‘m’ is the measurement. A training target ‘f’ can then be formulated using the measurements obtained at the specified tem- poral offsets τ i.e. [1,2,4,8,16,32],
below,
U = 0x Health + 0 x Health Packs + 1 x Poison
# 3.5. Experimental setup
(7)
# f=
The target can be used to train the neural network model with backpropagation and Mean Square Error (MSE) can be used as the loss
function to compute the error.
The experimental setup includes the environment and the archi-
tecture of the neural network explained as follows.
# (a) Neural network architecture
3.3. Supervised learning for reinforcement learning
The measurement and goal input modules are found less useful or in other words, found slightly detrimental to the performance so it is decided to use only the perception module as inputs to the model. The measurements are normalized and a goal vector ‘g’ of [1, 1, −1] i.e. coefficients for (Health, Health Packs, Poison) measurements are used. There are almost 50,000 episodes of DFP ran on the health gathering
# scenario.
DFP excels in environments where a stream of rich and temporally dense multidimensional feedbacks are available. In traditional re- inforcement learning settings, transforming the feedbacks into a single dimension scalar reward might result in loss of useful information which would detriment performance. It is also noted out that enrich- ment in measurements is the most important factor for the good per-
# formance of DFP.
3.4. Using health as the measurement
The neural network architecture consists of three input modules; the perception module is the environment state which is just screen pixels (input image). The three-layered convolutional neural network is used as the feature extractor to transform the screen pixels into a vector of length 512 and the three-layered fully connected network is used to parse the measurement module and goal module. The outputs of the three modules are concatenated to form a joint representation for fur- ther processing. The model is then split into two streams, the ex- pectation stream, and the action stream. Their respective outputs are summed to form the model’s prediction. In our proposed method the measurement size is three i.e. (health, health packs, poison), a number of time steps are 6 i.e. (1, 2, 4, 8, 16, 32) and the action size is three i.e. (Turn Left, Turn Right, and Move Forward). The model is trained and compiled using Adam optimizer [43] and the mean squared error (MSE) [44] as the loss metric. Similar to other reinforcement learning algo- rithms, most of the logic is contained in the update step. First, a mini- batch is sampled of sample trajectories from the experience replay buffer (memory) and initialize the corresponding states, measurements, goal and targets variables. Then target is computed for the model which
Health is considered in the implementation, as health packs and
is the difference between future and present measurements for the set of
5
(8)
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
# Table 1
# Test Setup Hardware Specification.
# 5. Advantage Actor-critic (A2C) and advantage Actor-Critic- long Short-Term memory (A2C-LSTM)
CPU Intel® Core™ i7-7700 CPU @3.60 GHz × 8 GPU NVIDIA GeForce GTX 1080/PCIe/SSE2 GPU RAM GiB DDR4
temporal offsets (1, 2, 4, 8, 16, 32). The target for each action is as- signed to the ‘f_action_target’ variable. The rest of the variables are filled up with the mini-batch samples drawn from the experience re- play. f_target is the ground truth label assigned to the model for training. Further, the training routine needs a call to perform gradient descent update. The overall learning and testing process using DFP lasted for 12 h on a powerful GPU machine whose specifications are
described in Table 1.
4. Direct future prediction (DFP) and asynchronous advantage Actor-Critic (A3C)
As the Direct Future Prediction (DFP) is explained in section 2 which is good at pursuing complex goals at test time. The architecture of DFP is shown in Fig. 4 and its performance is presented in a graph displayed in Fig. 5. With a performance of almost 95%, DFP can be chosen as one of the best machine learning technique for training game agents or bots. While on the other hand, the architecture of A3C utilizes the power of the deep neural networks (DNN) [45] by running multiple agents for training at the same time. Each agent then shares its results with the other agents. Since every agent makes different decisions, this approach reduces the chance for the AI to run into a local minimum. Additionally, it drastically reduces the average training time required to perform decently well at any given task. The A3C high-level archi- tecture is shown in Fig. 6. In A3C the global network and multiple worker agents each have their own set of network parameters. Each of these agents interacts with its own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent is that the experi- ence of each agent is independent of the experience of the others. In this way, the overall experience available for training becomes more di- verse. In addition to using the A3C algorithms for training the agent
RMSProp [46] is used as an optimizer during the experiments.
After both DFP and A3C are used to train the agents on health gathering scenario of the VizDoom Game AI research platform, then the agents were tested as well to analyze and observe their performances where DFP was found better than A3C as shown in Fig. 7. The agent trained with DFP gathered almost 95% health in the allotted time while
the agent trained with A3C gathered almost 90% health.
The overall learning and testing process is measured in time and
lasted for 12 h on a powerful GPU machine.
Advantage Actor-Critic (A2C) method performs best with large batch sizes by using the GPU’s effectively. The A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies, however, frailer in results and output than GPU-only A3C im- plementation. Both value-based methods and policy-based methods have drawbacks that’s why a new reinforcement learning method ‘Actor-Critic’ has been introduced where critic measures how good the action taken is (value-based) and Actor controls how the agent behaves (policy-based). Mastering such an architecture is essential to under- stand the state of the art algorithms such as Proximal Policy Optimization (PPO) [48] which is based on Advantage Actor-Critic (A2C). In the Actor-Critic method, the actor makes actions randomly and the Critic observes the actions and provides feedback. Learning from this feedback, the actor updates its policy and becomes better at playing the game. On the other hand, the Critic also updates its knowledge to provide feedback so it can become better next time. The approach of Actor-Critic is to have two neural networks run in parallel which can be estimated as Actor: a policy function Π(s, , ϴ) that controls how the agent acts, and, the Critic: a value function measures how good the actions are. In addition, as the value-based methods have high susceptibility so the advantage function can be used instead of value function to overcome this problem which can be de- fined as,
(9)
But even this advantage function has drawbacks because it requires two value functions - Q(s, a) and V(s) and this drawback can be over-
come by using the TD error as a good estimator.
In addition, the agents trained with A2C and A2C-LSTM algorithms can be analyzed and observed in Fig. 8, where the performance of A2C is better than A2C-LSTM. The curve of the A2C-LSTM stayed uniform until the last higher number of steps and never rose or declined to a high extent. While on the other hand, the beginning health gathering performance of the agent trained with A2C was found at most 2 to 3 percent better than A2C-LSTM and remain uniform until 10,000 steps where then the agent health gathering performance improved gradually stepwise and at ~15,000 steps the performance curve started touching 70% but its overall final performance declines to ~54% which can be seen in the left graph in Fig. 8. However, on the other side, to confirm the final conclusion to be accurate and authentic the A2C-LSTM agent was considered to be trained time and time again for further longer steps at least up to 300,000 steps which in number were more steps than the training steps of A2C agent just because to see any change or improvement in performance curve, however, despite of training for extraordinary steps still the curve was uniform. Therefore, it was con-
cluded that A2C should be preferred over A2C-LSTM for training game
# Table 2
# Table 2
Hyperparameters used in the experiments for training models (Agents or Bots).
Parameters Algorithms/Methods DFP A3C A2C A2C-LSTM Discount Factor (γ) 0.99 0.99 0.99 0.99 Learning Rate (α) 0.0001 0.0001 0.0001 0.0001 Experience Replay Memory 50,000 60,000 50,000 50,000 Screen Buffer 640, 480 640, 480 640, 480 640, 480 Batch Size 32 32 32 32 Initial Decay 1.0 0.9 1.0 0.9 Final Decay 0.0001 0.0001 0.0001 0.0001 History length 4 4 4 4 Frames per Action 4 4 4 4 Time Elapsed (Hours) 12 12 16 20 Reinforce DQN DRQN DuelingDQN Double DQN C51_ DDQN 0.99 0.99 0.99 0.99 0.99 0.99 0.0001 0.0001 0.0001 0.001 0.0001 0.0001 40,000 50,000 50,000 50,000 50,000 50,000 640, 480 640, 480 640, 480 640, 480 640, 480 640, 480 64 32 32 32 32 32 1.0 1.0 1.0 1.0 1.0 1.0 0.001 0.0001 0.0001 0.0001 0.0001 0.0001 6 4 4 4 4 5 4 4 4 4 4 170 12 12 12 22 19
6
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Agent's Health on Health Gathering Scenario using DFP 100 95 85 80 75 70 65 601 Health 8 — or 7 30000 of 10000 20000 T 7 40000 50000 60000 Steps 70000
Fig. 5. The health of the agent trained with DFP.
# agents or bots.
# 6. Asynchronous advantage Actor-Critic (A3C) and deep recurrent
# Q-Network (DRQN)
The advantage of A3C over DRQN is that it is more resource-effi- cient, since A3C can be run on multiple cores of a single machine, and does not require a large amount of RAM to store the replay buffer
compared to DRQN which requires a large amount of replay buffer. The
actor-critic aspect provides more accurate updates to the policy than a DQN update might provide. A3C is an on-policy algorithm that cannot explore the state-space as efficiently as DQN, so there are some trade- offs between the two algorithms. However, according to our experi- ments conducted on the health gathering scenario of the VizDoom platform A3C performs higher than DRQN as can be seen in Fig. 9. The DQN performs well than DRQN on fully observable environments (FOE) such as health gathering scenario shown in Fig. 11, however, it per- forms low on partially observable environments (POE) where for
Global Network Worker n t t
Fig. 6. Diagram of A3C high-level architecture [47].
Fig. 6. Diagram of A3C high-level architecture [47].
7
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Health of Agent trained on Health of Agent trained on m Health Gathering Scenario using DFP m Health Gathering Scenario using A3C 95 95 90 ” 85 85 80 80 B B 10 10 65 65 60 60 g 5 5 551 n 50 a 50 i 45 i 45 40 4 35 3 30 30 5 5 20 20 5 5 10 10 5 — DP 5 — ABC 0-— T T T T T T 0-— T T T T T - 0 10000 20000 30000 40000 50000 60000 70000 Steps T iğ 0 100000 200000 300000 400000 500000 600000 700000 Steps
Fig. 7. Comparison of the performance of DFP with A3C on health gathering Scenario(s) using the VizDoom AI platform.
overcoming this issue the recurrent feature (LSTM) of DRQN was in- troduced which makes it efficient over DQN by exploiting the experi- ences or sequential updates from memory however this feature does not make DRQN efficient over A3C algorithm and yet its performance is lower than A3C in many game environments.
The left graph in Fig. 9 shows the performance of A3C which was not impressive initially until around 375,000 steps. however, Later the per- formance improved but it took significantly longer to achieve ~90% re- sults while on the other hand, the right graph shows the steady perfor- mance of DRQN on the health gathering scenario which remains almost steady and uniform. DRQN never achieved results to a great extent and its
performance crest and trough always remain between ~50% and ~51%.
# 7. Direct future prediction (DFP) and deep recurrent Q-Network
(DRQN)
- 8. Deep Q-Network (DQN) and deep recurrent Q-Network (DRQN)
In DQN, a single agent is represented by a single neural network that interacts with a single environment. Deep Q-Networks are more capable of overcoming unstable learning by mainly 4 techniques i.e. Experience
# Replay [49], Target Network, Clipping Rewards and Skipping Frames.
Experience reply was originally proposed in 1993 in [50] to overcome the problem of overfitting as Deep Neural Networks (DNN) easily overfits current episodes and once the DNN gets over fit then it becomes hard to produce different experiences. So, for solving this issue, experience replay stores experiences including state transitions, rewards, and actions, which are necessary data to perform Q learning, and makes mini-batches to update neural networks. While calculating the temporal difference (TD) error the target Q-function (Target network) gets changed frequently with DNN’s as unstable target Q-function makes training difficult so target network technique fixes parameters of target Q-function Q (st+1, Ῥ) and
# replaces them with the latest network every thousand of steps.
In order to compare the performance of DFP with DRQN, two agents are trained on the VizDoom health gathering scenario using DFP and DRQN, then both the agents are tested on the health gathering scenario (maps) to see their performance difference where DFP outperformed DRQN as shown in Fig. 10. While gathering health packs the agent trained with DRQN was losing health on average and never manage to improve it to a high level. Thus its overall final performance remains average with ~50% to 51% as shown in the right-side graph. On the other hand, the agent trained with DFP performed very well in collecting health packs and the overall health of the agent achieved ~94% results which can be observed in the left graph for further consideration. It concludes that DFP is a better technique which is one among the state-of-the-art for training agents using the VizDoom Game-AI research platform.
(10)
Each computer game has different score scales. For example, in Pong, players can get 1 point when winning the play. Otherwise, players get −1 point. However, in Space Invaders, players get 10–30 points when defeating invaders. This difference would make training unstable. Thus Clipping Rewards technique clips scores, which all po-
sitive rewards are set +1 and all negative rewards are set −1.
The fourth technique of skipping frames that DQN uses to overcome unstable learning can be explained excellently by referring or with the help of the arcade learning environment (ALE) [51] which is capable of
rendering 60 images per second. But actually, players don’t take actions
8
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Health Agent Health on Health Gathering Scenario using a2c Agent Health on Health Gathering Scenario using a2c-LSTM 100 100 95 95 90 90 854 854 80 80 5 5 70 70 65 65 604 604 55 55 50 EY sa alli aid NE treet te 45 İp 40 40 351 35 30 30 25 25 204 204 5 5 104 104 5 — ax 5 — adclsTm 0 2500 5000 7500 10000 12500 15000 17500 0 50000 100000 150000 200000 250000 300000 Steps steps
Fig. 8. Comparison of the performance of A2C with A2C-LSTM on health gathering Scenario(s) using the VizDoom AI platform.
thus much in a second. AI doesn’t need to calculate Q-values every frame. So by employing skipping frames technique DQN calculates Q- values every 4 frames and uses past 4 frames as inputs. This reduces the
# computational cost and gathers more experiences.
Using these four techniques enables DQN to achieve stable training.
Table 3 shows that the performance of DQN increases if it uses
# Experience Replay technique along with the Target Network.
DQN and DRQN are the variants of Deep Q-learning introduced by Google DeepMind, London, the UK in 2013 and 2015 that performed with extraordinary results on Atari games and later were applied to different platforms. In this paper, we also applied them using the VizDoom AI platform to compare and differentiate their performance on the health
Health Agent Health on Health Gathering Scenario using A3C Agent Health on Health Gathering Scenario using DRQN 100 100 95 954 90 4 90 85 85 80 80 75 75 70 70 4 654 65 60 60 4 55 5 s0 Şİ a rr 45 4 E45 40 4 40 35 35 30 4 30 25 25 20 20 15 15 10 10 51 — asc 5 — DRON o o T i o 100000 200000 300000 400000 500000 600000 700000 Steps o 2000 4000 6000 8000 10000 12000 14000 Steps
Fig. 9. Comparison of the performance of A3C with DRQN on health gathering Scenario(s) using the VizDoom AI platform.
Fig. 9. Comparison of the performance of A3C with DRQN on health gathering Scenario(s) using the VizDoom AI platform.
9
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Agent Health on Health Gathering 100 Scenario using DFP 95 4 85 804 15 70 651 604 55 Health Ss 45 35 254 20 51 10 51 — o 0 10000 20000 30000 40000 50000 60000 70000 Steps Agent Health on Health Gathering Scenario using DRQN 100 95 4 85 804 75 70 65 60 55 Health 3 45 35 254 20 5 10 5 — DRON 0 2000 4000 6000 8000 10000 12000 14000 steps
Fig. 10. Performance Comparison of DFP with DRQN on health gathering Scenario(s) using the VizDoom AI platform.
# Table 3
DQN Performance with and without Experience Replay and Target Network
[52]
Replay ✓ ✓ ✗ ✗ Target ✓ ✗ ✓ ✗ Breakout 316.8 240.7 10.2 3.2 River Raid Sequest Space Invaders 7446.6 2894.4 1088.9 4102.8 822.6 826.3 2867.7 1003.0 373.2 1453.0 275.8 302.0
gathering scenario (map) by training the agents. The agent trained with DQN performed slightly better than the DRQN in gathering the health packs due to the fully observable environment and retained its health to ~53%. while on the other hand, the agent trained with DRQN remain below in performance in collecting health packs on the health gathering scenario (maps) by retaining its health to almost 51% as shown in Fig. 11. It concludes that the agents trained with DQN perform better than DRQN
# particularly on the health gathering scenario of the VizDoom AI platform.
# 9. Dueling deep Q-Network (DDQN) and double deep Q-Network
(DDQN)
actions to take in a given state. While this would be fine if all actions were always overestimated equally, but this never finds to be the case because if certain suboptimal actions regularly were given higher Q- values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to fix this issue, a simple trick was proposed: instead of taking the max over Q-values when computing the Target Q-value for training steps, the primary network is used to choose an action, and the target network is used to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, it was able to substantially reduce the overestimation, and train faster and more reliably. The new double-DQN equation for up- dating the target value could be represented as,
Q- Target = r+ y Q(s’, argmax(Q(s’, a, @)), 6’) ay
And, the advantage is calculated separately and then combined only
# at the final layer into a Q-value [17].
The reason behind the change in the architecture that dueling-DQN makes is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only
# at the final layer.
(12)
(11)
Dueling-DQN (DDQN) and double-DQN (DDQN) are two simple additional improvements to the DQN architecture or, in other words, these are close newer variants of DQN introduced by Google DeepMind, London, UK, in 2015-16 [17,53]. The dueling-DQN and double-DQN allow for improved performance, stability, and faster training time. The double-DQN basically uses 2 neural networks to perform the Bellman iteration, one for generating the prediction term and the other for generating the target term. It helps to alleviate the bias introduced by
the inaccuracies of Q-network at the beginning phase of training.
The regular DQN often overestimates the Q-values of the potential
In dueling DQN, Q can also be computed by using the below formula with value function V and a state-dependent action advantage function
A, as shown in Fig. 12.
al 1 S, a) = v(s) + A(s, a) — — A(s,a Qs, a) = vs) + AG. a) — 77D AG. a)
So, dueling-DQN and double-DQN are the two variants of DQN that performed well on Atari 2600 domain. In this paper, we use them particularly using the VizDoom AI platform to compare and differ-
entiate their performance on the health gathering scenario (map) by
10
10
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Agent Health on Health Gathering Scenario using DON Health — DON 15000 20000 25000 Steps 0 5000 10000 Agent Health on Health Gathering Scenario using DRON 100 96 4 Health 4 — DON 6000 8000 10000 12000 14000 Steps 0 2000 4000
Fig. 11. Comparison of the performance of DQN with DRQN on health gathering Scenario(s) using the VizDoom AI platform.
e pH 7 yo Vs) A(s,a) Dueling DQN Q(s,a)
Fig. 12. Above: Regular DQN with a single stream for Q-values. Below: Dueling DQN where the value.
training two agents. The agent trained with dueling-DQN performed a lot better than the double-DQN in gathering the health packs and re- tained its health to ~59%. On the other hand, the agent trained with double-DQN remain below in performance by collecting health packs on the map and its health retains to ~52% as shown in Fig. 13. It concludes that the agents trained with dueling-DQN perform better
# particularly on the VizDoom AI platform.
equation. The number 51 represents the use of 51 discrete values to parameterize the value distribution Z. During each update step, a transition is sampled from the environment and the target distribution is computed. is used to update the current distribution Z by minimizing the cross entropy loss between Z and . The pseudo code of the C51 Algorithm is as follows.
# Pseudo code of the C51 Algorithm [54]
Input a transition , ( ) Q( ) = 0, 0, ….., N − 1 For ,……, N − 1 do # Compute the projection of onto the support { } # Distribute probability of + ( + ( End for Output —
- 10. C51_DDQN and Reinforce
C51 is a viable algorithm introduced in [18] to perform an iterative
approximation of the value distribution Z using distributional Bellman
Similar to DQN, we first use a deep neural network to represent the value distribution Z. Since the inputs are screen pixels, the first 3 layers are convolutional layers. The neural network outputs 3 sets of value distribution predictions, one for each action i.e. (Health, Health Packs,
11
11
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Agent Health on Health Gathering Agent Health on Health Gathering Scenario using Dueling-DDQN Scenario using DDQN 100 100 95 951 90 90 4 85 854 80 801 Bİ D4 70 104 65 654 60 601 55 551 s s ——— —— 5 5 5 50 5 501 I ft 45 454 40 401 354 354 30 301 25 25 20 201 5 5 10 101 — Dueling DDON 7 — DDN 0-4 oz 7 7 T 7 7 7 0 20000 40000 60000 80000 100000 120000 Steps 7 T 7 7 7 : lu 0 5000 10000 15000 20000 25000 30000 35000 Steps
Fig. 13. Performance of Dueling-DQN with Double-DQN on health gathering Scenario(s) using the VizDoom AI platform.
Poison). Each set of prediction is a softmax layer with 51 units and the number of atoms is the number of discrete values (i.e. 51). The main logic of the algorithm is contained in the update step. First, a minibatch of sample trajectories is sampled from the Experience Replay buffer and the corresponding states, reward, and targets variables are initialized. The variable ‘m_prob’ stores the probability mass of the value dis- tribution Z. Next, a forward pass is carried out to get the next state distributions. As the model outputs 3 sets of value distributions, one for each action. Thus the one with the largest expected value to perform the update (similar to maxa′Q (s′, a′) in Q Learning) is only needed. Then the target distribution Z′ is computed (i.e. scale by γ and shift by reward r) and projected it to the 51 discrete supports.
# Pseudocode of the Reinforce Algorithm [18]
initialize θ for each episode {s1, a1, r2…sT−1, aT−1, rT} s sampled from policy πθ do for t = 1 to T − 1 do θ ← θ + α∇θ logπθ (st, at) Gt end for end for
However, ‘Reinforce’ suffers from high variance because the sampled rewards can be different from one episode to another that is why this
algorithm is normally used with a baseline subtracted from the policy.
The C51 significantly outperforms several state-of-the-art algo- rithms. In fact, C51 surpasses much state-of-the-art by a large margin in
a number of games, most notably Seaquest [51].
On the other hand, ‘Reinforce’ is a family of reinforcement learning methods which directly update the policy weights or in other words, it is a policy gradient-based method which samples the expected return directly from the episode where the expected return is actually the total episodic reward onward that step Gt. To be more specific, it iteratively updates agent's parameters by computing policy gradient and works well when episodes are reasonably short where lots of episodes can be simulated however value-function methods are better for longer epi- sodes because they can start learning before the end of a single episode. How this algorithm works can be understood further with the help of
In the same way, C51_DDQN and Reinforce are as well implemented and tested on the VizDoom health gathering scenario (maps). The agent trained with C51_DDQN performed a lot better and uniform than the Reinforce in gathering the health packs and retained its health to ~87%. On the other hand, the agent trained with Reinforce remains below in performance by collecting health packs on the health gath- ering scenario (maps) and retains its health finally to ~57% as its performance crest and trough can be observed in Fig. 14. It concludes that the agents trained with C51_DDQN perform better than the agents trained with Reinforce particularly using the VizDoom Game-AI re-
# search platform.
- 11. Techniques with better performance
the pseudocode as follows?
After training agents on health gathering scenario(s) of the VizDoom
AI platform using different machine learning methods and techniques,
12
12
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
Agent Health on Health Gathering Agent Health on Health Gathering Scenario using c51 DDQN Scenario using Reinforce 100 M$ 100 951 95 | 90 4 85 85 807 807 754 B4 704 701 65 651 604 604 & 5 © 507 5 50 v v I 45] > 454 401 4 35 54 30 mİ 51 51 204 204 154 54 104 104 54 — G1DDON 5 — Reinforce 0.--—-—_—_ — — — — 0 — —_ — — —— 0 20000 40000 60000 80000 100000 120000 140000 0 10000 20000 30000 40000 50000 60000 70000 Steps Steps
Fig. 14. Performance of C51_DDQN with Reinforce on health gathering Scenario(s) using the VizDoom AI platform.
Comparing Four Machine Learning Methods with Better Performance for Training Agents on the VizDoom's Health Gathering Scenario wi 804 =I ie 704 2 60 — o m — ABC — DDON-Dueling — €51-000N o 100000 200000 300000 400000 300000 600000 700000 Steps
Fig. 15. Comparison of the four techniques with the best performance on the health gathering Scenario(s) of the VizDoom AI platform.
Fig. 15. Comparison of the four techniques with the best performance on the health gathering Scenario(s) of the VizDoom AI platform.
13
13
A. Khan, et al.
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
# Table 4
Showing the results or performance of Algorithms used for training agents.
Algorithms/Methods DFP A3C A2C A2C-LSTM DQN ~95% ~90% ~54% ~52% ~53% DRQN ~51% Dueling DQN Double DQN C51_DDQN Reinforce ~59% ~52% ~87% ~57%
the performance of the following four methods is found better and accurate as shown in Fig. 15. The x-axis represents the training steps of thousands to million while the y-axis represents the health of the agents in percentage while gathering the health packs. The agents trained using the DFP method sustains almost 95% health while gathering health packs which is more than other methods and remains one of the best options for training the agents. The agents trained using A3C method follow DFP in performance and sustains almost 90% health on health gathering scenarios which means the second-best machine learning method after DFP for training agents, However, it takes A3C significantly longer to achieve those results and is performing worse than the others until around 375,000 steps. The C51_DDQN is the third- best machine learning technique that performed well in gathering health on the health gathering scenario of the VizDoom Game-AI platform with ~87% performance. Dueling DQN (DDQN) is the last method among the four best machine learning methods that perform well in gathering health packs on the health gathering scenarios and retained health to ~59%.
like to acknowledge and thank NVIDIA Corporation for two powerful sets of GPU machines donation.
# References
- ua J.D. Owens, et al., GPU computing, Proc. IEEE 96 (5) (2008) 879-899.
[1] J.D. Owens, et al., GPU computing, Proc. IEEE 96 (5) (2008) 879–899.
- [2] N.P. Jouppi, et al., In-datacenter performance analysis of a tensor processing unit, 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), IEEE, 2017.
- [3] M.T. Hagan, et al., Neural network design Vol. 20 Pws Pub, Boston, 1996.
- [4] F.J. Khan Adil, Shaohui Liu, Worku Jifara, Zhihong Tian, Yunsheng Fu, State-of-the- Art and Open Challenges in RTS Game-AI and Starcraft. (IJACSA), Int. J. Adv. Comput. Sci. App. 8 (12) (2017) 9.
- [5] V. Mnih, et al., Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
- [6] M. Lowney, R. Mahieu, Creating an Agent of Doom: A Visual Reinforcement Learning Approach.
- [7] Z.-J. Pang, et al., On Reinforcement Learning for Full-length Game of StarCraft. arXiv preprint arXiv:1809.09095, 2018.
- [8] Grand theft auto V. 2014, Xbox One. New York, NY: Rockstar Games, [2014] ©2014.
- [9] G. Ekaputra, C. Lim, K.I. Eng, Minecraft: A game as an education and scientific learning tool, ISICO 2013 (2013) 2013.
- 12. Discussion and future work
- [10] I. Grondman, et al., A survey of actor-critic reinforcement learning: Standard and natural policy gradients, IEEE Trans. Syst., Man, Cybernetics, Part C (App. Rev.) 42
(6) (2012) 1291–1307.
In this paper, the work has shown the effectiveness of using different machine learning techniques and methods within the context of com- plex 3D multi-agent environment with agents playing competitively against human players and inbuilt game agents using the VizDoom Game AI research platform. Such systems can be applied to commercial games to provide competent opponents, without the need for pre- defined manual coded instructions or scripts. In addition, this paper concludes that if the environment provides with a rich and temporally dense measurements signals, reformulating the reinforcement learning problem to supervised learning (e.g. DFP) leads to better performance and accelerated training. According to the experiments, more mea- surements certainly lead to better results. There is a beneficiary effect by allowing the model to generate a richer set of predictions, similar to the way auxiliary tasks enhance the performance of deep learning vi- sion classifier. The goal skeptical nature of DFP allows the agent to pursue complex goals at inference time. This lifts the limitation on learning and acting from a single objective by traditional reinforcement learning methods, and is one way to achieve transfer learning and one- shot learning for multiple tasks. Besides DFP, this paper as well con- cludes that machine learning techniques such as DFP, A3C, C51_DDQN, and Dueling DQN can be chosen as the first choice for training agents due to their efficient performance. The insightful summary of what the
- [11] J.X. Wang, et al., Learning to reinforcement learn. arXiv preprint arXiv:1611.05763,
2016.
- [12] V. Mnih, et al., Asynchronous methods for deep reinforcement learning. in
# International Conference on Machine Learning, 2016.
- [13] S. Freitas, et al., Exploration of DQN in ViZDoom, 2018.
# [13] S. Freitas, et al., Exploration of DQN in ViZDoom, 2018.
- [14] R. Brejl, H. Purwins, H. Schoenau-Fog, Exploring Deep Recurrent Q-Learning for Navigation in a 3D Environment, Eai Endorsed Transactions on Creative Technologies, 2018.
- [15] C. Schulze, M. Schulze, ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling. arXiv preprint arXiv:1801.01000,
2018.
- [16] W. Dabney, et al., Distributional reinforcement learning with quantile regression. arXiv preprint arXiv:1710.10044, 2017.
- [17] Z. Wang, et al., Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
- [18] R.J. Williams, Simple statistical gradient-following algorithms for connectionist
# reinforcement learning, Machine Learning 8 (3–4) (1992) 229–256.
- [19] M. Kempka, et al., ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning. arXiv preprint arXiv:1605.02097, 2016.
- [20] A. Khan, et al., Optimal Skipping Rates: Training Agents with Fine-Grained Control Using Deep Reinforcement Learning, J. Robot. 2019 (2019) 10.
- [21] A. Stooke, P. Abbeel, Accelerated methods for deep reinforcement learning. arXiv preprint arXiv:1803.02811, 2018.
- [22] G. Lample, D.S. Chaplot, Playing FPS games with deep reinforcement learning.
# arXiv preprint arXiv:1609.05521, 2016.
- [23] F.J. Khan Adil, Shaohui Liu, Aleksei Grigorev, B.B. Gupta, Seungmin Rho, Training an Agent for FPS Doom Game using Visual Reinforcement Learning and VizDoom, (IJACSA) Int. J. Adv. Comput. Sci. App. 8 (12) (2017).
- [24] C.J. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3–4) (1992) 279–292.
experiment results convey is presented in Table 4.
The experiments demonstrate that learning from raw pixels is a new era in artificial intelligence since of versatile and dynamic environ- ments such as VizDoom. Extending such approaches further in multiple
ways to broaden the range of behaviours for learning is our future work.
- [25] A. Dosovitskiy, V. Koltun, Learning to act by predicting the future. arXiv preprint
arXiv:1611.01779, 2016.
- [26] M.H. Leung, L.R.T. Michael, Applying Modern Reinforcement Learning to Play Video Games. 2017.
- [27] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Computation 9 (8)
(1997) 1735–1780.
- [28] D. Silver, et al., Mastering the game of Go with deep neural networks and tree
search, Nature 529 (7587) (2016) 484–489.
# Declaration of Competing Interest
- [29] O. Nachum, et al., Bridging the gap between value and policy based reinforcement
# learning. in Advances in Neural, Inform. Process, Syst. (2017).
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influ-
ence the work reported in this paper.
# Acknowledgment
This work is funded by the University of Peshawar, Pakistan and the
Higher Education Department KPK, Pakistan. The authors would also
- [30] A. Kassambara, Machine Learning Essentials: Practical Guide in R, STHDA, 2018.
- [31] M. Abadi, et al., Tensorflow: Large-scale machine learning on heterogeneous dis-
tributed systems. arXiv preprint arXiv:1603.04467, 2016.
- G. Bradski, A. Kaehler, OpenCV. Dr. Dobb’s journal of software tools 3 (2000).
# (ost
[32] G. Bradski, A. Kaehler, OpenCV. Dr. Dobb’s journal of software tools 3 (2000).
- [33] M. Fairbank, E. Alonso, The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Ap-proximation. arXiv preprint arXiv:1107.4606,
2011.
- [34] F.G. Glavin, M.G. Madden, Learning to Shoot in First Person Shooter Games by Stabilizing Actions and Clustering Rewards for Reinforcement Learning. arXiv
preprint arXiv:1806.05117, 2018.
14
14
A. Khan, et al.
- [35] D. Hafner, Deep Reinforcement Learning From Raw Pixels in Doom. arXiv preprint arXiv:1610.02164, 2016.
- [36] A. Smola, S. Vishwanathan, Introduction to machine learning, Cambridge University, UK, 2008, p. 34.
- [37] M. McPartland, M. Gallagher, Creating a multi-purpose first-person shooter bot with reinforcement learning, in: IEEE Symposium On Computational Intelligence and Games, 2008. CIG'08, IEEE, 2008.
- [38] M. Wydmuch, M. Kempka, W. Jaśkowski, ViZDoom Competitions: Playing Doom from Pixels. arXiv preprint arXiv:1809.03470, 2018.
- [39] D. Pérez-Liébana, et al., Analyzing the robustness of general video game playing agents, in: 2016 IEEE Conference on Computational Intelligence and Games (CIG), IEEE, 2016.
- [40] F.G. Glavin, M.G. Madden, DRE-Bot: A hierarchical First Person Shooter bot using multiple Sarsa (λ) reinforcement learners, in: 2012 17th International Conference on Computer Games (CGAMES), IEEE, 2012.
- [41] J. Powles, H. Hodson, Google DeepMind and healthcare in an age of algorithms, Health Technol. 7 (4) (2017) 351–367.
- [42] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444.
- [43] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
- [44] K. Das, J. Jiang, J. Rao, Mean squared error of empirical predictor, The Annals of Statistics 32 (2) (2004) 818–840.
- [45] J. Schmidhuber, Deep learning in neural networks: An overview, Neural Net. 61
(2015) 85–117.
- [46] S. Ruder, An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.
- [47] B. Tan, N. Xu, and B. Kong, Autonomous Driving in Reality with Reinforcement Learning and Image Translation. arXiv preprint arXiv:1801.05299, 2018.
- [48] J. Schulman, et al., Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347, 2017.
- [49] T. Schaul, et al., Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
- [50] L.-J. Lin, Reinforcement learning for robots using neural networks, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993.
- [51] M.G. Bellemare, et al., The arcade learning environment: An evaluation platform for general agents, J. Artificial Intelligence Res. 47 (2013) 253–279.
- [52] V. Mnih, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529–533.
- [53] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double q-
15
15
Entertainment Computing 34 (2020) 100357
# Entertainment Computing 34 (2020) 100357
learning, in: Thirtieth AAAI conference on artificial intelligence 2016 Mar 2. [54] M.G. Bellemare, W. Dabney, R. Munos, A distributional perspective on reinforce- ment learning. arXiv preprint arXiv:1707.06887, 2017.

Dr. Adil Khan http://orcid.org/0000-0003-2862-5718 (ORCID ID). He received C.T. from AIOU Islamabad, B. Ed from the University of Peshawar, BS Honors in Computer Science from Edwards College Peshawar, M.S in Computer Science from City University of Science and Information Technology Peshawar and Ph.D. from University of Peshawar, Peshawar, Pakistan. From 2014-2016, he was a Lecturer in Higher Education Department KPK, Pakistan and from 2016-2019 he was serving as a research scholar at the School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001 PR China. Currently Adil khan is working as an Assistant Professor at the Department of Computer Science, SZIC, University of
Peshawar. He has published many publications in top-tier
academic journals and conferences and is interested in Machine Learning, Game Artificial Intelligence (Game-AI), Neural networks, Real Time Strategy Games, First-Person-Shooter Games, Sandbox open world Games, Computer Vision, Image Processing (Breast Cancer Detection). He can be reached at personal E-mail: adil.adil25@yahoo.com.
Professor (Asst) & Dr. Muhammad Zubair Asghar. http://orcid.org/0000-0003-3320- 2074 (ORCID ID). He is an Assistant Professor at Institute of Computing and Information Technology, Gomal University, Dera Ismail Khan, KP, Pakistan and approved Ph.D. su- pervisor recognized by Higher Education Commission (HEC), Pakistan. His Ph.D. research interest includes Game-AI, First-person Shooter Games, Third-Person Shooter Games, Computational Intelligence, Computational Linguistics, Machine Learning, Text Mining, Opinion Mining, Sentiment Analysis, and Big Data Solutions for Social Networks. He has published more than 40 publications in journals of international reputation (JCR and ISI indexed). He has more than 15 years of University teaching and laboratory experience in Artificial Intelligence and Intelligent Systems Design. He is a Guest Editor of special issues in the journal of ‘Social Computing in Health Informatics and Business Intelligence’. He is also a reviewer of many impact factor journals and an Associate Editor of IEEE ACCESS and Plos One. He can be reached at official E-mail: zubair@gu.edu.pk.
