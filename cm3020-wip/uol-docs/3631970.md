<!-- image -->

## Testing of Deep Reinforcement Learning Agents with Surrogate Models

MATTEO BIAGIOLA and PAOLO TONELLA, Università della Svizzera italiana, Switzerland

Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents.

In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called I/n.sc/d.sc/a.sc/g.sc/o.sc, trains a classi/fier on failure and non-failure environment (i.e., pass) con/figurations resulting from the DRL training process. The classi/fier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment con/figuration induces a failure of the DRL agent under test. The failure prediction acts as a /fitness function, guiding the generation towards failure environment con/figurations, while saving computation time by deferring the execution of the DRL agent in the environment to those con/figurations that are more likely to expose failures.

Experimental results show that our search-based approach /finds 50% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78% more diverse; similarly, the behaviors of the DRL agent induced by failure con/figurations are 74% more diverse.

## CCS Concepts: · Software and its engineering → Software veri/fication and validation;

Additional Key Words and Phrases: Software testing, reinforcement learning

## ACMReference format:

Matteo Biagiola and Paolo Tonella. 2024. Testing of Deep Reinforcement Learning Agents with Surrogate Models. ACM Trans. Softw. Eng. Methodol. 33, 3, Article 73 (March 2024), 33 pages.

https://doi.org/10.1145/3631970

## 1 INTRODUCTION

Reinforcement Learning ( RL ) is a learning paradigm in which an agent interacts with the environment to complete a given task. Learning is driven by a reward signal returned to the agent by the environment. The ultimate goal of an agent is to learn a policy, i.e., a way to act in each environment state, that maximizes the amount of reward the agent earns in its lifetime. The /first RL algorithms were tabular [58] and could handle tasks with /finite and small environment states. With the advent of Deep Learning ( DL ), new algorithms (called Deep RL, or DRL for short) were proposed, which could deal with complex environment states (e.g., images) [42].

1049-331X/2024/03-ART73 $15.00

DRL has recently been applied in many practical contexts. An example is personalization, i.e., the problem of customizing a service to the needs of a particular user. For instance, Net/flix uses DRL to choose which movie artwork to show to a user in order to maximize engagement [28]. Similarly, Microsoft developed Personalizer [35], a service developers can use for content recommendation and ad placement. Meta proposed Horizon [29, 30] (also called ReAgent), an open source applied DRL platform, employed to deliver personalized noti/fications to their users, replacing the previous system based on supervised learning.

Another practical context in which the DRL paradigm is applied is continuous control. For instance, the automaker Audi [71] showcased that their 1:8 scale car could, using DRL, search for a parking place in an area of 9 square meters and park autonomously. Indeed, advancements in state representation learning and smooth exploration [36, 48, 68] made it possible to train DRL agents directly on real robots. Moreover, in recent years, simulators have become more realistic for a variety of tasks, besides achieving high parallelization thanks to GPU acceleration [26, 39]. In addition, domain randomization and learned actuator dynamics are reducing the sim-to-real gap in robotics research [25, 41, 52].

Despite the growing prevalence of DRL agents in the real world, methodologies for testing such agents are still largely unexplored. On the other hand, DRL agents present some peculiar characteristics. Indeed, DRL agents are trained online since they interact with the environment to learn the optimal actions to perform the task. Speci/fically, in order to increase generalization, DRL agents are usually trained on randomized environment con/figurations [15, 63] to prevent agents from memorizing how to behave in a particular instance of the environment (e.g., a self-driving car that drives only on a speci/fic track). Therefore, during training the DRL agent is presented with different environment con/figurations (e.g., di/fferent tracks) and it fails in some while it succeeds in others.

The current state-of-the-practice to test DRL agents is to run them on a set of environment con/figurations generated at random [43, 63]. However, testing a DRL agent on randomly generated environment con/figurations has two shortcomings. First, random generation [43] is unlikely to expose failures. As a consequence, their absence might lead the developer to overestimate the capabilities of the DRL agent and to the deployment of an unsafe agent. Secondly, even /finding challenging environment con/figurations by random exploration is di/fficult and computationally expensive, since many executions are needed and each execution requires running the DRL agent in a simulator or in the real world.

On the other hand, the interactions of the DRL agent with the environment during training provide clues about the weaknesses of the DRL agent that result from the training process. The intuition is that training failures are representative of critical environment con/figurations even for the DRL agent once it has been trained, and could be used as guidance for the generation of new environment con/figurations that will likely challenge it.

Our approach, implemented in a tool called I/n.sc/d.sc/a.sc/g.sc/o.sc, considers the interaction data produced during the DRL training process as a labeled dataset to train a surrogate model-i.e., a classi/fier-on failure and non-failure (i.e., pass) environment con/figurations. Then, I/n.sc/d.sc/a.sc/g.sc/o.sc uses such surrogate model as a proxy for the execution of the DRL agent in an environment with a newly generated con/figuration. I/n.sc/d.sc/a.sc/g.sc/o.sc uses a search-based approach to maximize the failure prediction for an environment con/figuration given by the surrogate model, instead of a random search [63]. Indeed, random search needs to generate a large set of environment con/figurations in order to be e/ffective, which makes it computationally expensive. On the other hand, I/n.sc/d.sc/a.sc/g.sc/o.sc uses a mutation operator guided by saliency-based input attribution [55] to identify mutations that have the maximum in/fluence on the failure prediction. This way, I/n.sc/d.sc/a.sc/g.sc/o.sc turns a non-promising environment con/figuration into a critical one for the DRL agent under test, making the search more e/fficient. I/n.sc/d.sc/a.sc/g.sc/o.sc executes the

DRL agent under test only on the most promising environment con/figurations, i.e., those with the highest failure predictions, hence saving computation time while maximizing failure exposures.

Our article makes the following contributions:

- (1) Failure Search with Surrogate Models: in this article, we propose an approach that makes use of a surrogate model of the environment to guide failure search while automatically generating environment con/figurations for a DRL agent. In particular, we train a classi/fier on the training interaction data and use its output as a /fitness function to maximize the failure prediction of a given environment con/figuration. Moreover, we use the saliency method to e/fficiently identify the most critical mutations for the given environment con/figurations.
- (2) The I/n.sc/d.sc/a.sc/g.sc/o.sc Tool: a practical tool, that implements the aforementioned approach, which wemakepublicly available [7]. We also release three DRL agents trained on as many complex environments, as well as the required infrastructure to test them.
- (3) Experimental Evaluation: we systematically compare di/fferent con/figurations of I/n.sc/d.sc/a.sc/g.sc/o.sc with the state-of-the-art sampling approach [63] that maximizes failure prediction by generating a large set of environment con/figurations. On three complex case studies, i.e., a parking task [37], a walking humanoid [62], and a self-driving car [61], our experiments show that, overall, I/n.sc/d.sc/a.sc/g.sc/o.sc is able to /find 50% more failure environment con/figurations than sampling. Moreover, we introduce a clustering-based technique to measure the diversity of failure environment con/figurations triggered by the competing approaches. Experimental results show that the failure environment con/figurations found by I/n.sc/d.sc/a.sc/g.sc/o.sc are 77% more diverse than those generated by sampling. Moreover, the behaviors of the DRL agent induced by such environment con/figurations are 74% more diverse.

## 2 RELATED WORK

Testing DRL agents is a rather unexplored area of research. Works that generate adversarial attacks [59]havebeenproposed[23, 38], showing that such DRL agents can be vulnerable to adversarial attacks, similarly to DL agents (i.e., agents trained using supervised learning). However, our approach is substantially di/fferent since it is not focused on perturbing the raw inputs of the DRL agent sensors but rather on generating con/figurations for the whole environment the DRL agent runs into. The most similar work to ours is that by Uesato et al. [63], who proposed the sampling approach we used as a baseline in our experiments. Our results show that our search-based approach outperforms it both in terms of number of failures triggered and their diversity in all case studies.

Biagiola et al. [8] proposed an approach to test the adaptation capabilities of DRL agents. In particular, the training of a DRL agent is resumed with environment con/figurations that are different from those experienced by the DRL agent during the previous training phase. Then, the proposed approach builds the adaptation frontier of the DRL agent, separating the con/figurations in which the DRL agent under test is able to adapt from those where adaptation is unsuccessful. Our approach is similar, in the sense that we also generate environment con/figurations. However, we are interested in testing the DRL agent to /find its weaknesses at testing time rather than its adaptation capabilities (i.e., we do not resume training).

Also related to our work is that by Ruderman et al. [15], who studied how to train and test agents in procedurally generated environments. In particular, they trained DRL agents on a set of procedurally generated mazes for a 3D navigation task. Then, at testing time, they adopt a local search process to modify generated mazes guided by the performance of the DRL agent. This process generates out-of-distribution mazes, i.e., environment con/figurations that are not possible under the training distribution. In our work, we minimize the computational cost of the search by using a surrogate model of the environment (i.e., a failure predictor), rather than executing the DRL

agent under test in the environment to measure its performance. Indeed, executing the DRL agent in the environment at each search iteration becomes prohibitively expensive in environments more complex than mazes. Moreover, our approach does not generate out-of-distribution environment con/figurations since all environment con/figurations generated at testing time are subject to the same validity constraints as the environment con/figurations generated during the DRL training process.

More recently, Tappler et al. [60] proposed a search approach to assess the quality of DRL agents. Their approach consists of searching for a reference trace that solves the RL task by sampling the environment. Such trace is built using a depth-/first search algorithm and it is composed of all the states not part of the backtracking branches of the search. In particular, the search backtracks when it encounters an unsafe state, which is a state where the environment terminates the episode unsuccessfully. The states in the search graph preceding both an unsafe state and at least a successor non-terminal state, are called boundary states. The pre/fixes of the reference trace that end up in a boundary state are safety tests since they are sequences of actions designed to bring the DRL agent under test into safety-critical situations. Our approach is complementary since our goal is to generate new environment con/figurations to test the DRL agent, rather than evaluating it in the same environment con/figuration.

The literature in testing DL agents is quite rich and includes a multitude of works summarized in di/fferent surveys on the topic [9, 24, 74]. Particularly relevant to our work are those that use search-based methods to generate test inputs [1, 2, 6, 19, 49].However,ourworkisdi/fferentsince it speci/fically targets DRL agents by exploiting the interaction data a DRL agent produces during training, which is not possible in DL testing since DL agents are trained o/ffline.

## 3 BACKGROUND AND MOTIVATION

## 3.1 Reinforcement Learning

Fundamentals and Notations. RL aims at learning a policy, which is a mapping from states to actions, in order to optimize a numerical reward signal [58]. The agent that acts in the environment needs to discover what actions result in a high reward through trial and error without the presence of a supervisor. The main assumption of this learning paradigm is the so-called reward hypothesis [58], stating that training goals can be expressed as the maximization of the cumulative reward.

More formally, at each timestep t an RL agent receives a state s as input from the environment, and it has to decide the action a to take. The executed action triggers a change of state and results in a reward value r given to the agent by the environment. Assuming that the task we formalize as an RL problem is episodic, i.e., it terminates once certain conditions hold, the goal of an RL agent is to maximize the cumulative reward (i.e., often called return) of the episode. In the general case, however, the RL objective is expressed as the maximization of the expected return since both the environment in which the agent operates and its policy can be stochastic. The main reason for the stochastic nature of a policy is due to a fundamental dilemma in RL, which is the explorationexploitation dilemma. In fact, on the one hand, the agent has to exploit the actions already known to be rewarding but, on the other hand, it has to explore unknown actions that might result in even more reward.

The most important component of an RL agent is its policy π : S→A ,where A is a set of actions and S is a set of states. The optimal policy π ∗ tells the RL agent how to act in each state in order to maximize the expected return. The optimal policy can be learned directly or can be extracted from value functions, namely the state-value function /v.alt π ( s ) and the action-value function q π ( s , a ) . The former quanti/fies the value of the state s , i.e., the expected return in s ,

Fig. 1. An initial configuration of the Parking environment in the HighwayEnv simulator [37]. The le/f\_t-hand side ( A ) shows how the configuration on the right-hand side ( B ) is rendered in the environment.

<!-- image -->

whereas the latter quanti/fies the value of the state-action pair ( s , a ) . The di/fference between the two functions is that /v.alt π provides the value of a state s by considering each possible action the agent can take in s (in other words, the average of the expected return in s for all the actions), while q π considers the value of a state s for a particular action a . Both functions satisfy the Bellman equations [58] which are recursive consistency equations relating the values of a state (or stateaction pair) to the values of all the possible successor states (or state-action pairs). In particular, by solving the Bellman equation for q π , we obtain q ∗ π from which we can extract π ∗ by choosing in each state s the action a that maximizes q ∗ π .

Deep RL Algorithms. Before DL, the RL problem was addressed using dynamic programming and approximate tabular methods, such as Monte Carlo methods and temporal di/fference learning [58]. However, such methods are not applicable to problems where the state dimensionality is high (e.g., images) and/or the action space is continuous (e.g., the throttle in a self-driving car). The advent of DL made it possible to create Deep RL (DRL) algorithms that work on such complex practical scenarios [42]. In particular, Deep RL (DRL) algorithms use non-linear function approximators, such as neural networks, to approximate high dimensional state and action spaces besides modeling the dynamics of the environment. Therefore, instead of having exact representations of policies s and value functions ( /v.alt π and q π ), neural networks can be used to approximate such quantities as well as to model the environment in which the agent operates.

In our experiments, we consider model-free DRL algorithms which do not use a model of the environment. There exist di/fferent categories of model-free algorithms, based on how the RL problem is addressed. Speci/fically, policy gradient algorithms, of which PPO is a notable example [54], directly solve the RL objective by representing the policy explicitly (i.e., with a neural network). Value-based algorithms, on the other hand, extract the policy by solving the Bellman equations, hence representing value functions explicitly. Examples of algorithms in such category are DQN and its improvements [14, 21, 22, 42, 53, 70]. Hybrid methods represent both policy and value functions to incorporate, the bene/fits of both policy gradients and value-based methods. SAC [20]andTQC[34] are the state-of-the-art algorithms in this category. Belonging to a special category is the algorithm HER [3], which was proposed as a wrapper on top of traditional DRL algorithms, to speed up learning in the context of goal-based tasks, e.g., parking a car from a starting position to a target parking spot.

## 3.2 Motivating Example

Figure 1 shows the Parking environment, created by Leurent et al. [37], in a particular con/figuration where the DRL agent needs to control the ego vehicle positioned at the center of the parking place.

The left-hand side of the /figure (i.e., Figure 1(A)) shows how the environment con/figuration on the right-hand side (i.e., Figure 1(B)) is rendered in the HighwayEnv simulator. In such environment con/figuration, the ego vehicle is at the center of the parking place, i.e., its position is ( 0 . 0 , 0 . 0 ) ,and it has a heading of 0 . 0 (this parameter ranges in the interval [0 . 0 , 1 . 0 ) , representing a complete rotation). In the parking place, there are 20 parking spots, 5 cars parked at lanes 3, 5, 6, 8, and 13 and the target parking spot is at lane 20. The task of the DRL agent in this environment is to park the ego vehicle inside the target parking spot, with the proper heading.

The action space of the DRL agent is composed of two actions, namely, throttle and steering, both of which are continuous. The DRL agent receives a negative reward at each timestep, proportional to the Euclidean distance of the ego vehicle from the target. Moreover, it receives a constant positive reward when the target is reached and a big constant negative reward when it collides with a parked vehicle. The task is episodic with an episode /finishing when either the ego vehicle is parked in the right target spot with the right heading, or it collides with a parked vehicle, or the timeout, measured in number of timesteps, expires.

We made the environment con/figurable , such that the parameters of the con/figuration (i.e., the initial conditions at the beginning of each episode) can be changed programmatically. Correspondingly, an environment con/figuration needs to be valid , i.e., it has to respect the constraints imposed by the environment. Such constraints are designed by the developers of the environment to ensure that valid con/figurations are solvable by the DRL agent. In other words, any agent would be able to solve the task when starting from a valid initial con/figuration, since the environment does not contain any physically insurmountable obstacle or impediment.

The constraints de/fined for the Parking environment are the following: there cannot be a parked vehicle in the goal lane ( goal\_lane /nelement pvehicles ) since, otherwise, it would be impossible for the DRLagent to successfully park the vehicle in the target spot. Moreover, goal\_lane and pvehicles elements can vary in the interval [1 , 20],i.e.,thetargetcannotbeoutoftheparkingplaceand there cannot be vehicles outside the parking spots. The head\_ego parameter can vary in the interval [0 . 0 , 1 . 0 ) . The parameters pos\_ego.x and pos\_ego.y can vary in the intervals [ -10 , 10] and [ -5 , 5] , respectively; in the former case the constraint ensures that the ego vehicle is not too far from the parking place while the latter constraint avoids the ego vehicle to be too close to parked vehicles that would make any maneuver impossible and, as a consequence, the task unsolvable.

## 4 APPROACH

The goal of our approach is to exploit the data resulting from the interaction between the DRL agent and the environment during training in order to discover the weaknesses of the DRL agent at testing time. The interaction data we consider is in the form of pairs ( e i , c i ) where e i is the environment con/figuration at episode i during training and c i is a class label, i.e., a boolean value indicating whether the DRL agent failed ( c i = 1) or not ( c i = 0) at the task in episode i .

Our approach exploits the information on the failures that happened during training with the objective of generating new critical test cases, i.e., environment con/figurations, in which the DRL agent under test (i.e., the DRL agent at the end of training) is likely to fail. Since the execution of the DRL agent in the environment is computationally expensive we avoid the execution of candidate new environment con/figurations that are not promising, i.e., that are unlikely to lead to the exposure of a failure. The training interaction data can be leveraged to predict which, among the newly generated environment con/figurations at testing time, are more likely to produce a failure and the DRL agent can be executed only in environments with such promising con/figurations.

The current state-of-the-practice to test DRL agents is to evaluate them for a certain number of episodes, each with an environment con/figuration generated at random [43, 63]. However, environment con/figurations generated at random are unlikely to expose failures, although speci/fic

Fig. 2. Overall approach for testing DRL agents. Our approach takes as input a trained DRL agent that produced the interaction data ( A ) we use in the first step of our approach ( B ), i.e., Surrogate Model Training ❶ . The second step of our approach is Failure Search ❷ , that selects the environment configuration in which the DRL agent is more likely to fail. In the evaluation ( C ), we execute the selected environment configuration on the real environment to check whether the execution results in a failure or not.

<!-- image -->

environment con/figurations may exist that are challenging for the DRL agent under test. On the other hand, the DRL training process o/ffers a valuable source of information exploitable at testing time to e/fficiently expose failures of the DRL agent under test. The intuition is that the failures experienced during training by weaker versions of the DRL agent under test are representative of critical environment con/figurations of the DRL agent at the end of training, and can be used to guide the generation of new environment con/figurations that can challenge it.

Figure 2 summarizes the overall approach to use interaction data produced during the DRL training process to test a DRL agent. Our approach (i.e., Figure 2(B)) takes as input a trained DRL agent (i.e., Figure 2(A)) as well as the output of the DRL training process (i.e., pairs of interaction data ( e i , c i ) ). The /first step of our approach ❶ consists of training a surrogate model of the environment on the set of interaction data. The surrogate model is used in the next step (i.e., failure search) to predict whether unseen environment con/figurations are likely to be failures. The failure search step ❷ uses the surrogate model to generate environment con/figurations in which the DRL agent is more likely to fail, by acting as a proxy for the execution of the DRL agent in the environment with such con/figurations. The output of step ❷ is the environment con/figuration that, considering the surrogate model a classi/fier, has the highest failure prediction among the candidates (in Figure 2 the selected environment con/figuration is Env1 and it is encircled with a dashed line). Finally, we evaluate the e/ffectiveness of our approach by executing the DRL agent (i.e., Figure 2(C)) in an environment with such con/figuration, to check whether the execution results in an actual failure or not.

## 4.1 Surrogate Model Training

4.1.1 Classifier. The simplest implementation for the surrogate model is to train a classi/fier to predict whether, given an environment con/figuration, the DRL agent under test will fail in it or not. The interaction dataset D = { ( e 1 , c 1 ) ,..., ( e N , c N ) } is used to train such classi/fier. In particular, we train a softmax classi/fier (i.e., a neural network) to minimize the cross-entropy loss.

The classi/fication problem to address in step ❶ presents some peculiar characteristics. In particular, the dataset D might be unbalanced , i.e., the number of environment con/figurations in which the agent fails ( c = 1) is much lower than the number of environment con/figurations in which the agent does not fail ( c = 0). The reason is that at the beginning of the training process the DRL agent fails in most of the environment con/figurations while, as training goes on, the DRL agent fails less and less until the training process converges and failures become rare. One of the strategies to train a classi/fier in situations of class unbalance is to introduce a weight vector W in the loss function [12]. Such vector has two components, i.e., one for each class, in order to scale the loss

function for each i th datapoint w.r.t. the class the datapoint belongs to. The idea is to give more weight to the datapoints of the underrepresented class (i.e., the failure class) such that the classi/fier can learn to classify datapoints of both classes equally. In the literature [13, 32], there are various proposals for the computation of W . Our implementation choice is described in Section 5.2.

4.1.2 Regressor. When the number of failures in interaction dataset D is too small to train a reasonable classi/fier, another implementation choice for the surrogate model is to train a regressor. Contrary to the classi/fier, where it is straightforward to log when episodes are successful/unsuccessful during training, training a regressor requires de/fining a function that computes a continuous value for each episode. Such value should be small when the agent is close to a failure in a certain episode while it should be increasingly larger as long as the agent is far from a failure. The labels c 1 ,..., c N in the interaction dataset D are continuous values and the training objective is to minimize the Mean Squared Error between the prediction of the regressor and the ground-truth value in the dataset.

Similarly to the classi/fier, the interaction dataset D can be unbalanced, with the majority of the data having a continuous value which represents far-from-failure situations. Also in this case, we use weighted training to cope with the unbalanced dataset (see Section 5.2).

In the following sections (i.e., Sections 4.2 and 4.3), we describe our approach considering the classi/fier as an implementation choice for the surrogate model. Without loss of generality, the same applies when the surrogate model is implemented as a regressor.

## 4.2 Failure Prediction

The classi/fier is used to predict the class of any environment con/figuration that is not present in the dataset D of interaction data. Therefore, it is a proxy for the actual execution of the DRL agent in the environment with a speci/fic con/figuration. More formally, it is a function f : E → [0 , 1] ∈ R that takes as input an environment con/figuration e ∈ E and outputs a failure prediction.

Given the classi/fier, our objective is to generate an environment con/figuration that maximizes the failure prediction, i.e., to solve ˆ e = arg max e f ( e ) , and then execute the DRL agent in an environment with con/figuration ˆ e . It should be noticed that the real failure prediction function f ∗ is only approximated by our classi/fier, i.e., f ≈ f ∗ . The reason for the approximation is two-fold: (1) the dataset D of interaction data might not be large and diverse enough to best represent all realworld conditions; (2) during training the DRL agent performs non-deterministic actions to better explore the state space. As a consequence, a failure that happens during training might be due to some non-deterministic actions carried out in a speci/fic episode. Despite the mismatch between f and f ∗ , failure search (see Figure 2) can still e/ffectively use the classi/fier that implements f , provided its feedback on the failure prediction of a new environment con/figuration can be reliably used to converge toward inputs that challenge the DRL agent. In fact, failure search does not require perfect guidance toward optimal inputs, it just needs a direction where the search should be directed [31].

## 4.3 Search-based Failure Search with Surrogate Models

For the failure search step ❷ , we use the output of the classi/fier as a /fitness function to be optimized by a search-based algorithm. In particular, we consider two search-based algorithms, i.e., Hill Climbing and Genetic Algorithm , to generate the environment con/figurations where to execute the DRL agent.

Hill climbing is a local search algorithm that, starting from an arbitrary candidate solution to the problem (i.e., an environment con/figuration), incrementally changes such solution to create new ones. If a better solution is found the process is repeated until the current solution can no

<!-- image -->

longer be improved or a timeout expires. Genetic algorithm is a population-based algorithm that combines global and local search to avoid getting stuck in local optima while improving existing solutions.

Algorithm 1 shows the pseudocode of the hill climbing algorithm for the generation of environment con/figurations. It takes as input the classi/fier f , the size of the neighborhood NS of the current solution e and an optional environment con/figuration in which the DRL agent failed during training e f . If such environment con/figuration is not provided, the initial solution is generated at random (see if statement at Lines 2-6). The for loop at Lines 13-19 computes, at each i th iteration, the neighbors of the current solution e . Indeed, function /m.sc/u.sc/t.sc/a.sc/t.sc/e.scE/n.sc/v.scC/o.sc/n.sc/f.sc/i.sc/g.sc at Line 15 takes care of mutating the current solution e and ensuring the validity of the result, i.e., the mutation is applied to e only if the new environment con/figuration e i is valid. The mutated solution e i is then evaluated by the classi/fier to compute its failure prediction at Line 16. At Lines 17-18 each mutation of the current solution, i.e., e i , is stored as well as its failure prediction fp i .Attheend of the loop, the index j of the neighboring solution with the maximum failure prediction is computed (Line 21) which is used to retrieve the corresponding neighboring solution e (Line 22). The outermost loop at Lines 8-23 is repeated until there is a search budget (i.e., the timeout )thathas not expired. Finally, the best solution e is assigned to ˆ e and returned.

The Genetic Algorithm showninAlgorithm2 takes as input the classi/fier f , the population size PS , the crossover rate cr , and an optional set of environment con/figurations in which the DRL agent failed during training E f . Such a set can be used to /fill the initial population when it is available; otherwise, the initial population is generated randomly (Line 2). The /c.sc/o.sc/m.sc/p.sc/u.sc/t.sc/e.scF/i.sc/t.sc/n.sc/e.sc/s.sc/s.sc

<!-- image -->

procedure at Line 3 computes the /fitness value for each solution in the population, which is its failure prediction as computed by the classi/fier f . At Line 8 a new population with the best solutions from the current population is instantiated ( elitism ). The while loop at Lines 10-26 is the evolution part of the algorithm which terminates when the size of the new population reaches the target population size PS . At the beginning of the loop, two solutions are selected based on their /fitness (Lines 12-13) and are crossed over according to the crossover probability cr (Lines 18-20); the solutions oe 1 and oe 2 are modi/fied only if the crossover results are two valid con/figurations. Afterward, the solutions are mutated (Lines 22-23, also in this case oe 1 and oe 2 are modi/fied only if the changes result in valid con/figurations) and /finally, the best solutions (either the parents pe 1, pe 2 or the o/ffsprings oe 1, oe 2) are stored in the new population (Line 25). At the end of the while

loop the current population is assigned the new population and the /fitness value of each solution is computed (Lines 28-29). The /r.sc/e.sc/s.sc/e.sc/e.sc/d.scP/o.sc/p.sc/u.sc/l.sc/a.sc/t.sc/i.sc/o.sc/n.sc procedure at Line 31 takes care of avoiding stagnation by reseeding the population according to the currentIteration variable.Ifthesetoffailure environment con/figurations E f is available, the solutions with the worst /fitness in the current population are replaced by randomly sampled solutions from the set E f ; otherwise, the worst solutions are replaced by randomly generated individuals. The main loop (Lines 6-33) terminates when the search budget expires. Finally, the solution with the best /fitness ˆ e is taken at Line 35 and returned.

Both search algorithms support seeding from existing environment con/figurations that caused a failure of the agent during training. In particular, let us suppose that the parameter e f is not null in Algorithm 1 (similarly, E f is not empty in Algorithm 2). The DRL agent may not fail in such environment con/figurations because it may have encountered similar environment con/figurations later during training, and it might have adapted to them. However, despite adaptation, the DRL agent might not perform well and a proper change in such environment con/figuration has the potential to trigger a failure. For example, let us suppose that the environment con/figuration e f = [20 , 0 . 0 , { 3 , 5 , 6 , 8 , 13 , 19 } , ( 0 . 0 , 0 . 0 ) ] causes a failure of the DRL agent during training in the Parking environment (it is the same environment con/figuration shown in Figure 1, except that there is a parked vehicle beside the target spot), but the DRL agent does not fail in e f at testing time. However, changing the heading of the ego vehicle from 0.0 to 0.5, i.e., the opposite direction w.r.t. the target spot, might result in the DRL agent not being able to turn the vehicle and park it with the right heading, since the mutation has succeeded in making the environment more challenging, eventually exposing a failure.

4.3.1 Mutation Function. Knowing what to change in the environment con/figuration is important for /finding failures. The function that makes this decision is the /m.sc/u.sc/t.sc/a.sc/t.sc/e.scE/n.sc/v.scC/o.sc/n.sc/f.sc/i.sc/g.sc function (Line 13 in Algorithm 1 and Lines 14-15 in Algorithm 2). We propose two implementations of the /m.sc/u.sc/t.sc/a.sc/t.sc/e.scE/n.sc/v.scC/o.sc/n.sc/f.sc/i.sc/g.sc function: the /first one randomly changes a parameter of the given environment con/figuration, choosing among all parameters with equal probability. The second one only changes the parameter of an environment con/figuration that contributes the most to the failure prediction. The idea is that some parameters are more critical than others when considering how they a/ffect failure prediction. Hence, changing them is more bene/ficial for generating failure environment con/figurations than changing the other parameters. In particular, we propose to use the saliency method to compute input attribution [55], i.e., to determine how much an input in/fluences a prediction made by a neural network (our classi/fier). Given an environment con/figuration and the classi/fier, the saliency method computes the input gradient, i.e., the partial derivatives over all the individual parameters of the environment con/figuration. The absolute value of each gradient indicates which input parameter is more critical to the failure prediction and its sign indicates the direction (i.e., positive or negative) of change. In our saliency-guided implementation of the /m.sc/u.sc/t.sc/a.sc/t.sc/e.scE/n.sc/v.scC/o.sc/n.sc/f.sc/i.sc/g.sc function we take the output computed by saliency and change the parameter in the environment con/figuration whose corresponding gradient is the highest one; the direction of change, either positive or negative, depends on the sign of the gradient.

For instance, let us consider the environment con/figuration: e = [20 , 0 . 0 , { 3 , 5 , 6 , 8 , 13 , 19 } , ( 0 . 0 , 0 . 0 ) ], i.e., an environment con/figuration of the Parking environment. The input attribution for this environment con/figuration is an array of the same size as the input, as it contains the partial derivatives over each input. Let us suppose that the highest value in the array is in the second position, i.e., the position corresponding to the parameter head\_ego , and that such value is positive. This means that the parameter head\_ego is the most critical one a/ffecting the failure

prediction of the classi/fier and that changing it in the positive direction, i.e., increasing it will also increase the failure prediction for the resulting environment con/figuration.

- 4.3.2 Crossover Function. The crossover function is speci/fic to the genetic algorithm (Line 12 in Algorithm 2). We propose a single-point crossover implementation where, given two environment con/figurations, the cut point is determined randomly; then the elements in the two con/figurations before and after the cut point are swapped. We chose this simple implementation for crossover because it can be applied to con/figurations of any case study with little to no modi/fications. Custom implementations that take into account the speci/fic features of each case study remain possible in our implementation.

For instance, let us suppose that e 1 = [20 , 0 . 0 , { 3 , 5 , 6 , 8 , 13 , 19 } , ( 0 . 0 , 0 . 0 ) ]and e 2 = [15 , 0 . 5 , { 1 , 3 , 9 } , ( -1 . 0 , 7 . 5 ) ] are two Parking environment con/figurations. The cut point is computed based on the number of parameters in the environment con/figuration, which in the case of Parking is equal to four (i.e., goal\_lane , head\_ego , pvehicles ,and pos\_ego ). Let us suppose that the cut point is at /first position. The two environment con/figurations after crossover will be as follows: ce 1 = [20 , 0 . 5 , { 1 , 3 , 9 } , ( -1 . 0 , 7 . 5 ) ]and ce 2 = [15 , 0 . 0 , { 3 , 5 , 6 , 8 , 13 , 19 } , ( 0 . 0 , 0 . 0 ) ].

## 4.4 Implementation

Weimplemented our approach in a Python tool called I/n.sc/d.sc/a.sc/g.sc/o.sc (Latin for 'search') which is publicly available [7]. The DRL agents are implemented by the stable-baselines [47] library and we use Pytorch [44]and scikit-learn [45] to implement the training and inference of the classi/fier. The saliency method is implemented by the captum library [33].

## 5 EMPIRICAL EVALUATION

We consider the following research questions:

RQ1 (E/ffectiveness): What is the e/ffectiveness of the proposed approach? Can it generate failure environment con/figurations for the DRL agent under test?

- RQ2 (Comparison): How does I/n.sc/d.sc/a.sc/g.sc/o.sc compare against the random baseline? How does it compare against the state-of-the-art sampling approach?
- RQ3 (Hyperparameters): What is the impact of the key hyperparameters of I/n.sc/d.sc/a.sc/g.sc/o.sc?
- RQ4 (Ine/ffective Failure Seeds): How does the e/ffectiveness of I/n.sc/d.sc/a.sc/g.sc/o.sc change when failure seeds are ine/ffective at testing time?
- RQ1 evaluates the e/ffectiveness of I/n.sc/d.sc/a.sc/g.sc/o.sc, i.e., its capability to generate failure environment con/figurations for the DRL agent under test. This research question acts as a sanity check to make sure that our approach is able to generate failures that we can analyze and study in the next research question.

RQ2 compares I/n.sc/d.sc/a.sc/g.sc/o.sc with the random baseline and the state-of-the-art sampling approach [63], both w.r.t. the number of generated failure environment con/figurations and their diversity.

RQ3 investigates the key hyperparameters of I/n.sc/d.sc/a.sc/g.sc/o.sc both w.r.t. the number of failure environment con/figurations that are generated by each hyperparameter setting and their diversity. We /first analyze the choice of the search algorithm, i.e., hill climbing vs. genetic algorithm. Second, since our approach can work both with and without the provisioning of failure seeds ( e f and E f are optional parameters in Algorithms 1 and 2, respectively), we want to investigate which of the two seed strategies is more convenient to use. Third, when mutating the environment con/figurations I/n.sc/d.sc/a.sc/g.sc/o.sc can either choose them randomly or it can focus on those that have the highest in/fluence on the failure prediction, as determined by the saliency method. Hence, we want to evaluate the impact of the mutation strategy when using I/n.sc/d.sc/a.sc/g.sc/o.sc.

Testing of Deep Reinforcement Learning Agents with Surrogate Models

In RQ4, we study how I/n.sc/d.sc/a.sc/g.sc/o.sc performs when failure seeds, i.e., failure con/figurations causing a failure of the agent during training, are not e/ffective for the DRL agent at testing time. In this research question, we want to study whether I/n.sc/d.sc/a.sc/g.sc/o.sc is still e/ffective at /finding failures when there is no guidance from the failure seeds.

## 5.1 Case Studies

Parking. We already introduced the /first environment, i.e., Parking [37], in Section 3,wherewe also describe the representation of the environment con/figuration (see Figure 1). Such environment has been used in several studies, especially to evaluate the capabilities of DRL agents [10, 11, 16, 17, 57, 72, 76].

The encoding adopted to train a classi/fier for failure prediction consists of an array of 24 elements where the /first two values are for the two single-value parameters of the environment con/figuration (i.e., goal\_lane and head\_ego ), followed by 20 values corresponding to the one-hot encoding of the pvehicles parameter (i.e., each position has a value of 1 only when there is a vehicle in the respective spot; a value of 0 otherwise) and /finally the two values of the pos\_ego parameter.

Regarding the mutation operators, the goal\_lane parameter value is increased or decreased, with equal probability, and the change amount is an integer in the interval [1 , 20]. The heading\_ego parameter value is increased or decreased, with equal probability, and the change amount is a random /floating point number in the interval [0 , 1 ) . The coordinates of the pos\_ego parameter tuple are increased or decreased, with equal probability, by a random /float in the interval [0 , 1]. For what concerns the pvehicles parameter, with equal probability parked cars in the pvehicles set are added/removed or the parked car occupancy indexes are mutated. In the former case, a certain number of indices is selected at random for adding or removing parked cars with equal probability. In the latter case, a certain number of parked car indices is selected at random to be mutated, i.e., increased or decreased by a random integer in the interval [1 , 20]. Moreover, the crossover operator considers the parameters of the Parking environment con/figurations as unique features. An example of crossover for the Parking environment is in Section 4.3.2.

Humanoid. The second environment we consider is Humanoid , built using the Mujoco simulator [62]. Mujoco is a very popular simulator in the DRL community, especially to benchmark DRL algorithms in continuous control tasks. The 3D physics simulator has di/fferent pre-built environments, with Humanoid being one of the most challenging environments for DRL algorithms [69].

In Humanoid the DRL agent needs to control a bipedal robot in a 3D space and make it walk on a smooth surface. Each environment con/figuration is composed of two arrays, i.e., joints\_pos and joints\_vel . The former has size 24 and consists of the positions and rotations of the joints of the robot while the latter has size 23 and consists of the linear and angular velocities of those joints (more information are available in the Gym Library wiki [18]). All the angles are in radians, represented as /floating point numbers. Figure 3 shows two con/figurations of the Humanoid environment; we can see that the initial joints positions can be altered, forming di/fferent initial con/figurations for the Humanoid robot (Figure 3(A) and (B) shows the rendering of two di/fferent environment con/figurations, while Figure 3(C) shows how an environment con/figuration is encoded.).

The state space of the DRL agent is an array of size 376, composed of joint positions, angles, and relative velocities, plus other components as center of mass inertia and velocity. The action space is composed of 17 elements that are the degrees of freedom of the robot, i.e., the actuated joints of the robot. The reward function encourages the DRL agent to walk as fast as possible plus a bonus for each timestep. An episode terminates when the abdomen /y.alt coordinate of the robot goes out of the range ( 1 , 2 ) , which indicates that the robot has fallen down, or when a timeout expires (we

Fig. 3. Two initial configurations ( A and B )ofthe Humanoid environment in the Mujoco simulator [62]. The right-hand side ( C ), shows an example of environment configuration.

<!-- image -->

set such timeout to 300 timesteps). When the robot falls we deem the episode unsuccessful; on the other hand, when the timeout expires we consider the episode successful. We changed the environment interface to be con/figurable such that the parameters joints\_pos and joints\_vel can be set programmatically at the beginning of each episode. The original implementation initializes joints\_pos to [0 , 0 , 1 . 4 , 1 , 0 ,..., 0] and joints\_vel to all zeros; then, to generate a new environment con/figuration, each value in both arrays can be changed by adding or subtracting a small quantity m (which is set to 0.03). We followed the original implementation to de/fine validity , i.e., we consider an environment con/figuration to be valid only if the values of its parameters, i.e., joints\_pos and joints\_vel , are within the interval [ -m , m ] w.r.t. the initial values of such parameters. For instance, a valid environment con/figuration can have the third value of its joints\_pos array within the interval [1 . 4 -m , 1 . 4 + m ], since that value is initialized to 1.4.

The encoding we adopt as input to the classi/fier is the concatenation of the two arrays that de/fine the environment con/figuration, i.e., joints\_pos and joints\_vel . Regarding the mutation operators, they are de/fined in the same way for both parameters, i.e., once an index of either joints\_pos or joints\_vel is chosen at random, the value at that index is either decreased or increased, with equal probability, by a random /floating point quantity in the interval [ -m , m ]. Moreover, the crossover operator is based on the parameters of the Humanoid environment, as in Parking. In Humanoid there are only two parameters, i.e., joints\_pos and joints\_vel ;therefore, during crossover, the entire joints\_pos and joints\_vel vectors are swapped between two di/fferent environment con/figurations.

Driving. The third environment we consider is Driving , built using the DonkeyCar simulator [61]. This platform has been used in previous works to train and test self-driving car software both based on supervised learning and RL [66, 67, 75, 77].

In this environment, the DRL agent controls a car which drives along a track. The con/figuration determines the shape of the track in which the car drives (see Figure 4(A)). The track is represented as a list of 12 pairs, where each pair consists of two elements, i.e., a command and a value (see Figure 4(C)). The possible commands are S which indicates a straight line, R which indicates a right curve, L which indicates a left curve and DY which signals the beginning of a curve. The value associated with each command represents the number of road units, except for the DY command, where the associated value represents the individual angle of rotation for each road unit. For example, the sequence of commands [(S,2), (DY,15.2), (L,3)] (i.e., the /first curve of the track represented by the environment con/figuration in Figure 4(B)) instructs the road engine to build a straight road for two road units followed by a left curve which is three road units long, for a total of 45.6 degrees (i.e., 3 × 15 . 2).

The state space of the DRL agent is an RGB image of size (160, 120) taken by the front camera of the car, while the action space is two-dimensional, i.e., steering angle and throttle. The DRL

<!-- image -->

<!-- image -->

Fig. 4. An initial configuration of the Driving environment in the DonkeyCar simulator [61]. The le/f\_t-hand side shows both the plot ( A ) of the environment configuration on the right-hand side ( C ), and how the configuration is rendered in the DonkeyCar simulator ( B ).

<!-- image -->

agent receives a small negative reward per timestep plus a positive reward every time it crosses a waypoint in the track, minus a penalty related to the cross track error , i.e., the distance between the center of mass of the car and the center of the track. The /first component of the reward encourages the DRL agent to go as fast as possible, the second privileges the progress on the track, and the third forces the DRL agent to drive as close to the center of the track as possible. Moreover, the DRL agent receives a large negative reward when it goes o/ff-road. In such cases, the episode terminates unsuccessfully, whereas when the DRL agent crosses the end line of the track, the episode is deemed successful.

We modi/fied the DonkeyCar simulator to make it con/figurable , such that at the beginning of each episode the track passed as input can be instantiated in the simulator. Regarding validity ,thereare a number of constraints that need to be respected in order for the track to be valid. First, each track should start and end with an S command. Moreover, after a DY command there must be either an L or R command. Afterward, the track must not contain loops (i.e., self-intersections), it must not have very sharp turns (i.e., with a rotation angle > 170 · ) and it must have at least 3 curves, one of which must be with a rotation angle of at least 120 · . The last two constraints ensure that the generated tracks are non-trivial.

The encoding we adopt as input to the classi/fier is the concatenation of two arrays, i.e., the array of all commands, where each command is given a unique integer identi/fier, and the array of all values. Regarding the mutation operators, we de/fine one for commands and one for values and, when analyzing a command-value pair, we either change the command or the value with equal probability. The change command operator can only change an L command to an R command or vice-versa. The change value operator /first analyzes the associated command and, if it is a DY command, it either increases or decreases, with equal probability, the current value by a random /floating point number in the interval [0 , 50]. Otherwise, it increases or decreases, with equal probability, the current value by a random integer number in the interval [1 , 20]. Moreover, the crossover operator considers the Driving con/figuration as a list of command-value pairs. The cut point is a value in the interval [1 , 12] such that after crossover the resulting Driving environment con/figurations have command-value pairs coming from both parent environment con/figurations. We customized the crossover operator for this environment by retrying crossover a certain number of times, until either both resulting con/figurations are valid or the maximum counter is reached.

## 5.2 Experimental Setup

5.2.1 Procedure. We trained the DRL agents in each environment using the hyperparameters recommended by Ra/ffin et al. [46, 47]. For RQ4, we simpli/fied the training environments in order to make the resulting agents more robust against training failure con/figurations. In particular, we

Table 1. Case Studies Training Metrics

|                                      |                 | RQ1-RQ3           | RQ1-RQ3           | RQ1-RQ3           | RQ4               | RQ4               | RQ4               |
|--------------------------------------|-----------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|
|                                      | DRL algo        | # t r . timesteps | # tr . episo d es | # t r . failur es | # t r . timesteps | # tr . episo d es | # t r . failur es |
| P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc      | HER [3]+TQC[34] | 200 k             | 8,789             | 206               | 80 k              | 3,159             | 17                |
| H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc | TQC [34]        | 1,500 k           | 6,965             | 270               | 300 k             | 1,141             | 318               |
| D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc      | SAC [20]        | 10,00 k           | 11,274            | 194               | 500 k             | 1,445             | 13                |

removed the parked vehicles in Parking, we decreased the variation of the two parameter vectors determining the initial con/figurations in Humanoid (i.e., we set m to 0.01 instead of 0.03, see the respective paragraph in Section 5.1), and we constrained the number of curves in Driving to be ≥ 2 instead of ≥ 3, of which the hardest one must be ≤ 130 · instead of ≤ 170 · .Table1 shows in Column 1 the algorithms we used for each environment. Column 2 (respectively, Column 5 for RQ4) shows the number of training timesteps which corresponds to the number of episodes shown in Column 3 (respectively, Column 6 for RQ4). In each setting we trained each DRL agent until convergence, i.e., until the success rate in the last 100 episodes was 100%. Columns 4 and 7 show the number of training failures in the two settings after /filtering the initial 30% of the data. We can observe that, in the /first setting (i.e., Column 4), the number of failures remaining after /filtering is comparable and close to 200 on average. In the second setting (i.e., Column 7), the number of failures is much lower in Parking and Driving (i.e., ≈ 20 vs. ≈ 200), while in Humanoid the DRL agent experiences a drop in performance after the initial training phase in both settings, making the number of training failures after /filtering comparable.

Given the low number of failures in the second setting, we resort to a regressor-based surrogate model. Indeed, a regressor, as opposed to a classi/fier, can learn from near-failure interactions during training, i.e., where the agent got close to failing but ultimately such interactions were successful. For the Parking environment, we considered the length of the episode as a continuous value. In Parking, an episode /finishes unsuccessfully when the agent is not able to park the vehicle in a certain amount of time. A long episode indicates a challenging environment con/figuration where the agent struggled to complete the parking task.

In Humanoid, we consider the abdomen latitudinal coordinate of the robot, which determines when the robot has fallen down; for a given episode we consider the minimum distance between the current coordinate and the respective lower and upper bounds. In Driving, we use the maximum cross-track error in an episode as a continuous value for the regressor.

In all case studies, we normalize the continuous values between 0 and 1, where 0 indicates a failure. When using the regressor-based surrogate model for failure search, our objective is to generate environment con/figurations that minimize the prediction of the regressor.

Classi/fier Training. To compensate for the unbalanced dataset when training the failure classi/fier, we compute a weight vector W as follows: given N datapoints, the array of class targets Y , with | C | being the number of classes:

W = N | C |· hist ( Y ) , (1)

where hist( Y ) is a function that outputs an array of size | C | indicating the number of datapoints for each class. Such formula, gives a higher weight to the underrepresented classes [32].

Testing of Deep Reinforcement Learning Agents with Surrogate Models

We chose a multi-layer perceptron as the classi/fier architecture. The reason is twofold: /first, the size of the available training data is small (at most 10 k examples for both failure and non-failure classes, see Table 1). Secondly, our environment con/figurations are small size one-dimensional feature vectors (24 for Parking, 47 for Humanoid, and 24 for Driving). Furthermore, since the output of the classi/fier is used as a /fitness function during testing, adopting complex models to process simple inputs would be ine/fficient and prone to over/fitting.

When training the failure predictor, there are two important hyperparameters to consider. The /first one is the amount of initial interaction data to /filter out. Indeed, at the beginning of training the agent carries out random actions and, as a consequence, it often fails regardless of the environment con/figuration. This means that the earliest failures are not useful to predict the failures of the DRL agent under test. The other hyperparameter that depends on the case study is the number of hidden layers of the multi-layer perceptron.

In our experiments, we considered nine levels of /filtering (i.e., 5, 10-80), where, for instance, 5% /filtering means that the /first 5% of the environment con/figurations are not considered for training the classi/fier. Moreover, we chose four di/fferent numbers of hidden layers, i.e., from 1 to 4, where each hidden layer is composed of 32 units followed by a batch normalization layer [27]and adropoutlayer[56] with the probability of dropout of 0.5 (except when the network has only one hidden layer). For each pair /filter-layers we trained the classi/fier 10 times (with all the other training hyperparameters, e.g., learning rate, /fixed), every time with a di/fferent random seed. We used a validation set formed using 20% of the data to save the best model during training which we evaluated on a held-out test set. We built such a test set by choosing a /filtering level of 5% for each case study and by selecting 10% of the data at random. During hyperparameter tuning, we measured precision and recall of the classi/fier considering the failure class as the positive class. We deem precision more important than recall since in order to guide failure search it is more important for the classi/fier to be as precise as possible (i.e., few false positives) even at the cost of missing some failures (i.e., false negatives). We chose the best classi/fier model by looking at the results of the 10 training runs for each /filter-layers pair, and we selected the model with the highest precision and with a recall of at least 10%, in order not to miss too many failures.

The best classi/fier for Parking has four hidden layers and a /filtering level of 50%, reaching a precision of 24% and a recall of 17% on the held-out test set. The best classi/fier for Humanoid has one hidden layer and a /filtering level of 10%, reaching a precision of 62% and a recall of 48% on the held-out test set. Finally, the best classi/fier for Driving has four hidden layers and a /filtering level of 30%, reaching a precision of 25% and a recall of 12% on the held-out test set. Such classi/fiers are used in the failure search step for each case study.

Regressor Training. In order to deal with the unbalanced regression problem, we use the Label Distribution Smoothing ( LDS ) approach proposed by Yuzhe et al. [73]. The approach starts by convolving a symmetric kernel with the empirical distribution of continuous values (e.g., obtained by binning the continuous values). The output is a smoothed version of the distribution that we use in the loss function to proportionally weight each continuous value based on their frequency.

The best regressor for Parking has two hidden layers and a /filtering level of 10%; the best regressor for Humanoid has two hidden layers and a /filtering level of 30%; the best regressor for Driving has two hidden layers and a /filtering level of 5%. Such regressors are used in the failure search step for each case study when training failures are ine/ffective at testing time and correspondingly a regressor is potentially a better surrogate model than a classi/fier.

Baselines. We compared I/n.sc/d.sc/a.sc/g.sc/o.sc with two approaches. The /first approach is the random baseline, where environment con/figurations are generated at random. Such baseline is useful to understand whether the proposed approach is able to outperform the state-of-the-practice in testing DRL agents [43, 63]. The second approach is the state-of-the-art sampling approach by Uesato

et al. [63]. This simple approach consists of generating a large initial set of T environment con/figurations at random and choosing the one that, according to the classi/fier f , has the highest failure prediction.

I/n.sc/d.sc/a.sc/g.sc/o.sc. We considered hill climbing and genetic algorithm each with four di/fferent settings. In the /first one, the seed environment con/figurations to evolve are generated at random (hcrnd and garnd). In the second, we used hill climbing and genetic algorithm to evolve failure environment con/figurations (hcfail and gafail). Regarding the settings with the failure seeds, we always /filtered out the initial 30% of the environment con/figurations, in order not to include failures that are likely not representative of the failures of the DRL agent under test. Furthermore, we considered hill climbing and genetic algorithm guided by saliency, evolving both random (hcsal+rnd and gasal+rnd) and failure (hcsal+fail and gasal+fail) environment con/figurations. For I/n.sc/d.sc/a.sc/g.sc/o.sc and the sampling approach we considered a /fixed search budget B per environment con/figuration. In particular, we chose B = 5 seconds for Parking and Humanoid, while B = 30 seconds for Driving, as from preliminary experiments we observed that such budget was enough to reach /fitness convergence for all approaches.

During failure search, we generated T = 100 environment con/figurations for each case study and each approach, and we evaluated the respective DRL agent in each environment con/figuration. Since Humanoid and Driving simulators are non-deterministic we evaluated the respective DRL agents in each environment con/figuration 10 times. Moreover, we repeated the experiments 10 times for each failure search approach in order to cope with the intrinsic randomness of the approaches. Overall, we have three case studies, 10 techniques (eight settings of I/n.sc/d.sc/a.sc/g.sc/o.sc, including hill climbing vs. genetic algorithm, random vs. failure seed, two mutation strategies (i.e., random and saliency), plus the random and the sampling approaches), each generating 100 environment con/figurations (each repeated 10 times in Humanoid and Driving). Finally, each technique is executed 10 times for a total of 210 k simulations.

Hardware Resources. Due to the high number of simulations, we resorted to the university cluster with 20 CPU nodes, and parallelized the execution of the experiments to obtain the results in a reasonable amount of time. We did not make use of GPU nodes in the experiments, as DRL models are typically small and simulators could be executed headless on CPUs.

5.2.2 Metrics. In order to assess the e/ffectiveness of our failure search approach (RQ1)andto compare it with the baselines (RQ2), we measured the number of failures each approach triggers given the number of environment con/figurations to generate (i.e., T ) and the /fixed search budget B per environment con/figuration. In non-deterministic environments, such as Humanoid and Driving, we measured the failure probability of each environment con/figuration out of 10 runs and we considered an environment con/figuration to cause a failure if its failure probability is > 0 . 5. As each failure search approach is executed 10 times, we determined whether there is a statistical di/fference between the failures triggered by each pair of failure search approaches (including the 8 settings of I/n.sc/d.sc/a.sc/g.sc/o.sc) by computing the Mann-Whitney U Test [4, 40]. To measure the e/ffect size, we computed the Vargha-Delaney metric ˆ A 12 [65].

To compare the competing approaches (RQ2) we also considered two types of diversity regarding the failures generated by each approach, namely, input and output diversity. For input diversity , we /first clustered all the environment con/figurations that caused a failure, across all considered failure search techniques, obtaining a single partition of all such failing environment con/figurations. Similarly, for output diversity , we clustered the trajectories (i.e., positions over time) of the DRL agents on the failure inducing environment con/figurations. In Parking and Driving, we considered the trajectories of the vehicle while performing the task, i.e., parking the vehicle in the former case and driving along the track in the latter. In Humanoid, we considered

the trajectory of the height of the robot, which determines whether the robot falls or not. Since trajectories can have di/fferent lengths, we extended them with zero-padding to the maximum observed length. For both input and output diversity, we used the k-means clustering algorithm [5] and we determined the optimal number of clusters k ∗ by performing silhouette analysis [51], optimizing the balance between density and separation of the clusters. In our experiments, we varied the number of clusters k between two and the number of inputs (i.e., environment con/figurations or trajectories) to be clustered, and computed the silhouette score for each candidate. We increased k ∗ to a higher value only if the new silhouette was at least 20% greater than the best silhouette score found so far, in order to /filter out random /fluctuations of the silhouette score.

After applying clustering, we computed two diversity metrics for each failure search approach, namely coverage and entropy . Given a failure search approach A and the optimal number of clusters k ∗ , the coverage of the clusters for the approach A is de/fined as

C A = ∑ k ∗ i = 1 γ A ( i ) k ∗ , (2)

where the function γ A ( i ) : Z + → 0 | 1 determines whether a certain cluster labeled by i is covered by the failure search approach A , i.e., whether at least one failure generated by the approach A belongs to the cluster with label i .

The second metric, entropy , measures how uniformly the di/fferent failures are distributed across the clusters. Given the number of failures triggered by the failure search approach A in the i th cluster, F A ( i ) , the probability of /finding a failure generated by the approach A in cluster i is given by

and entropy is de/fined as

η A i = F A ( i ) ∑ i F A ( i ) , (3)

H A = k ∗ ∑ i = 1 η A i · log 2 ( η A i ) . (4)

In particular, entropy is zero when all failures are concentrated in one cluster, while it is maximum and equal to log 2 ( k ∗ ) when failures are distributed uniformly across all the clusters. Hence, we can use the following formula as a normalized measure of entropy ranging between 0 and 1:

H A = H A log 2 ( k ∗ ) . (5)

We ran clustering 10 times to account for randomness and took the average of coverage and entropy across the 10 runs. Then, we compared statistically the coverage and entropy averages for each failure search approach across the respective 10 runs by computing the Mann-Whitney U Test and the Vargha-Delaney e/ffect size.

To evaluate the impact of algorithm, seed, and mutation strategies on I/n.sc/d.sc/a.sc/g.sc/o.sc (RQ3), we used the same metrics: number of failures and input/output diversity, the latter quanti/fied with coverage and entropy.

## 5.3 Results

E/ffectiveness (RQ1). Column # Failures in Table 2 shows the average number of failure environment con/figurations triggered by each failure search approach out of 10 runs for each case study. Considering I/n.sc/d.sc/a.sc/g.sc/o.sc in all its settings, such number is between 4 and 27 in the Parking environment, between 1 and 4 in the Humanoid environment and between 2 and 42 in the Driving environment.

Table 2. Number of Failures and Input/Output Diversity Measured by Cluster Coverage or Entropy

|                            | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   |
|----------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|
|                            |                                             | Input                                       | Input                                       | Output                                      | Output                                      |                                                  | Input                                            | Input                                            | Output                                           | Output                                           |                                             | Input                                       | Input                                       | Output                                      | Output                                      |
|                            | #F a i l u r e s                            | Co v e rage (%)                             | Entr op y ( %)                              | Co v e rage (%)                             | Entr op y ( %)                              | #F a i l u r e s                                 | Co v e rage (%)                                  | Entr op y ( %)                                   | Co v e rage (%)                                  | Entr op y ( %)                                   | #F a i l u r e s                            | Co v e rage (%)                             | Entr op y ( %)                              | Co v e rage (%)                             | Entr op y ( %)                              |
| Baselines                  |                                             |                                             |                                             |                                             |                                             |                                                  |                                                  |                                                  |                                                  |                                                  |                                             |                                             |                                             |                                             |                                             |
| random                     | 1                                           | 55.00                                       | 18.37                                       | 31.94                                       | 12.13                                       | 1                                                | 16.08                                            | 0.00                                             | 30.00                                            | 0.00                                             | 1                                           | 10.00                                       | 0.00                                        | 10.00                                       | 0.00                                        |
| sampling                   | 13                                          | 50.00                                       | 0.00                                        | 41.10                                       | 22.91                                       | 1                                                | 14.72                                            | 12.62                                            | 25.00                                            | 0.00                                             | 5                                           | 80.00                                       | 54.97                                       | 75.00                                       | 37.50                                       |
| I/n.sc/d.sc/a.sc/g.sc/o.sc |                                             |                                             |                                             |                                             |                                             |                                                  |                                                  |                                                  |                                                  |                                                  |                                             |                                             |                                             |                                             |                                             |
| hc rnd                     | 4                                           | 75.00                                       | 41.54                                       | 44.72                                       | 26.90                                       | 1                                                | 29.57                                            | 19.03                                            | 40.00                                            | 0.00                                             | 3                                           | 65.00                                       | 23.80                                       | 56.00                                       | 10.11                                       |
| hc fail                    | 5                                           | 85.00                                       | 64.87                                       | 45.45                                       | 33.95                                       | 2                                                | 27.37                                            | 18.00                                            | 50.00                                            | 16.23                                            | 4                                           | 90.00                                       | 73.61                                       | 95.00                                       | 77.20                                       |
| hc sal+rnd                 | 6                                           | 90.00                                       | 57.37                                       | 48.01                                       | 31.59                                       | 4                                                | 59.73                                            | 61.32                                            | 95.00                                            | 77.75                                            | 2                                           | 60.00                                       | 28.37                                       | 60.00                                       | 26.48                                       |
| hc sal+fail                | 13                                          | 100.00                                      | 93.23                                       | 66.03                                       | 58.91                                       | 3                                                | 56.98                                            | 55.66                                            | 80.00                                            | 53.70                                            | 11                                          | 100.00                                      | 83.80                                       | 100.00                                      | 86.69                                       |
| ga rnd                     | 6                                           | 55.00                                       | 5.44                                        | 43.39                                       | 25.68                                       | 2                                                | 35.95                                            | 31.21                                            | 55.00                                            | 29.18                                            | 6                                           | 90.00                                       | 72.00                                       | 80.00                                       | 52.86                                       |
| ga fail                    |                                             | 50.00                                       | 0.00                                        | 46.55                                       | 29.64                                       | 2                                                | 24.99                                            |                                                  |                                                  | 43.48                                            | 22                                          | 90.00                                       | 85.63                                       |                                             | 19.16                                       |
| ga sal+rnd                 | 11                                          |                                             | 10.31                                       | 47.73                                       | 29.77                                       | 1                                                | 23.87                                            | 12.90 17.45                                      | 63.50 40.00                                      | 19.52                                            |                                             |                                             |                                             | 70.00                                       |                                             |
| ga                         | 8                                           | 60.00                                       |                                             |                                             |                                             |                                                  |                                                  |                                                  |                                                  |                                                  | 5                                           | 83.00                                       | 59.70                                       | 60.00                                       | 17.22                                       |
| sal+fail                   | 27                                          | 55.00                                       | 2.01                                        | 51.95                                       | 29.22                                       | 4                                                | 38.11                                            | 24.99                                            | 75.00                                            | 44.04                                            | 42                                          | 100.00                                      | 97.61                                       | 70.00                                       | 12.73                                       |

Values represent the average among ten runs. Bold faced values indicate a statistically signi/ficant di/fference between I/n.sc/d.sc/a.sc/g.sc/o.sc and sampling. Underlined values indicate a large e/ffect size.

RQ1 : Overall, I/n.sc/d.sc/a.sc/g.sc/o.sc successfully challenged the DRL agent under test in all case studies, by generating a signi/ficant number of failure environment con/figurations. On average, the sampling approach generated 6 failures while I/n.sc/d.sc/a.sc/g.sc/o.sc generated from 3 to 24 failures.

Comparison (RQ2). Table 2 shows the results for the /first setting: bold values indicate a statistically signi/ficant di/fference (at level α = 0 . 05) between I/n.sc/d.sc/a.sc/g.sc/o.sc and sampling; values are underlined when the e/ffect size is large. In particular, in Parking the best approach is gasal+fail that is able to generate more failures than other approaches (i.e., 27 on average) and the di/fference w.r.t. the sampling approach (i.e., 13 failures on average) is statistically signi/ficant with a large e/ffect size. In Humanoid, several settings of I/n.sc/d.sc/a.sc/g.sc/o.sc are signi/ficantly better than sampling (which exposes only 1 failure on average). In Driving, gasal+fail and hcsal+fail are the best approaches with, on average, 42 and 11 failures, respectively. Their di/fference w.r.t. the sampling approach (which exposes 5 failures on average) is statistically signi/ficant, and the e/ffect size is large.

Regarding the comparison with the random approach, in all case studies, classi/fier-based approaches (i.e., sampling and I/n.sc/d.sc/a.sc/g.sc/o.sc) found signi/ficantly more failures with a large e/ffect size. Hence, our empirical results show that the failures experienced during training are indeed related to and informative of the failures of the DRL agent under test.

The macro-columns Diversity in Table 2 shows the average out of 10 runs of the Coverage and Entropy metrics regarding input and output diversity for each failure search approach. Although in Parking the gasal+fail approach generates more failures than sampling, the two approaches are comparable in terms of input and output diversity (both considering coverage and entropy). On the other hand, the hcsal+fail approach is able to generate inputs that are both signi/ficantly di/fferent from those of sampling (i.e., higher coverage) and better distributed among clusters (i.e., higher entropy). Output coverage and entropy values are higher for hcsal+fail than those of sampling (66.03% and 58.91% vs. 41.10% and 22.91%, respectively), with output coverage being

comparable and output entropy being statistically better with a large e/ffect size. This means that, although hcsal+fail and sampling generated the same number of failures (i.e., 13 on average), the failures produced by hcsal+fail exercise more diverse behaviors of the DRL agent under test.

For what concerns Humanoid, the two best approaches in terms of input and output diversity are settings of I/n.sc/d.sc/a.sc/g.sc/o.sc (hcsal+rnd and hcsal+fail) both considering coverage and entropy (the gasal+fail setting is comparable to them). All of them are signi/ficantly better than sampling with a large e/ffect size, except for gasal+fail, whose input entropy (24.99%) is not signi/ficantly di/fferent from that of sampling (12.62%). In Driving, the best approaches in terms of input and output diversity are hcsal+fail and gasal+fail, the former signi/ficantly better than sampling on input coverage and output diversity and the latter on input diversity.

When comparing I/n.sc/d.sc/a.sc/g.sc/o.sc with the random approach in terms of diversity, most of the I/n.sc/d.sc/a.sc/g.sc/o.sc settings, especially those with the saliency-based mutations, are signi/ficantly better than random with a large e/ffect size in all case studies.

We identify two reasons for the higher diversity achieved by I/n.sc/d.sc/a.sc/g.sc/o.sc: (1) the search-based approach of I/n.sc/d.sc/a.sc/g.sc/o.sc implicitly favors diversity by locally exploring the environment con/figuration space until failures are discovered. In this way, it is more likely for I/n.sc/d.sc/a.sc/g.sc/o.sc to obtain failure con/figurations starting from diverse seeds. This is more di/fficult when failure con/figurations are to be randomly generated from scratch (e.g., by the sampling approach). (2) The failure environment con/figurations generated at training time a/ffect di/fferent versions of the agent under test, which is evolving during its training process. Hence, these con/figurations tend to be quite diverse. I/n.sc/d.sc/a.sc/g.sc/o.sc e/ffectively uses such seed con/figurations by retaining their diversity while at the same time increasing their failure exposure.

RQ2 : Overall, both considering the number of failures triggered and their input and output diversity, hcsal+fail is the best setting of I/n.sc/d.sc/a.sc/g.sc/o.sc, generating 50% more failures than the sampling approach as well as failures being 78% more input-diverse and 74% more outputdiverse.

Hyperparameters (RQ3). On average and considering all case studies, hcsal+fail generates failures that are the most diverse both in terms of input and output diversity. Speci/fically, hcsal+fail has the best input coverage (85.66% vs. 69.91% of the second best hcsal+rnd), the best input entropy (77.56% vs. 52.16% of the second best hcfail), the best output coverage (82.01% vs. 67.67% of the second best hcsal+rnd) and the best output entropy (66.43% vs. 45.27% of the second best hcsal+rnd).

Ontheother hand, gasal+fail is the setting of I/n.sc/d.sc/a.sc/g.sc/o.sc that generates the highest number of failures (i.e., 24 on average). However, such failures cover fewer clusters and have a lower entropy than the failures generated by hcsal+fail. Therefore, considering the number of failures and their diversity, hcsal+fail is the preferable I/n.sc/d.sc/a.sc/g.sc/o.sc setting.

Across all case studies, the settings of I/n.sc/d.sc/a.sc/g.sc/o.sc that use the saliency-based mutations are more e/ffective than their counterparts, i.e., the di/fference is always statistically signi/ficant with a large e/ffect size, except for Humanoid, where gasal+fail is comparable to garnd. Overall, this shows that the guidance o/ffered by the saliency-based mutation operator is e/ffective at /finding failure environment con/figurations.

In Table 3, we report a further comparison between the di/fferent settings of I/n.sc/d.sc/a.sc/g.sc/o.sc, focused on the impact of the seed strategy (random, rnd ,vs.failure, fail ). In each cell of the table the three case studies are separated by a forward slash '/' symbol. The symbol 'F' (respectively, 'R') indicates that failure seeding (respectively, random seeding) is statistically better than random (respectively, failure) seeding. A dash symbol indicates no statistically signi/ficant di/fference. For instance, for the raw hcrnd vs. hcfail and the macro-column # Failures, the hcrnd and the hcfail settings are equivalent

Table 3. Impact of the Seed Strategy

|                          | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc / H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc / D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc / H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc / D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc / H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc / D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc / H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc / D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc / H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc / D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   |
|--------------------------|----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|
|                          |                                                                                                                      |                                                                                                                      |                                                                                                                      | Output                                                                                                               | Output                                                                                                               |
|                          | # Failur e s                                                                                                         | Co v erage                                                                                                           | Entr op                                                                                                              | Co v erage                                                                                                           | Entr op                                                                                                              |
|                          |                                                                                                                      |                                                                                                                      | y                                                                                                                    |                                                                                                                      | y                                                                                                                    |
| hcrnd vs. hcfail         |                                                                                                                      | -/-/F -/-/F -/-/F -/-/F -/-/F                                                                                        |                                                                                                                      |                                                                                                                      |                                                                                                                      |
| garnd vs. gafail         |                                                                                                                      | F/-/F -/-/-                                                                                                          | -/-/-                                                                                                                | -/-/-                                                                                                                | -/-/-                                                                                                                |
| hcsal+rnd vs. hcsal+fail |                                                                                                                      |                                                                                                                      |                                                                                                                      | F/-/F -/-/F F/-/F -/-/F F/-/F                                                                                        |                                                                                                                      |
| gasal+rnd vs. gasal+fail |                                                                                                                      |                                                                                                                      |                                                                                                                      | F/F/F -/-/F -/-/F -/F/- -/-/-                                                                                        |                                                                                                                      |
| rnd vs. fail             | 0 vs. 8                                                                                                              | 0 vs. 3                                                                                                              | 0 vs. 4                                                                                                              | 0 vs. 3                                                                                                              | 0 vs. 3                                                                                                              |

In each row, the three case studies (Parking / Humanoid / Driving) are separated by a forward slash '/' symbol. An 'F' symbol indicates a statistically signi/ficant di/fference in favor of the approach with the failure seed; a dash '-' symbol indicates that the two approaches are indistinguishable and an 'R' symbol (missing in the table) would indicate a statistically signi/ficant di/fference in favor of the approach with the random seed. As for the comparisons in the /first two research questions, we compute statistical signi/ficance using the Mann-Whitney U Test.

in the /first two case studies, i.e., Parking and Humanoid (hence the two '-' symbols separated by the '/' symbol), while the hcfail setting is statistically better than hcrnd in the last case study, i.e., Driving (hence the last symbol is 'F'). In terms of number of failures, we can see that, in most cases (i.e., 8/12) the settings with the failure seeds are signi/ficantly better than their random seeds counterparts. In particular, the gasal+fail setting is signi/ficantly better than the gasal+rnd setting in all case studies (while, for instance, the hcsal+fail is signi/ficantly better than hcsal+rnd in Parking and Driving but not in Humanoid). From the point of view of diversity, the settings of I/n.sc/d.sc/a.sc/g.sc/o.sc with the failure seeds are mostly comparable to the settings with the random seeds, except for Driving, where the failure seeds are critical to generate more diverse failures. Random seeding is never a better choice (indeed the 'R' symbol is completely missing in the table), across all settings of I/n.sc/d.sc/a.sc/g.sc/o.sc and across all three case studies.

RQ3 : Overall, the settings of I/n.sc/d.sc/a.sc/g.sc/o.sc with the saliency mutation strategy and failure seeds are either comparable or signi/ficantly better than their random counterparts. In particular, failure seeds are signi/ficantly better than random seeds in 21 comparisons out of 60. Between hill climbing and genetic algorithm, the former is preferable because it generates more diverse failure scenarios (i.e., on average hill climbing failures are 60% more inputdiverse and 80% more output-diverse than genetic algorithm failures). The latter might be considered when the number of exposed failures is important, regardless of their diversity.

Ine/ffective Failure Seeds (RQ4). RQ3 shows that the saliency mutation operator is e/ffective at guiding the search towards failure-inducing con/figurations and that hill climbing generates more diverse failures w.r.t. genetic algorithm. To test the e/ffectiveness of I/n.sc/d.sc/a.sc/g.sc/o.sc in the presence of ine/ffective failure seeds, we consider its best con/figuration, i.e., hill climbing with the saliency mutation operator and random seeds. We chose to use random seeds to test whether the surrogate model is able to guide the search without relying on ine/ffective failure seeds. In Table 4, we indicate

Table 4. Number of Failures and Input/Output Diversity Measured by Cluster Coverage or Entropy

|                            | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   |                            |
|----------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|----------------------------|
|                            | Input                                       | Output                                      | Output                                      |                                             | Input                                            | Input                                            | Output                                           | Output                                           |                                                  |                                             |                                             | Output                                      | Output                                      | Output                                      |                            |
| #F a i l u r e s           | Co v e rage (%)                             | Entr op y ( %)                              | Co v e rage (%)                             | Entr op y ( %)                              | #F a i l u r e s                                 | Co v e rage (%)                                  | Entr op y ( %)                                   | Co v e rage (%)                                  | Entr op y ( %)                                   | #F a i l u r e s                            | Co v e rage (%)                             | Entr op y ( %)                              | Co v e rage (%)                             | Entr op y ( %)                              |                            |
| Baselines random           | 1 46.67                                     | 12.10                                       | 55.00                                       | 19.18                                       | 1                                                | 30.83                                            | 0.00                                             | 37.50                                            | 5.00                                             | 0                                           | 23.72                                       | 0.00                                        | 20.00                                       | 0.00                                        |                            |
| train. fail.               | 1 45.00                                     | 0.00                                        | 50.00                                       | 0.00                                        | 1                                                | 25.83                                            | 0.00                                             | 35.00                                            | 10.00                                            | 2                                           | 33.34                                       | 9.70                                        | 60.00                                       | 20.00                                       |                            |
| I/n.sc/d.sc/a.sc/g.sc/o.sc | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                       | I/n.sc/d.sc/a.sc/g.sc/o.sc                       | I/n.sc/d.sc/a.sc/g.sc/o.sc                       | I/n.sc/d.sc/a.sc/g.sc/o.sc                       | I/n.sc/d.sc/a.sc/g.sc/o.sc                       | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc                  | I/n.sc/d.sc/a.sc/g.sc/o.sc |
| hc sal+rnd                 | 3 71.67                                     | 57.22                                       | 80.00                                       | 62.42                                       | 3                                                | 43.42                                            | 7.29                                             | 75.00                                            | 48.08                                            | 3                                           | 57.97                                       | 48.61                                       | 80.00                                       | 56.23                                       |                            |
| hc sal+rnd                 | 3 80.00                                     | 62.06                                       | 80.00                                       | 52.88                                       | 12                                               | 81.92                                            | 57.35                                            | 100.00                                           | 86.02                                            | 10                                          | 91.16                                       | 88.10                                       | 100.00                                      | 80.48                                       |                            |

Values represent the average among ten runs. Bold faced values indicate a statistically signi/ficant di/fference between I/n.sc/d.sc/a.sc/g.sc/o.sc and the best of the two baselines. Underlined values indicate a large e/ffect size. We indicate with a bar the setting of I/n.sc/d.sc/a.sc/g.sc/o.sc using the /fitness-based surrogate model, i.e., hc sal+rnd .

with hcsal+rnd the version of I/n.sc/d.sc/a.sc/g.sc/o.sc using the classi/fier, while we indicate with hcsal+rnd the version of I/n.sc/d.sc/a.sc/g.sc/o.sc that uses the regressor-based surrogate model.

Table 4 shows that replaying training failures (row train. fail. ) is ine/ffective at testing time, being indistinguishable from random in Parking and Humanoid. In Driving, training failure replay generates signi/ficantly more failures than random (i.e., on average 2 vs. 0, respectively), although regarding diversity, only the output coverage of training failures is statistically di/fferent than the output coverage of failures generated by random.

Regarding I/n.sc/d.sc/a.sc/g.sc/o.sc, the version with the classi/fier-based surrogate model (i.e., hcsal+rnd) generates signi/ficantly more failures than random and training failure replay in Parking and Humanoid. Such failures are also signi/ficantly more diverse than the two baselines in Parking and Humanoid, although in the latter there is statistical signi/ficance only for output coverage. Regarding Driving, hcsal+rnd failures have a signi/ficantly higher input entropy, while for output coverage there is no statistical signi/ficance, although the trend is in favor of hcsal+rnd (i.e., 80 vs. 60 for output coverage and 56 vs. 20 for output entropy).

The version of I/n.sc/d.sc/a.sc/g.sc/o.sc with the regressor-based surrogate model (i.e., hcsal+rnd) is equally e/ffective at /finding failures as the classi/fier-based one in Parking. In Humanoid and Driving, it shows statistical signi/ficance of the di/fferences w.r.t. the two baselines, while the classi/fier-based one does not (e.g., on input diversity in Humanoid; number of failures, input and output coverage in Driving).

RQ4 : Overall, when failure seeds are ine/ffective, I/n.sc/d.sc/a.sc/g.sc/o.sc is able to trigger more failures of the DRL agent under test than replaying the training con/figurations at testing time in all three case studies (i.e., on average 3 vs. 1 in Parking, 12 vs. 1 in Humanoid and 10 vs. 2 in Driving), with the di/fferences being statistically signi/ficant with a large e/ffect size.

## 6 DISCUSSION

## 6.1 Solvability of the Failures

For each approach, we resumed training of the three DRL agents under test by feeding all the failure environment con/figurations found by each approach in every run of the environment generation

Fig. 5. t-SNE projection in 2D of trajectories associated with hc sal+fail ( ) and training failures ( ) configurations. The C symbol indicates the centroid of a cluster. The le/f\_tmost plot ( A ) shows trajectories for the Parking environment, the center plot ( B ) for the Humanoid environment, and the rightmost plot ( C )forthe Driving environment.

<!-- image -->

process. In every case study and for each approach, we found that the given DRL agent could learn how to successfully terminate the respective episodes by performing some additional training. This shows that the failure environment con/figurations generated by all approaches, and indeed by I/n.sc/d.sc/a.sc/g.sc/o.sc, are solvable by the DRL agents under test.

This also indicates that, although a generated environment con/figuration can be challenging for the DRL agent under test, there exists a sequence of actions that let the DRL agent solve the task successfully. For instance, after additional training, the DRL agent in the Parking environment is always able to /find trajectories for the vehicle to reach the target spot, which is instead missed by the original DRL agent under test. Similarly, in the Driving environment, the generated failure environment con/figurations do not induce track shapes that are beyond the mechanical capabilities of the vehicle. Regarding Humanoid, the initial positions of the joints as well as their velocities, resulting from the generated failure environment con/figurations, do not prevent the DRL agent to control and regain the balance of the robot.

Solvability of the failure environment con/figurations generated by I/n.sc/d.sc/a.sc/g.sc/o.sc shows that such environment con/figurations represent real weaknesses of the DRL agent under test, which could realistically occur during the operation of the DRL agent in production.

## 6.2 Training Failures

6.2.1 /Q\_ualitative Analysis. A simple way to test a DRL agent is to replay the training failures except for the earliest ones (e.g., by /filtering out the /first 30% of the failure environment con/figurations). However, this has two major downsides: (1) the DRL agent under test may have adapted to failure environment con/figurations in which weaker versions of itself failed, despite the exclusion of early failures; (2) such an approach would not be generative and by design it can only replay existing failure environment con/figurations. Generative approaches like hcsal+fail produce diverse and potentially unlimited challenging inputs exposing the limitations of the DRL agent, and potentially improving it through retraining [15].

For the sake of completeness, we replayed at testing time the failure environment con/figurations that happened during the DRL training process, and we compared them with the failures generated by hcsal+fail. In particular, we clustered the DRL agents trajectories associated with those failures, following the same process described in Section 5.2.2. Figure 5 shows a 2D t-SNE [64] projection of the failure trajectories of the three DRL agents under all failure environment con/figurations in each case study. Cluster centroids are indicated with the letter C . In Humanoid (Figure 5(B)) the trajectories associated with the failure environment con/figurations discovered by I/n.sc/d.sc/a.sc/g.sc/o.sc, cover

Table 5. Comparison between Training Failures (i.e., Train. Fail.) and Failures Found by the Best I/n.sc/d.sc/a.sc/g.sc/o.sc Se/t\_tings (i.e., hc sal+fail and ga sal+fail )

| P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | P/a.sc/r.sc/k.sc/i.sc/n.sc/g.sc Diversity   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | H/u.sc/m.sc/a.sc/n.sc/o.sc/i.sc/d.sc   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Diversity   |
|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|
| Input                                       | Input                                       | Output                                      | Output                                      | Output                                      | Output                                      | Input                                  | Input                                  | Input                                  | Diversity Output                       | Diversity Output                       | Diversity Output                       | Diversity Output                       | Input                                       | Input                                       | Input                                       | Output                                      | Output                                      | Output                                      | Output                                      |
| i l u r e s (%)                             | op y ( %)                                   | Purity (%)                                  | e rage (%)                                  | op y ( %)                                   | Purity (%)                                  | i l u r e s                            |                                        | op y ( %)                              | Purity (%)                             | v e rage (%)                           | op y ( %)                              | Purity (%)                             | a i l u r e s                               | (%)                                         |                                             | Purity (%)                                  | v e rage (%)                                | op y ( %)                                   | Purity (%)                                  |
| #F a Co v e rage                            | Entr                                        | Gini                                        | Co v                                        | Entr                                        | Gini                                        | #F a                                   | Co v e rage (%)                        | Entr                                   | Gini                                   | Co                                     | Entr                                   | Gini                                   | #F                                          | Co v e rage                                 | Entr op y ( %)                              | Gini                                        | Co                                          | Entr                                        | Gini                                        |
|                                             | 31 100.00 98.10                             | 60.48                                       | 95.00 85.00                                 | 40.62 42.24                                 | 66.93                                       | 1 3                                    | 35.00 100.00                           | 10.00 66.19                            | 83.42                                  | 35.00 100.00                           | 10.00 63.70                            | 87.40                                  |                                             | 13 100.00 78.32                             |                                             | 53.84                                       | 100.00                                      | 74.66                                       |                                             |
| 13 100.00 93.53 27 50.00                    | 31 100.00 85.32 0.00                        | 81.55                                       | 89.83 48.84                                 | 51.49 25.17                                 | 81.54                                       | 1 4                                    | 28.33 91.67                            | 10.00 54.02                            | 89.84                                  | 30.00 90.00                            | 10.00 50.51                            | 89.41                                  | 13 95.00                                    | 11 100.00 81.95 42 100.00 97.61             | 66.99                                       | 72.76                                       | 100.00 100.00 64.50                         | 83.40 75.57 6.78                            | 53.23 69.63                                 |

The comparison is in terms of number of failures, input/output diversity measured by cluster coverage or entropy and Gini purity coe/fficient. Values represent the average among ten runs. Bold faced values indicate a statistically signi/ficant di/fference. Underlined values indicate a large e/ffect size.

more clusters than the trajectories associated with training failure environment con/figurations. In Parking (Figure 5(A)) and Driving (Figure 5(C)) the two classes of trajectories are complementary since they cover the same clusters but with a di/fferent intra-cluster distribution, showing that the generative approach of I/n.sc/d.sc/a.sc/g.sc/o.sc can explore new clusters or new regions within a cluster.

6.2.2 /Q\_uantitative Analysis. We also carried out a quantitative analysis by measuring the number of failures triggered when replaying the failure environment con/figurations that happened during training and their diversity w.r.t. the failure con/figurations found by the two best settings of I/n.sc/d.sc/a.sc/g.sc/o.sc, i.e., hcsal+fail and gasal+fail.Table5 shows the results for each case study.

In Parking, the training failure con/figurations trigger signi/ficantly more failures than both I/n.sc/d.sc/a.sc/g.sc/o.sc's settings (i.e., 31 vs. 13 and 27, respectively). Such failures are equivalent to those found by hcsal+fail in terms of both input and output diversity, but signi/ficantly more diverse than those found by gasal+fail. In Humanoid, both I/n.sc/d.sc/a.sc/g.sc/o.sc's settings are better than replaying failure con/figurations in all dimensions, i.e., in terms of number of failures and their diversity. In Driving, hcsal+fail and the training failures are equivalent, while gasal+fail is signi/ficantly better in terms of number of failures (i.e., 42 vs. 13) and their input entropy (i.e., 97.61 vs. 66.99), but signi/ficantly worse in terms of output diversity.

Since the comparison between failure replay and I/n.sc/d.sc/a.sc/g.sc/o.sc is not conclusive on input and output diversity (i.e., none of the two is superior w.r.t. the other in all case studies), we measured the complementarity between the two competing approaches. In fact, high diversity could be achieved by the two approaches either with similar or with complementary distribution of the inputs/outputs among the same clusters. To measure such complementarity between training failures and the failures that I/n.sc/d.sc/a.sc/g.sc/o.sc generates, we use the Gini purity coe/fficient [50] for each cluster, de/fined as follows:

G ( i ) = | C | ∑ c f = 1 p 2 c f , (6)

where i indicates the i th cluster, | C | is the number of classes and p c f represents the probability of /finding a data point of class c f in cluster i . The number of classes is | C | = 2, since we are measuring the complementarity of two approaches. Equation (6) gives the probability of having a failure in a cluster belonging to one speci/fic class. In particular, G ( i ) = 1 means that cluster i is pure, i.e., all the data points in the cluster belong to a single class.

Fig. 6. Representation of di/fferent trajectories associated with hc sal+fail failure configurations. The trajectory of the vehicle is shown in blue for the Parking environment ( A ) and in red for the Driving environment ( C ). In Humanoid ( B ), the trajectory of the robot is the latitudinal position over time.

<!-- image -->

The Gini Purity columns in Table 5 show the average Gini purity coe/fficients for each case study. Overall, we can observe that, in both settings of I/n.sc/d.sc/a.sc/g.sc/o.sc, i.e., hcsal+fail and gasal+fail, the majority of the clusters are highly pure (i.e., the Gini purity is always greater than 50%). In particular, the setting gasal+fail shows a higher degree of complementarity with the training failures than hcsal+fail (across all the case studies, the Gini purity coe/fficients are 80.79% and 67.71%, respectively).

Hence, we conclude that replayed failures and new failures generated by I/n.sc/d.sc/a.sc/g.sc/o.sc are highly complementary. Developers should use both during RL testing.

## 6.3 /Q\_ualitative Analysis of I/n.sc/d.sc/a.sc/g.sc/o.sc Failures

Figure 6 shows the failure trajectories associated to failure environment con/figurations generated by I/n.sc/d.sc/a.sc/g.sc/o.sc. For all case studies, we selected two failure trajectories belonging to di/fferent clusters representing two di/fferent failure modes.

In Parking (Figure 6(A)), the /first failure mode (at the top) consists of the DRL agent not being able to park the vehicle in the target spot with the right heading given a certain amount of time; the second failure mode (at the bottom) is when the DRL agent does not complete the parking maneuver due to a crash with another vehicle parked beside the target spot.

In Humanoid (Figure 6(B)), the abdomen latitudinal coordinate of the robot should be among the two horizontal lines shown in Figure 6 called healthy range [18], otherwise, the robot falls and the episode terminates unsuccessfully. The two failure trajectories associated with I/n.sc/d.sc/a.sc/g.sc/o.sc failure environment con/figurations are di/fferent: the one at the top shows a trajectory that monotonically goes down in a relatively short amount of time (i.e., just above 70 timesteps). The trajectory at the bottom is more noisy indicating that the DRL agent under test is more uncertain. Indeed, the DRL

agent is able to recover the partial fall around the middle of the episode (i.e., around 120 timesteps) while eventually failing after ≈ 200 timesteps.

In Driving (Figure 6(C)), the /first failure mode shows the DRL agent failing to drive the last curve of the track, while the trajectory at the bottom represents a track with a di/fficult left curve at the beginning.

## 6.4 Discrete vs. Continuous Configurations

In the Parking environment, the number of failures found by the sampling approach is equivalent to the failures found by the best I/n.sc/d.sc/a.sc/g.sc/o.sc setting, i.e., hcsal+fail (i.e., 13 on average), although the failures generated by hcsal+fail have a signi/ficantly higher input diversity and output entropy. Parking environment con/figurations are composed of several discrete and only a few continuous parameters, and they are subject to a few constraints. As a consequence, sampling Parking environment con/figurations at random is e/fficient, as a large number of candidate environment con/figurations can be generated within the given search budget, among which it is more likely to /find environment con/figurations where the failure prediction is high.

Also in the Humanoid environment, there are few validity constraints, hence it is e/fficient to sample environment con/figurations at random. However, the space of environment con/figurations is larger than in Parking (almost twice as much, i.e., 47 vs. 24 parameters) and all parameters are in the continuous domain. As a consequence, /finding challenging environment con/figurations by sampling at random is not e/ffective. Indeed, our results show that the sampling approach generates, on average, 1 failure, while hcsal+fail generates 3 failures (a 200% increase), with a signi/ficantly higher input and output diversity. This shows that I/n.sc/d.sc/a.sc/g.sc/o.sc works much better than random sampling when environment con/figurations have continuous parameters.

The Driving environment o/ffers yet another perspective. Indeed, similarly to Parking, Driving environment con/figurations are mostly composed of discrete parameters but, on the other hand, such environment con/figurations are subject to complex constraints. Therefore, sampling Driving environment con/figurations at random is not e/fficient, contrary to modifying existing environment con/figurations. Indeed, the sampling approach generates, on average, 5 failures, while hcsal+fail generates 11 failures (i.e., a 120% increase), with a signi/ficantly higher input and output diversity. Although, the Driving environment con/figurations have mostly discrete parameters our search-based approach is more e/ffective than sampling since it is able to use the search budget more e/fficiently by modifying existing environment con/figurations which satisfy the complex constraints that hold in this environment.

## 6.5 Ablation Study on the Driving Environment

When generating roads for the driving environment, we make sure that they are challenging enough for the agent under test. Indeed, we constrain the generation to roads that have at least three curves, one of which must have a rotation angle of at least 120 · . However, the driving agent under test might experience failures also when roads do not obey such constraints. To test this hypothesis, we executed I/n.sc/d.sc/a.sc/g.sc/o.sc with random seeds instead of failure seeds (as failure seeds obey the constraints by construction) and the sampling approach. Table 6 shows the number of failures found when executing the failure search with constraints enabled (i.e., column w/ Constr. )and disabled (i.e., column w/o Constr. ). Results show that for sampling, hcsal+rnd and gasal+rnd there is no statistical signi/ficant di/fference between the number of failures generated by the respective techniques with and without road constraints. However, hcrnd and garnd /find signi/ficantly more failures when constraints are enabled, showing that restricting the inputs during the search can be bene/ficial to discover more failures.

Table 6. Comparison between Failures Found by I/n.sc/d.sc/a.sc/g.sc/o.sc Se/t\_tings on the Driving Environment with and without Enabling the Road Constraints when Generating New Configurations

|                            | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Constr .   | D/r.sc/i.sc/v.sc/i.sc/n.sc/g.sc Constr .   |
|----------------------------|--------------------------------------------|--------------------------------------------|
|                            | # Failures                                 |                                            |
| I/n.sc/d.sc/a.sc/g.sc/o.sc |                                            |                                            |
| sampling                   | 5                                          | 4                                          |
| hcrnd                      | 3                                          | 1                                          |
| hcsal+rnd                  | 2                                          | 1                                          |
| garnd                      | 6                                          | 3                                          |
| gasal+rnd                  | 5                                          | 3                                          |

## 6.6 Regressor-based vs. Classifier-based Surrogate Model

Due to the low number of failures in Parking and Driving in the setting for RQ4, we resorted to a regressor-based surrogate model to guide the search toward failure con/figurations. We observe that in Parking, the regressor-based surrogate model is as e/ffective as the classi/fier-based surrogate model. On the other hand, the former is signi/ficantly better than the latter in both Humanoid (in terms of higher number of failures, better input diversity and output coverage) and Driving (in terms of higher number of failures and better input diversity).

One explanation of this phenomenon is in the function we use to compute the continuous values for each episode in the di/fferent case studies. In Parking, we use the episode length as guidance towards failures, which is not more e/ffective than using a binary classi/fier. Indeed, the episode length only gives indirect guidance towards a failure; a long episode might point to con/figurations where the vehicle is simply far from the parking target spot. On the other hand, in Humanoid, we track the height of the robot throughout the episode; the lower the minimum height the higher the chance that the initial con/figuration is challenging for the DRL agent under test. Likewise, in Driving we use the minimum cross track error in an episode, which guides the regressor-based surrogate model toward con/figurations with challenging curves.

In conclusion, the regressor-based surrogate model outperforms the classi/fier-based one in two out of three case studies, while being equivalent to the classi/fier-based surrogate model in Parking. In such contexts (see the RQ4 macro-column in Table 1), the number of failures is low (Parking and Driving) or training failures are ine/ffective (Humanoid), and a continuous-valued function provides a /finer-grained signal when training the surrogate model w.r.t. boolean values. However, it requires domain knowledge of the task at hand to de/fine a continuous-valued function that measures how challenging a certain environment con/figuration is for the agent.

## 6.7 Threats to Validity

6.7.1 Internal Validity. A threat to internal validity may come from an unfair comparison of the considered approaches. We gave the same search budget to all approaches, and we generated the same number of environment con/figurations. Moreover, we considered the same DRL agents for each approach and executed the tests on the same environments.

- 6.7.2 External Validity. Using a limited number of subjects poses an external validity threat, in terms of generalizability of our results. To mitigate such threat, we chose three environments which are widely used in the DRL community and have di/fferent characteristics that challenge the capabilities of each failure search approach.
- 6.7.3 Conclusion Validity. A conclusion validity threat may come from the wrong interpretation of the results due to random variations and inappropriate use of statistical tests. We mitigated this threat by executing each failure search approach multiple times, as well as repeating multiple times the execution of environment con/figurations on non-deterministic environments. When computing diversity, we executed clustering multiple times to account for the randomness of the k -means algorithm. Moreover, we compared the di/fferent approaches, both in terms of number of failures and in terms of their diversity, using rigorous statistical tests such as the MannWhitney U Test for computing the p -value and the Vargha-Delaney metric to measure the e/ffect size.
- 6.7.4 Reproducibility. In terms of reproducibility, we publish our replication package [7], making our evaluation repeatable and our results reproducible.

## 7 CONCLUSION AND FUTURE WORK

Our approach to test DRL agents uses the interaction data produced by the DRL agents during training to train a surrogate model-i.e., a classi/fier-on failure and non-failure (i.e., pass) environment con/figurations. Then, it uses the failure prediction output of the surrogate model, as a /fitness function to be maximized, to achieve high failure-exposure capabilities of the generated environment con/figurations while saving computation time. Our empirical results show that our search-based approach is able to generate 50% more failures than the state-of-the-art sampling approach and that such failures are more diverse in all case studies (on average, 78% more diverse regarding input diversity, and 74% more diverse regarding output diversity). When training failures are e/ffective at testing time, our results show that I/n.sc/d.sc/a.sc/g.sc/o.sc generates failures that are complementary. On the other hand, when training failures are ine/ffective at testing time, I/n.sc/d.sc/a.sc/g.sc/o.sc triggers signi/ficantly more, and more diverse failures. In our future work, we plan to increase the diversity of the generated failures by incorporating it into the /fitness function.

## REFERENCES

- [1] Raja Ben Abdessalem, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. 2016. Testing advanced driver assistance systems using multi-objective search and neural networks. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE . 63-74.
- [2] Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. 2018. Testing autonomous cars for feature interaction failures using many-objective search. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE 2018) . ACM, 143-154. DOI: https://doi.org/10.1145/ 3238147.3238192
- [3] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5048-5058. https://proceedings.neurips.cc/paper/2017/hash/ 453fadbd8a1a3af50a9df4df899537b5-Abstract.html
- [4] Andrea Arcuri and Lionel Briand. 2014. A hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering. Software Testing, Veri/fication and Reliability 24, 3 (2014), 219-250.
- [5] David Arthur and Sergei Vassilvitskii. 2006. k-means++: The Advantages of Careful Seeding . Technical Report. Stanford.

| [6] R. Ben Abdessalem, S. Nejati, L. C. Briand, and T. Stifter. 2018. Testing vision-based control systems using learnable evolutionary algorithms. In IEEE/ACM 40th International Conference on Software Engineering (ICSE'18) . 1016-1026. https://doi.org/10.1145/3180155.3180160                                                                                                                |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [7] Matteo Biagiola. 2023. Replication package. https://www.github.com/matteobiagiola/drl-testing-experiments.On- line; accessed November 2023.                                                                                                                                                                                                                                                     |
| [8] Matteo Biagiola and Paolo Tonella. 2022. Testing the plasticity of reinforcement learning-based systems. ACM Trans. Softw. Eng. Methodol. 31, 4 (2022), 80:1-80:46. https://doi.org/10.1145/3511701                                                                                                                                                                                             |
| [9] Houssem Ben Braiek and Foutse Khomh. 2020. On testing machine learning programs. Journal of Systems and Soft- ware 164 (2020), 110542.                                                                                                                                                                                                                                                          |
| https://doi.org/10.1109/TITS.2022.3160936 [11] Dong Chen, Mohammad R. Hajidavalloo, Zhaojian Li, Kaian Chen, Yongqiang Wang, Longsheng Jiang, and Yue Wang. 2023. Deep multi-agent reinforcement learning for highway on-ramp merging in mixed tra/ffic. IEEE Trans. Intell. Transp. Syst. 24, 11 (2023), 11623-11638. https://doi.org/10.1109/TITS.2023.3285442                                    |
| [12] Torch Contributors. 2021. Torch CrossEntropyLoss class documentation. https://pytorch.org/docs/stable/generated/ torch.nn.CrossEntropyLoss.html. Online; accessed November 2023.                                                                                                                                                                                                               |
| [13] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-balanced loss based on e/ffective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9268- 9277.                                                                                                                                                         |
| [14] Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2018. Distributional reinforcement learning with quantile regression. In Proceedings of the 32nd AAAI Conference on Arti/ficial Intelligence .                                                                                                                                                                                   |
| [15] Richard Everett. 2019. Strategically Training and Evaluating Agents in Procedurally Generated Environments .Ph.D. Dissertation. University of Oxford.                                                                                                                                                                                                                                          |
| [16] Ben Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. 2021. Robust predictable control. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,                                                                     |
| Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27813-27825. https://proceedings.neurips.cc/paper/2021/hash/ e9f85782949743dcc42079e629332b5f-Abstract.html                                                                                                                                                                                                                                       |
| [17] Shuo Feng, Xintao Yan, Haowei Sun, Yiheng Feng, and Henry X. Liu. 2021. Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment. Nature Communications 12, 1 (2021), 1-14.                                                                                                                                                                 |
| [18] Farama Foundation. 2022. Humanoid. https://www.gymlibrary.dev/environments/mujoco/humanoid/. Online; ac- cessed November 2023.                                                                                                                                                                                                                                                                 |
| [19] Alessio Gambi, Marc Müller, and Gordon Fraser. 2019. Automatically testing self-driving cars with search-based procedural content generation. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA . 318-328. [20] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: O/ff-policy maximum en-          |
| [21] Hado van Hasselt. 2010. Double Q-learning. In nual Conference on Neural Information Processing Systems 2010. Proceedings of a Meeting held 6-9 December 2010, Vancouver, British Columbia, Canada ,                                                                                                                                                                                            |
| Advances in Neural Information Processing Systems 23: 24th An- John D. La/fferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta (Eds.). Curran Associates, Inc., 2613-2621. https://proceedings.neurips.cc/paper/2010/hash/                                                                                                                                     |
| [22] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2018. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the 32nd AAAI Conference on Arti/ficial Intelligence .                                                                                             |
| [23] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial attacks on neural network policies. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings . OpenReview.net. https://openreview.net/forum?id=ryvlRyBKl                                                      |
| [24] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning systems. In Proceedings of the 42nd International Conference on Software Engineering (ICSE'20) .ACM,12pages. [25] Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. 2019. Learning |

## Testing of Deep Reinforcement Learning Agents with Surrogate Models

- [26] Jemin Hwangbo, Joonho Lee, and Marco Hutter. 2018. Per-contact iteration method for solving contact dynamics. IEEE Robotics and Automation Letters 3, 2 (2018), 895-902.
- [27] Sergey Io/ffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning . PMLR, 448-456.
- [28] Justin Basilico J. Ashok Chandrashekar, Fernando Amat and Tony Jebara. 2017. Artwork Personalization at Net/flix . https://net/flixtechblog.com/artwork-personalization-c589f074ad76. Online; accessed November 2023.
- [29] Kittipat Virochsiri Jason Gauci and Edoardo Conti. 2018. Horizon: The /first open source reinforcement learning platform for large-scale products and services. https://engineering.fb.com/2018/11/01/ml-applications/horizon/. Online; accessed November 2023.
- [30] Kittipat Virochsiri Jason Gauci and Edoardo Conti. 2021. A platform for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.). https://github.com/facebookresearch/ReAgent. Online; accessed November 2023.
- [31] Yaochu Jin. 2005. A comprehensive survey of /fitness approximation in evolutionary computation. Soft Computing 9, 1 (2005), 3-12.
- [32] Gary King and Langche Zeng. 2001. Logistic regression in rare events data. Political Analysis 9, 2 (2001), 137-163.
- [33] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A uni/fied and generic model interpretability library for PyTorch. CoRR abs/2009.07896 (2020). arXiv:2009.07896 https://arxiv.org/ abs/2009.07896
- [34] Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. 2020. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In Proceedings of the International Conference on Machine Learning . PMLR, 5556-5566.
- [35] Jennifer Langston. 2020. With reinforcement learning, Microsoft brings a new class of AI solutions to customers. https://blogs.microsoft.com/ai/reinforcement-learning/. Online; accessed November 2023.
- [36] Timothée Lesort, Natalia Díaz Rodríguez, Jean-François Goudou, and David Filliat. 2018. State representation learning for control: An overview. Neural Networks 108 (2018), 379-392. https://doi.org/10.1016/J.NEUNET.2018.07.006
- [37] Edouard Leurent. 2018. An Environment for Autonomous Driving Decision-Making. https://github.com/eleurent/ highway-env. Online; accessed November 2023.
- [38] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th International Joint Conference on Arti/ficial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017 , Carles Sierra (Ed.). ijcai.org, 3756-3762. https://doi.org/10.24963/IJCAI.2017/525
- [39] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. 2021. Isaac Gym: High performance GPU based physics simulation for robot learning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual , Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 28dd2c7955ce926456240b2/ff0100bde-Abstract-round2.html
- [40] Henry B. Mann and Donald R. Whitney. 1947. On a test of whether one of two random variables is stochastically larger than the other. The Annals of Mathematical Statistics 18, 1 (1947), 50-60. https://doi.org/10.1214/aoms/ 1177730491
- [41] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. 2022. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics 7, 62 (2022), eabk2822.
- [42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nat. 518, 7540 (2015), 529-533. https://doi.org/10.1038/NATURE14236
- [43] OpenAI and collaborators. 2021. Gym Leaderboard. https://github.com/openai/gym/wiki/Leaderboard. Online; accessed November 2023.
- [44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 8024-8035. https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740Abstract.html
- [45] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,

ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.

Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12 (2011), 2825-2830. https://doi.org/10.5555/1953048.2078195

- [46] Antonin Ra/ffin. 2020. RL Baselines3 Zoo . https://github.com/DLR-RM/rl-baselines3-zoo. Online; accessed November 2023.
- [47] Antonin Ra/ffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. 2021. StableBaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research 22, 268 (2021), 1-8. Retrieved from http://jmlr.org/papers/v22/20-1364.html
- [48] Antonin Ra/ffin, Jens Kober, and Freek Stulp. 2022. Smooth exploration for robotic reinforcement learning. In Proceedings of the Conference on Robot Learning . PMLR, 1634-1644.
- [49] Vincenzo Riccio and Paolo Tonella. 2020. Model-based exploration of the frontier of behaviours for deep learning system testing. In Proceedings of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE'20) .
- [50] Lior Rokach and Oded Maimon. 2005. Top-down induction of decision trees classi/fiers-a survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 35, 4 (2005), 476-487.
- [51] Peter J. Rousseeuw. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math. 20 (1987), 53-65. https://doi.org/10.1016/0377-0427(87)90125-7
- [52] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. 2022. Learning to walk in minutes using massively parallel deep reinforcement learning. In Proceedings of the Conference on Robot Learning . PMLR, 91-100.
- [53] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2016. Prioritized experience replay. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.05952
- [54] John Schulman, FilipWolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017). arXiv:1707.06347 http://arxiv.org/abs/1707.06347
- [55] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classi/fication models and saliency maps. In 2nd International Conference on Learning Representations, ICLR 2014, Ban/ff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http: //arxiv.org/abs/1312.6034

[56] Nitish Srivastava, Geo/ffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A

simple way to prevent neural networks from over/fitting.

The Journal of Machine Learning Research

15, 1 (2014),

1929-1958.

- [57] Haowei Sun, Shuo Feng, Xintao Yan, and Henry X. Liu. 2021. Corner case generation and analysis for safety assessment of autonomous vehicles. Transportation Research Record 2675, 11 (2021), 587-600.
- [58] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction . MIT Press.
- [59] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, Ban/ff, AB, Canada, April 14-16, 2014, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http: //arxiv.org/abs/1312.6199
- [60] Martin Tappler, Filip Cano Córdoba, Bernhard K. Aichernig, and Bettina Könighofer. 2022. Search-based testing of reinforcement learning. In Proceedings of the 31st International Joint Conference on Arti/ficial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org, 503-510. https://doi.org/10.24963/IJCAI.2022/72
- [61] Maxime Ellerbach Tawn Kramer and contributors. 2021. Self Driving Car Sandbox. https://github.com/tawnkramer/ sdsandbox. Online; accessed November 2023.
- [62] Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine for model-based control. In Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems . IEEE, 5026-5033.
- [63] Jonathan Uesato, Ananya Kumar, Csaba Szepesvári, Tom Erez, Avraham Ruderman, Keith Anderson, Krishnamurthy (Dj) Dvijotham, Nicolas Heess, and Pushmeet Kohli. 2019. Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=B1xhQhRcK7
- [64] Laurens van der Maaten and Geo/ffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research 9, 86 (2008), 2579-2605. http://jmlr.org/papers/v9/vandermaaten08a.html
- [65] András Vargha and Harold D. Delaney. 2000. A critique and improvement of the CL common language e/ffect size statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics 25, 2 (2000), 101-132.
- [66] Ankit Verma, Siddhesh Bagkar, Naga Venkata SaiTeja Allam, Adhiti Raman, Matthias Schmid, and Venkat N. Krovi. 2021. Implementation and validation of behavior cloning using scaled vehicles. In Proceedings of the SAE WCX Digital Summit . SAE International. DOI: https://doi.org/10.4271/2021-01-0248
- [67] Ari Viitala, Rinu Boney, and Juho Kannala. 2020. Learning to drive small scale cars from scratch. CoRR abs/2008.00715 (2020). arXiv:2008.00715 https://arxiv.org/abs/2008.00715

## Testing of Deep Reinforcement Learning Agents with Surrogate Models

- [68] Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, and Juho Kannala. 2021. Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning. In 20th International Conference on Advanced Robotics, ICAR 2021, Ljubljana, Slovenia, December 6-10, 2021 . IEEE, 275-281. https://doi.org/10.1109/ICAR53236.2021.9659342
- [69] Che Wang, Yanqiu Wu, Quan Vuong, and Keith W. Ross. 2019. Towards Simplicity in Deep Reinforcement Learning: Streamlined O/ff-Policy Learning. CoRR abs/1910.02208 (2019). arXiv:1910.02208 http://arxiv.org/abs/1910.02208
- [70] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016. Dueling network architectures for deep reinforcement learning. In Proceedings of the International Conference on Machine Learning . PMLR, 1995-2003.
- [71] Automotive World. 2016. Automatic Intelligent Parking: Audi at NIPS in Barcelona. Retrieved July 05, 2023 from https://www.automotiveworld.com/news-releases/automatic-intelligent-parking-audi-nips-barcelona/
- [72] Mengdi Xu, Peide Huang, Fengpei Li, Jiacheng Zhu, Xuewei Qi, Kentaro Oguchi, Zhiyuan Huang, Henry Lam, and Ding Zhao. 2021. Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sampling . CoRR abs/2106.10566 (2021). arXiv:2106.10566 https://arxiv.org/abs/2106.10566
- [73] Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. 2021. Delving into deep imbalanced regression. In Proceedings of the International Conference on Machine Learning . PMLR, 11842-11851.
- [74] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering (2020).
- [75] Qi Zhang and Tao Du. 2019. Self-driving scale car trained by Deep reinforcement Learning. CoRR abs/1909.03467 (2019). arXiv:1909.03467 http://arxiv.org/abs/1909.03467
- [76] Songan Zhang, Lu Wen, Huei Peng, and H. Eric Tseng. 2021. Quick learner automated vehicle adapting its roadmanship to varying tra/ffic cultures with meta reinforcement learning. In Proceedings of the 2021 IEEE International Intelligent Transportation Systems Conference (ITSC'21) . IEEE, 1745-1752.
- [77] Hongli Zhou, Xiaolei Chen, Guanwen Zhang, and Wei Zhou. 2021. Deep reinforcement learning for autonomous driving by transferring visual features. In Proceedings of the 2020 25th International Conference on Pattern Recognition (ICPR'21) . DOI: https://doi.org/10.1109/ICPR48806.2021.9412011

Received 26 September 2022; revised 30 October 2023; accepted 31 October 2023