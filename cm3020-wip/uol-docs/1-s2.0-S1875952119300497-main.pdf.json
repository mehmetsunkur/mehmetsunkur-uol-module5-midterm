[
    {
        "element_id": "692e9f6f6c6cc28bafbc9de2c0ae77be",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        624.2,
                        99.9
                    ],
                    [
                        624.2,
                        121.3
                    ],
                    [
                        1026.1,
                        121.3
                    ],
                    [
                        1026.1,
                        99.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81134,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "340d98db5ba4282308b5ff6b5612338b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        627.8,
                        107.2
                    ],
                    [
                        627.8,
                        127.2
                    ],
                    [
                        1025.8,
                        127.2
                    ],
                    [
                        1025.8,
                        107.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "692e9f6f6c6cc28bafbc9de2c0ae77be"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "78436ae6ccda63a6d9054b724314cc26",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.6,
                        168.8
                    ],
                    [
                        104.6,
                        349.9
                    ],
                    [
                        269.6,
                        349.9
                    ],
                    [
                        269.6,
                        168.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "LSEVIER",
        "type": "Image"
    },
    {
        "element_id": "85b2a5a832fe0255fbaf2df47ff541bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        633.7,
                        168.2
                    ],
                    [
                        633.7,
                        198.2
                    ],
                    [
                        1028.2,
                        198.2
                    ],
                    [
                        1028.2,
                        168.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.40586,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Contents lists available at ScienceDirect",
        "type": "NarrativeText"
    },
    {
        "element_id": "484cbba035fd3210b3000692c9ea96a6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        602.7,
                        239.7
                    ],
                    [
                        602.7,
                        288.7
                    ],
                    [
                        1054.7,
                        288.7
                    ],
                    [
                        1054.7,
                        239.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.44616,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Entertainment Computing",
        "type": "Title"
    },
    {
        "element_id": "a153900b45ebb91419fe6079f274ec23",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        528.5,
                        326.1
                    ],
                    [
                        528.5,
                        349.7
                    ],
                    [
                        1133.1,
                        349.7
                    ],
                    [
                        1133.1,
                        326.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.27088,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "484cbba035fd3210b3000692c9ea96a6"
        },
        "text": "journal homepage: www.elsevier.com/locate/entcom",
        "type": "NarrativeText"
    },
    {
        "element_id": "915249446485b81843a96c018170e1ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1392.0,
                        152.3
                    ],
                    [
                        1392.0,
                        350.9
                    ],
                    [
                        1549.3,
                        350.9
                    ],
                    [
                        1549.3,
                        152.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Entertainment Computing",
        "type": "Image"
    },
    {
        "element_id": "b130d10b591b956e211258b6ab7de059",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        459.1
                    ],
                    [
                        104.4,
                        552.6
                    ],
                    [
                        1337.3,
                        552.6
                    ],
                    [
                        1337.3,
                        459.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66127,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Playing first-person shooter games with machine learning techniques and methods using the VizDoom Game-AI research platform",
        "type": "NarrativeText"
    },
    {
        "element_id": "7616f0fc1c7c225a12324b897bedcb9f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1391.1,
                        459.8
                    ],
                    [
                        1391.1,
                        537.6
                    ],
                    [
                        1468.9,
                        537.6
                    ],
                    [
                        1468.9,
                        459.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "T ",
        "type": "Image"
    },
    {
        "element_id": "f7bcb7c2eb6973d8865254ab6aac575d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        569.4
                    ],
                    [
                        104.4,
                        610.0
                    ],
                    [
                        282.0,
                        610.0
                    ],
                    [
                        282.0,
                        569.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Adil Khana,b,\u204e",
        "type": "Title"
    },
    {
        "element_id": "5ec1e579e904563b3775a58a33cbfd96",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        282.0,
                        573.1
                    ],
                    [
                        282.0,
                        610.0
                    ],
                    [
                        1289.1,
                        610.0
                    ],
                    [
                        1289.1,
                        573.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": ", Muhammad Naeema, Muhammad Zubair Asgharc, Aziz Ud Dinb, Atif Khand",
        "type": "Title"
    },
    {
        "element_id": "4e4e5a17409ad2dc29f602b127e48051",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        98.6,
                        623.0
                    ],
                    [
                        98.6,
                        646.4
                    ],
                    [
                        660.2,
                        646.4
                    ],
                    [
                        660.2,
                        623.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5165,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "5ec1e579e904563b3775a58a33cbfd96"
        },
        "text": "a Department of Computer Science, University of Peshawar, KP, Pakistan",
        "type": "NarrativeText"
    },
    {
        "element_id": "57d722f46b665c72ad2ad167eb8d4a64",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.8,
                        646.9
                    ],
                    [
                        103.8,
                        670.3
                    ],
                    [
                        713.8,
                        670.3
                    ],
                    [
                        713.8,
                        646.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.40345,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "5ec1e579e904563b3775a58a33cbfd96"
        },
        "text": "b Department of Computer Science, SZIC, University of Peshawar, KP, Pakistan",
        "type": "ListItem"
    },
    {
        "element_id": "3beb566cb43376b26da168996f344e3a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        92.3,
                        671.2
                    ],
                    [
                        92.3,
                        694.1
                    ],
                    [
                        846.3,
                        694.1
                    ],
                    [
                        846.3,
                        671.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66963,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "5ec1e579e904563b3775a58a33cbfd96"
        },
        "text": "c Institute of Computing and Information Technology, Gomal University, D. I. Khan, KP, Pakistan",
        "type": "ListItem"
    },
    {
        "element_id": "dd3660f59f07ed1ab5e83be84e8a4973",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        93.6,
                        694.6
                    ],
                    [
                        93.6,
                        717.9
                    ],
                    [
                        671.0,
                        717.9
                    ],
                    [
                        671.0,
                        694.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67165,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "5ec1e579e904563b3775a58a33cbfd96"
        },
        "text": "d Department of Computer Science, Islamia College, Peshawar, KP, Pakistan",
        "type": "ListItem"
    },
    {
        "element_id": "a2a195c9cb0d805a609399766901f9f3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        786.3
                    ],
                    [
                        104.4,
                        809.9
                    ],
                    [
                        312.6,
                        809.9
                    ],
                    [
                        312.6,
                        786.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.41842,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "A R T I C L E I N F O",
        "type": "Title"
    },
    {
        "element_id": "28232d9305b6dc146be60e1f44a95b86",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        560.6,
                        786.8
                    ],
                    [
                        560.6,
                        809.9
                    ],
                    [
                        717.9,
                        809.9
                    ],
                    [
                        717.9,
                        786.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.35456,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a2a195c9cb0d805a609399766901f9f3"
        },
        "text": "A B S T R A C T",
        "type": "NarrativeText"
    },
    {
        "element_id": "faded39f751032875c05ba1947bd6823",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.3,
                        847.4
                    ],
                    [
                        103.3,
                        1026.9
                    ],
                    [
                        402.4,
                        1026.9
                    ],
                    [
                        402.4,
                        847.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71343,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a2a195c9cb0d805a609399766901f9f3"
        },
        "text": "Keywords: Artificial Intelligence Artificial Neural Network Autonomous Systems Computational Intelligence Intelligent agents Visual Deep Reinforcement Learning",
        "type": "NarrativeText"
    },
    {
        "element_id": "a4d3ca894e5db378ca79c4696f52194a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1014.0
                    ],
                    [
                        104.4,
                        1031.7
                    ],
                    [
                        246.3,
                        1031.7
                    ],
                    [
                        246.3,
                        1014.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Machine Learning",
        "type": "Title"
    },
    {
        "element_id": "078f6cf6ceb35ecfaa1e7bec63ca2a2b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        564.3,
                        840.1
                    ],
                    [
                        564.3,
                        1184.8
                    ],
                    [
                        1549.6,
                        1184.8
                    ],
                    [
                        1549.6,
                        840.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93581,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "Artificial Intelligence in the form of machine learning is employed in games to control non-human computer- players, agents or bots. However, most of these games such as Atari took place in 2D environments that were not fully observable to the agents. Currently, it is of extreme significance to employ such machine learning tech- niques and methods in 3D environments such as Doom. Therefore, In this paper, we train agents on the health gathering scenario of the classical first-person shooter game Doom by first presenting the Direct Future Prediction to train an agent that uses a simple architecture with no additional supervisory signals, then differ- entiate and compare the performance of the agents trained by using several different machine learning tech- niques, and the AI reinforcement learning platform \u2018VizDoom\u2019, a 3D partially observable environment, with interesting enhanced properties that makes agents to stand out from inbuilt AI agents and human players. We have continued to use computer games as a benchmark for the performance of AI as having been so successful in the past. We also compared the results of our findings to conclude the performance of the agents trained with different machine learning techniques. The agents performed well against both human players and inbuilt game agents.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9bb63bb1ed492d6abcefaafc4f696491",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1285.7
                    ],
                    [
                        104.4,
                        1307.8
                    ],
                    [
                        536.0,
                        1307.8
                    ],
                    [
                        536.0,
                        1285.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "1. Introduction and research motivation",
        "type": "ListItem"
    },
    {
        "element_id": "35e016bcf10e664c4186543ad8887dd9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.0,
                        1339.6
                    ],
                    [
                        101.0,
                        1824.5
                    ],
                    [
                        802.3,
                        1824.5
                    ],
                    [
                        802.3,
                        1339.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95181,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "In the last few decades, due to the progress in artificial intelligence, a revolution and sudden change has been observed in the technology both in hardware and software. This change is seeping and taking over in our lives up to a certain extent, affecting how we live, work and entertain ourselves such as employing domestic robots servants, healthcare uses, electronic trading, remote sensing, expert systems, traffic control systems, autonomously-powered self-driving vehicles, and from behavioural algorithms to suggestive searches, etc. In the same way, gaming is a widely recognized part of our cultural landscape and as old as our human ancestors. The earliest computers were very slow and the interaction with the user was limited to basic principles. In the early '1940 s, computers evolved, and programmers commenced to develop new virtual worlds and surprising ways of interaction between the user and the machine. But now due to advancements in technology such as GPU\u2019s [1], TPU\u2019s [2] and the revolution in deep neural networks [3] it has become possible for artificial intelligence to step-in in video",
        "type": "NarrativeText"
    },
    {
        "element_id": "93a975db95ab4e32be8b72a702397269",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.1,
                        1282.4
                    ],
                    [
                        851.1,
                        1814.3
                    ],
                    [
                        1552.1,
                        1814.3
                    ],
                    [
                        1552.1,
                        1282.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94508,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "be more specific a huge amount of multidimensional data is required to be processed and executed [4]. In the recent past, machine learning techniques and methods were employed in Atari games for training agents, where later, the agents performed on 49 different Atari games with better and improved results. However, most of these Atari games took place in 2D environments that were fully observable to the agents [5]. Currently, it is of extreme significance to employ such machine learning techniques and methods in 3D environments such as Doom [6] a first-person shooter game shown in Fig. 1, StarCraft [7] a third-person shooter game based on real-time strategies, and sandbox open-world games such as Grand Theft Auto V [8] and Minecraft [9] because the research community in AI think and consider that computer video games are the best test-beds for testing different artificial intelligence techniques, methods, and algorithms before evaluating them in the real world. Thus, in this paper, state-of-the-art machine learning techniques that were before partially tested in 2D environments are now employed in a 3D environment known as Doom, to train, differentiate and com- pare agents performances, such as advantage actor-critic (A2C) [10],",
        "type": "NarrativeText"
    },
    {
        "element_id": "2ff44e48acefb9ee9e124b6d25f21a0d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1808.7
                    ],
                    [
                        104.4,
                        1830.8
                    ],
                    [
                        801.9,
                        1830.8
                    ],
                    [
                        801.9,
                        1808.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "games as well where massive graphical data in the form of frames, or to",
        "type": "UncategorizedText"
    },
    {
        "element_id": "b594ca1c1ea1888a93ccabfe60c30f88",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1808.7
                    ],
                    [
                        851.6,
                        1830.8
                    ],
                    [
                        1549.1,
                        1830.8
                    ],
                    [
                        1549.1,
                        1808.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "advantage actor-critic long short-term memory (A2C-LSTM) [11],",
        "type": "NarrativeText"
    },
    {
        "element_id": "eb555d73590ee050e35caf053e078ba5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        122.2,
                        1898.5
                    ],
                    [
                        122.2,
                        1911.8
                    ],
                    [
                        129.7,
                        1911.8
                    ],
                    [
                        129.7,
                        1898.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "a4d3ca894e5db378ca79c4696f52194a"
        },
        "text": "\u204e",
        "type": "UncategorizedText"
    },
    {
        "element_id": "0d64dbe94748cb94f897a1271ee8645d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        114.3,
                        1902.4
                    ],
                    [
                        114.3,
                        1919.6
                    ],
                    [
                        334.6,
                        1919.6
                    ],
                    [
                        334.6,
                        1902.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.45707,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "* Corresponding author.",
        "type": "Footer"
    },
    {
        "element_id": "3aa164a272efdf9ded981049ad3e4be2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        133.9,
                        1906.3
                    ],
                    [
                        133.9,
                        1926.2
                    ],
                    [
                        333.2,
                        1926.2
                    ],
                    [
                        333.2,
                        1906.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "Corresponding author.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3e5b1c496752c4b0576a5ecb3adfca64",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        135.6,
                        1927.4
                    ],
                    [
                        135.6,
                        1947.2
                    ],
                    [
                        717.3,
                        1947.2
                    ],
                    [
                        717.3,
                        1927.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.61171,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "E-mail addresses: adil \u00a9uop.edu.pk, Dradil@hit.edu.pk (A. Khan).",
        "type": "NarrativeText"
    },
    {
        "element_id": "2935e5f7bd4d063f2f48dc5af9715e10",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        136.1,
                        1932.8
                    ],
                    [
                        136.1,
                        1952.7
                    ],
                    [
                        711.4,
                        1952.7
                    ],
                    [
                        711.4,
                        1932.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "E-mail addresses: adil@uop.edu.pk, Dradil@hit.edu.pk (A. Khan).",
        "type": "Title"
    },
    {
        "element_id": "c7c9fb6bfe8db5b93f27cf31c7d55e02",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.5,
                        1972.7
                    ],
                    [
                        105.5,
                        1992.9
                    ],
                    [
                        531.5,
                        1992.9
                    ],
                    [
                        531.5,
                        1972.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67833,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "https://doi.org/10.1016/j.entcom.2020.100357",
        "type": "Title"
    },
    {
        "element_id": "dff09a0e3e47a8085f9938874989939f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1978.7
                    ],
                    [
                        104.4,
                        1998.7
                    ],
                    [
                        528.9,
                        1998.7
                    ],
                    [
                        528.9,
                        1978.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "c7c9fb6bfe8db5b93f27cf31c7d55e02"
        },
        "text": "https://doi.org/10.1016/j.entcom.2020.100357",
        "type": "UncategorizedText"
    },
    {
        "element_id": "20e330e8075263dbceb92be7b26f5b3b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        95.4,
                        1998.6
                    ],
                    [
                        95.4,
                        2068.8
                    ],
                    [
                        965.1,
                        2068.8
                    ],
                    [
                        965.1,
                        1998.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.52497,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1,
            "parent_id": "c7c9fb6bfe8db5b93f27cf31c7d55e02"
        },
        "text": "Received 2 April 2019; Received in revised form 14 January 2020; Accepted 15 February 2020 Available online 19 February 2020",
        "type": "NarrativeText"
    },
    {
        "element_id": "e2ac40cd23b451b4468572e05f4aa25a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2060.2
                    ],
                    [
                        104.4,
                        2080.2
                    ],
                    [
                        522.2,
                        2080.2
                    ],
                    [
                        522.2,
                        2060.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 1
        },
        "text": "1875-9521/ \u00a9 2020 Published by Elsevier B.V.",
        "type": "Title"
    },
    {
        "element_id": "d5c5d49ebe50c2ad77fcfcd21c2d9cee",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.3,
                        103.2
                    ],
                    [
                        101.3,
                        124.3
                    ],
                    [
                        217.1,
                        124.3
                    ],
                    [
                        217.1,
                        103.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.512,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "ab635a5b1aa0f173b6c9522964ab4993",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1207.8,
                        102.6
                    ],
                    [
                        1207.8,
                        120.6
                    ],
                    [
                        1548.7,
                        120.6
                    ],
                    [
                        1548.7,
                        102.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76192,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "b7e13e8111ceddcbcca3c32cdb05547b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "ab635a5b1aa0f173b6c9522964ab4993"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "8ca8c699404e898960ae964927140f15",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        346.5,
                        158.3
                    ],
                    [
                        346.5,
                        723.6
                    ],
                    [
                        1307.1,
                        723.6
                    ],
                    [
                        1307.1,
                        158.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-2-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "HEALTH FrAG",
        "type": "Image"
    },
    {
        "element_id": "242143fa8c958e122cf2b3f5d0204f77",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        498.9,
                        743.6
                    ],
                    [
                        498.9,
                        769.4
                    ],
                    [
                        1153.1,
                        769.4
                    ],
                    [
                        1153.1,
                        743.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.893,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "Fig. 1. A sample screen from Doom showing the first-person perspective.",
        "type": "FigureCaption"
    },
    {
        "element_id": "8f46eb6bcb0c3c8bff858524217e56c9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.5,
                        809.5
                    ],
                    [
                        103.5,
                        1099.0
                    ],
                    [
                        806.7,
                        1099.0
                    ],
                    [
                        806.7,
                        809.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95705,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "asynchronous advantage actor-critic (A3C) [12], Deep Q-network (DQN) [13], Deep recurrent Q-network (DRQN) [14], Double deep Q- network (DDQN) [15], C51-DDQN [16], Dueling deep Q-network (DDQN) [17], and Reinforce [18] whereafter applying them most of the agents are found useful and effective. In addition, this paper presents one of the 4 best techniques that performed well on the VizDoom Game- AI research platform [19]. It was suggested that making such research available is beneficial for the community researching on first-person shooter games which may set up a base for further research and im- provement.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f0c85bbd0156da6cbe8abe9fb3a89130",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1145.7
                    ],
                    [
                        104.4,
                        1167.8
                    ],
                    [
                        529.8,
                        1167.8
                    ],
                    [
                        529.8,
                        1145.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "1.1. The academic motivation of AI in games",
        "type": "Title"
    },
    {
        "element_id": "156bdd02c5f59f68ea095a53e5c36ae9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.0,
                        1200.7
                    ],
                    [
                        103.0,
                        1394.4
                    ],
                    [
                        804.4,
                        1394.4
                    ],
                    [
                        804.4,
                        1200.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95645,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "Why use machine learning (AI) techniques and algorithms to re- search on games? because the future belongs to artificial intelligence in games, particularly machine learning has immense potential and role in games designing and development. The possibilities abound, however, the challenges are also innumerable. Without a doubt, game develop- ment will experience a proliferation of these machine learning con-",
        "type": "NarrativeText"
    },
    {
        "element_id": "109f9d6b0b5d1483f24db77359f76e85",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1378.1
                    ],
                    [
                        104.4,
                        1400.3
                    ],
                    [
                        475.6,
                        1400.3
                    ],
                    [
                        475.6,
                        1378.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "cepts, which is only a matter of time.",
        "type": "NarrativeText"
    },
    {
        "element_id": "57156ae80a410ade96b5ef5d4dfabb14",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.8,
                        1402.6
                    ],
                    [
                        101.8,
                        1801.4
                    ],
                    [
                        805.2,
                        1801.4
                    ],
                    [
                        805.2,
                        1402.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95669,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "In addition, the more primary use of AI in games is to train games agents or bots in an intelligent way so that they could perform and act intelligently similar to human being\u2019s. By achieving such objectives, it creates more fun, challenge, and understanding to human players as for as playing or interacting with games is experienced and concerned. While playing against skilful human players AI agents or bots need to understand what a player does and how a player feels during the play. To gauge and enhance AI agent\u2019s vs human player\u2019s in-game experience, machine learning scientists, practitioners, games researchers, and de- velopers use machine learning methods, such as reinforcement learning, supervised learning like support vector machines or neural networks to build and train the models to make them more effective and intelligent. Such advancements are particularly significant for the",
        "type": "NarrativeText"
    },
    {
        "element_id": "7b06f0455fc560dd1bb202786ca67b72",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        807.0
                    ],
                    [
                        851.6,
                        1244.2
                    ],
                    [
                        1555.0,
                        1244.2
                    ],
                    [
                        1555.0,
                        807.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95248,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "OpenAI gym, Unity and VizDoom which is based on first-person shooter (FPS) game Doom used for visual deep reinforcement learning from raw screen pixels in 3D game environments. The speed of the learning agent greatly depends on the number of frames the agent is permitted to skip. In other words, the frame skipping rate influences the agent\u2019s learning and final performance greatly particularly using deep Q-learning, ex- perience replay memory, and the VizDoom Game AI research platform. The agents can be trained and tested on several of Doom\u2019s scenarios or maps in order to obtain good results and compare them with the ex- isting state-of-the-art research work on Doom-based AI agents. So far the experiments performed on Doom\u2019s scenarios demonstrate that the profitable and optimal frame skipping rate falls in the range of 3 to 11 that provides the best balance between the learning speed and the final performance of the agent which exhibits human-like behaviour and outperforms an average human player and inbuilt game agents [20].",
        "type": "NarrativeText"
    },
    {
        "element_id": "f6616d076d4a0196f334601384519a0e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1247.3
                    ],
                    [
                        851.6,
                        1563.9
                    ],
                    [
                        1553.0,
                        1563.9
                    ],
                    [
                        1553.0,
                        1247.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95834,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "Moreover, there is a lot of existing research that relate to playing FPS games with visual deep reinforcement learning [21] such as the work introduced in [22], in which a method is presented to augment the models to exploit game feature information such as the presence of enemies or items. Similarly, another work described in [23] in which a competitive agent is proposed that is trained on the Doom\u2019s basic sce- nario(s) in the same semi-realistic 3D environment VizDoom using convolutional deep learning with Q-learning [24] that considers only the screens raw pixels for exhibiting agent\u2019s usefulness. The era of re- search using games changed when agents were trained using only the screen raw pixels.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d2aa726980f7aa751a78b9e152d170cc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1566.3
                    ],
                    [
                        851.6,
                        1790.3
                    ],
                    [
                        1553.0,
                        1790.3
                    ],
                    [
                        1553.0,
                        1566.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95538,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "f0c85bbd0156da6cbe8abe9fb3a89130"
        },
        "text": "Another more related work is proposed in [25] in which there is supposedly no reward signal (like there is in Atari games via the score). Instead, the authors used matching future state (measurements) pre- dictions as a replacement to a reward signal. However, Probably, it should be noted that there is a constant health reduction in e.g. the basic room (walking on the radioactive ground) kind of resemblance to typical reinforcement learning r = \u22121 reward for each step when",
        "type": "NarrativeText"
    },
    {
        "element_id": "041974fc346f982ce98bf1aafecd6ef3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1784.9
                    ],
                    [
                        104.4,
                        1807.0
                    ],
                    [
                        715.8,
                        1807.0
                    ],
                    [
                        715.8,
                        1784.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "progress and development of computer video games industry.",
        "type": "Title"
    },
    {
        "element_id": "747104da8fdfd77b68399261acce7f3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1849.7
                    ],
                    [
                        104.4,
                        1897.7
                    ],
                    [
                        738.3,
                        1897.7
                    ],
                    [
                        738.3,
                        1849.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88807,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "2. Research on Doom using the VizDoom Game-AI research",
        "type": "Title"
    },
    {
        "element_id": "fa8cf3be0e36f94b138f0224e0d254e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1882.8
                    ],
                    [
                        104.4,
                        1905.0
                    ],
                    [
                        195.9,
                        1905.0
                    ],
                    [
                        195.9,
                        1882.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2
        },
        "text": "platform",
        "type": "Title"
    },
    {
        "element_id": "090d55f46550c05869c442146811cdfb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.0,
                        1934.4
                    ],
                    [
                        102.0,
                        2079.3
                    ],
                    [
                        806.5,
                        2079.3
                    ],
                    [
                        806.5,
                        1934.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94282,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "fa8cf3be0e36f94b138f0224e0d254e3"
        },
        "text": "These days, game AI is one of the focused and active research areas in artificial intelligence as computer games are the best test-beds for testing theoretical ideas in AI before practically applying them in the real world. In this regard, many Game AI research platforms are fa- miliarized for research on computer video games such as DeepMind,",
        "type": "NarrativeText"
    },
    {
        "element_id": "983430bd41d6396d87bc2e3c8ad60a10",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1774.2
                    ],
                    [
                        851.6,
                        1796.3
                    ],
                    [
                        1204.9,
                        1796.3
                    ],
                    [
                        1204.9,
                        1774.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "fa8cf3be0e36f94b138f0224e0d254e3"
        },
        "text": "trying to reach a goal state quickly.",
        "type": "NarrativeText"
    },
    {
        "element_id": "af517936d0efff7487d0ea1ce0e8ad9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1798.4
                    ],
                    [
                        851.6,
                        2028.8
                    ],
                    [
                        1555.3,
                        2028.8
                    ],
                    [
                        1555.3,
                        1798.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95314,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "fa8cf3be0e36f94b138f0224e0d254e3"
        },
        "text": "Sometimes Reinforcement Learning [26] environments with dis- crete actions are not getting it properly. They don't simulate human muscles easily. In other words, it\u2019s not easy for human players to wiggle the joystick at 114 microseconds between left and right as muscles get tired soon. Moreover, the author's proposed network doesn't employ LSTMs [27] to memorize transactions due to their proposed approach as the second-best agent in the visual Doom AI competition used LSTM but its simple feedforward architecture was 50% efficient.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1dc94da4a2201d36616d7a18c4fcd3b9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        886.3,
                        2035.8
                    ],
                    [
                        886.3,
                        2057.9
                    ],
                    [
                        1549.0,
                        2057.9
                    ],
                    [
                        1549.0,
                        2035.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "fa8cf3be0e36f94b138f0224e0d254e3"
        },
        "text": "In addition, each year a visual doom AI competition is organized for",
        "type": "NarrativeText"
    },
    {
        "element_id": "4e82bf5c66818c10b293b2aaa3f1110a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 2,
            "parent_id": "fa8cf3be0e36f94b138f0224e0d254e3"
        },
        "text": "2",
        "type": "UncategorizedText"
    },
    {
        "element_id": "5cec95795407e266f939c6035bcb2356",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.7,
                        103.3
                    ],
                    [
                        101.7,
                        124.3
                    ],
                    [
                        216.6,
                        124.3
                    ],
                    [
                        216.6,
                        103.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54156,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "15081d6bb9cf6f60d18baa9834eff817",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1210.1,
                        103.2
                    ],
                    [
                        1210.1,
                        120.6
                    ],
                    [
                        1550.3,
                        120.6
                    ],
                    [
                        1550.3,
                        103.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7518,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "9879558ab4fdc8e7f0afd8b51c888a6f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "15081d6bb9cf6f60d18baa9834eff817"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "1ef0ca61a33532c6b903867834772621",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.3,
                        163.1
                    ],
                    [
                        103.3,
                        335.9
                    ],
                    [
                        810.2,
                        335.9
                    ],
                    [
                        810.2,
                        163.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95553,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "9879558ab4fdc8e7f0afd8b51c888a6f"
        },
        "text": "evaluating AI agents on two different tracks: limited death-match on a known map and a full death-match on an unknown map by using ma- chine learning techniques. In this paper, agents are trained, tested, and compared using different machine learning techniques and methods. The methodology and experimental works are presented in several sections as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f898e136e7526bf999b7029a0925250",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        383.0
                    ],
                    [
                        104.4,
                        405.1
                    ],
                    [
                        288.4,
                        405.1
                    ],
                    [
                        288.4,
                        383.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "15081d6bb9cf6f60d18baa9834eff817"
        },
        "text": "2.1. Basic objective",
        "type": "Title"
    },
    {
        "element_id": "56099d62a8f674f5a377fd3ac7d49b33",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.2,
                        436.9
                    ],
                    [
                        102.2,
                        753.8
                    ],
                    [
                        805.2,
                        753.8
                    ],
                    [
                        805.2,
                        436.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95768,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "9f898e136e7526bf999b7029a0925250"
        },
        "text": "The purpose of the experiments is to train competent, well balanced and robust agents using machine learning techniques such as re- inforcement learning and supervised learning [28] that can adapt to learn, act in complex and dynamic 3D environments. Such agents ac- cept raw sensory input and core measurements to show that employed techniques are better at outperforming human players and other inbuilt game agents on the \u2018health gathering scenario(s)\u2019 of the VizDoom platform where only a few limited actions are allowed. Besides, the purpose also includes to prove, differentiate and compare several ma- chine learning techniques by training agents on the VizDoom health gathering scenario(s).",
        "type": "NarrativeText"
    },
    {
        "element_id": "5aff6b9f2d14aebe48a01545c9750b5e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        168.5
                    ],
                    [
                        851.6,
                        190.7
                    ],
                    [
                        1289.5,
                        190.7
                    ],
                    [
                        1289.5,
                        168.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "9f898e136e7526bf999b7029a0925250"
        },
        "text": "2.4. The environment used for the experiments",
        "type": "NarrativeText"
    },
    {
        "element_id": "8eb683f6ad43624c612672d5cc4f5c43",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        224.0
                    ],
                    [
                        851.6,
                        800.7
                    ],
                    [
                        1554.0,
                        800.7
                    ],
                    [
                        1554.0,
                        224.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94628,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "9f898e136e7526bf999b7029a0925250"
        },
        "text": "All the experiments are performed in Pycharm 2017.2 professional version using ViZDoom 1.1.5, Tensorflow 1.5.0 [31], Keras 1.2.2/2.0.5, OpenCV 3.3 [32], CMake 2.8+, GCC 4.6+, and Python 3.6 (64-bit) with Numpy on an Ubuntu Server 16.04.3 LTS Operating System with Intel\u00ae Core\u2122 i7-7700 CPU @3. 60 GHz \u00d7 8 and NVIDIA GeForce GTX 1080/PCIe/SSE2 powerful GPU machine for processing the CNN\u2019s. The agents are trained for thousand to millions of steps consisting of per- forming actions, observing transitions, and updating the networks. The hyperparameter settings for all the experiments listed in Table 2. are met and found after hundreds of runs. The parameters are kept tuned until the models were found converged accurately and properly to meet the desired or expected results. The discount factor is set to \u03b3 = 0.99 for almost all algorithms, the learning rate \u03b1 varied from 0.001 to 0.0001, Experience replay buffer memory capacity is set from 50,000 elements to 60,000, the screen-buffer is set to 640, 480 and remain the same for almost all of the algorithms, the batch-size is set to 32 and 64 for some algorithms, initial decay varied from 0.9 to 1, the final decay is set from 0.001 to 0.0001 and frame-per-action to 4 and 5. The overall learning and testing process is measured by the number of hours it takes to complete on a set of powerful GPU machines.",
        "type": "NarrativeText"
    },
    {
        "element_id": "52b64c8c783f15651e12a9028c6ade2f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        801.1
                    ],
                    [
                        104.4,
                        823.3
                    ],
                    [
                        616.5,
                        823.3
                    ],
                    [
                        616.5,
                        801.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "15081d6bb9cf6f60d18baa9834eff817"
        },
        "text": "2.2. VizDoom health gathering environment (scenario)",
        "type": "Title"
    },
    {
        "element_id": "ee89de8f96dc7cd55be19eaa0c19578a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.4,
                        854.9
                    ],
                    [
                        103.4,
                        1113.8
                    ],
                    [
                        808.3,
                        1113.8
                    ],
                    [
                        808.3,
                        854.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95773,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "52b64c8c783f15651e12a9028c6ade2f"
        },
        "text": "We demonstrate implementing DFP on the health gathering scenario shown in Fig. 2, provided by the VizDoom Game-AI research platform. The main objective of the agent is to survive as long as possible. However, at each time step, the agent\u2019s health decreases, so in order to live longer, the agent has to locate and pick up the health packs scat- tered across different parts of the environment or map. At the same time, the agent also needs to avoid running into poison jars which will take away its health. There are no active opponents in this simple health gathering scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f07c74d25684a516a3ccceb698401cde",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1161.0
                    ],
                    [
                        104.4,
                        1183.1
                    ],
                    [
                        424.2,
                        1183.1
                    ],
                    [
                        424.2,
                        1161.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "15081d6bb9cf6f60d18baa9834eff817"
        },
        "text": "2.3. Machine learning approaches",
        "type": "Title"
    },
    {
        "element_id": "bc5a192bd312c9120d14ba3cf111ce45",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1213.8
                    ],
                    [
                        104.4,
                        1379.6
                    ],
                    [
                        809.5,
                        1379.6
                    ],
                    [
                        809.5,
                        1213.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95483,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "f07c74d25684a516a3ccceb698401cde"
        },
        "text": "Several Artificial Intelligence approaches are used to train agents such as using policy optimization [29] value optimization [30], and DFP as shown in Fig. 3. In this paper, similar to traditional reinforce- ment learning, we suggest and compare training agents that learn through the response provided by interacting with the environment",
        "type": "NarrativeText"
    },
    {
        "element_id": "29fff4675a7b9817d74e60ab5bf59f11",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        841.3
                    ],
                    [
                        851.6,
                        863.4
                    ],
                    [
                        1202.1,
                        863.4
                    ],
                    [
                        1202.1,
                        841.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "f07c74d25684a516a3ccceb698401cde"
        },
        "text": "3. Direct future prediction (DFP)",
        "type": "ListItem"
    },
    {
        "element_id": "9ef5353a3abbd094b780eccf9e3e5a55",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        894.3
                    ],
                    [
                        851.6,
                        1380.1
                    ],
                    [
                        1551.0,
                        1380.1
                    ],
                    [
                        1551.0,
                        894.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95129,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "f07c74d25684a516a3ccceb698401cde"
        },
        "text": "Direct future prediction (DFP) is a machine learning technique and it has one of the major ability and benefit to pursuing complex goals at test time. Normally, in reinforcement learning settings, learning is guided by a series of scalar reward signals [33], but in complex en- vironments, the scalar rewards can be sparse and delayed, which means that sometimes it is not easy to tell which action or sequence of actions are responsible for a particular positive reward that happens several time-steps later, this problem is known as credit assignment [34]. Be- sides rewards, if the environment provides some kind of rich and temporally dense multidimensional feedback, for example, measure- ments like kills, health, ammunition levels in a first-person shooter game, the agent can be programmed to learn to predict such rich and temporally dense measurements feedback instead [35]. It is possible for agents at inference time to observe the effects of different actions on such measurement streams and choose the action that maximizes an objective that can be expressed as a function of the predicted mea-",
        "type": "NarrativeText"
    },
    {
        "element_id": "0270b4ec104910cedc5aeb21f2f14a85",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1364.3
                    ],
                    [
                        104.4,
                        1386.4
                    ],
                    [
                        454.1,
                        1386.4
                    ],
                    [
                        454.1,
                        1364.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "f07c74d25684a516a3ccceb698401cde"
        },
        "text": "using machine learning techniques.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2dcd7e3ffe74575421494c737b939cd2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1364.3
                    ],
                    [
                        851.6,
                        1387.0
                    ],
                    [
                        1549.0,
                        1387.0
                    ],
                    [
                        1549.0,
                        1364.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3,
            "parent_id": "f07c74d25684a516a3ccceb698401cde"
        },
        "text": "surements at time \u2018t\u2019 (i.e. mt), for example, if the scenario(s) of the first-",
        "type": "NarrativeText"
    },
    {
        "element_id": "cfc1ed185eb65cf9234c6ce5f2c8be4b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        346.5,
                        1412.3
                    ],
                    [
                        346.5,
                        2023.6
                    ],
                    [
                        1307.1,
                        2023.6
                    ],
                    [
                        1307.1,
                        1412.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-3-5.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "9793d828f0c7d13d2a6ad3d39f9f1768",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        529.8,
                        2043.6
                    ],
                    [
                        529.8,
                        2069.4
                    ],
                    [
                        1119.7,
                        2069.4
                    ],
                    [
                        1119.7,
                        2043.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86244,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3
        },
        "text": "Fig. 2. The Health gathering scenario(s) used in the experiments.",
        "type": "FigureCaption"
    },
    {
        "element_id": "f17be3e4db3bbbb2c248455697781ab1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 3
        },
        "text": "3",
        "type": "UncategorizedText"
    },
    {
        "element_id": "318fbdaec099ae1ceabb34acc0f204d9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.3,
                        102.9
                    ],
                    [
                        101.3,
                        124.3
                    ],
                    [
                        217.0,
                        124.3
                    ],
                    [
                        217.0,
                        102.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72602,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "c1e2fdf5c01add6451498d70e43527ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.1,
                        102.7
                    ],
                    [
                        1208.1,
                        120.6
                    ],
                    [
                        1547.8,
                        120.6
                    ],
                    [
                        1547.8,
                        102.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69269,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "296dd41c1dcc537fbee93f6c8102caa6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "c1e2fdf5c01add6451498d70e43527ab"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "d02173bd47c65f5daac33be58ee76e54",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        334.5,
                        158.4
                    ],
                    [
                        334.5,
                        661.6
                    ],
                    [
                        1318.9,
                        661.6
                    ],
                    [
                        1318.9,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-6.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "71148e2d0d473561a008f2aaf96c6231",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        652.7,
                        682.4
                    ],
                    [
                        652.7,
                        707.2
                    ],
                    [
                        997.3,
                        707.2
                    ],
                    [
                        997.3,
                        682.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84429,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "Fig. 3. Machine Learning Approaches.",
        "type": "FigureCaption"
    },
    {
        "element_id": "e3708b08906b3af1416370582ef94077",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        747.5
                    ],
                    [
                        104.4,
                        827.8
                    ],
                    [
                        801.9,
                        827.8
                    ],
                    [
                        801.9,
                        747.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92702,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "person shooter game (FPS) Doom is considered, and if the predicted measurements vector is (Kills, Ammo-used, Health) and the objective is",
        "type": "NarrativeText"
    },
    {
        "element_id": "e3dfcf314cfefffe22161a56f7a2607a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        811.4
                    ],
                    [
                        104.4,
                        833.5
                    ],
                    [
                        754.3,
                        833.5
                    ],
                    [
                        754.3,
                        811.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "to maximize the number of kills, then the objective can be set as,",
        "type": "NarrativeText"
    },
    {
        "element_id": "8a9482f4797d795ca6fec0a496bfe6cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.1,
                        846.6
                    ],
                    [
                        108.1,
                        869.0
                    ],
                    [
                        749.5,
                        869.0
                    ],
                    [
                        749.5,
                        846.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8128,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "U = f(m,) = g-m; = 1 x Kills \u2014 0.5 x Ammo used + 0.5 x Health",
        "type": "NarrativeText"
    },
    {
        "element_id": "4c96ec52fa20fea5b13dd4e15da0bb1e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        921.1
                    ],
                    [
                        104.4,
                        1029.7
                    ],
                    [
                        807.5,
                        1029.7
                    ],
                    [
                        807.5,
                        921.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93334,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "where g = [1, \u22120.5, 0.5] is known as the goal vector. The \u22120.5 wt assigned to the Ammo-used measurement just informs the agent it\u2019s not good to waste ammo. Such an approach has two major benefits that are",
        "type": "NarrativeText"
    },
    {
        "element_id": "61e01fb3cac82e35f4444ce40d31f6eb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1013.6
                    ],
                    [
                        104.4,
                        1035.7
                    ],
                    [
                        208.8,
                        1035.7
                    ],
                    [
                        208.8,
                        1013.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "as follows,",
        "type": "NarrativeText"
    },
    {
        "element_id": "0052b93cbd4561160090b025277c106b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        771.4,
                        882.4
                    ],
                    [
                        771.4,
                        910.3
                    ],
                    [
                        802.9,
                        910.3
                    ],
                    [
                        802.9,
                        882.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.37949,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "(1)",
        "type": "NarrativeText"
    },
    {
        "element_id": "5a6a01a8403293396f02dc73ce4b6ac6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        747.9
                    ],
                    [
                        851.7,
                        797.1
                    ],
                    [
                        1550.1,
                        797.1
                    ],
                    [
                        1550.1,
                        747.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90192,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "to saying pick the action which will lead to an increase in expected kill",
        "type": "NarrativeText"
    },
    {
        "element_id": "3453c0e67917f941ea6adb70334735bc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        782.2
                    ],
                    [
                        851.7,
                        804.4
                    ],
                    [
                        921.7,
                        804.4
                    ],
                    [
                        921.7,
                        782.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "counts.",
        "type": "Title"
    },
    {
        "element_id": "203d698b2c51b85b437159addf0e51b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        806.3
                    ],
                    [
                        851.7,
                        885.2
                    ],
                    [
                        1549.1,
                        885.2
                    ],
                    [
                        1549.1,
                        806.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93168,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "Further, if the health-level drops below a particular threshold, a different objective to the agent can be assigned so that it could focus on",
        "type": "NarrativeText"
    },
    {
        "element_id": "d3e247283c882c1590b92008a98db97d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        869.5
                    ],
                    [
                        851.7,
                        891.6
                    ],
                    [
                        1532.3,
                        891.6
                    ],
                    [
                        1532.3,
                        869.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "picking up health packs in order to improve health and avoid dying.",
        "type": "NarrativeText"
    },
    {
        "element_id": "de78023f9834a8f635dd0a1e15e5e621",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.6,
                        906.4
                    ],
                    [
                        850.6,
                        926.7
                    ],
                    [
                        1310.7,
                        926.7
                    ],
                    [
                        1310.7,
                        906.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.52291,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "U = 0x kills + 0 x Heath + 1 x Health Packs",
        "type": "NarrativeText"
    },
    {
        "element_id": "d8e3a02a2fa0509a53944437b0af9023",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        948.2
                    ],
                    [
                        851.7,
                        1055.1
                    ],
                    [
                        1553.0,
                        1055.1
                    ],
                    [
                        1553.0,
                        948.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93671,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "Under this goal vector [0, 0, 1], the agent concentrates obsessively to pick up health packs. Once the health level goes back to normal, the goal vector can be switched back to [1, 0, 0] so that the agent could",
        "type": "NarrativeText"
    },
    {
        "element_id": "14f762d09de82043e639a69a51fc2460",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1039.7
                    ],
                    [
                        851.7,
                        1061.8
                    ],
                    [
                        1032.4,
                        1061.8
                    ],
                    [
                        1032.4,
                        1039.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "start killing again.",
        "type": "NarrativeText"
    },
    {
        "element_id": "244b43b49589bc452c62f10c6fc6c463",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1073.4
                    ],
                    [
                        104.4,
                        1095.5
                    ],
                    [
                        514.1,
                        1095.5
                    ],
                    [
                        514.1,
                        1073.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "3.1. To stabilize and accelerate the training",
        "type": "NarrativeText"
    },
    {
        "element_id": "46fc9801190696faeadb36498291f94c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.2,
                        1126.2
                    ],
                    [
                        102.2,
                        1298.8
                    ],
                    [
                        808.9,
                        1298.8
                    ],
                    [
                        808.9,
                        1126.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95386,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "Dealing supervised learning [36], with concrete labels such as multidimensional measurements attached to each input state for ex- ample pixels\u2019 input, the agent is able to learn from a richer and denser signal than a single scalar reward stream can provide. Training per- formance can be greatly enhanced and stabilized as a result, just like in typical supervised learning tasks such as image classification.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ebfa23ba4b9b7ebee26ee062b924ff7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1065.4
                    ],
                    [
                        851.7,
                        1317.3
                    ],
                    [
                        1552.0,
                        1317.3
                    ],
                    [
                        1552.0,
                        1065.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95613,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "The ability to pursue complex goals at inference time has great implications for reinforcement learning, so a truly intelligent agent needs to be able to adapt itself to different goals under different cir- cumstances [39]. However, these days most traditional reinforcement learning methods limit learning to only a single objective following the guidance of the scalar reward which is not the true way of learning for intelligent agents to behave. In fact, similar to human beings, re- inforcement learning agents need to possess the innate ability to switch",
        "type": "NarrativeText"
    },
    {
        "element_id": "a7379c5f6a9044392f509057861c7430",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1301.1
                    ],
                    [
                        851.7,
                        1323.3
                    ],
                    [
                        1290.6,
                        1323.3
                    ],
                    [
                        1290.6,
                        1301.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "3453c0e67917f941ea6adb70334735bc"
        },
        "text": "goals based on different circumstances [40].",
        "type": "NarrativeText"
    },
    {
        "element_id": "5a57058b084609505afab6e618b58e92",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1336.6
                    ],
                    [
                        104.4,
                        1358.7
                    ],
                    [
                        449.7,
                        1358.7
                    ],
                    [
                        449.7,
                        1336.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "3.2. Complex goals at inference time",
        "type": "Title"
    },
    {
        "element_id": "0b459f8255f4f5a0df77c73695f9e83b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        1390.1
                    ],
                    [
                        104.0,
                        1556.0
                    ],
                    [
                        807.9,
                        1556.0
                    ],
                    [
                        807.9,
                        1390.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95032,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "It is one of the more interesting aspects of this approach. In tradi- tional reinforcement learning the objective is to maximize the expected future rewards, to be more specific, it implies that the agent only knows how to act based on the objective given. The agent cannot be simply instructed to behave differently (i.e. with another objective) in any",
        "type": "NarrativeText"
    },
    {
        "element_id": "cba47b4449e3409402f0a0563760ca47",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1540.0
                    ],
                    [
                        104.4,
                        1562.2
                    ],
                    [
                        507.2,
                        1562.2
                    ],
                    [
                        507.2,
                        1540.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "meaningful sense at inference time [37].",
        "type": "NarrativeText"
    },
    {
        "element_id": "594243c5f295131529475f4db50144a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1326.7
                    ],
                    [
                        851.7,
                        1462.5
                    ],
                    [
                        1554.3,
                        1462.5
                    ],
                    [
                        1554.3,
                        1326.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93887,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "As in traditional reinforcement learning, responses are received in the form of scalar rewards. In the same way, in DFP responses are re- ceived in the form of measurements (m) which can be thought of as a multidimensional vector with each element capturing some aspects of",
        "type": "NarrativeText"
    },
    {
        "element_id": "76af9579f5d1a46bd86c35d5be1da857",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1446.5
                    ],
                    [
                        851.7,
                        1468.6
                    ],
                    [
                        1295.9,
                        1468.6
                    ],
                    [
                        1295.9,
                        1446.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "the game e.g. kills, ammunition, health, etc.,",
        "type": "NarrativeText"
    },
    {
        "element_id": "25e08686d0445f6277218eb6c98eb2fc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1471.4
                    ],
                    [
                        851.7,
                        1549.3
                    ],
                    [
                        1549.3,
                        1549.3
                    ],
                    [
                        1549.3,
                        1471.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92987,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "Let [\u03c41\u2026, \u03c4n] be a set of temporal offsets that the model has to learn to predict the differences between future and present measurements can",
        "type": "NarrativeText"
    },
    {
        "element_id": "c30894f086c5d19946110797e54e3835",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1533.7
                    ],
                    [
                        851.7,
                        1555.9
                    ],
                    [
                        1101.8,
                        1555.9
                    ],
                    [
                        1101.8,
                        1533.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "be formulated as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "bb1954e4a3604c1367771dfcfa49099b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1564.1
                    ],
                    [
                        104.4,
                        1729.9
                    ],
                    [
                        803.0,
                        1729.9
                    ],
                    [
                        803.0,
                        1564.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94545,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4,
            "parent_id": "5a57058b084609505afab6e618b58e92"
        },
        "text": "In contrast, the presented supervised learning approach enables the agent to flexibly pursue different objectives (i.e. goals) or a combina- tion of multiple objectives at inference time. It can be achieved through the model under supervised learning settings that outputs the predic- tion of measurements. The objective can be basically expressed as a",
        "type": "NarrativeText"
    },
    {
        "element_id": "f6ace258a86e551b803cba379f029cf6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1714.3
                    ],
                    [
                        104.4,
                        1736.5
                    ],
                    [
                        504.7,
                        1736.5
                    ],
                    [
                        504.7,
                        1714.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "function of the predicted measurements.",
        "type": "Title"
    },
    {
        "element_id": "d31e4ff974cd2be59c80d2ce2c24828b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.6,
                        1570.3
                    ],
                    [
                        854.6,
                        1593.1
                    ],
                    [
                        1561.0,
                        1593.1
                    ],
                    [
                        1561.0,
                        1570.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6234,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "[Myc \u2014 My Men \u2014 M] (4)",
        "type": "Formula"
    },
    {
        "element_id": "6495a18b630f5f2f66039c627e63ec6a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1616.5
                    ],
                    [
                        851.7,
                        1723.9
                    ],
                    [
                        1553.2,
                        1723.9
                    ],
                    [
                        1553.2,
                        1616.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93836,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "It is beneficial to use [1,2,4,8,16,32] as the set of temporal offsets. In practice, the model outputs a set of \u2018f\u2019, one for each action. At in- ference time, the agent simply picks the action that maximizes the",
        "type": "NarrativeText"
    },
    {
        "element_id": "c43bd0b9a1873d77379e5aed024b44dd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1708.2
                    ],
                    [
                        851.7,
                        1730.3
                    ],
                    [
                        1298.1,
                        1730.3
                    ],
                    [
                        1298.1,
                        1708.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "objective U that can be computed as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "88cf04da607c2c6f12f03aeaa7038b73",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        99.0,
                        1738.5
                    ],
                    [
                        99.0,
                        1939.8
                    ],
                    [
                        810.8,
                        1939.8
                    ],
                    [
                        810.8,
                        1738.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94839,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "In an FPS game, the environment provides at least three measure- ments for every time step (Kills, Health, Health Packs). A health pack is a box scattered around the environment that can be picked up by the agent to improve its health. Health Packs measures the number of health packs picked up by the agent, to be more specific, it is a rea- sonable objective to simply tell the agent to maximize the number of kills [38],",
        "type": "NarrativeText"
    },
    {
        "element_id": "4c8bbc70065ad44cc89536c3f635041c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.4,
                        1744.4
                    ],
                    [
                        852.4,
                        1765.8
                    ],
                    [
                        918.7,
                        1765.8
                    ],
                    [
                        918.7,
                        1744.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58569,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "U= gf",
        "type": "NarrativeText"
    },
    {
        "element_id": "762ffb85eec25f6e5df968231870a972",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1790.8
                    ],
                    [
                        851.7,
                        1956.0
                    ],
                    [
                        1551.5,
                        1956.0
                    ],
                    [
                        1551.5,
                        1790.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95083,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "Here \u2018g\u2019 denotes the goal vector that controls the behaviour of the agent, the scalar reward is used as the only measurement and set \u2018g\u2019 as a vector of discounted factors i.e. (1, \u03b3, \u03b3 2, \u2026.\u2026..) then the resulting objective function resembles the Q value, which is the sum of dis- counted future rewards. Therefore, sometimes in a sense, DQN is va-",
        "type": "NarrativeText"
    },
    {
        "element_id": "fc8d4a5a99dbfeaa36a83fa2d99e8acd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        80.4,
                        1953.4
                    ],
                    [
                        80.4,
                        1972.3
                    ],
                    [
                        649.2,
                        1972.3
                    ],
                    [
                        649.2,
                        1953.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63987,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "U = 1x kills + 0 x Heath + 0 x Health Packs",
        "type": "NarrativeText"
    },
    {
        "element_id": "9917e8add75668e335425c4263432a10",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1994.4
                    ],
                    [
                        104.4,
                        2073.0
                    ],
                    [
                        801.9,
                        2073.0
                    ],
                    [
                        801.9,
                        1994.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92988,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "The coefficients of the measurements i.e. [1, 0, 0] represent the goal vector (g). Then at each time step, the agent will pick the action i.e.",
        "type": "NarrativeText"
    },
    {
        "element_id": "68861ac97f7ab7588036f02790aa790a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2057.2
                    ],
                    [
                        104.4,
                        2079.3
                    ],
                    [
                        801.8,
                        2079.3
                    ],
                    [
                        801.8,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "Turn Left, Turn Right, or Shoot that maximizes U, which is equivalent",
        "type": "NarrativeText"
    },
    {
        "element_id": "b0056a53aa04ca8c803d7cf278371480",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        772.8,
                        1955.5
                    ],
                    [
                        772.8,
                        1977.6
                    ],
                    [
                        801.9,
                        1977.6
                    ],
                    [
                        801.9,
                        1955.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "(2)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4b11a76b26ee50268c9f34981e34835a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1941.0
                    ],
                    [
                        851.7,
                        1963.1
                    ],
                    [
                        1388.6,
                        1963.1
                    ],
                    [
                        1388.6,
                        1941.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "guely viewed and considered as a special case of DFP.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1f07b967cf7b8ba9fd40a7596db38b03",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1966.5
                    ],
                    [
                        851.7,
                        2073.1
                    ],
                    [
                        1551.4,
                        2073.1
                    ],
                    [
                        1551.4,
                        1966.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94141,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "The network model used in the implementation consists of three inputs modules i.e. a perception module S(s), a measurement module M (m) and a goal module G(g) as shown in Fig. 4. If \u2018s\u2019 is an image, then",
        "type": "NarrativeText"
    },
    {
        "element_id": "48a2f27010a011d7b5e2d56b75971d0b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        2057.2
                    ],
                    [
                        851.7,
                        2079.3
                    ],
                    [
                        1549.1,
                        2079.3
                    ],
                    [
                        1549.1,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "the perception module \u2018S\u2019 is implemented as a convolutional neural",
        "type": "NarrativeText"
    },
    {
        "element_id": "7c17c61c049cf37f4fba2c46f2bbcf3b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "4",
        "type": "UncategorizedText"
    },
    {
        "element_id": "b3b6c7605e77ec5d9680e9fd27f6a8f9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1520.0,
                        908.1
                    ],
                    [
                        1520.0,
                        930.2
                    ],
                    [
                        1549.1,
                        930.2
                    ],
                    [
                        1549.1,
                        908.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "(3)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ad959179a0ccd0725328809cef4a1968",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1520.0,
                        1576.6
                    ],
                    [
                        1520.0,
                        1598.7
                    ],
                    [
                        1549.1,
                        1598.7
                    ],
                    [
                        1549.1,
                        1576.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "(4)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "0b3b388ec5b21eec147583788f45d9f9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1520.0,
                        1751.2
                    ],
                    [
                        1520.0,
                        1773.3
                    ],
                    [
                        1549.1,
                        1773.3
                    ],
                    [
                        1549.1,
                        1751.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 4
        },
        "text": "(5)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "41c5b235e7d575bf85a0e3e97761b2d8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.4,
                        102.4
                    ],
                    [
                        101.4,
                        124.3
                    ],
                    [
                        217.0,
                        124.3
                    ],
                    [
                        217.0,
                        102.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70577,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "6b7aa581d2e3e0f0c6a69f7fe95e3d98",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.0,
                        102.7
                    ],
                    [
                        1208.0,
                        120.5
                    ],
                    [
                        1548.0,
                        120.5
                    ],
                    [
                        1548.0,
                        102.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69734,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "40efbe5ada44a6520ca81486f2c75159",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "6b7aa581d2e3e0f0c6a69f7fe95e3d98"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "02a4dadcea60873f7ee046e5bb592d8a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        334.3,
                        158.4
                    ],
                    [
                        334.3,
                        797.2
                    ],
                    [
                        1319.1,
                        797.2
                    ],
                    [
                        1319.1,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-7.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "Input Image m. \u00fck rae Measurements(m) FCNN m) m E = = EXPECTATION > de) ep ACTION Ah x Ag e0 p Duplicate Prediction Target Action Taken f Normalize",
        "type": "Image"
    },
    {
        "element_id": "40bd999d71f26ef1cf5b1ee109a5f04a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        634.2,
                        819.0
                    ],
                    [
                        634.2,
                        842.8
                    ],
                    [
                        1013.2,
                        842.8
                    ],
                    [
                        1013.2,
                        819.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81085,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "Fig. 4. DFP Neural Network Architecture.",
        "type": "FigureCaption"
    },
    {
        "element_id": "da168c6143fdd79c48b021f3001effb5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        884.8
                    ],
                    [
                        104.4,
                        992.0
                    ],
                    [
                        801.9,
                        992.0
                    ],
                    [
                        801.9,
                        884.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93796,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "network. The measurement and goal modules are fully-connected net- works. The outputs of the three input modules are concatenated, forming the joint input representation (j) which is used for subsequent",
        "type": "NarrativeText"
    },
    {
        "element_id": "af038dfaebf6e1839d6111de5f4574a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        975.9
                    ],
                    [
                        104.4,
                        998.1
                    ],
                    [
                        213.4,
                        998.1
                    ],
                    [
                        213.4,
                        975.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "processing:",
        "type": "Title"
    },
    {
        "element_id": "04ba462faedd85d657e881e2ee97e05e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        113.5,
                        1011.7
                    ],
                    [
                        113.5,
                        1041.1
                    ],
                    [
                        814.0,
                        1041.1
                    ],
                    [
                        814.0,
                        1011.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63867,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "(6)",
        "type": "Formula"
    },
    {
        "element_id": "bffd090f0ecf59343ba26e511c6e130a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1058.7
                    ],
                    [
                        104.4,
                        1166.2
                    ],
                    [
                        801.8,
                        1166.2
                    ],
                    [
                        801.8,
                        1058.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93602,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "The model consists of two streams, the expectation stream E(j) and the Action Stream A(j). Usually using such two separate streams leads to better performance and this approach is based on the dueling ar-",
        "type": "NarrativeText"
    },
    {
        "element_id": "74fed1080c6202857893844107cfbe4e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1150.4
                    ],
                    [
                        104.4,
                        1172.6
                    ],
                    [
                        596.0,
                        1172.6
                    ],
                    [
                        596.0,
                        1150.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "chitecture introduced by Google Deep Mind [41].",
        "type": "NarrativeText"
    },
    {
        "element_id": "3a5098fd5b633e95c67c39f8406940fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        882.3
                    ],
                    [
                        851.7,
                        1195.8
                    ],
                    [
                        1554.8,
                        1195.8
                    ],
                    [
                        1554.8,
                        882.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95384,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "Poison are derivative measurements from health. This is quite sur- prising and unreasonable that the performance of DFP deteriorates by 50% if just \u2018health\u2019 is used as the only measurement, even though health packs and poisons are derived from the change in health. Further, there is a beneficial effect by allowing the model to generate a richer set of predictions, similar to the way auxiliary tasks enhance the performance of deep learning vision classifier [42]. One of the best things about DFP is its capability to pursue different goals at inference time. For illus- trating, the trained model can be used and the goal vector \u2018g\u2019 can be altered from (1, 1, \u22121) to (0, 0, 1). Where the objective becomes as",
        "type": "NarrativeText"
    },
    {
        "element_id": "121280c134de67504e771d8c41c8d9d9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1174.7
                    ],
                    [
                        104.4,
                        1317.9
                    ],
                    [
                        801.9,
                        1317.9
                    ],
                    [
                        801.9,
                        1174.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93771,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "While training, each transition produces a tuple (s, a, m) using the environment. where \u2018s\u2019 represents the state e.g. image pixels, \u2018a\u2019 is the action taken, and \u2018m\u2019 is the measurement. A training target \u2018f\u2019 can then be formulated using the measurements obtained at the specified tem- poral offsets \u03c4 i.e. [1,2,4,8,16,32],",
        "type": "NarrativeText"
    },
    {
        "element_id": "af32807c12c51d29d95e2f3a39442411",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1179.4
                    ],
                    [
                        851.7,
                        1201.5
                    ],
                    [
                        916.9,
                        1201.5
                    ],
                    [
                        916.9,
                        1179.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "below,",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d2f5e7980f57ac438852b3758b1dcc27",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.3,
                        1217.1
                    ],
                    [
                        850.3,
                        1238.7
                    ],
                    [
                        1335.3,
                        1238.7
                    ],
                    [
                        1335.3,
                        1217.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73303,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "U = 0x Health + 0 x Health Packs + 1 x Poison",
        "type": "NarrativeText"
    },
    {
        "element_id": "f22cbc2a86dd72d8882ef3d855d66285",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1301.8
                    ],
                    [
                        851.7,
                        1323.9
                    ],
                    [
                        1075.6,
                        1323.9
                    ],
                    [
                        1075.6,
                        1301.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "3.5. Experimental setup",
        "type": "Title"
    },
    {
        "element_id": "688c0ab3f341426455d0e138913bb3e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        116.9,
                        1332.6
                    ],
                    [
                        116.9,
                        1363.7
                    ],
                    [
                        807.3,
                        1363.7
                    ],
                    [
                        807.3,
                        1332.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5211,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "(7)",
        "type": "Formula"
    },
    {
        "element_id": "ba74cf66a2ac3aa72c21ec43f6ab858c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        1333.0
                    ],
                    [
                        104.0,
                        1349.0
                    ],
                    [
                        125.0,
                        1349.0
                    ],
                    [
                        125.0,
                        1333.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "f=",
        "type": "Title"
    },
    {
        "element_id": "ad54c9e934d4a424a146434882f1b90f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.3,
                        1379.4
                    ],
                    [
                        103.3,
                        1457.1
                    ],
                    [
                        801.9,
                        1457.1
                    ],
                    [
                        801.9,
                        1379.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92912,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "ba74cf66a2ac3aa72c21ec43f6ab858c"
        },
        "text": "The target can be used to train the neural network model with backpropagation and Mean Square Error (MSE) can be used as the loss",
        "type": "NarrativeText"
    },
    {
        "element_id": "423295046c2dccdc9015921378ebbd06",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1440.8
                    ],
                    [
                        104.4,
                        1462.9
                    ],
                    [
                        405.8,
                        1462.9
                    ],
                    [
                        405.8,
                        1440.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "ba74cf66a2ac3aa72c21ec43f6ab858c"
        },
        "text": "function to compute the error.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5fe6d46bc0b205e147fc1765713ab93c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.9,
                        1355.6
                    ],
                    [
                        858.9,
                        1405.2
                    ],
                    [
                        1549.0,
                        1405.2
                    ],
                    [
                        1549.0,
                        1355.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92648,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "ba74cf66a2ac3aa72c21ec43f6ab858c"
        },
        "text": "The experimental setup includes the environment and the archi-",
        "type": "NarrativeText"
    },
    {
        "element_id": "65e3776a13a9ae18460cd9c9cea310e7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1388.8
                    ],
                    [
                        851.7,
                        1411.0
                    ],
                    [
                        1357.1,
                        1411.0
                    ],
                    [
                        1357.1,
                        1388.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "ba74cf66a2ac3aa72c21ec43f6ab858c"
        },
        "text": "tecture of the neural network explained as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "946782f6e743656c01ebedb16fc5f22b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1446.9
                    ],
                    [
                        851.7,
                        1469.1
                    ],
                    [
                        1168.2,
                        1469.1
                    ],
                    [
                        1168.2,
                        1446.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "(a) Neural network architecture",
        "type": "Title"
    },
    {
        "element_id": "7e95f1f311a7e967642acdc53962a9e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1502.1
                    ],
                    [
                        104.4,
                        1524.2
                    ],
                    [
                        581.5,
                        1524.2
                    ],
                    [
                        581.5,
                        1502.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "946782f6e743656c01ebedb16fc5f22b"
        },
        "text": "3.3. Supervised learning for reinforcement learning",
        "type": "NarrativeText"
    },
    {
        "element_id": "e61790630345645788640d1eead2d116",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.5,
                        1554.8
                    ],
                    [
                        102.5,
                        1749.3
                    ],
                    [
                        803.5,
                        1749.3
                    ],
                    [
                        803.5,
                        1554.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95424,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "946782f6e743656c01ebedb16fc5f22b"
        },
        "text": "The measurement and goal input modules are found less useful or in other words, found slightly detrimental to the performance so it is decided to use only the perception module as inputs to the model. The measurements are normalized and a goal vector \u2018g\u2019 of [1, 1, \u22121] i.e. coefficients for (Health, Health Packs, Poison) measurements are used. There are almost 50,000 episodes of DFP ran on the health gathering",
        "type": "NarrativeText"
    },
    {
        "element_id": "9e0c6fddd1137a96222834908eb028f8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1734.5
                    ],
                    [
                        104.4,
                        1756.6
                    ],
                    [
                        191.8,
                        1756.6
                    ],
                    [
                        191.8,
                        1734.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "scenario.",
        "type": "Title"
    },
    {
        "element_id": "8ca6c4bc2c9d5ae6b94acafb05c91c5a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.5,
                        1758.3
                    ],
                    [
                        103.5,
                        1953.1
                    ],
                    [
                        804.0,
                        1953.1
                    ],
                    [
                        804.0,
                        1758.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95518,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "9e0c6fddd1137a96222834908eb028f8"
        },
        "text": "DFP excels in environments where a stream of rich and temporally dense multidimensional feedbacks are available. In traditional re- inforcement learning settings, transforming the feedbacks into a single dimension scalar reward might result in loss of useful information which would detriment performance. It is also noted out that enrich- ment in measurements is the most important factor for the good per-",
        "type": "NarrativeText"
    },
    {
        "element_id": "76c9461b6a92a6828d9add7cad925075",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1937.8
                    ],
                    [
                        104.4,
                        1959.9
                    ],
                    [
                        276.6,
                        1959.9
                    ],
                    [
                        276.6,
                        1937.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5
        },
        "text": "formance of DFP.",
        "type": "Title"
    },
    {
        "element_id": "67746ab116f9a5b7be3b4684f84235c7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1999.1
                    ],
                    [
                        104.4,
                        2021.2
                    ],
                    [
                        458.9,
                        2021.2
                    ],
                    [
                        458.9,
                        1999.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "3.4. Using health as the measurement",
        "type": "NarrativeText"
    },
    {
        "element_id": "415df9b6f969b9653a981762992140b3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.5,
                        1501.2
                    ],
                    [
                        851.5,
                        2072.3
                    ],
                    [
                        1549.1,
                        2072.3
                    ],
                    [
                        1549.1,
                        1501.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95046,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "The neural network architecture consists of three input modules; the perception module is the environment state which is just screen pixels (input image). The three-layered convolutional neural network is used as the feature extractor to transform the screen pixels into a vector of length 512 and the three-layered fully connected network is used to parse the measurement module and goal module. The outputs of the three modules are concatenated to form a joint representation for fur- ther processing. The model is then split into two streams, the ex- pectation stream, and the action stream. Their respective outputs are summed to form the model\u2019s prediction. In our proposed method the measurement size is three i.e. (health, health packs, poison), a number of time steps are 6 i.e. (1, 2, 4, 8, 16, 32) and the action size is three i.e. (Turn Left, Turn Right, and Move Forward). The model is trained and compiled using Adam optimizer [43] and the mean squared error (MSE) [44] as the loss metric. Similar to other reinforcement learning algo- rithms, most of the logic is contained in the update step. First, a mini- batch is sampled of sample trajectories from the experience replay buffer (memory) and initialize the corresponding states, measurements, goal and targets variables. Then target is computed for the model which",
        "type": "NarrativeText"
    },
    {
        "element_id": "488360ce10ed395233c57bdcdccb261a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        139.1,
                        2057.2
                    ],
                    [
                        139.1,
                        2079.3
                    ],
                    [
                        801.8,
                        2079.3
                    ],
                    [
                        801.8,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "Health is considered in the implementation, as health packs and",
        "type": "NarrativeText"
    },
    {
        "element_id": "99310700b0b5f08e10be72e26ae55d87",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        2057.2
                    ],
                    [
                        851.7,
                        2079.3
                    ],
                    [
                        1549.0,
                        2079.3
                    ],
                    [
                        1549.0,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "is the difference between future and present measurements for the set of",
        "type": "NarrativeText"
    },
    {
        "element_id": "4e81dce459dc998e05e78899247125da",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "5",
        "type": "UncategorizedText"
    },
    {
        "element_id": "10bb96c331c455553a15b8bb1971e01c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1520.0,
                        1219.5
                    ],
                    [
                        1520.0,
                        1241.7
                    ],
                    [
                        1549.1,
                        1241.7
                    ],
                    [
                        1549.1,
                        1219.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 5,
            "parent_id": "76c9461b6a92a6828d9add7cad925075"
        },
        "text": "(8)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "e0ca07f5b581490c83def19f8d5d8bef",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.0,
                        102.2
                    ],
                    [
                        102.0,
                        124.3
                    ],
                    [
                        216.4,
                        124.3
                    ],
                    [
                        216.4,
                        102.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64668,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "0893bd08ff4f3cb1f0c237697f6d7074",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1209.8,
                        103.0
                    ],
                    [
                        1209.8,
                        120.6
                    ],
                    [
                        1550.5,
                        120.6
                    ],
                    [
                        1550.5,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78203,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "f469bb7e3f016ba42fecbb01dce01c5d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "0893bd08ff4f3cb1f0c237697f6d7074"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "eedd658531acaefe88ca872549f4bb6b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        126.1,
                        164.0
                    ],
                    [
                        126.1,
                        189.7
                    ],
                    [
                        198.0,
                        189.7
                    ],
                    [
                        198.0,
                        164.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.45914,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "0893bd08ff4f3cb1f0c237697f6d7074"
        },
        "text": "Table 1",
        "type": "Title"
    },
    {
        "element_id": "57ab2b206aa9b9d54c85c8707c6070c7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        126.1,
                        196.2
                    ],
                    [
                        126.1,
                        216.1
                    ],
                    [
                        438.3,
                        216.1
                    ],
                    [
                        438.3,
                        196.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "0893bd08ff4f3cb1f0c237697f6d7074"
        },
        "text": "Test Setup Hardware Specification.",
        "type": "Title"
    },
    {
        "element_id": "c018904ca8fa611024823aa01eeb94d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        845.2,
                        165.2
                    ],
                    [
                        845.2,
                        219.6
                    ],
                    [
                        1548.7,
                        219.6
                    ],
                    [
                        1548.7,
                        165.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77666,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "0893bd08ff4f3cb1f0c237697f6d7074"
        },
        "text": "5. Advantage Actor-critic (A2C) and advantage Actor-Critic- long Short-Term memory (A2C-LSTM)",
        "type": "Title"
    },
    {
        "element_id": "feff5ea1fe4d56d6d34bfd5f82d0dbe2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        132.0,
                        228.8
                    ],
                    [
                        132.0,
                        302.6
                    ],
                    [
                        772.1,
                        302.6
                    ],
                    [
                        772.1,
                        228.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73964,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-6-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5",
            "text_as_html": "<table><tbody><tr><td>GPU</td><td>Intel\u00ae Core\u201d i7-7700 CPU \u00a93.60 GHz x 8</td></tr><tr><td>GPU</td><td>NVIDIA GeForce GTX 1080/PCle/SSE2 GPU</td></tr><tr><td>RAM</td><td>GiB DDR4</td></tr></tbody></table>"
        },
        "text": "CPU Intel\u00ae Core\u2122 i7-7700 CPU @3.60 GHz \u00d7 8 GPU NVIDIA GeForce GTX 1080/PCIe/SSE2 GPU RAM GiB DDR4",
        "type": "Table"
    },
    {
        "element_id": "4d8defd1662926998d87acdb0b3aec9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.3,
                        359.2
                    ],
                    [
                        102.3,
                        582.6
                    ],
                    [
                        805.5,
                        582.6
                    ],
                    [
                        805.5,
                        359.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95499,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "temporal offsets (1, 2, 4, 8, 16, 32). The target for each action is as- signed to the \u2018f_action_target\u2019 variable. The rest of the variables are filled up with the mini-batch samples drawn from the experience re- play. f_target is the ground truth label assigned to the model for training. Further, the training routine needs a call to perform gradient descent update. The overall learning and testing process using DFP lasted for 12 h on a powerful GPU machine whose specifications are",
        "type": "NarrativeText"
    },
    {
        "element_id": "f8c92e76e22dcb7fd890fc01b8c34b97",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        566.5
                    ],
                    [
                        104.4,
                        588.6
                    ],
                    [
                        312.2,
                        588.6
                    ],
                    [
                        312.2,
                        566.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "described in Table 1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "86b64a5eb9dc281873546d20e619d4ec",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        649.3
                    ],
                    [
                        104.4,
                        704.8
                    ],
                    [
                        776.0,
                        704.8
                    ],
                    [
                        776.0,
                        649.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81455,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "4. Direct future prediction (DFP) and asynchronous advantage Actor-Critic (A3C)",
        "type": "NarrativeText"
    },
    {
        "element_id": "2038c5c96e9674b449512da015684d0d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        735.4
                    ],
                    [
                        104.4,
                        1338.4
                    ],
                    [
                        805.8,
                        1338.4
                    ],
                    [
                        805.8,
                        735.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94867,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "As the Direct Future Prediction (DFP) is explained in section 2 which is good at pursuing complex goals at test time. The architecture of DFP is shown in Fig. 4 and its performance is presented in a graph displayed in Fig. 5. With a performance of almost 95%, DFP can be chosen as one of the best machine learning technique for training game agents or bots. While on the other hand, the architecture of A3C utilizes the power of the deep neural networks (DNN) [45] by running multiple agents for training at the same time. Each agent then shares its results with the other agents. Since every agent makes different decisions, this approach reduces the chance for the AI to run into a local minimum. Additionally, it drastically reduces the average training time required to perform decently well at any given task. The A3C high-level archi- tecture is shown in Fig. 6. In A3C the global network and multiple worker agents each have their own set of network parameters. Each of these agents interacts with its own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent is that the experi- ence of each agent is independent of the experience of the others. In this way, the overall experience available for training becomes more di- verse. In addition to using the A3C algorithms for training the agent",
        "type": "NarrativeText"
    },
    {
        "element_id": "8ac0d7d3233d9f3a9616636059e1ac69",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1321.9
                    ],
                    [
                        104.4,
                        1344.0
                    ],
                    [
                        725.0,
                        1344.0
                    ],
                    [
                        725.0,
                        1321.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "RMSProp [46] is used as an optimizer during the experiments.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a185ce9c5a8b5d787e7c089770c30d38",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1347.4
                    ],
                    [
                        104.4,
                        1512.1
                    ],
                    [
                        807.0,
                        1512.1
                    ],
                    [
                        807.0,
                        1347.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95092,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "After both DFP and A3C are used to train the agents on health gathering scenario of the VizDoom Game AI research platform, then the agents were tested as well to analyze and observe their performances where DFP was found better than A3C as shown in Fig. 7. The agent trained with DFP gathered almost 95% health in the allotted time while",
        "type": "NarrativeText"
    },
    {
        "element_id": "3f3f03a2d0efd1d1376a3f4c1be03213",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1496.2
                    ],
                    [
                        104.4,
                        1518.4
                    ],
                    [
                        666.9,
                        1518.4
                    ],
                    [
                        666.9,
                        1496.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "the agent trained with A3C gathered almost 90% health.",
        "type": "NarrativeText"
    },
    {
        "element_id": "90de03495448fdaa3209de49070a9036",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.3,
                        1521.7
                    ],
                    [
                        103.3,
                        1570.6
                    ],
                    [
                        801.8,
                        1570.6
                    ],
                    [
                        801.8,
                        1521.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91679,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "The overall learning and testing process is measured in time and",
        "type": "NarrativeText"
    },
    {
        "element_id": "13997153d6a97f0a994bd65ea3d88af2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1554.3
                    ],
                    [
                        104.4,
                        1576.5
                    ],
                    [
                        543.4,
                        1576.5
                    ],
                    [
                        543.4,
                        1554.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "lasted for 12 h on a powerful GPU machine.",
        "type": "NarrativeText"
    },
    {
        "element_id": "684e1b86a012d881a46e152f3fca209d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        255.6
                    ],
                    [
                        851.6,
                        946.1
                    ],
                    [
                        1551.0,
                        946.1
                    ],
                    [
                        1551.0,
                        255.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94685,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "c018904ca8fa611024823aa01eeb94d5"
        },
        "text": "Advantage Actor-Critic (A2C) method performs best with large batch sizes by using the GPU\u2019s effectively. The A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies, however, frailer in results and output than GPU-only A3C im- plementation. Both value-based methods and policy-based methods have drawbacks that\u2019s why a new reinforcement learning method \u2018Actor-Critic\u2019 has been introduced where critic measures how good the action taken is (value-based) and Actor controls how the agent behaves (policy-based). Mastering such an architecture is essential to under- stand the state of the art algorithms such as Proximal Policy Optimization (PPO) [48] which is based on Advantage Actor-Critic (A2C). In the Actor-Critic method, the actor makes actions randomly and the Critic observes the actions and provides feedback. Learning from this feedback, the actor updates its policy and becomes better at playing the game. On the other hand, the Critic also updates its knowledge to provide feedback so it can become better next time. The approach of Actor-Critic is to have two neural networks run in parallel which can be estimated as Actor: a policy function \u03a0(s, , \u03f4) that controls how the agent acts, and, the Critic: a value function measures how good the actions are. In addition, as the value-based methods have high susceptibility so the advantage function can be used instead of value function to overcome this problem which can be de- fined as,",
        "type": "NarrativeText"
    },
    {
        "element_id": "c5f1f4dc699c82816c1c33b31ebab3df",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.1,
                        964.1
                    ],
                    [
                        858.1,
                        991.9
                    ],
                    [
                        1560.5,
                        991.9
                    ],
                    [
                        1560.5,
                        964.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62523,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "(9)",
        "type": "Formula"
    },
    {
        "element_id": "cc9c09b676ea10b5eb02442fc820826f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1014.2
                    ],
                    [
                        851.7,
                        1092.2
                    ],
                    [
                        1556.6,
                        1092.2
                    ],
                    [
                        1556.6,
                        1014.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93286,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "But even this advantage function has drawbacks because it requires two value functions - Q(s, a) and V(s) and this drawback can be over-",
        "type": "NarrativeText"
    },
    {
        "element_id": "65d6bc212af35971f53e06ec21b9baba",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1075.8
                    ],
                    [
                        851.7,
                        1097.9
                    ],
                    [
                        1333.1,
                        1097.9
                    ],
                    [
                        1333.1,
                        1075.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "come by using the TD error as a good estimator.",
        "type": "NarrativeText"
    },
    {
        "element_id": "93a89bd0e28673f1d4316df508b407b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.2,
                        1101.9
                    ],
                    [
                        850.2,
                        1615.4
                    ],
                    [
                        1553.5,
                        1615.4
                    ],
                    [
                        1553.5,
                        1101.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95106,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "In addition, the agents trained with A2C and A2C-LSTM algorithms can be analyzed and observed in Fig. 8, where the performance of A2C is better than A2C-LSTM. The curve of the A2C-LSTM stayed uniform until the last higher number of steps and never rose or declined to a high extent. While on the other hand, the beginning health gathering performance of the agent trained with A2C was found at most 2 to 3 percent better than A2C-LSTM and remain uniform until 10,000 steps where then the agent health gathering performance improved gradually stepwise and at ~15,000 steps the performance curve started touching 70% but its overall final performance declines to ~54% which can be seen in the left graph in Fig. 8. However, on the other side, to confirm the final conclusion to be accurate and authentic the A2C-LSTM agent was considered to be trained time and time again for further longer steps at least up to 300,000 steps which in number were more steps than the training steps of A2C agent just because to see any change or improvement in performance curve, however, despite of training for extraordinary steps still the curve was uniform. Therefore, it was con-",
        "type": "NarrativeText"
    },
    {
        "element_id": "507da2e09df5bc79e6285c16929975b4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1598.9
                    ],
                    [
                        851.7,
                        1621.1
                    ],
                    [
                        1549.1,
                        1621.1
                    ],
                    [
                        1549.1,
                        1598.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "cluded that A2C should be preferred over A2C-LSTM for training game",
        "type": "NarrativeText"
    },
    {
        "element_id": "b2dded2d96cc78018436824b0c6f70aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.9,
                        1657.9
                    ],
                    [
                        103.9,
                        1676.2
                    ],
                    [
                        178.9,
                        1676.2
                    ],
                    [
                        178.9,
                        1657.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72626,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "Table 2",
        "type": "Title"
    },
    {
        "element_id": "a94b9fb9cc454cd7f49954fc68427e60",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1662.5
                    ],
                    [
                        104.4,
                        1682.4
                    ],
                    [
                        175.0,
                        1682.4
                    ],
                    [
                        175.0,
                        1662.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6
        },
        "text": "Table 2",
        "type": "Title"
    },
    {
        "element_id": "9fda7e8e7cd58e2edb0d593c490ea595",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1689.1
                    ],
                    [
                        104.4,
                        1709.1
                    ],
                    [
                        810.0,
                        1709.1
                    ],
                    [
                        810.0,
                        1689.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "a94b9fb9cc454cd7f49954fc68427e60"
        },
        "text": "Hyperparameters used in the experiments for training models (Agents or Bots).",
        "type": "NarrativeText"
    },
    {
        "element_id": "0eedee4d975cadca7c341e27d64e1bf9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.3,
                        1708.6
                    ],
                    [
                        102.3,
                        2056.1
                    ],
                    [
                        1550.5,
                        2056.1
                    ],
                    [
                        1550.5,
                        1708.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92426,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-6-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "a94b9fb9cc454cd7f49954fc68427e60",
            "text_as_html": "<table><thead><tr><th rowspan=\"2\">Parameters</th><th colspan=\"9\">Algorithms/Methods</th><th rowspan=\"2\">Reinforce</th></tr><tr><th>DEP</th><th>A3C</th><th>A2C</th><th>A2CISTM</th><th>(O DON</th><th>DRON</th><th>DuelingDQN</th><th>DoubleDQN</th><th>(o C51 DDQN</th></tr></thead><tbody><tr><td>Discount Factor (y)</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td></tr><tr><td>Learning Rate (0)</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.001</td><td>0.0001</td><td>0.0001</td></tr><tr><td>Experience Replay Memory</td><td>50,000</td><td>60,000</td><td>50,000</td><td>50,000</td><td>40,000</td><td>50,000</td><td>50,000</td><td>50,000</td><td>50,000</td><td>50,000</td></tr><tr><td>Screen Buffer</td><td>640, 480</td><td>640,</td><td>480-640,</td><td>480-640, 480</td><td>640, 480</td><td>640,</td><td>480-640, 480</td><td>640, 480</td><td>640, 480</td><td>640, 480</td></tr><tr><td>Batch Size</td><td>32</td><td>32</td><td>32</td><td>32</td><td>64</td><td>32</td><td>32</td><td>32</td><td>32</td><td>32</td></tr><tr><td>Initial Decay</td><td>1.0</td><td>0.9</td><td>1.0</td><td>0.9</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>Final Decay</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td><td>0.0001</td></tr><tr><td>History length</td><td>4</td><td>4</td><td>4</td><td>4</td><td></td><td>6</td><td>4</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Frames per Action</td><td>4</td><td>4</td><td>4</td><td>4</td><td>5</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Time Elapsed (Hours)</td><td>12</td><td>12</td><td>16</td><td>20</td><td>170</td><td>12</td><td>12</td><td>12</td><td>22</td><td>19</td></tr></tbody></table>"
        },
        "text": "Parameters Algorithms/Methods DFP A3C A2C A2C-LSTM Discount Factor (\u03b3) 0.99 0.99 0.99 0.99 Learning Rate (\u03b1) 0.0001 0.0001 0.0001 0.0001 Experience Replay Memory 50,000 60,000 50,000 50,000 Screen Buffer 640, 480 640, 480 640, 480 640, 480 Batch Size 32 32 32 32 Initial Decay 1.0 0.9 1.0 0.9 Final Decay 0.0001 0.0001 0.0001 0.0001 History length 4 4 4 4 Frames per Action 4 4 4 4 Time Elapsed (Hours) 12 12 16 20 Reinforce DQN DRQN DuelingDQN Double DQN C51_ DDQN 0.99 0.99 0.99 0.99 0.99 0.99 0.0001 0.0001 0.0001 0.001 0.0001 0.0001 40,000 50,000 50,000 50,000 50,000 50,000 640, 480 640, 480 640, 480 640, 480 640, 480 640, 480 64 32 32 32 32 32 1.0 1.0 1.0 1.0 1.0 1.0 0.001 0.0001 0.0001 0.0001 0.0001 0.0001 6 4 4 4 4 5 4 4 4 4 4 170 12 12 12 22 19",
        "type": "Table"
    },
    {
        "element_id": "ff5368f9a51ae50255eaf6057a7c6023",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 6,
            "parent_id": "a94b9fb9cc454cd7f49954fc68427e60"
        },
        "text": "6",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d66ba6615b72242979ebf4b9c6641a60",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.0,
                        103.3
                    ],
                    [
                        101.0,
                        124.3
                    ],
                    [
                        217.1,
                        124.3
                    ],
                    [
                        217.1,
                        103.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62753,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "a68665efe31156aad61c00342c9ad317",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.6,
                        102.9
                    ],
                    [
                        1208.6,
                        120.6
                    ],
                    [
                        1549.4,
                        120.6
                    ],
                    [
                        1549.4,
                        102.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79836,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "2fbc3bc586071a1500284906441a5db2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7,
            "parent_id": "a68665efe31156aad61c00342c9ad317"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "c88464898cc833d3532361e3d257108b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        336.5,
                        158.3
                    ],
                    [
                        336.5,
                        935.1
                    ],
                    [
                        1316.9,
                        935.1
                    ],
                    [
                        1316.9,
                        158.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-8.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Agent's Health on Health Gathering Scenario using DFP 100 95 85 80 75 70 65 601 Health 8 \u2014 or 7 30000 of 10000 20000 T 7 40000 50000 60000 Steps 70000",
        "type": "Image"
    },
    {
        "element_id": "57bbcd32c3269e523bfa8f29aef1ce0a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        606.8,
                        960.9
                    ],
                    [
                        606.8,
                        980.9
                    ],
                    [
                        1046.8,
                        980.9
                    ],
                    [
                        1046.8,
                        960.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Fig. 5. The health of the agent trained with DFP.",
        "type": "NarrativeText"
    },
    {
        "element_id": "004ccf91a3b233f8bcbecf69d623c61d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1026.8
                    ],
                    [
                        104.4,
                        1048.9
                    ],
                    [
                        249.4,
                        1048.9
                    ],
                    [
                        249.4,
                        1026.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "agents or bots.",
        "type": "Title"
    },
    {
        "element_id": "59100ed79c1de3295314fef8cc88cd9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1094.1
                    ],
                    [
                        104.4,
                        1142.9
                    ],
                    [
                        807.7,
                        1142.9
                    ],
                    [
                        807.7,
                        1094.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70437,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "6. Asynchronous advantage Actor-Critic (A3C) and deep recurrent",
        "type": "Title"
    },
    {
        "element_id": "8f410af96c8a6f6f34b3b4854084dc59",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1127.3
                    ],
                    [
                        104.4,
                        1149.4
                    ],
                    [
                        308.8,
                        1149.4
                    ],
                    [
                        308.8,
                        1127.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Q-Network (DRQN)",
        "type": "Title"
    },
    {
        "element_id": "cad189ea81229fa14f76a4ee4d9c24ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1180.4
                    ],
                    [
                        104.4,
                        1288.1
                    ],
                    [
                        801.9,
                        1288.1
                    ],
                    [
                        801.9,
                        1180.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93889,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7,
            "parent_id": "8f410af96c8a6f6f34b3b4854084dc59"
        },
        "text": "The advantage of A3C over DRQN is that it is more resource-effi- cient, since A3C can be run on multiple cores of a single machine, and does not require a large amount of RAM to store the replay buffer",
        "type": "NarrativeText"
    },
    {
        "element_id": "65d31add2989ea5bf9a401bdd867adac",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1272.5
                    ],
                    [
                        104.4,
                        1294.6
                    ],
                    [
                        801.8,
                        1294.6
                    ],
                    [
                        801.8,
                        1272.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7,
            "parent_id": "8f410af96c8a6f6f34b3b4854084dc59"
        },
        "text": "compared to DRQN which requires a large amount of replay buffer. The",
        "type": "NarrativeText"
    },
    {
        "element_id": "127575e5c8fbe61c1037f58eed6b9d8b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        849.3,
                        1023.3
                    ],
                    [
                        849.3,
                        1281.4
                    ],
                    [
                        1552.0,
                        1281.4
                    ],
                    [
                        1552.0,
                        1023.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95298,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7,
            "parent_id": "8f410af96c8a6f6f34b3b4854084dc59"
        },
        "text": "actor-critic aspect provides more accurate updates to the policy than a DQN update might provide. A3C is an on-policy algorithm that cannot explore the state-space as efficiently as DQN, so there are some trade- offs between the two algorithms. However, according to our experi- ments conducted on the health gathering scenario of the VizDoom platform A3C performs higher than DRQN as can be seen in Fig. 9. The DQN performs well than DRQN on fully observable environments (FOE) such as health gathering scenario shown in Fig. 11, however, it per- forms low on partially observable environments (POE) where for",
        "type": "NarrativeText"
    },
    {
        "element_id": "d9af42d920e6c812fd73d0fc5369a757",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        334.2,
                        1320.6
                    ],
                    [
                        334.2,
                        2023.8
                    ],
                    [
                        1319.4,
                        2023.8
                    ],
                    [
                        1319.4,
                        1320.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-9.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Global Network Worker n t t",
        "type": "Image"
    },
    {
        "element_id": "b2b65c63650c07c832379b2ebbddca40",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        593.7,
                        2043.4
                    ],
                    [
                        593.7,
                        2064.4
                    ],
                    [
                        1054.5,
                        2064.4
                    ],
                    [
                        1054.5,
                        2043.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79377,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Fig. 6. Diagram of A3C high-level architecture [47].",
        "type": "FigureCaption"
    },
    {
        "element_id": "6d891919c3454aa0e0b75df32b4346cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        592.6,
                        2049.5
                    ],
                    [
                        592.6,
                        2069.4
                    ],
                    [
                        1060.9,
                        2069.4
                    ],
                    [
                        1060.9,
                        2049.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "Fig. 6. Diagram of A3C high-level architecture [47].",
        "type": "NarrativeText"
    },
    {
        "element_id": "429fa1dc604470355bc62a77b8e3e33f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 7
        },
        "text": "7",
        "type": "UncategorizedText"
    },
    {
        "element_id": "64cfcae82ceb4f5b90203fbdb709dcd9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.3,
                        102.9
                    ],
                    [
                        101.3,
                        124.3
                    ],
                    [
                        215.7,
                        124.3
                    ],
                    [
                        215.7,
                        102.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56214,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "A. Khan, et al.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0889698e2633c087b29a89a060968ca4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.5,
                        102.6
                    ],
                    [
                        1208.5,
                        120.7
                    ],
                    [
                        1550.1,
                        120.7
                    ],
                    [
                        1550.1,
                        102.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73536,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "d4aa6ed2fd2039c7a2e2a663fe444283",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "0889698e2633c087b29a89a060968ca4"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "246a88a8a156ee41a45bd2b6f615c77b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        198.9,
                        158.4
                    ],
                    [
                        198.9,
                        1125.2
                    ],
                    [
                        1454.5,
                        1125.2
                    ],
                    [
                        1454.5,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-8-10.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "Health of Agent trained on Health of Agent trained on m Health Gathering Scenario using DFP m Health Gathering Scenario using A3C 95 95 90 \u201d 85 85 80 80 B B 10 10 65 65 60 60 g 5 5 551 n 50 a 50 i 45 i 45 40 4 35 3 30 30 5 5 20 20 5 5 10 10 5 \u2014 DP 5 \u2014 ABC 0-\u2014 T T T T T T 0-\u2014 T T T T T - 0 10000 20000 30000 40000 50000 60000 70000 Steps T i\u011f 0 100000 200000 300000 400000 500000 600000 700000 Steps",
        "type": "Image"
    },
    {
        "element_id": "c9e069856d0ef1b93aa825708339bca6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        285.2,
                        1150.9
                    ],
                    [
                        285.2,
                        1170.8
                    ],
                    [
                        1368.4,
                        1170.8
                    ],
                    [
                        1368.4,
                        1150.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "Fig. 7. Comparison of the performance of DFP with A3C on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "89a52df92838de5933f9656c81d4510a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1211.5
                    ],
                    [
                        104.4,
                        1355.1
                    ],
                    [
                        801.9,
                        1355.1
                    ],
                    [
                        801.9,
                        1211.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91619,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "overcoming this issue the recurrent feature (LSTM) of DRQN was in- troduced which makes it efficient over DQN by exploiting the experi- ences or sequential updates from memory however this feature does not make DRQN efficient over A3C algorithm and yet its performance is lower than A3C in many game environments.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3723ba84d7024b9b4836b47fbd7750be",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1357.3
                    ],
                    [
                        104.4,
                        1552.2
                    ],
                    [
                        803.8,
                        1552.2
                    ],
                    [
                        803.8,
                        1357.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95336,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "The left graph in Fig. 9 shows the performance of A3C which was not impressive initially until around 375,000 steps. however, Later the per- formance improved but it took significantly longer to achieve ~90% re- sults while on the other hand, the right graph shows the steady perfor- mance of DRQN on the health gathering scenario which remains almost steady and uniform. DRQN never achieved results to a great extent and its",
        "type": "NarrativeText"
    },
    {
        "element_id": "25bb775263d2995da99ce87df05e0e20",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1536.4
                    ],
                    [
                        104.4,
                        1558.5
                    ],
                    [
                        801.9,
                        1558.5
                    ],
                    [
                        801.9,
                        1536.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "performance crest and trough always remain between ~50% and ~51%.",
        "type": "NarrativeText"
    },
    {
        "element_id": "28a37f89c948e510ddb816c03101b9a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.6,
                        1617.0
                    ],
                    [
                        102.6,
                        1667.0
                    ],
                    [
                        795.3,
                        1667.0
                    ],
                    [
                        795.3,
                        1617.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86656,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "7. Direct future prediction (DFP) and deep recurrent Q-Network",
        "type": "Title"
    },
    {
        "element_id": "c5b88c566fc59bf758d46d710c59136e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1650.4
                    ],
                    [
                        104.4,
                        1672.6
                    ],
                    [
                        186.6,
                        1672.6
                    ],
                    [
                        186.6,
                        1650.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "28a37f89c948e510ddb816c03101b9a3"
        },
        "text": "(DRQN)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "5cba79d223824a24e3f9173b1b82306b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1216.7
                    ],
                    [
                        851.6,
                        1238.9
                    ],
                    [
                        1549.1,
                        1238.9
                    ],
                    [
                        1549.1,
                        1216.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "28a37f89c948e510ddb816c03101b9a3"
        },
        "text": "8. Deep Q-Network (DQN) and deep recurrent Q-Network (DRQN)",
        "type": "ListItem"
    },
    {
        "element_id": "3070c068c4c7f82e504ed1d70ba0c32f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1272.0
                    ],
                    [
                        851.6,
                        1377.7
                    ],
                    [
                        1549.1,
                        1377.7
                    ],
                    [
                        1549.1,
                        1272.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93244,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "28a37f89c948e510ddb816c03101b9a3"
        },
        "text": "In DQN, a single agent is represented by a single neural network that interacts with a single environment. Deep Q-Networks are more capable of overcoming unstable learning by mainly 4 techniques i.e. Experience",
        "type": "NarrativeText"
    },
    {
        "element_id": "e851a9ba2b3ed90e7372bf731bb8831e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1362.1
                    ],
                    [
                        851.6,
                        1384.2
                    ],
                    [
                        1546.5,
                        1384.2
                    ],
                    [
                        1546.5,
                        1362.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "Replay [49], Target Network, Clipping Rewards and Skipping Frames.",
        "type": "Title"
    },
    {
        "element_id": "26fef8e20dae9cb63fc8ef647be17b6f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1386.5
                    ],
                    [
                        851.6,
                        1697.9
                    ],
                    [
                        1549.4,
                        1697.9
                    ],
                    [
                        1549.4,
                        1386.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95536,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "e851a9ba2b3ed90e7372bf731bb8831e"
        },
        "text": "Experience reply was originally proposed in 1993 in [50] to overcome the problem of overfitting as Deep Neural Networks (DNN) easily overfits current episodes and once the DNN gets over fit then it becomes hard to produce different experiences. So, for solving this issue, experience replay stores experiences including state transitions, rewards, and actions, which are necessary data to perform Q learning, and makes mini-batches to update neural networks. While calculating the temporal difference (TD) error the target Q-function (Target network) gets changed frequently with DNN\u2019s as unstable target Q-function makes training difficult so target network technique fixes parameters of target Q-function Q (st+1, \u1fec) and",
        "type": "NarrativeText"
    },
    {
        "element_id": "357bae34a932a39c2e9c9b2155aa7763",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1681.8
                    ],
                    [
                        851.7,
                        1703.9
                    ],
                    [
                        1456.9,
                        1703.9
                    ],
                    [
                        1456.9,
                        1681.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "replaces them with the latest network every thousand of steps.",
        "type": "Title"
    },
    {
        "element_id": "500282f8e73296f50b1a59e88bc62320",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.8,
                        1702.8
                    ],
                    [
                        101.8,
                        2079.3
                    ],
                    [
                        804.8,
                        2079.3
                    ],
                    [
                        804.8,
                        1702.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95657,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8,
            "parent_id": "357bae34a932a39c2e9c9b2155aa7763"
        },
        "text": "In order to compare the performance of DFP with DRQN, two agents are trained on the VizDoom health gathering scenario using DFP and DRQN, then both the agents are tested on the health gathering scenario (maps) to see their performance difference where DFP outperformed DRQN as shown in Fig. 10. While gathering health packs the agent trained with DRQN was losing health on average and never manage to improve it to a high level. Thus its overall final performance remains average with ~50% to 51% as shown in the right-side graph. On the other hand, the agent trained with DFP performed very well in collecting health packs and the overall health of the agent achieved ~94% results which can be observed in the left graph for further consideration. It concludes that DFP is a better technique which is one among the state-of-the-art for training agents using the VizDoom Game-AI research platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "25a7bc7c831bae5c756d12f0019f8a13",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.9,
                        1722.0
                    ],
                    [
                        858.9,
                        1768.0
                    ],
                    [
                        1565.7,
                        1768.0
                    ],
                    [
                        1565.7,
                        1722.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78448,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "(10)",
        "type": "Formula"
    },
    {
        "element_id": "333284b386c5cebb52f6fa02df4d6521",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1791.5
                    ],
                    [
                        851.7,
                        1956.4
                    ],
                    [
                        1550.5,
                        1956.4
                    ],
                    [
                        1550.5,
                        1791.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95162,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "Each computer game has different score scales. For example, in Pong, players can get 1 point when winning the play. Otherwise, players get \u22121 point. However, in Space Invaders, players get 10\u201330 points when defeating invaders. This difference would make training unstable. Thus Clipping Rewards technique clips scores, which all po-",
        "type": "NarrativeText"
    },
    {
        "element_id": "59b97914420644de1d3472416f9d9ec1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1941.0
                    ],
                    [
                        851.7,
                        1963.1
                    ],
                    [
                        1476.4,
                        1963.1
                    ],
                    [
                        1476.4,
                        1941.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "sitive rewards are set +1 and all negative rewards are set \u22121.",
        "type": "NarrativeText"
    },
    {
        "element_id": "09340649145b84e72064ddd90b8c53be",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1966.4
                    ],
                    [
                        851.7,
                        2073.1
                    ],
                    [
                        1551.9,
                        2073.1
                    ],
                    [
                        1551.9,
                        1966.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9417,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "The fourth technique of skipping frames that DQN uses to overcome unstable learning can be explained excellently by referring or with the help of the arcade learning environment (ALE) [51] which is capable of",
        "type": "NarrativeText"
    },
    {
        "element_id": "acd838da154abf9b86bbdb54cfddc584",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        2057.2
                    ],
                    [
                        851.7,
                        2079.3
                    ],
                    [
                        1549.1,
                        2079.3
                    ],
                    [
                        1549.1,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "rendering 60 images per second. But actually, players don\u2019t take actions",
        "type": "NarrativeText"
    },
    {
        "element_id": "723bd42008f368528effec3e0065af5b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 8
        },
        "text": "8",
        "type": "UncategorizedText"
    },
    {
        "element_id": "b3a34b7b0aca50c8400744b41077af8e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.5,
                        103.1
                    ],
                    [
                        101.5,
                        124.3
                    ],
                    [
                        215.7,
                        124.3
                    ],
                    [
                        215.7,
                        103.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48568,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "A. Khan, et al.",
        "type": "NarrativeText"
    },
    {
        "element_id": "06b614fcaec06f759c103c8573caf55a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1209.2,
                        103.1
                    ],
                    [
                        1209.2,
                        120.6
                    ],
                    [
                        1547.6,
                        120.6
                    ],
                    [
                        1547.6,
                        103.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78602,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "5b5921068e51983d83ff473820ede388",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9,
            "parent_id": "06b614fcaec06f759c103c8573caf55a"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "da99be576632716e24a1b908ca59864d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        197.2,
                        158.3
                    ],
                    [
                        197.2,
                        1025.5
                    ],
                    [
                        1456.4,
                        1025.5
                    ],
                    [
                        1456.4,
                        158.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-9-11.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Health Agent Health on Health Gathering Scenario using a2c Agent Health on Health Gathering Scenario using a2c-LSTM 100 100 95 95 90 90 854 854 80 80 5 5 70 70 65 65 604 604 55 55 50 EY sa alli aid NE treet te 45 \u0130p 40 40 351 35 30 30 25 25 204 204 5 5 104 104 5 \u2014 ax 5 \u2014 adclsTm 0 2500 5000 7500 10000 12500 15000 17500 0 50000 100000 150000 200000 250000 300000 Steps steps",
        "type": "Image"
    },
    {
        "element_id": "16bd34291fdfad2a9fd82d98e4e2f7a5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        256.4,
                        1051.3
                    ],
                    [
                        256.4,
                        1071.3
                    ],
                    [
                        1397.2,
                        1071.3
                    ],
                    [
                        1397.2,
                        1051.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Fig. 8. Comparison of the performance of A2C with A2C-LSTM on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "211fb3988d62823949cde1a4b6b8b1f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.7,
                        1113.4
                    ],
                    [
                        101.7,
                        1219.6
                    ],
                    [
                        801.9,
                        1219.6
                    ],
                    [
                        801.9,
                        1113.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93318,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "thus much in a second. AI doesn\u2019t need to calculate Q-values every frame. So by employing skipping frames technique DQN calculates Q- values every 4 frames and uses past 4 frames as inputs. This reduces the",
        "type": "NarrativeText"
    },
    {
        "element_id": "0b136e03e04f10b92cfc529ca2ee274a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1204.3
                    ],
                    [
                        104.4,
                        1226.4
                    ],
                    [
                        600.1,
                        1226.4
                    ],
                    [
                        600.1,
                        1204.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "computational cost and gathers more experiences.",
        "type": "Title"
    },
    {
        "element_id": "3b2e6a46ce9848f2190fc2fd35d07655",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.2,
                        1229.7
                    ],
                    [
                        101.2,
                        1277.9
                    ],
                    [
                        801.9,
                        1277.9
                    ],
                    [
                        801.9,
                        1229.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91882,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9,
            "parent_id": "0b136e03e04f10b92cfc529ca2ee274a"
        },
        "text": "Using these four techniques enables DQN to achieve stable training.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ef2339eaf371690e4d1f126e5c5d2958",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1262.4
                    ],
                    [
                        104.4,
                        1284.5
                    ],
                    [
                        801.8,
                        1284.5
                    ],
                    [
                        801.8,
                        1262.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9,
            "parent_id": "0b136e03e04f10b92cfc529ca2ee274a"
        },
        "text": "Table 3 shows that the performance of DQN increases if it uses",
        "type": "NarrativeText"
    },
    {
        "element_id": "fc33c1ec38073f74db94f85bcb024f76",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1117.2
                    ],
                    [
                        851.6,
                        1139.3
                    ],
                    [
                        1457.2,
                        1139.3
                    ],
                    [
                        1457.2,
                        1117.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Experience Replay technique along with the Target Network.",
        "type": "Title"
    },
    {
        "element_id": "4ecebe4433d01d22aff9c360e1958a1a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        848.6,
                        1139.7
                    ],
                    [
                        848.6,
                        1284.5
                    ],
                    [
                        1549.1,
                        1284.5
                    ],
                    [
                        1549.1,
                        1139.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9392,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9,
            "parent_id": "fc33c1ec38073f74db94f85bcb024f76"
        },
        "text": "DQN and DRQN are the variants of Deep Q-learning introduced by Google DeepMind, London, the UK in 2013 and 2015 that performed with extraordinary results on Atari games and later were applied to different platforms. In this paper, we also applied them using the VizDoom AI platform to compare and differentiate their performance on the health",
        "type": "NarrativeText"
    },
    {
        "element_id": "9b630460fe654f834b79655f7b0a4c96",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        196.7,
                        1323.8
                    ],
                    [
                        196.7,
                        2023.8
                    ],
                    [
                        1456.7,
                        2023.8
                    ],
                    [
                        1456.7,
                        1323.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-9-12.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Health Agent Health on Health Gathering Scenario using A3C Agent Health on Health Gathering Scenario using DRQN 100 100 95 954 90 4 90 85 85 80 80 75 75 70 70 4 654 65 60 60 4 55 5 s0 \u015e\u0130 a rr 45 4 E45 40 4 40 35 35 30 4 30 25 25 20 20 15 15 10 10 51 \u2014 asc 5 \u2014 DRON o o T i o 100000 200000 300000 400000 500000 600000 700000 Steps o 2000 4000 6000 8000 10000 12000 14000 Steps",
        "type": "Image"
    },
    {
        "element_id": "06d39f72e643ab4856019f73e10c8388",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        274.3,
                        2044.5
                    ],
                    [
                        274.3,
                        2064.1
                    ],
                    [
                        1382.2,
                        2064.1
                    ],
                    [
                        1382.2,
                        2044.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86418,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Fig. 9. Comparison of the performance of A3C with DRQN on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "FigureCaption"
    },
    {
        "element_id": "e3727fb4bbc2788874c409f5c900bdba",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        275.3,
                        2049.5
                    ],
                    [
                        275.3,
                        2069.4
                    ],
                    [
                        1378.1,
                        2069.4
                    ],
                    [
                        1378.1,
                        2049.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "Fig. 9. Comparison of the performance of A3C with DRQN on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6023d0e715cac19f2e30b97100990fa7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        821.8,
                        2119.5
                    ],
                    [
                        821.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2137.2
                    ],
                    [
                        831.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 9
        },
        "text": "9",
        "type": "UncategorizedText"
    },
    {
        "element_id": "cebb812981a5a0d1a71e17e78fa3a662",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.5,
                        103.0
                    ],
                    [
                        100.5,
                        124.3
                    ],
                    [
                        216.7,
                        124.3
                    ],
                    [
                        216.7,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48644,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "22ad79a199afbc2db40fd6dcdcbb26a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1209.5,
                        103.1
                    ],
                    [
                        1209.5,
                        120.6
                    ],
                    [
                        1549.7,
                        120.6
                    ],
                    [
                        1549.7,
                        103.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77112,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "715fb28bc4afa56c1caa6e019b8358de",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "22ad79a199afbc2db40fd6dcdcbb26a4"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "4b58155fce0fd8f39b54f46517daf6e7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        197.2,
                        158.4
                    ],
                    [
                        197.2,
                        1051.2
                    ],
                    [
                        1456.4,
                        1051.2
                    ],
                    [
                        1456.4,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-10-13.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Agent Health on Health Gathering 100 Scenario using DFP 95 4 85 804 15 70 651 604 55 Health Ss 45 35 254 20 51 10 51 \u2014 o 0 10000 20000 30000 40000 50000 60000 70000 Steps Agent Health on Health Gathering Scenario using DRQN 100 95 4 85 804 75 70 65 60 55 Health 3 45 35 254 20 5 10 5 \u2014 DRON 0 2000 4000 6000 8000 10000 12000 14000 steps",
        "type": "Image"
    },
    {
        "element_id": "d9b8f5695bd476413d885403734faeef",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        299.1,
                        1076.9
                    ],
                    [
                        299.1,
                        1096.8
                    ],
                    [
                        1354.5,
                        1096.8
                    ],
                    [
                        1354.5,
                        1076.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Fig. 10. Performance Comparison of DFP with DRQN on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8faefcc39fb5ba6ac95da98d6e252681",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1141.0
                    ],
                    [
                        104.4,
                        1166.5
                    ],
                    [
                        178.6,
                        1166.5
                    ],
                    [
                        178.6,
                        1141.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70916,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Table 3",
        "type": "Title"
    },
    {
        "element_id": "801b0a9bc5f9f7477aefe20956f384d1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1169.0
                    ],
                    [
                        104.4,
                        1212.2
                    ],
                    [
                        801.8,
                        1212.2
                    ],
                    [
                        801.8,
                        1169.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79421,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "8faefcc39fb5ba6ac95da98d6e252681"
        },
        "text": "DQN Performance with and without Experience Replay and Target Network",
        "type": "FigureCaption"
    },
    {
        "element_id": "f8bf918ed0788b76a3dc5a114240fcd1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1199.8
                    ],
                    [
                        104.4,
                        1219.8
                    ],
                    [
                        143.7,
                        1219.8
                    ],
                    [
                        143.7,
                        1199.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "8faefcc39fb5ba6ac95da98d6e252681"
        },
        "text": "[52]",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4b8ae5052bbd6997cc256a3c256dd5ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.2,
                        1219.3
                    ],
                    [
                        108.2,
                        1385.4
                    ],
                    [
                        795.5,
                        1385.4
                    ],
                    [
                        795.5,
                        1219.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91606,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-10-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "8faefcc39fb5ba6ac95da98d6e252681",
            "text_as_html": "<table><tbody><tr><td>Replay</td><td></td><td></td><td></td><td></td></tr><tr><td>Target</td><td></td><td></td><td></td><td></td></tr><tr><td>Breakout</td><td>316.8</td><td>240.7</td><td>10.2</td><td>3.2</td></tr><tr><td>River Raid</td><td>7446.6</td><td>4102.8</td><td>2867.7</td><td>1453.0</td></tr><tr><td>Sequest</td><td>2894.4</td><td>822.6</td><td>1003.0</td><td>275.8</td></tr><tr><td>Space Invaders</td><td>1088.9</td><td>826.3</td><td>373.2</td><td>302.0</td></tr></tbody></table>"
        },
        "text": "Replay \u2713 \u2713 \u2717 \u2717 Target \u2713 \u2717 \u2713 \u2717 Breakout 316.8 240.7 10.2 3.2 River Raid Sequest Space Invaders 7446.6 2894.4 1088.9 4102.8 822.6 826.3 2867.7 1003.0 373.2 1453.0 275.8 302.0",
        "type": "Table"
    },
    {
        "element_id": "e7fbb9672b0d9edeeca31f9fffa9dc41",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.1,
                        1433.8
                    ],
                    [
                        103.1,
                        1657.5
                    ],
                    [
                        802.2,
                        1657.5
                    ],
                    [
                        802.2,
                        1433.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95328,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "8faefcc39fb5ba6ac95da98d6e252681"
        },
        "text": "gathering scenario (map) by training the agents. The agent trained with DQN performed slightly better than the DRQN in gathering the health packs due to the fully observable environment and retained its health to ~53%. while on the other hand, the agent trained with DRQN remain below in performance in collecting health packs on the health gathering scenario (maps) by retaining its health to almost 51% as shown in Fig. 11. It concludes that the agents trained with DQN perform better than DRQN",
        "type": "NarrativeText"
    },
    {
        "element_id": "d6729b970a0fd738251c859f0b77e858",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1641.4
                    ],
                    [
                        104.4,
                        1663.6
                    ],
                    [
                        801.9,
                        1663.6
                    ],
                    [
                        801.9,
                        1641.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "particularly on the health gathering scenario of the VizDoom AI platform.",
        "type": "Title"
    },
    {
        "element_id": "364aa41f7b200eec405806c6b49878e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1704.7
                    ],
                    [
                        104.4,
                        1752.0
                    ],
                    [
                        794.2,
                        1752.0
                    ],
                    [
                        794.2,
                        1704.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86733,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "9. Dueling deep Q-Network (DDQN) and double deep Q-Network",
        "type": "Title"
    },
    {
        "element_id": "27b352227ae958825228a9795632935d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1737.5
                    ],
                    [
                        104.4,
                        1759.6
                    ],
                    [
                        187.0,
                        1759.6
                    ],
                    [
                        187.0,
                        1737.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "364aa41f7b200eec405806c6b49878e6"
        },
        "text": "(DDQN)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "98d9adb251867b42992e026b836af321",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        1138.2
                    ],
                    [
                        851.6,
                        1484.5
                    ],
                    [
                        1550.8,
                        1484.5
                    ],
                    [
                        1550.8,
                        1138.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9545,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "364aa41f7b200eec405806c6b49878e6"
        },
        "text": "actions to take in a given state. While this would be fine if all actions were always overestimated equally, but this never finds to be the case because if certain suboptimal actions regularly were given higher Q- values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to fix this issue, a simple trick was proposed: instead of taking the max over Q-values when computing the Target Q-value for training steps, the primary network is used to choose an action, and the target network is used to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, it was able to substantially reduce the overestimation, and train faster and more reliably. The new double-DQN equation for up- dating the target value could be represented as,",
        "type": "NarrativeText"
    },
    {
        "element_id": "d8994c63eb6c2598b51549934f49f452",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.2,
                        1498.3
                    ],
                    [
                        850.2,
                        1520.8
                    ],
                    [
                        1555.5,
                        1520.8
                    ],
                    [
                        1555.5,
                        1498.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64328,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Q- Target = r+ y Q(s\u2019, argmax(Q(s\u2019, a, @)), 6\u2019) ay",
        "type": "Formula"
    },
    {
        "element_id": "226a261da588a5189a8883758468cee3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.2,
                        1545.6
                    ],
                    [
                        858.2,
                        1595.1
                    ],
                    [
                        1551.9,
                        1595.1
                    ],
                    [
                        1551.9,
                        1545.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91797,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "And, the advantage is calculated separately and then combined only",
        "type": "NarrativeText"
    },
    {
        "element_id": "0bf3822284e0be63cbcf614196518eee",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1579.5
                    ],
                    [
                        851.7,
                        1601.7
                    ],
                    [
                        1222.9,
                        1601.7
                    ],
                    [
                        1222.9,
                        1579.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "at the final layer into a Q-value [17].",
        "type": "Title"
    },
    {
        "element_id": "528698399896d37fa70e90b7ab72f01b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1604.0
                    ],
                    [
                        851.7,
                        1712.0
                    ],
                    [
                        1549.6,
                        1712.0
                    ],
                    [
                        1549.6,
                        1604.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93819,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10,
            "parent_id": "0bf3822284e0be63cbcf614196518eee"
        },
        "text": "The reason behind the change in the architecture that dueling-DQN makes is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only",
        "type": "NarrativeText"
    },
    {
        "element_id": "e52c3f49171984e4e7ec2726d4784629",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1695.8
                    ],
                    [
                        851.7,
                        1717.9
                    ],
                    [
                        1023.3,
                        1717.9
                    ],
                    [
                        1023.3,
                        1695.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "at the final layer.",
        "type": "Title"
    },
    {
        "element_id": "2c159622f4b06dcd3ab24fccd210f8cc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        859.8,
                        1732.5
                    ],
                    [
                        859.8,
                        1760.0
                    ],
                    [
                        1559.8,
                        1760.0
                    ],
                    [
                        1559.8,
                        1732.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.60072,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "(12)",
        "type": "Formula"
    },
    {
        "element_id": "e18863236c19887290ac67b31bc7a16f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1507.4,
                        1506.2
                    ],
                    [
                        1507.4,
                        1528.3
                    ],
                    [
                        1549.0,
                        1528.3
                    ],
                    [
                        1549.0,
                        1506.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "(11)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f8dde3ac6d9924774773a0a15b1e7e9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.4,
                        1791.2
                    ],
                    [
                        103.4,
                        2043.2
                    ],
                    [
                        804.5,
                        2043.2
                    ],
                    [
                        804.5,
                        1791.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95669,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "Dueling-DQN (DDQN) and double-DQN (DDQN) are two simple additional improvements to the DQN architecture or, in other words, these are close newer variants of DQN introduced by Google DeepMind, London, UK, in 2015-16 [17,53]. The dueling-DQN and double-DQN allow for improved performance, stability, and faster training time. The double-DQN basically uses 2 neural networks to perform the Bellman iteration, one for generating the prediction term and the other for generating the target term. It helps to alleviate the bias introduced by",
        "type": "NarrativeText"
    },
    {
        "element_id": "0ec0beede4ce873c54573ae2b26f8d3b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2028.2
                    ],
                    [
                        104.4,
                        2050.3
                    ],
                    [
                        752.3,
                        2050.3
                    ],
                    [
                        752.3,
                        2028.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "the inaccuracies of Q-network at the beginning phase of training.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8aa556fc0d9303b7facffe27d24b5b59",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        139.1,
                        2057.2
                    ],
                    [
                        139.1,
                        2079.3
                    ],
                    [
                        801.9,
                        2079.3
                    ],
                    [
                        801.9,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "The regular DQN often overestimates the Q-values of the potential",
        "type": "NarrativeText"
    },
    {
        "element_id": "77eddf484bb621e1f9e0e5947f7d4c48",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1777.0
                    ],
                    [
                        851.7,
                        1855.8
                    ],
                    [
                        1549.1,
                        1855.8
                    ],
                    [
                        1549.1,
                        1777.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93258,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "In dueling DQN, Q can also be computed by using the below formula with value function V and a state-dependent action advantage function",
        "type": "NarrativeText"
    },
    {
        "element_id": "028de76ff218c6a3324c01d86084ff5d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1840.3
                    ],
                    [
                        851.7,
                        1862.5
                    ],
                    [
                        1080.1,
                        1862.5
                    ],
                    [
                        1080.1,
                        1840.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "A, as shown in Fig. 12.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a0c044bb14e1c6e759b816ca98ffd06",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.5,
                        1879.1
                    ],
                    [
                        855.5,
                        1940.0
                    ],
                    [
                        1259.8,
                        1940.0
                    ],
                    [
                        1259.8,
                        1879.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82551,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "al 1 S, a) = v(s) + A(s, a) \u2014 \u2014 A(s,a Qs, a) = vs) + AG. a) \u2014 77D AG. a)",
        "type": "Formula"
    },
    {
        "element_id": "c790f8cc2c5ceab1cda3fc76e7d3cfeb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1965.2
                    ],
                    [
                        851.7,
                        2073.2
                    ],
                    [
                        1549.1,
                        2073.2
                    ],
                    [
                        1549.1,
                        1965.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94337,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "So, dueling-DQN and double-DQN are the two variants of DQN that performed well on Atari 2600 domain. In this paper, we use them particularly using the VizDoom AI platform to compare and differ-",
        "type": "NarrativeText"
    },
    {
        "element_id": "7a8531c4cf876f9f73ccabfb04d4da0d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        2057.2
                    ],
                    [
                        851.7,
                        2079.3
                    ],
                    [
                        1549.1,
                        2079.3
                    ],
                    [
                        1549.1,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "entiate their performance on the health gathering scenario (map) by",
        "type": "NarrativeText"
    },
    {
        "element_id": "52a8eb5efe545332e8ba327aa742cce3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        817.4,
                        2112.0
                    ],
                    [
                        817.4,
                        2130.8
                    ],
                    [
                        837.1,
                        2130.8
                    ],
                    [
                        837.1,
                        2112.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80216,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "10",
        "type": "Footer"
    },
    {
        "element_id": "e38687413b64eaadf4cc268ddd5b65cf",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 10
        },
        "text": "10",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4640f79c770bda81d1414aa72050850f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.6,
                        103.1
                    ],
                    [
                        101.6,
                        124.3
                    ],
                    [
                        215.5,
                        124.3
                    ],
                    [
                        215.5,
                        103.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.46918,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "A. Khan, et al.",
        "type": "NarrativeText"
    },
    {
        "element_id": "236c81a63f3184f7ba758ec2de548df4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1209.0,
                        103.0
                    ],
                    [
                        1209.0,
                        120.6
                    ],
                    [
                        1547.9,
                        120.6
                    ],
                    [
                        1547.9,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73924,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "f6ce291bfa777e5aa6094a2dd3b4f012",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "236c81a63f3184f7ba758ec2de548df4"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "ae1dcd7a84dbebf636d80783e682bbfc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        196.7,
                        158.4
                    ],
                    [
                        196.7,
                        1155.6
                    ],
                    [
                        1456.7,
                        1155.6
                    ],
                    [
                        1456.7,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-11-14.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "Agent Health on Health Gathering Scenario using DON Health \u2014 DON 15000 20000 25000 Steps 0 5000 10000 Agent Health on Health Gathering Scenario using DRON 100 96 4 Health 4 \u2014 DON 6000 8000 10000 12000 14000 Steps 0 2000 4000",
        "type": "Image"
    },
    {
        "element_id": "d0aab5605da4d8d972f9c43f0218df48",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        266.6,
                        1181.3
                    ],
                    [
                        266.6,
                        1201.2
                    ],
                    [
                        1387.0,
                        1201.2
                    ],
                    [
                        1387.0,
                        1181.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "Fig. 11. Comparison of the performance of DQN with DRQN on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e0d95034fa8573bef9ff86e2f13840d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        126.9,
                        1237.6
                    ],
                    [
                        126.9,
                        1592.4
                    ],
                    [
                        779.3,
                        1592.4
                    ],
                    [
                        779.3,
                        1237.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-11-15.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "e pH 7 yo Vs) A(s,a) Dueling DQN Q(s,a)",
        "type": "Image"
    },
    {
        "element_id": "786be19676c590f048d8a65d33f079f6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.1,
                        1612.9
                    ],
                    [
                        101.1,
                        1664.8
                    ],
                    [
                        801.9,
                        1664.8
                    ],
                    [
                        801.9,
                        1612.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91749,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "Fig. 12. Above: Regular DQN with a single stream for Q-values. Below: Dueling DQN where the value.",
        "type": "FigureCaption"
    },
    {
        "element_id": "173ec1d98822c9e1d38a05a3dc6bc8ef",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.1,
                        1704.5
                    ],
                    [
                        104.1,
                        1901.6
                    ],
                    [
                        808.6,
                        1901.6
                    ],
                    [
                        808.6,
                        1704.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95601,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "training two agents. The agent trained with dueling-DQN performed a lot better than the double-DQN in gathering the health packs and re- tained its health to ~59%. On the other hand, the agent trained with double-DQN remain below in performance by collecting health packs on the map and its health retains to ~52% as shown in Fig. 13. It concludes that the agents trained with dueling-DQN perform better",
        "type": "NarrativeText"
    },
    {
        "element_id": "7e56b6e37bc93989527d79b85925483d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1885.1
                    ],
                    [
                        104.4,
                        1907.2
                    ],
                    [
                        514.1,
                        1907.2
                    ],
                    [
                        514.1,
                        1885.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "particularly on the VizDoom AI platform.",
        "type": "Title"
    },
    {
        "element_id": "3c596f360e92bb374d08de447a181280",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        849.7,
                        1241.9
                    ],
                    [
                        849.7,
                        1438.4
                    ],
                    [
                        1549.1,
                        1438.4
                    ],
                    [
                        1549.1,
                        1241.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9336,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "7e56b6e37bc93989527d79b85925483d"
        },
        "text": "equation. The number 51 represents the use of 51 discrete values to parameterize the value distribution Z. During each update step, a transition is sampled from the environment and the target distribution is computed. is used to update the current distribution Z by minimizing the cross entropy loss between Z and . The pseudo code of the C51 Algorithm is as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4aefa7f6369b86c3f8361ad7ff8ccddb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        867.0,
                        1488.7
                    ],
                    [
                        867.0,
                        1512.7
                    ],
                    [
                        1200.0,
                        1512.7
                    ],
                    [
                        1200.0,
                        1488.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.31642,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "Pseudo code of the C51 Algorithm [54]",
        "type": "Title"
    },
    {
        "element_id": "051568e2fb7f8963a0621523db472a42",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        864.4,
                        1526.6
                    ],
                    [
                        864.4,
                        1933.4
                    ],
                    [
                        1317.9,
                        1933.4
                    ],
                    [
                        1317.9,
                        1526.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81696,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "4aefa7f6369b86c3f8361ad7ff8ccddb"
        },
        "text": "Input a transition , ( ) Q( ) = 0, 0, \u2026.., N \u2212 1 For ,\u2026\u2026, N \u2212 1 do # Compute the projection of onto the support { } # Distribute probability of + ( + ( End for Output \u2014",
        "type": "NarrativeText"
    },
    {
        "element_id": "66477f305a513346db91b1496a1c513c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1970.1
                    ],
                    [
                        104.4,
                        1992.2
                    ],
                    [
                        417.3,
                        1992.2
                    ],
                    [
                        417.3,
                        1970.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "4aefa7f6369b86c3f8361ad7ff8ccddb"
        },
        "text": "10. C51_DDQN and Reinforce",
        "type": "ListItem"
    },
    {
        "element_id": "557f979a7f7eabde6351e09ef38b4103",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.8,
                        2022.4
                    ],
                    [
                        106.8,
                        2073.0
                    ],
                    [
                        801.8,
                        2073.0
                    ],
                    [
                        801.8,
                        2022.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9213,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "4aefa7f6369b86c3f8361ad7ff8ccddb"
        },
        "text": "C51 is a viable algorithm introduced in [18] to perform an iterative",
        "type": "NarrativeText"
    },
    {
        "element_id": "7dde2cfb705bf56a0f22cd803058f97f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2057.2
                    ],
                    [
                        104.4,
                        2079.3
                    ],
                    [
                        801.9,
                        2079.3
                    ],
                    [
                        801.9,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "4aefa7f6369b86c3f8361ad7ff8ccddb"
        },
        "text": "approximation of the value distribution Z using distributional Bellman",
        "type": "NarrativeText"
    },
    {
        "element_id": "a2ac878cb82399219d958b983db46448",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1954.6
                    ],
                    [
                        851.7,
                        2069.9
                    ],
                    [
                        1551.9,
                        2069.9
                    ],
                    [
                        1551.9,
                        1954.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93043,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11,
            "parent_id": "4aefa7f6369b86c3f8361ad7ff8ccddb"
        },
        "text": "Similar to DQN, we first use a deep neural network to represent the value distribution Z. Since the inputs are screen pixels, the first 3 layers are convolutional layers. The neural network outputs 3 sets of value distribution predictions, one for each action i.e. (Health, Health Packs,",
        "type": "NarrativeText"
    },
    {
        "element_id": "8dcaa6f8f91056673c3a6d76ee36d57a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        817.4,
                        2112.1
                    ],
                    [
                        817.4,
                        2131.3
                    ],
                    [
                        834.9,
                        2131.3
                    ],
                    [
                        834.9,
                        2112.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7398,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "11",
        "type": "Footer"
    },
    {
        "element_id": "fe54710c883bdebe759826f7f1c6b9dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 11
        },
        "text": "11",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4afc3b827fa218d85ed9264fd5df2617",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.5,
                        102.9
                    ],
                    [
                        101.5,
                        124.3
                    ],
                    [
                        215.6,
                        124.3
                    ],
                    [
                        215.6,
                        102.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48525,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "A. Khan, et al.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c8a07801f1967e96dae74716a5c78454",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.7,
                        102.9
                    ],
                    [
                        1208.7,
                        120.6
                    ],
                    [
                        1548.2,
                        120.6
                    ],
                    [
                        1548.2,
                        102.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76663,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "d0a8db48aeb0039724301894944a8bde",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "c8a07801f1967e96dae74716a5c78454"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "2e237c28a313a98138eae2d68a748b8d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        191.0,
                        158.3
                    ],
                    [
                        191.0,
                        1209.1
                    ],
                    [
                        1462.6,
                        1209.1
                    ],
                    [
                        1462.6,
                        158.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-12-16.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "Agent Health on Health Gathering Agent Health on Health Gathering Scenario using Dueling-DDQN Scenario using DDQN 100 100 95 951 90 90 4 85 854 80 801 B\u0130 D4 70 104 65 654 60 601 55 551 s s \u2014\u2014\u2014 \u2014\u2014 5 5 5 50 5 501 I ft 45 454 40 401 354 354 30 301 25 25 20 201 5 5 10 101 \u2014 Dueling DDON 7 \u2014 DDN 0-4 oz 7 7 T 7 7 7 0 20000 40000 60000 80000 100000 120000 Steps 7 T 7 7 7 : lu 0 5000 10000 15000 20000 25000 30000 35000 Steps",
        "type": "Image"
    },
    {
        "element_id": "0e81f13da4473fc74993d26b3f7f6b7d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        286.1,
                        1234.8
                    ],
                    [
                        286.1,
                        1254.7
                    ],
                    [
                        1367.4,
                        1254.7
                    ],
                    [
                        1367.4,
                        1234.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "Fig. 13. Performance of Dueling-DQN with Double-DQN on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "34a3ee59d88351047f2ad1bc98542792",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1296.5
                    ],
                    [
                        104.4,
                        1642.5
                    ],
                    [
                        804.2,
                        1642.5
                    ],
                    [
                        804.2,
                        1296.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95522,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "Poison). Each set of prediction is a softmax layer with 51 units and the number of atoms is the number of discrete values (i.e. 51). The main logic of the algorithm is contained in the update step. First, a minibatch of sample trajectories is sampled from the Experience Replay buffer and the corresponding states, reward, and targets variables are initialized. The variable \u2018m_prob\u2019 stores the probability mass of the value dis- tribution Z. Next, a forward pass is carried out to get the next state distributions. As the model outputs 3 sets of value distributions, one for each action. Thus the one with the largest expected value to perform the update (similar to maxa\u2032Q (s\u2032, a\u2032) in Q Learning) is only needed. Then the target distribution Z\u2032 is computed (i.e. scale by \u03b3 and shift by reward r) and projected it to the 51 discrete supports.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9fdf9e2c466ccec9c6410a179a343313",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        868.3,
                        1316.5
                    ],
                    [
                        868.3,
                        1339.1
                    ],
                    [
                        1245.9,
                        1339.1
                    ],
                    [
                        1245.9,
                        1316.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.41005,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "Pseudocode of the Reinforce Algorithm [18]",
        "type": "Title"
    },
    {
        "element_id": "14023ed96d47599590932a761eb73216",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        857.1,
                        1346.8
                    ],
                    [
                        857.1,
                        1504.3
                    ],
                    [
                        1488.9,
                        1504.3
                    ],
                    [
                        1488.9,
                        1346.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.25254,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-12-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313",
            "text_as_html": "<table><tbody><tr><td>initialize 0</td></tr><tr><td>for each episode (s\u0131, a1, E&gt;...Sr\u20141, r\u0131, Er) $ sampled from policy</td></tr><tr><td>fort = 1toT \u2014 1do</td></tr><tr><td>0 = 0 + ao logit (5, a) Ge</td></tr><tr><td>end for</td></tr><tr><td>end for</td></tr></tbody></table>"
        },
        "text": "initialize \u03b8 for each episode {s1, a1, r2\u2026sT\u22121, aT\u22121, rT} s sampled from policy \u03c0\u03b8 do for t = 1 to T \u2212 1 do \u03b8 \u2190 \u03b8 + \u03b1\u2207\u03b8 log\u03c0\u03b8 (st, at) Gt end for end for",
        "type": "Table"
    },
    {
        "element_id": "68ef8a8adfc7429f46b5632cea35539c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1522.1
                    ],
                    [
                        851.7,
                        1600.6
                    ],
                    [
                        1549.0,
                        1600.6
                    ],
                    [
                        1549.0,
                        1522.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9264,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "However, \u2018Reinforce\u2019 suffers from high variance because the sampled rewards can be different from one episode to another that is why this",
        "type": "NarrativeText"
    },
    {
        "element_id": "fcc58fa3508609d24435e6be4643d372",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1585.5
                    ],
                    [
                        851.7,
                        1607.7
                    ],
                    [
                        1549.1,
                        1607.7
                    ],
                    [
                        1549.1,
                        1585.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "algorithm is normally used with a baseline subtracted from the policy.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f0be8ad461eaa7daaae66f6b4f7b4926",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1645.6
                    ],
                    [
                        104.4,
                        1721.9
                    ],
                    [
                        801.8,
                        1721.9
                    ],
                    [
                        801.8,
                        1645.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93315,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "The C51 significantly outperforms several state-of-the-art algo- rithms. In fact, C51 surpasses much state-of-the-art by a large margin in",
        "type": "NarrativeText"
    },
    {
        "element_id": "a938e8b4c2a61d10962fc216c486ddaa",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1707.6
                    ],
                    [
                        104.4,
                        1729.7
                    ],
                    [
                        586.5,
                        1729.7
                    ],
                    [
                        586.5,
                        1707.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "a number of games, most notably Seaquest [51].",
        "type": "NarrativeText"
    },
    {
        "element_id": "f7dd85059bf613d61aed1281f2ac2247",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1731.0
                    ],
                    [
                        104.4,
                        2042.9
                    ],
                    [
                        807.5,
                        2042.9
                    ],
                    [
                        807.5,
                        1731.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95685,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "On the other hand, \u2018Reinforce\u2019 is a family of reinforcement learning methods which directly update the policy weights or in other words, it is a policy gradient-based method which samples the expected return directly from the episode where the expected return is actually the total episodic reward onward that step Gt. To be more specific, it iteratively updates agent's parameters by computing policy gradient and works well when episodes are reasonably short where lots of episodes can be simulated however value-function methods are better for longer epi- sodes because they can start learning before the end of a single episode. How this algorithm works can be understood further with the help of",
        "type": "NarrativeText"
    },
    {
        "element_id": "8bdb16322713d9c64a282ceb62660444",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1608.2
                    ],
                    [
                        851.7,
                        1920.4
                    ],
                    [
                        1552.1,
                        1920.4
                    ],
                    [
                        1552.1,
                        1608.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95316,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "9fdf9e2c466ccec9c6410a179a343313"
        },
        "text": "In the same way, C51_DDQN and Reinforce are as well implemented and tested on the VizDoom health gathering scenario (maps). The agent trained with C51_DDQN performed a lot better and uniform than the Reinforce in gathering the health packs and retained its health to ~87%. On the other hand, the agent trained with Reinforce remains below in performance by collecting health packs on the health gath- ering scenario (maps) and retains its health finally to ~57% as its performance crest and trough can be observed in Fig. 14. It concludes that the agents trained with C51_DDQN perform better than the agents trained with Reinforce particularly using the VizDoom Game-AI re-",
        "type": "NarrativeText"
    },
    {
        "element_id": "79ec168fb60c9fc68e5be11f1f20f67d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1905.2
                    ],
                    [
                        851.7,
                        1927.4
                    ],
                    [
                        1012.4,
                        1927.4
                    ],
                    [
                        1012.4,
                        1905.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "search platform.",
        "type": "Title"
    },
    {
        "element_id": "74732b6a39f3b8e75e21385335e01b11",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1970.1
                    ],
                    [
                        851.7,
                        1992.2
                    ],
                    [
                        1282.6,
                        1992.2
                    ],
                    [
                        1282.6,
                        1970.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "79ec168fb60c9fc68e5be11f1f20f67d"
        },
        "text": "11. Techniques with better performance",
        "type": "ListItem"
    },
    {
        "element_id": "7ac9ad6c98574ca55e4f0e81e86def63",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2027.3
                    ],
                    [
                        104.4,
                        2049.4
                    ],
                    [
                        374.0,
                        2049.4
                    ],
                    [
                        374.0,
                        2027.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "79ec168fb60c9fc68e5be11f1f20f67d"
        },
        "text": "the pseudocode as follows?",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f68942e5c306cea0abf82e612120bec",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.8,
                        2023.5
                    ],
                    [
                        856.8,
                        2072.7
                    ],
                    [
                        1551.8,
                        2072.7
                    ],
                    [
                        1551.8,
                        2023.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92895,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "79ec168fb60c9fc68e5be11f1f20f67d"
        },
        "text": "After training agents on health gathering scenario(s) of the VizDoom",
        "type": "NarrativeText"
    },
    {
        "element_id": "7352354438d07200c23aadbe487ed44e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        2057.2
                    ],
                    [
                        851.7,
                        2079.3
                    ],
                    [
                        1549.1,
                        2079.3
                    ],
                    [
                        1549.1,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12,
            "parent_id": "79ec168fb60c9fc68e5be11f1f20f67d"
        },
        "text": "AI platform using different machine learning methods and techniques,",
        "type": "NarrativeText"
    },
    {
        "element_id": "912c107bc4f0707124373a8d445854ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.9,
                        2111.4
                    ],
                    [
                        816.9,
                        2131.1
                    ],
                    [
                        837.0,
                        2131.1
                    ],
                    [
                        837.0,
                        2111.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72919,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "12",
        "type": "Footer"
    },
    {
        "element_id": "ca2dca0ed1b0a8b3af52757bdd9878e9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 12
        },
        "text": "12",
        "type": "UncategorizedText"
    },
    {
        "element_id": "feb88bbd5255eaeab553f350cbac3ace",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.7,
                        102.7
                    ],
                    [
                        100.7,
                        124.3
                    ],
                    [
                        216.6,
                        124.3
                    ],
                    [
                        216.6,
                        102.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48592,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "8731fdcd8caf99f1f14090ed65a4a7c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1209.3,
                        103.0
                    ],
                    [
                        1209.3,
                        120.6
                    ],
                    [
                        1549.7,
                        120.6
                    ],
                    [
                        1549.7,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79961,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "85ae88d500097834c0b5b72e931ece7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13,
            "parent_id": "8731fdcd8caf99f1f14090ed65a4a7c1"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "568d3cad4c3aaacbc157c8a249293818",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        196.7,
                        158.4
                    ],
                    [
                        196.7,
                        1160.0
                    ],
                    [
                        1456.7,
                        1160.0
                    ],
                    [
                        1456.7,
                        158.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-13-17.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Agent Health on Health Gathering Agent Health on Health Gathering Scenario using c51 DDQN Scenario using Reinforce 100 M$ 100 951 95 | 90 4 85 85 807 807 754 B4 704 701 65 651 604 604 & 5 \u00a9 507 5 50 v v I 45] > 454 401 4 35 54 30 m\u0130 51 51 204 204 154 54 104 104 54 \u2014 G1DDON 5 \u2014 Reinforce 0.--\u2014-\u2014_\u2014_ \u2014 \u2014 \u2014 \u2014 0 \u2014 \u2014_ \u2014 \u2014 \u2014\u2014 0 20000 40000 60000 80000 100000 120000 140000 0 10000 20000 30000 40000 50000 60000 70000 Steps Steps",
        "type": "Image"
    },
    {
        "element_id": "8bd61da21889097378397afe2116f58a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        309.8,
                        1185.7
                    ],
                    [
                        309.8,
                        1205.6
                    ],
                    [
                        1343.7,
                        1205.6
                    ],
                    [
                        1343.7,
                        1185.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Fig. 14. Performance of C51_DDQN with Reinforce on health gathering Scenario(s) using the VizDoom AI platform.",
        "type": "NarrativeText"
    },
    {
        "element_id": "66970598217d7d5e8a5ae77fc68cc995",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        337.3,
                        1243.4
                    ],
                    [
                        337.3,
                        1963.8
                    ],
                    [
                        1316.1,
                        1963.8
                    ],
                    [
                        1316.1,
                        1243.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-13-18.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Comparing Four Machine Learning Methods with Better Performance for Training Agents on the VizDoom's Health Gathering Scenario wi 804 =I ie 704 2 60 \u2014 o m \u2014 ABC \u2014 DDON-Dueling \u2014 \u20ac51-000N o 100000 200000 300000 400000 300000 600000 700000 Steps",
        "type": "Image"
    },
    {
        "element_id": "0dcfb93bb245170930e0ddca59704ee9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        223.3,
                        1984.6
                    ],
                    [
                        223.3,
                        2003.8
                    ],
                    [
                        1425.5,
                        2003.8
                    ],
                    [
                        1425.5,
                        1984.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85594,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Fig. 15. Comparison of the four techniques with the best performance on the health gathering Scenario(s) of the VizDoom AI platform.",
        "type": "FigureCaption"
    },
    {
        "element_id": "eaebb0276f57955990e4a207b032d231",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        221.6,
                        1989.5
                    ],
                    [
                        221.6,
                        2009.4
                    ],
                    [
                        1431.8,
                        2009.4
                    ],
                    [
                        1431.8,
                        1989.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "Fig. 15. Comparison of the four techniques with the best performance on the health gathering Scenario(s) of the VizDoom AI platform.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "e36cf07843317e28ca1bd19e1f3b38f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2111.9
                    ],
                    [
                        816.8,
                        2130.8
                    ],
                    [
                        837.0,
                        2130.8
                    ],
                    [
                        837.0,
                        2111.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71993,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "13",
        "type": "Footer"
    },
    {
        "element_id": "f2ff6499032ea4042c1592891d5518e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 13
        },
        "text": "13",
        "type": "UncategorizedText"
    },
    {
        "element_id": "2635bb74f5b86f71018f971a0f803b85",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.4,
                        103.6
                    ],
                    [
                        101.4,
                        124.3
                    ],
                    [
                        215.8,
                        124.3
                    ],
                    [
                        215.8,
                        103.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65012,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14
        },
        "text": "A. Khan, et al.",
        "type": "Header"
    },
    {
        "element_id": "3a6222989750beb0c20284b9541cb412",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1210.6,
                        103.0
                    ],
                    [
                        1210.6,
                        120.4
                    ],
                    [
                        1549.0,
                        120.4
                    ],
                    [
                        1549.0,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79313,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "b6bd13e60fb182a6efdd0261cf3d28b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "1ff6cfaf0151ca9a73111d96f68bf054",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.2,
                        165.3
                    ],
                    [
                        104.2,
                        189.7
                    ],
                    [
                        179.0,
                        189.7
                    ],
                    [
                        179.0,
                        165.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62107,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "Table 4",
        "type": "Title"
    },
    {
        "element_id": "b9218337296c2c80a4dee35572595fa8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        196.2
                    ],
                    [
                        104.4,
                        216.1
                    ],
                    [
                        774.4,
                        216.1
                    ],
                    [
                        774.4,
                        196.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "1ff6cfaf0151ca9a73111d96f68bf054"
        },
        "text": "Showing the results or performance of Algorithms used for training agents.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6bceddf1705f41ea2dcf5577a927a02b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.2,
                        202.6
                    ],
                    [
                        107.2,
                        311.3
                    ],
                    [
                        1533.4,
                        311.3
                    ],
                    [
                        1533.4,
                        202.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58743,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-14-5.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "1ff6cfaf0151ca9a73111d96f68bf054",
            "text_as_html": "<table><thead><tr><th colspan=\"10\">Algorithms/Methods</th></tr><tr><th>DFP</th><th>A3C</th><th>A2C</th><th>A2C-LSTM</th><th>DON</th><th>DRON</th><th>Dueling DON</th><th>Double DON</th><th>CSI DDON</th><th>Reinforce</th></tr></thead><tbody><tr><td>~95%</td><td>~90%</td><td>~54%</td><td>~52%</td><td>~53%</td><td>~51%</td><td>~59%</td><td>~52%</td><td>~87%</td><td>~57%</td></tr></tbody></table>"
        },
        "text": "Algorithms/Methods DFP A3C A2C A2C-LSTM DQN ~95% ~90% ~54% ~52% ~53% DRQN ~51% Dueling DQN Double DQN C51_DDQN Reinforce ~59% ~52% ~87% ~57%",
        "type": "Table"
    },
    {
        "element_id": "798aa31d173302b92a1ab4e02751d593",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.9,
                        359.6
                    ],
                    [
                        102.9,
                        879.2
                    ],
                    [
                        805.4,
                        879.2
                    ],
                    [
                        805.4,
                        359.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95187,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "1ff6cfaf0151ca9a73111d96f68bf054"
        },
        "text": "the performance of the following four methods is found better and accurate as shown in Fig. 15. The x-axis represents the training steps of thousands to million while the y-axis represents the health of the agents in percentage while gathering the health packs. The agents trained using the DFP method sustains almost 95% health while gathering health packs which is more than other methods and remains one of the best options for training the agents. The agents trained using A3C method follow DFP in performance and sustains almost 90% health on health gathering scenarios which means the second-best machine learning method after DFP for training agents, However, it takes A3C significantly longer to achieve those results and is performing worse than the others until around 375,000 steps. The C51_DDQN is the third- best machine learning technique that performed well in gathering health on the health gathering scenario of the VizDoom Game-AI platform with ~87% performance. Dueling DQN (DDQN) is the last method among the four best machine learning methods that perform well in gathering health packs on the health gathering scenarios and retained health to ~59%.",
        "type": "NarrativeText"
    },
    {
        "element_id": "20d50e5f467361c0eac89abeedb4d2c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        358.8
                    ],
                    [
                        851.6,
                        414.3
                    ],
                    [
                        1549.1,
                        414.3
                    ],
                    [
                        1549.1,
                        358.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89728,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "1ff6cfaf0151ca9a73111d96f68bf054"
        },
        "text": "like to acknowledge and thank NVIDIA Corporation for two powerful sets of GPU machines donation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b64a6576863521dd7f2cbc1280e74998",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        450.3
                    ],
                    [
                        851.6,
                        472.4
                    ],
                    [
                        966.1,
                        472.4
                    ],
                    [
                        966.1,
                        450.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "References",
        "type": "Title"
    },
    {
        "element_id": "a62f56380366e1b070ae3f88575f3532",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        860.1,
                        504.8
                    ],
                    [
                        860.1,
                        524.0
                    ],
                    [
                        1471.0,
                        524.0
                    ],
                    [
                        1471.0,
                        504.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76102,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "ua J.D. Owens, et al., GPU computing, Proc. IEEE 96 (5) (2008) 879-899.",
        "type": "ListItem"
    },
    {
        "element_id": "8565808027e232e72be5bc290af37467",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        510.9
                    ],
                    [
                        861.6,
                        528.6
                    ],
                    [
                        1460.6,
                        528.6
                    ],
                    [
                        1460.6,
                        510.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[1] J.D. Owens, et al., GPU computing, Proc. IEEE 96 (5) (2008) 879\u2013899.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0c2ed913441494c94e2cc07e95bf34b3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        527.5
                    ],
                    [
                        861.6,
                        595.0
                    ],
                    [
                        1556.7,
                        595.0
                    ],
                    [
                        1556.7,
                        527.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91693,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[2] N.P. Jouppi, et al., In-datacenter performance analysis of a tensor processing unit, 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), IEEE, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "e6afa5b6e1136cee9655556e0c069487",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        594.9
                    ],
                    [
                        861.6,
                        617.1
                    ],
                    [
                        1515.2,
                        617.1
                    ],
                    [
                        1515.2,
                        594.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84495,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[3] M.T. Hagan, et al., Neural network design Vol. 20 Pws Pub, Boston, 1996.",
        "type": "ListItem"
    },
    {
        "element_id": "c62821c20f4c61604d04f680f2e4f352",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        616.8
                    ],
                    [
                        861.6,
                        683.5
                    ],
                    [
                        1549.0,
                        683.5
                    ],
                    [
                        1549.0,
                        616.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91786,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[4] F.J. Khan Adil, Shaohui Liu, Worku Jifara, Zhihong Tian, Yunsheng Fu, State-of-the- Art and Open Challenges in RTS Game-AI and Starcraft. (IJACSA), Int. J. Adv. Comput. Sci. App. 8 (12) (2017) 9.",
        "type": "ListItem"
    },
    {
        "element_id": "3d9e1365b5fa4eb288ce18994547708e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        682.5
                    ],
                    [
                        861.6,
                        727.8
                    ],
                    [
                        1517.1,
                        727.8
                    ],
                    [
                        1517.1,
                        682.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9074,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[5] V. Mnih, et al., Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "92c9f9956bafc9e9932f634e68ec6c21",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        727.3
                    ],
                    [
                        861.6,
                        772.0
                    ],
                    [
                        1507.9,
                        772.0
                    ],
                    [
                        1507.9,
                        727.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91197,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[6] M. Lowney, R. Mahieu, Creating an Agent of Doom: A Visual Reinforcement Learning Approach.",
        "type": "ListItem"
    },
    {
        "element_id": "5fe5094c71ac007d69b5fa83e2fe965a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        771.7
                    ],
                    [
                        861.6,
                        816.3
                    ],
                    [
                        1528.8,
                        816.3
                    ],
                    [
                        1528.8,
                        771.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90896,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[7] Z.-J. Pang, et al., On Reinforcement Learning for Full-length Game of StarCraft. arXiv preprint arXiv:1809.09095, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "4967e04eff183066686a676e4bd7e72f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        815.5
                    ],
                    [
                        861.6,
                        860.7
                    ],
                    [
                        1512.4,
                        860.7
                    ],
                    [
                        1512.4,
                        815.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89738,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[8] Grand theft auto V. 2014, Xbox One. New York, NY: Rockstar Games, [2014] \u00a92014.",
        "type": "ListItem"
    },
    {
        "element_id": "bae47c644628f191da973ef266f50806",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.6,
                        859.6
                    ],
                    [
                        861.6,
                        904.9
                    ],
                    [
                        1525.8,
                        904.9
                    ],
                    [
                        1525.8,
                        859.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90339,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[9] G. Ekaputra, C. Lim, K.I. Eng, Minecraft: A game as an education and scientific learning tool, ISICO 2013 (2013) 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "29952a35eb3a1d083e8b6ef7ff6afa48",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        918.1
                    ],
                    [
                        104.4,
                        940.3
                    ],
                    [
                        441.1,
                        940.3
                    ],
                    [
                        441.1,
                        918.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "12. Discussion and future work",
        "type": "ListItem"
    },
    {
        "element_id": "dfe2a724c0e60edebf0eec3ba4f1334e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        904.6
                    ],
                    [
                        851.7,
                        965.9
                    ],
                    [
                        1549.1,
                        965.9
                    ],
                    [
                        1549.1,
                        904.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93402,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[10] I. Grondman, et al., A survey of actor-critic reinforcement learning: Standard and natural policy gradients, IEEE Trans. Syst., Man, Cybernetics, Part C (App. Rev.) 42",
        "type": "ListItem"
    },
    {
        "element_id": "70459bffc6973c5850eec2be50edda48",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        953.7
                    ],
                    [
                        897.6,
                        971.4
                    ],
                    [
                        1079.6,
                        971.4
                    ],
                    [
                        1079.6,
                        953.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "(6) (2012) 1291\u20131307.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "fc03b9d5d59aeae745464bbd5dec8e7a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        974.8
                    ],
                    [
                        104.4,
                        1628.4
                    ],
                    [
                        804.9,
                        1628.4
                    ],
                    [
                        804.9,
                        974.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94666,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "In this paper, the work has shown the effectiveness of using different machine learning techniques and methods within the context of com- plex 3D multi-agent environment with agents playing competitively against human players and inbuilt game agents using the VizDoom Game AI research platform. Such systems can be applied to commercial games to provide competent opponents, without the need for pre- defined manual coded instructions or scripts. In addition, this paper concludes that if the environment provides with a rich and temporally dense measurements signals, reformulating the reinforcement learning problem to supervised learning (e.g. DFP) leads to better performance and accelerated training. According to the experiments, more mea- surements certainly lead to better results. There is a beneficiary effect by allowing the model to generate a richer set of predictions, similar to the way auxiliary tasks enhance the performance of deep learning vi- sion classifier. The goal skeptical nature of DFP allows the agent to pursue complex goals at inference time. This lifts the limitation on learning and acting from a single objective by traditional reinforcement learning methods, and is one way to achieve transfer learning and one- shot learning for multiple tasks. Besides DFP, this paper as well con- cludes that machine learning techniques such as DFP, A3C, C51_DDQN, and Dueling DQN can be chosen as the first choice for training agents due to their efficient performance. The insightful summary of what the",
        "type": "NarrativeText"
    },
    {
        "element_id": "5958e23c4f386f1e8ae5661e384851c9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        970.2
                    ],
                    [
                        851.7,
                        1011.1
                    ],
                    [
                        1549.0,
                        1011.1
                    ],
                    [
                        1549.0,
                        970.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91362,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[11] J.X. Wang, et al., Learning to reinforcement learn. arXiv preprint arXiv:1611.05763,",
        "type": "ListItem"
    },
    {
        "element_id": "c96f903b5756deaa66e494f252888fe6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        997.9
                    ],
                    [
                        897.6,
                        1015.6
                    ],
                    [
                        942.5,
                        1015.6
                    ],
                    [
                        942.5,
                        997.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "2016.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f12beab4361f9733c64c1097101c434f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1014.6
                    ],
                    [
                        851.7,
                        1055.2
                    ],
                    [
                        1493.8,
                        1055.2
                    ],
                    [
                        1493.8,
                        1014.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89408,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "b64a6576863521dd7f2cbc1280e74998"
        },
        "text": "[12] V. Mnih, et al., Asynchronous methods for deep reinforcement learning. in",
        "type": "ListItem"
    },
    {
        "element_id": "875fd882b95cf3d735c556b48100a46d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1042.2
                    ],
                    [
                        897.6,
                        1059.9
                    ],
                    [
                        1322.2,
                        1059.9
                    ],
                    [
                        1322.2,
                        1042.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "International Conference on Machine Learning, 2016.",
        "type": "Title"
    },
    {
        "element_id": "e3a6a6146d4b37a6cf1aadfafdeb9e4b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.1,
                        1059.6
                    ],
                    [
                        861.1,
                        1077.5
                    ],
                    [
                        1359.0,
                        1077.5
                    ],
                    [
                        1359.0,
                        1059.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81543,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "875fd882b95cf3d735c556b48100a46d"
        },
        "text": "[13] S. Freitas, et al., Exploration of DQN in ViZDoom, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "122e6d51dbe1044701c1b4f56f5b29c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1064.2
                    ],
                    [
                        851.7,
                        1081.9
                    ],
                    [
                        1345.3,
                        1081.9
                    ],
                    [
                        1345.3,
                        1064.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "[13] S. Freitas, et al., Exploration of DQN in ViZDoom, 2018.",
        "type": "Title"
    },
    {
        "element_id": "1be0637c04e8b2dc4dec68a06dbefa16",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1081.0
                    ],
                    [
                        851.7,
                        1148.4
                    ],
                    [
                        1536.2,
                        1148.4
                    ],
                    [
                        1536.2,
                        1081.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92299,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "[14] R. Brejl, H. Purwins, H. Schoenau-Fog, Exploring Deep Recurrent Q-Learning for Navigation in a 3D Environment, Eai Endorsed Transactions on Creative Technologies, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "1c6863f601ad601172801acb302bdc69",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1147.0
                    ],
                    [
                        851.7,
                        1209.0
                    ],
                    [
                        1518.8,
                        1209.0
                    ],
                    [
                        1518.8,
                        1147.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93115,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "[15] C. Schulze, M. Schulze, ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling. arXiv preprint arXiv:1801.01000,",
        "type": "ListItem"
    },
    {
        "element_id": "3e36cbec93efb19170296207e4648551",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1197.1
                    ],
                    [
                        897.6,
                        1214.9
                    ],
                    [
                        942.5,
                        1214.9
                    ],
                    [
                        942.5,
                        1197.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "2018.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8fa020723597523a0b16e5c773a24ec0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1214.3
                    ],
                    [
                        851.7,
                        1259.1
                    ],
                    [
                        1542.5,
                        1259.1
                    ],
                    [
                        1542.5,
                        1214.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91356,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "[16] W. Dabney, et al., Distributional reinforcement learning with quantile regression. arXiv preprint arXiv:1710.10044, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "3083434c11dc060eaf2dccc7df9cafc0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1258.6
                    ],
                    [
                        851.7,
                        1303.4
                    ],
                    [
                        1526.9,
                        1303.4
                    ],
                    [
                        1526.9,
                        1258.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90726,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "[17] Z. Wang, et al., Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "d02126329f54ab499e17fa148cf9574f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1302.1
                    ],
                    [
                        851.7,
                        1342.5
                    ],
                    [
                        1526.3,
                        1342.5
                    ],
                    [
                        1526.3,
                        1302.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90155,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "122e6d51dbe1044701c1b4f56f5b29c1"
        },
        "text": "[18] R.J. Williams, Simple statistical gradient-following algorithms for connectionist",
        "type": "ListItem"
    },
    {
        "element_id": "ea345cf9c83d0368778b7e782211cac4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1329.9
                    ],
                    [
                        897.6,
                        1347.6
                    ],
                    [
                        1433.3,
                        1347.6
                    ],
                    [
                        1433.3,
                        1329.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "reinforcement learning, Machine Learning 8 (3\u20134) (1992) 229\u2013256.",
        "type": "Title"
    },
    {
        "element_id": "db262c0830a976657ce89339010c12fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1347.3
                    ],
                    [
                        851.7,
                        1392.0
                    ],
                    [
                        1521.1,
                        1392.0
                    ],
                    [
                        1521.1,
                        1347.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90282,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ea345cf9c83d0368778b7e782211cac4"
        },
        "text": "[19] M. Kempka, et al., ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning. arXiv preprint arXiv:1605.02097, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "bc9ec28f8ba7b13bfdd213310448d5d3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1390.9
                    ],
                    [
                        851.7,
                        1436.3
                    ],
                    [
                        1549.0,
                        1436.3
                    ],
                    [
                        1549.0,
                        1390.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91051,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ea345cf9c83d0368778b7e782211cac4"
        },
        "text": "[20] A. Khan, et al., Optimal Skipping Rates: Training Agents with Fine-Grained Control Using Deep Reinforcement Learning, J. Robot. 2019 (2019) 10.",
        "type": "ListItem"
    },
    {
        "element_id": "d8722934caede070d7638d72aac03f26",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1435.9
                    ],
                    [
                        851.7,
                        1480.5
                    ],
                    [
                        1545.0,
                        1480.5
                    ],
                    [
                        1545.0,
                        1435.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91026,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ea345cf9c83d0368778b7e782211cac4"
        },
        "text": "[21] A. Stooke, P. Abbeel, Accelerated methods for deep reinforcement learning. arXiv preprint arXiv:1803.02811, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "378603c668d3f52c9d82ffa185753816",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1479.0
                    ],
                    [
                        851.7,
                        1519.9
                    ],
                    [
                        1522.8,
                        1519.9
                    ],
                    [
                        1522.8,
                        1479.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89582,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ea345cf9c83d0368778b7e782211cac4"
        },
        "text": "[22] G. Lample, D.S. Chaplot, Playing FPS games with deep reinforcement learning.",
        "type": "ListItem"
    },
    {
        "element_id": "649aaa8f2c888cb2c589e717e6eae87b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1507.1
                    ],
                    [
                        897.6,
                        1524.8
                    ],
                    [
                        1212.4,
                        1524.8
                    ],
                    [
                        1212.4,
                        1507.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "arXiv preprint arXiv:1609.05521, 2016.",
        "type": "Title"
    },
    {
        "element_id": "88a38fe69a119c11db86ed75d2e12c5d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1524.1
                    ],
                    [
                        851.7,
                        1591.2
                    ],
                    [
                        1549.1,
                        1591.2
                    ],
                    [
                        1549.1,
                        1524.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92539,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[23] F.J. Khan Adil, Shaohui Liu, Aleksei Grigorev, B.B. Gupta, Seungmin Rho, Training an Agent for FPS Doom Game using Visual Reinforcement Learning and VizDoom, (IJACSA) Int. J. Adv. Comput. Sci. App. 8 (12) (2017).",
        "type": "ListItem"
    },
    {
        "element_id": "56c49ea78524c1c6fa858595ad8a5b1e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1590.9
                    ],
                    [
                        851.7,
                        1613.3
                    ],
                    [
                        1527.7,
                        1613.3
                    ],
                    [
                        1527.7,
                        1590.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85052,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[24] C.J. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3\u20134) (1992) 279\u2013292.",
        "type": "ListItem"
    },
    {
        "element_id": "e92f03c7c9f09c74a277cd19f6b0ae1f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1615.4
                    ],
                    [
                        104.4,
                        1637.6
                    ],
                    [
                        602.3,
                        1637.6
                    ],
                    [
                        602.3,
                        1615.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "experiment results convey is presented in Table 4.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b158ed2ac465f47ff4a0c7e1b30ce304",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.0,
                        1639.8
                    ],
                    [
                        103.0,
                        1747.6
                    ],
                    [
                        801.9,
                        1747.6
                    ],
                    [
                        801.9,
                        1639.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93787,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "The experiments demonstrate that learning from raw pixels is a new era in artificial intelligence since of versatile and dynamic environ- ments such as VizDoom. Extending such approaches further in multiple",
        "type": "NarrativeText"
    },
    {
        "element_id": "ba186aebca593d4467b8b0d4006ed684",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1731.7
                    ],
                    [
                        104.4,
                        1753.8
                    ],
                    [
                        801.9,
                        1753.8
                    ],
                    [
                        801.9,
                        1731.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "ways to broaden the range of behaviours for learning is our future work.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1cb0ea6f1e95f9e0afc5ec80058ec1a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1613.0
                    ],
                    [
                        851.7,
                        1652.5
                    ],
                    [
                        1544.9,
                        1652.5
                    ],
                    [
                        1544.9,
                        1613.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90093,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[25] A. Dosovitskiy, V. Koltun, Learning to act by predicting the future. arXiv preprint",
        "type": "ListItem"
    },
    {
        "element_id": "9c78a7cabd4a4bbf1953a8c5dd407fb0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1640.0
                    ],
                    [
                        897.6,
                        1657.7
                    ],
                    [
                        1094.9,
                        1657.7
                    ],
                    [
                        1094.9,
                        1640.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "arXiv:1611.01779, 2016.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ce146438e24ecf09e11e7e3d6d5bfa20",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1656.3
                    ],
                    [
                        851.7,
                        1701.9
                    ],
                    [
                        1520.5,
                        1701.9
                    ],
                    [
                        1520.5,
                        1656.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91122,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[26] M.H. Leung, L.R.T. Michael, Applying Modern Reinforcement Learning to Play Video Games. 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "9abd04f387b6316b5738dcf97b3cac16",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1701.0
                    ],
                    [
                        851.7,
                        1741.5
                    ],
                    [
                        1549.1,
                        1741.5
                    ],
                    [
                        1549.1,
                        1701.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91478,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[27] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Computation 9 (8)",
        "type": "ListItem"
    },
    {
        "element_id": "712cc1072180a797dee515110b22235c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1728.5
                    ],
                    [
                        897.6,
                        1746.2
                    ],
                    [
                        1050.3,
                        1746.2
                    ],
                    [
                        1050.3,
                        1728.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "(1997) 1735\u20131780.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "5645f21bfabedf6a6ab6ca00fc08b0c3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1745.3
                    ],
                    [
                        851.7,
                        1785.9
                    ],
                    [
                        1522.8,
                        1785.9
                    ],
                    [
                        1522.8,
                        1745.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90594,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "[28] D. Silver, et al., Mastering the game of Go with deep neural networks and tree",
        "type": "ListItem"
    },
    {
        "element_id": "8f1bbff5c219c015b658c2587d2653c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1772.7
                    ],
                    [
                        897.6,
                        1790.4
                    ],
                    [
                        1245.6,
                        1790.4
                    ],
                    [
                        1245.6,
                        1772.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "649aaa8f2c888cb2c589e717e6eae87b"
        },
        "text": "search, Nature 529 (7587) (2016) 484\u2013489.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "706023afbf913b40a5c7dd547c936522",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1792.8
                    ],
                    [
                        104.4,
                        1814.9
                    ],
                    [
                        465.2,
                        1814.9
                    ],
                    [
                        465.2,
                        1792.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "Declaration of Competing Interest",
        "type": "Title"
    },
    {
        "element_id": "bfd25243320c831cdd9485648e8ecbb8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1789.1
                    ],
                    [
                        851.7,
                        1830.0
                    ],
                    [
                        1549.0,
                        1830.0
                    ],
                    [
                        1549.0,
                        1789.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90113,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "706023afbf913b40a5c7dd547c936522"
        },
        "text": "[29] O. Nachum, et al., Bridging the gap between value and policy based reinforcement",
        "type": "ListItem"
    },
    {
        "element_id": "0f28a8b9646eb34e1784dde094bc9b01",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1817.0
                    ],
                    [
                        897.6,
                        1834.7
                    ],
                    [
                        1393.1,
                        1834.7
                    ],
                    [
                        1393.1,
                        1817.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "learning. in Advances in Neural, Inform. Process, Syst. (2017).",
        "type": "Title"
    },
    {
        "element_id": "b08e22f138c81481984a0ab0aedd84a9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.5,
                        1846.0
                    ],
                    [
                        102.5,
                        1924.7
                    ],
                    [
                        802.3,
                        1924.7
                    ],
                    [
                        802.3,
                        1846.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93463,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "0f28a8b9646eb34e1784dde094bc9b01"
        },
        "text": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influ-",
        "type": "NarrativeText"
    },
    {
        "element_id": "a211c3951aedca0b4fba1233d87e36b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1909.0
                    ],
                    [
                        104.4,
                        1931.1
                    ],
                    [
                        474.5,
                        1931.1
                    ],
                    [
                        474.5,
                        1909.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "0f28a8b9646eb34e1784dde094bc9b01"
        },
        "text": "ence the work reported in this paper.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca55c365ce9372a550d789282a466644",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1970.1
                    ],
                    [
                        104.4,
                        1992.2
                    ],
                    [
                        288.0,
                        1992.2
                    ],
                    [
                        288.0,
                        1970.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "Acknowledgment",
        "type": "Title"
    },
    {
        "element_id": "48171b7da5584f536bc1feb505b4ed36",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        109.3,
                        2022.9
                    ],
                    [
                        109.3,
                        2073.0
                    ],
                    [
                        801.8,
                        2073.0
                    ],
                    [
                        801.8,
                        2022.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92585,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "This work is funded by the University of Peshawar, Pakistan and the",
        "type": "NarrativeText"
    },
    {
        "element_id": "f66cf01e51089a868b118c04c3edc799",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        2057.2
                    ],
                    [
                        104.4,
                        2079.3
                    ],
                    [
                        801.8,
                        2079.3
                    ],
                    [
                        801.8,
                        2057.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "Higher Education Department KPK, Pakistan. The authors would also",
        "type": "NarrativeText"
    },
    {
        "element_id": "4e7f3589fb20cd8e0d9591ba2d4e777a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        847.9,
                        1834.1
                    ],
                    [
                        847.9,
                        1856.9
                    ],
                    [
                        1542.0,
                        1856.9
                    ],
                    [
                        1542.0,
                        1834.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84656,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "[30] A. Kassambara, Machine Learning Essentials: Practical Guide in R, STHDA, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "0fa8ec674d9ec10842c05b9b52804349",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1855.8
                    ],
                    [
                        851.7,
                        1895.8
                    ],
                    [
                        1542.9,
                        1895.8
                    ],
                    [
                        1542.9,
                        1855.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8903,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "[31] M. Abadi, et al., Tensorflow: Large-scale machine learning on heterogeneous dis-",
        "type": "ListItem"
    },
    {
        "element_id": "17ec902c137dca34d5f2791b80eb9da8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1883.4
                    ],
                    [
                        897.6,
                        1901.1
                    ],
                    [
                        1352.4,
                        1901.1
                    ],
                    [
                        1352.4,
                        1883.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "tributed systems. arXiv preprint arXiv:1603.04467, 2016.",
        "type": "NarrativeText"
    },
    {
        "element_id": "efc205160946b87125aced37f519d901",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.4,
                        1900.1
                    ],
                    [
                        856.4,
                        1918.8
                    ],
                    [
                        1543.3,
                        1918.8
                    ],
                    [
                        1543.3,
                        1900.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84957,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "ca55c365ce9372a550d789282a466644"
        },
        "text": "G. Bradski, A. Kaehler, OpenCV. Dr. Dobb\u2019s journal of software tools 3 (2000).",
        "type": "ListItem"
    },
    {
        "element_id": "8b7725e6323d283a9117257a10ded1f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.0,
                        1902.0
                    ],
                    [
                        854.0,
                        1939.0
                    ],
                    [
                        884.0,
                        1939.0
                    ],
                    [
                        884.0,
                        1902.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "3a6222989750beb0c20284b9541cb412"
        },
        "text": "(ost",
        "type": "Title"
    },
    {
        "element_id": "12b1f632c2fc77d11d10f3f2f2c15ac8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1905.6
                    ],
                    [
                        851.7,
                        1923.4
                    ],
                    [
                        1521.9,
                        1923.4
                    ],
                    [
                        1521.9,
                        1905.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "8b7725e6323d283a9117257a10ded1f7"
        },
        "text": "[32] G. Bradski, A. Kaehler, OpenCV. Dr. Dobb\u2019s journal of software tools 3 (2000).",
        "type": "NarrativeText"
    },
    {
        "element_id": "26cb2d42eec92896ed7c27164b84636b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1922.6
                    ],
                    [
                        851.7,
                        1983.8
                    ],
                    [
                        1549.1,
                        1983.8
                    ],
                    [
                        1549.1,
                        1922.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92499,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "8b7725e6323d283a9117257a10ded1f7"
        },
        "text": "[33] M. Fairbank, E. Alonso, The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Ap-proximation. arXiv preprint arXiv:1107.4606,",
        "type": "ListItem"
    },
    {
        "element_id": "5e9d26d6bfeb059a7b00e271d5158d72",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        1971.9
                    ],
                    [
                        897.6,
                        1989.6
                    ],
                    [
                        942.5,
                        1989.6
                    ],
                    [
                        942.5,
                        1971.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "8b7725e6323d283a9117257a10ded1f7"
        },
        "text": "2011.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f46a2d3d8faf2b153159e83f9a74ab94",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        1989.9
                    ],
                    [
                        851.7,
                        2050.9
                    ],
                    [
                        1528.0,
                        2050.9
                    ],
                    [
                        1528.0,
                        1989.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93111,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "8b7725e6323d283a9117257a10ded1f7"
        },
        "text": "[34] F.G. Glavin, M.G. Madden, Learning to Shoot in First Person Shooter Games by Stabilizing Actions and Clustering Rewards for Reinforcement Learning. arXiv",
        "type": "ListItem"
    },
    {
        "element_id": "1a854f6da50aa765583c807e909e21e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        897.6,
                        2038.4
                    ],
                    [
                        897.6,
                        2056.1
                    ],
                    [
                        1164.5,
                        2056.1
                    ],
                    [
                        1164.5,
                        2038.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14,
            "parent_id": "8b7725e6323d283a9117257a10ded1f7"
        },
        "text": "preprint arXiv:1806.05117, 2018.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d8cb9865de821e44b51f8b7dc7f15ee6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        817.0,
                        2112.3
                    ],
                    [
                        817.0,
                        2131.0
                    ],
                    [
                        836.6,
                        2131.0
                    ],
                    [
                        836.6,
                        2112.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14
        },
        "text": "14",
        "type": "Footer"
    },
    {
        "element_id": "ce7dfed8f5008458c522c599fffdce75",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 14
        },
        "text": "14",
        "type": "UncategorizedText"
    },
    {
        "element_id": "a0e5aa0ab00560aae4f6c5263a4c9a8c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.9,
                        104.0
                    ],
                    [
                        101.9,
                        124.3
                    ],
                    [
                        216.5,
                        124.3
                    ],
                    [
                        216.5,
                        104.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.47353,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "A. Khan, et al.",
        "type": "NarrativeText"
    },
    {
        "element_id": "78b13feab27e76c950daf8d122cc67b4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        166.8
                    ],
                    [
                        104.4,
                        210.8
                    ],
                    [
                        801.9,
                        210.8
                    ],
                    [
                        801.9,
                        166.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90972,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[35] D. Hafner, Deep Reinforcement Learning From Raw Pixels in Doom. arXiv preprint arXiv:1610.02164, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "8bcc9b76035d639b962672ba0e87689e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        211.3
                    ],
                    [
                        104.4,
                        255.0
                    ],
                    [
                        742.6,
                        255.0
                    ],
                    [
                        742.6,
                        211.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90973,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[36] A. Smola, S. Vishwanathan, Introduction to machine learning, Cambridge University, UK, 2008, p. 34.",
        "type": "ListItem"
    },
    {
        "element_id": "9d31a64ba846da2c3388dd0b2d5094c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        255.4
                    ],
                    [
                        104.4,
                        321.5
                    ],
                    [
                        791.3,
                        321.5
                    ],
                    [
                        791.3,
                        255.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91788,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[37] M. McPartland, M. Gallagher, Creating a multi-purpose first-person shooter bot with reinforcement learning, in: IEEE Symposium On Computational Intelligence and Games, 2008. CIG'08, IEEE, 2008.",
        "type": "ListItem"
    },
    {
        "element_id": "a3bf4dcc97ceae085d366b711d5426ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        320.3
                    ],
                    [
                        104.4,
                        365.7
                    ],
                    [
                        791.2,
                        365.7
                    ],
                    [
                        791.2,
                        320.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90852,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[38] M. Wydmuch, M. Kempka, W. Ja\u015bkowski, ViZDoom Competitions: Playing Doom from Pixels. arXiv preprint arXiv:1809.03470, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "0e1acdd15ce59c2c1aa9122a2d26b04a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        366.0
                    ],
                    [
                        104.4,
                        432.2
                    ],
                    [
                        801.9,
                        432.2
                    ],
                    [
                        801.9,
                        366.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92852,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[39] D. P\u00e9rez-Li\u00e9bana, et al., Analyzing the robustness of general video game playing agents, in: 2016 IEEE Conference on Computational Intelligence and Games (CIG), IEEE, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "326e29c2057aad2deb781b74b776d262",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        432.4
                    ],
                    [
                        104.4,
                        498.6
                    ],
                    [
                        801.0,
                        498.6
                    ],
                    [
                        801.0,
                        432.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92867,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[40] F.G. Glavin, M.G. Madden, DRE-Bot: A hierarchical First Person Shooter bot using multiple Sarsa (\u03bb) reinforcement learners, in: 2012 17th International Conference on Computer Games (CGAMES), IEEE, 2012.",
        "type": "ListItem"
    },
    {
        "element_id": "be3fc4e54b56b629d0fd979588183308",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        498.4
                    ],
                    [
                        104.4,
                        542.9
                    ],
                    [
                        789.2,
                        542.9
                    ],
                    [
                        789.2,
                        498.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91139,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[41] J. Powles, H. Hodson, Google DeepMind and healthcare in an age of algorithms, Health Technol. 7 (4) (2017) 351\u2013367.",
        "type": "ListItem"
    },
    {
        "element_id": "5e0d53cecd7c9971c2b27f5a7cdee8ef",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        543.1
                    ],
                    [
                        104.4,
                        564.9
                    ],
                    [
                        801.8,
                        564.9
                    ],
                    [
                        801.8,
                        543.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83176,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[42] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436\u2013444.",
        "type": "ListItem"
    },
    {
        "element_id": "f329e7c9c83adb8960e54666f859c572",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        564.9
                    ],
                    [
                        104.4,
                        609.3
                    ],
                    [
                        791.1,
                        609.3
                    ],
                    [
                        791.1,
                        564.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91031,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[43] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
        "type": "ListItem"
    },
    {
        "element_id": "a4b51072a24e3b7453fdd17401488988",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        609.4
                    ],
                    [
                        104.4,
                        653.6
                    ],
                    [
                        800.0,
                        653.6
                    ],
                    [
                        800.0,
                        609.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90796,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[44] K. Das, J. Jiang, J. Rao, Mean squared error of empirical predictor, The Annals of Statistics 32 (2) (2004) 818\u2013840.",
        "type": "ListItem"
    },
    {
        "element_id": "46524d15e2bbac67d476152041ef2cf9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        653.1
                    ],
                    [
                        104.4,
                        693.3
                    ],
                    [
                        787.8,
                        693.3
                    ],
                    [
                        787.8,
                        653.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90144,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[45] J. Schmidhuber, Deep learning in neural networks: An overview, Neural Net. 61",
        "type": "ListItem"
    },
    {
        "element_id": "5237e0564206404099fc403f02f10626",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        150.4,
                        680.1
                    ],
                    [
                        150.4,
                        697.8
                    ],
                    [
                        273.3,
                        697.8
                    ],
                    [
                        273.3,
                        680.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "(2015) 85\u2013117.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ced0886c807e1c95fa04acde66485300",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        697.6
                    ],
                    [
                        104.4,
                        742.1
                    ],
                    [
                        801.9,
                        742.1
                    ],
                    [
                        801.9,
                        697.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91948,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[46] S. Ruder, An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "451ba7f7fba1328e8de919a20d2986c4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        742.5
                    ],
                    [
                        104.4,
                        786.4
                    ],
                    [
                        784.1,
                        786.4
                    ],
                    [
                        784.1,
                        742.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91769,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[47] B. Tan, N. Xu, and B. Kong, Autonomous Driving in Reality with Reinforcement Learning and Image Translation. arXiv preprint arXiv:1801.05299, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "b9c81741b9aeb2257882121bfe4f1bf0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        785.6
                    ],
                    [
                        104.4,
                        825.9
                    ],
                    [
                        766.6,
                        825.9
                    ],
                    [
                        766.6,
                        785.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90955,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[48] J. Schulman, et al., Proximal policy optimization algorithms. arXiv preprint",
        "type": "ListItem"
    },
    {
        "element_id": "b8c13cf8b9fc726f56e07ce1f130b2e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        150.4,
                        812.9
                    ],
                    [
                        150.4,
                        830.6
                    ],
                    [
                        347.7,
                        830.6
                    ],
                    [
                        347.7,
                        812.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "arXiv:1707.06347, 2017.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f4be3dc5f16693da905fed1a3d9707eb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        830.9
                    ],
                    [
                        104.4,
                        875.0
                    ],
                    [
                        788.2,
                        875.0
                    ],
                    [
                        788.2,
                        830.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9058,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[49] T. Schaul, et al., Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "e306c00f62803e085c7be80ca40e1486",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        874.5
                    ],
                    [
                        104.4,
                        919.3
                    ],
                    [
                        801.9,
                        919.3
                    ],
                    [
                        801.9,
                        874.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91086,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[50] L.-J. Lin, Reinforcement learning for robots using neural networks, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993.",
        "type": "ListItem"
    },
    {
        "element_id": "5749125eaeb108e60ded5f451751116a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        918.7
                    ],
                    [
                        104.4,
                        963.5
                    ],
                    [
                        801.9,
                        963.5
                    ],
                    [
                        801.9,
                        918.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90462,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[51] M.G. Bellemare, et al., The arcade learning environment: An evaluation platform for general agents, J. Artificial Intelligence Res. 47 (2013) 253\u2013279.",
        "type": "ListItem"
    },
    {
        "element_id": "23ee79a2694dff6b79d8eb903094cc22",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        962.2
                    ],
                    [
                        104.4,
                        1007.8
                    ],
                    [
                        799.9,
                        1007.8
                    ],
                    [
                        799.9,
                        962.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89666,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[52] V. Mnih, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529\u2013533.",
        "type": "ListItem"
    },
    {
        "element_id": "b7fce857c2f2445499dadcc7b01e0dd9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1007.6
                    ],
                    [
                        104.4,
                        1030.0
                    ],
                    [
                        778.2,
                        1030.0
                    ],
                    [
                        778.2,
                        1007.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85395,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "[53] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double q-",
        "type": "ListItem"
    },
    {
        "element_id": "e30d50addf169f1f657ce378275b6a1c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        817.2,
                        2111.7
                    ],
                    [
                        817.2,
                        2130.9
                    ],
                    [
                        836.6,
                        2130.9
                    ],
                    [
                        836.6,
                        2111.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66137,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "15",
        "type": "Footer"
    },
    {
        "element_id": "94ec98c29d64d892a5ec5792f6c1e5f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        816.8,
                        2119.5
                    ],
                    [
                        816.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2137.2
                    ],
                    [
                        836.8,
                        2119.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "15",
        "type": "UncategorizedText"
    },
    {
        "element_id": "489b4afad7d6e4e4065003e2108f1ba5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1208.2,
                        103.0
                    ],
                    [
                        1208.2,
                        120.6
                    ],
                    [
                        1550.7,
                        120.6
                    ],
                    [
                        1550.7,
                        103.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73688,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Header"
    },
    {
        "element_id": "99d1e86b5e6d6f19a29d054e74f45174",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1211.2,
                        108.3
                    ],
                    [
                        1211.2,
                        126.0
                    ],
                    [
                        1549.0,
                        126.0
                    ],
                    [
                        1549.0,
                        108.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15,
            "parent_id": "489b4afad7d6e4e4065003e2108f1ba5"
        },
        "text": "Entertainment Computing 34 (2020) 100357",
        "type": "Title"
    },
    {
        "element_id": "ff8db003c514eeab7cfd3ef1394a7ecf",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.6,
                        167.1
                    ],
                    [
                        851.6,
                        233.0
                    ],
                    [
                        1539.4,
                        233.0
                    ],
                    [
                        1539.4,
                        167.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85746,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15,
            "parent_id": "99d1e86b5e6d6f19a29d054e74f45174"
        },
        "text": "learning, in: Thirtieth AAAI conference on artificial intelligence 2016 Mar 2. [54] M.G. Bellemare, W. Dabney, R. Munos, A distributional perspective on reinforce- ment learning. arXiv preprint arXiv:1707.06887, 2017.",
        "type": "NarrativeText"
    },
    {
        "element_id": "88549f954c9f26473271ebdad14176bd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        277.7
                    ],
                    [
                        851.7,
                        553.1
                    ],
                    [
                        1048.3,
                        553.1
                    ],
                    [
                        1048.3,
                        277.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-15-19.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "547e874adbc040806b66ddb21f9dca40",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1081.9,
                        275.3
                    ],
                    [
                        1081.9,
                        580.5
                    ],
                    [
                        1549.5,
                        580.5
                    ],
                    [
                        1549.5,
                        275.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92814,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "Dr. Adil Khan http://orcid.org/0000-0003-2862-5718 (ORCID ID). He received C.T. from AIOU Islamabad, B. Ed from the University of Peshawar, BS Honors in Computer Science from Edwards College Peshawar, M.S in Computer Science from City University of Science and Information Technology Peshawar and Ph.D. from University of Peshawar, Peshawar, Pakistan. From 2014-2016, he was a Lecturer in Higher Education Department KPK, Pakistan and from 2016-2019 he was serving as a research scholar at the School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001 PR China. Currently Adil khan is working as an Assistant Professor at the Department of Computer Science, SZIC, University of",
        "type": "NarrativeText"
    },
    {
        "element_id": "a7f653edca5ead0afbc99b554c8e4193",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1084.1,
                        567.9
                    ],
                    [
                        1084.1,
                        585.6
                    ],
                    [
                        1549.1,
                        585.6
                    ],
                    [
                        1549.1,
                        567.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "Peshawar. He has published many publications in top-tier",
        "type": "NarrativeText"
    },
    {
        "element_id": "38962f2fc396d62e904f7f07005ec97f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.8,
                        581.7
                    ],
                    [
                        850.8,
                        674.1
                    ],
                    [
                        1553.1,
                        674.1
                    ],
                    [
                        1553.1,
                        581.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8516,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "academic journals and conferences and is interested in Machine Learning, Game Artificial Intelligence (Game-AI), Neural networks, Real Time Strategy Games, First-Person-Shooter Games, Sandbox open world Games, Computer Vision, Image Processing (Breast Cancer Detection). He can be reached at personal E-mail: adil.adil25@yahoo.com.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2515391a9294240ce59ae80a86833534",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.7,
                        701.8
                    ],
                    [
                        851.7,
                        990.6
                    ],
                    [
                        1554.2,
                        990.6
                    ],
                    [
                        1554.2,
                        701.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95657,
            "file_directory": "./uol-docs",
            "filename": "1-s2.0-S1875952119300497-main.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:23:08",
            "page_number": 15
        },
        "text": "Professor (Asst) & Dr. Muhammad Zubair Asghar. http://orcid.org/0000-0003-3320- 2074 (ORCID ID). He is an Assistant Professor at Institute of Computing and Information Technology, Gomal University, Dera Ismail Khan, KP, Pakistan and approved Ph.D. su- pervisor recognized by Higher Education Commission (HEC), Pakistan. His Ph.D. research interest includes Game-AI, First-person Shooter Games, Third-Person Shooter Games, Computational Intelligence, Computational Linguistics, Machine Learning, Text Mining, Opinion Mining, Sentiment Analysis, and Big Data Solutions for Social Networks. He has published more than 40 publications in journals of international reputation (JCR and ISI indexed). He has more than 15 years of University teaching and laboratory experience in Artificial Intelligence and Intelligent Systems Design. He is a Guest Editor of special issues in the journal of \u2018Social Computing in Health Informatics and Business Intelligence\u2019. He is also a reviewer of many impact factor journals and an Associate Editor of IEEE ACCESS and Plos One. He can be reached at official E-mail: zubair@gu.edu.pk.",
        "type": "NarrativeText"
    }
]