<!-- image -->

glyph<c=11,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=21,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=19,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=21,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=20,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=12,font=/YUSQVD+TimesNewRomanPS-ItalicMT>

glyph<c=20,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=19,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=25,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=25,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=23,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=19,font=/YUSQVD+TimesNewRomanPS-ItalicMT>

Contents lists available at ScienceDirect

## Computers in Human Behavior

journal homepage: http://www.elsevier.com/locate/comphumbeh

Full length article

## Decoding emotional changes of android-gamers using a fused Type-2 fuzzy deep neural network

Lidia Ghosh a , Sriparna Saha b,* , Amit Konar a

- a Department of Electronics & Tele-Communication Engineering, Jadavpur University, West Bengal, India
- b Department of Computer Science & Engineering, Maulana Abul Kalam Azad University of Technology, West Bengal, India

## A R T I C L E  I N F O

Keywords: BCI Electroencephalography Deep learning Type-2 fuzzy set Emotion recognition Android games

## 1. Introduction

Playing android games and beating the scores of others has become a 'happening ' thing for young individuals for quite some time now. People used to play games on their desktops for leisure purpose earlier. But with the  rapid  technological  improvements,  newer  android  game-playing through mobile phones is getting popularized by a larger mass of people. The matter of concern is that if only good games are played for recreational purposes, then that would have positive impacts in terms of releasing stress. However, people especially young minds tend to get addicted to games (Ryan et al., 2006) irrespective of focusing on their works. The obsession for android games and lack of focus in day-to-day activities bring negative impacts (Schneider et al., 2004). On the other hand, there are several violent games, available in the online market, which can be freely downloadable and playing the games for a longer time create severe disorder on the mental condition of the players.

Already  some  works  are  present  in  the  existing  literature

A B S T R A C T

With the fastest growing popularity of gaming applications on android phone, analyzing emotion changes of steadfast android-gamers have become a study of utmost interest among most of the psychologists. Recently, some android games are producing negative impacts to the gamers; even in the worst cases the effect is becoming life-threatening too. Most of the existing research works are based on psychological view-point of exploring the impact (positive/negative) of playing android games for the child and adult age-group. However, the online recognition  of  emotional  state  changes  of  the  android-gamers  while  playing  video  games  may  be  relatively unexplored.  To  fill  this  void,  the  present  study  proposes  a  novel  method  of  identifying  the  emotional  state changes of android-gamers by decoding their brain signals and facial images simultaneously during playing video games. Besides above, the second novelty of the paper lies in designing a multimodal fusion method between brain signals and facial images for the said application. To address this challenge, the paper proposes a fused type-2 fuzzy deep neural network (FT2FDNN) which integrates the brain signal processing approach by a general type-2 fuzzy reasoning algorithm with the flavor of the image/video processing approach using a deep convolutional neural network. FT2FDNN uses multiple modalities to extract the similar information (here, emotional changes) simultaneously from the type-2 fuzzy and deep neural representations. The proposed fused type-2 fuzzy deep learning paradigm demonstrates promising results in classifying the emotional changes of gamers with high classification accuracy. Thus the proposed work explores a new era for future researchers.

(Andersonet al., 2010)- (Carnagey et al., 2007) which justify our claim of  emotional changes due to playing android games (Andersonet al., 2010). Also, it is to be added that not only mood swings, but the adverse impacts of playing such violent games also can be much deeper in young minds.  Now  a  days,  cyber  criminals  are  targeting  through  gaming platforms also (Messias et al., 2011). Just a few years ago in 2016, a treacherous  game  named  as  'Blue  Whale  Challenge ' released.  Many people had even committed suicide by just playing that game. This type of incidents inspire us to develop an algorithm which can automatically identify the emotional state change of android gamers. The multi-modal approach utilizes  both  the  electroencephalographic  (EEG)  signal  and facial images of the gamers while playing video games. The automatic recognition of emotional state (Cowie and Cornelius, 2003) of android game player can be used for self-monitoring of the gamers and/or for parental control as well.

Kühn et al. in (Kühn et al., 2019) showed that how playing violent android  games  can  have  detrimental  effect  rather  than  playing

E-mail addresses: lidiaghosh.bits@gmail.com (L. Ghosh), sahasriparna@gmail.com (S. Saha), konaramit@yahoo.co.in (A. Konar).

<!-- image -->

<!-- image -->

L. Ghosh et al.

Fig. 1. Architectural overview of the FT2FDNN.

<!-- image -->

non-violent games. Przybylski and Weinstein measured the aggression level in adolescents spending their time in android games and found more aggression in them (Przybylski & Weinstein, 2019). In a similar type  of  work,  Prescott,  Sargent  and  Hull  claimed  that  the  generated physical aggression increases over time with more involvement in violent games (Prescott et al., 2018). Likewise, Hasan et al. checked the long-term effects of violent android games (Hasan et al., 2013). Arriaga et al. studied the effects of playing violent android games for a longer duration for college students (Arriaga et al., 2011). Carnagey et al. reported that psychological damage can occur in the chronic game players (Carnagey et al., 2007).

To grasp emotion quantitatively, different physiological (Balters & Steinert,  2017)-  (Purves  et  al.,  2012,  p.  713),  behavioral  (Coulson, 2004)- (Gottman & Krokoff, 1989) and subjective (Fredrickson et al., 2003)-  (Watson  et  al.,  1988)  measures  were  adopted  in  the  existing literature.  The  physiological  changes,  that  express  human  emotions, actually  constitute  energy  in  motion,  which  can  be  quantified  using various  physiological  sensors  more  accurately  as  compared  to  the

All the above cited papers show that how playing games for a longer duration  could  be  detrimental  to  our  health.  The  studies  that  have already  been  done  mainly  are  from  the  psychological  perspectives. There is hardly any work in the technical ground to justify the claims of the  psychologists  in  terms  of  machine  learning  (ML)  algorithms  to decode the emotional changes of the gamers in real-time. The proposed work is a novel one proposed by the authors in terms of mainly applicability of ML approaches in recognizing changes in emotions of players by processing facial expressions and EEG signals simultaneously. The authors did not find any relevant work done so far where these two modalities have been combined to classify emotions of gamers. Although there  exist  a  few  works,  where  both  of  these  modalities  have  been incorporated for the application in other domains, but not for detecting emotional state changes of android-game addicted people.

behavioral and subjective aspects. Thus, a greater amount of work has been  carried  out  to  decode  the  physiological  responses  for  distinct emotions. In this context, several types of measures, such as, Autonomic Nervous System (ANS)- mediated changes (Balters & Steinert, 2017) like heart-rate changes (using electrocardiogram sensor) (Anttonen & Surakka, 2005)- (Pollatos et al., 2007), blood pressure changes (using pulse oximeter,  manual  auscultatory  and  digital  oscillometric  techniques) (Sarlo  et  al.,  2005),  breathing  rate  changes  (using  respiratory  transducer) (Homma & Masaoka, 2008), changes in brain signal characteristics  (using  EEG)  (Murugappan  et  al.,  2008)  and  brain  activations (using  functional  near  infrared  spectroscopy/fNIRs  and/or  functional magnetic resonance imaging/fMRI) (FakhrHosseini et al., 2015)- (Kesler et al., 2001) etc. have been used in the literature. However, the recent advances in brain wave and brain image analysis enable offering more sophisticated and refined testing of emotional contents (T Dasborough et al., 2008).

Huang et al. in (Huang et al., 2017) propose a work to recognize four emotional states, where movie clips have been used as stimulus to the subjects. The study shows that multi-modal approach is far better than using a single mode of either facial images or EEG signals, when neural network is taken as the classifier. This motivated us to use multi-modal approach. Another work by Sokolov et al. shows that Hjorth parameters can be used as EEG features and for extracting features from facial images, principal component analysis can be undertaken (Sokolov et al., 2017). However, the work lacks in achieving a high accuracy, whereas the present approach surely increases the accuracy. On the other hand, Petrantonakis and Hadjileon in (Petrantonakis & Hadjileontiadis, 2010) implemented  hybrid  adaptive  filtering  approach  for  extracting  the relevant EEG features. In the current paper, the authors implemented a novel EEG feature extraction method using phase-sensitive CSP algorithm,  which  successfully  outperforms  the  standard  CSP  algorithm (Delorme & Makeig,  2004)-  (Abdi & Williams,  2010)  and  the  other

L. Ghosh et al.

Fig. 2. Vertical Slice based GT2FS.

<!-- image -->

existing feature extraction techniques.

The  organization  of  the  remainder  in  this  paper  is  as  follows.  In section 2, the principles & methodologies adopted to design proposed algorithm  of  this  paper  are  discussed  from  the  viewpoint  of  mathematics. Section 3 gives an overview of the experimental framework, EEG data acquisition during playing the android games, and the results obtained from the experiments are presented in Section 4 and 5. The results of all the experiments are summarized in the discussion section 6 and concluding remarks are listed in Section 7.

Apart from designing CSP feature extractor, another primary motivation  here  is  to  design  a  fused  type-2  fuzzy  deep  neural  network (FT2FDNN) using the concept of multimodal fusion for online emotion recognition of gamers when they engaged themselves playing android games. The proposed model simultaneously extracts information from the  brain  signal  using  general  type-2  fuzzy  set  (GT2FS)  induced reasoning and from the image/video data of facial expression (Friesen and Ekman, 1978) using a deep convolutional neural network (CNN) representation.  The  knowledge  learnt  by  the  GT2FS  and  CNN  representations are then combined together in the fusion layer to produce the final  fused  data-representation  for  getting  an  ultimate  solution  for pattern-classification.  From  our  previous  experience  (Ghosh  et  al., 2018) -(Saha et al., 2016) and based on the existing literature (Ghosh et al., 2019), it is evident that brain signals have wide fluctuations over time which results in uncertainties in the data representation. We select general type-2 fuzzy logic (GT2FL) for EEG data analysis for its inherent capability of handling such uncertainties (Rakshit et al., 2016). On the other hand, convolutional neural network (CNN) (Lawrence et al., 1997) has proved its efficacy dealing with image/video data while removing the noise from the original data. This inspires us to use a 3-dimensional (3D)  CNN  for  image  (video)  data  analysis.  The  proposed  FT2FDNN utilizes  the  outputs  obtained by the GT2FS and 3D-CNN to form the fused representation and finally fed to a classifier unit to get the desired six  emotion  classes:  happiness,  sadness,  anger,  surprise,  disgust  and neutral.  Thereby,  FT2FDNN  is  more  potentially  suitable  for  online classification  of  emotional  contents  of  the  gamers  as  the  model  can efficiently handle the high level of data ambiguity and noise.

## 2. The proposed approach

In this section, the methodology adopted to design an efficient online emotion detector of android gamers, has been discussed. A Fused Type-2

Fuzzy Deep Neural Network (FT2FDNN) is designed, which integrates the classical image-dataset processing approach using a 3-dimensional convolutional neural network (3D-CNN) (Lawrence et al., 1997) with the flavor of EEG signal (Acharya et al., 2018) classification using type-2 fuzzy set (T2FS). The proposed FT2FDNN is shown in Fig. 1. As shown in the figure, the proposed system consists of 3 major modules: i) Image processing  by  3D-CNN,  ii)  EEG  signal  processing  by  type-2  fuzzy reasoning and iii) Fusion module. To detect emotional contents of the human subject based on their brain signals as well as images of facial expressions, the proposed system takes both the information parallelly as input. Image data (i.e., video) are fed to the 3D-CNN module and EEG time-series data are fed to the type-2 fuzzy reasoning (GT2FR) module simultaneously. Subsequently, the data representations obtained from 3D-CNN and GT2FR modules are fused together in the fusion part with an ultimate aim of data classification. All the three modules are discussed briefly in the following sub-sections.

## 2.1. Image/video signal processing using 3D-CNN

Facial  image  processing  using  3D-CNN  to  accurately  decode  the emotional expressions from videos, a robust video feature representation scheme is required which can incorporate both the global facial gestures and local facial gestures which involves the active contour information of different facial parts including lip-contour and eye-contour (with eye-brow). The reason behind choosing the lip and eye-contour as local facial gestures is straight-forward as there exist many literature which validates the fact that the detection of the changes in lip (Halder et al., 2011) and eye-contours (Zheng et al., 2014) play a significant role in emotion recognition (Liu et al., 2010), (Peng et al., 2005). As shown in Fig. 2, a three-stream 3D-CNN is designed based on the model proposed by Huang et al. in 2018 (Huang et al., 2018), which can extract both the spatial  and  temporal  image  features  from  the  video  clip  of  facial expression  changes  during  playing  android  games.  A  video  clip  containing  adjacent  16  frames,  captured  by  front  camera  (8  MP  front camera of mobile phone) is used as the input to the 3D-CNN. As shown in Fig. 2, the first (upper-most) input data-stream of the 3D-CNN is resized (227 × 227) complete video frames to extract the global facial gestures. The lower two input data-streams contain the local detailed information about lip-contour and eye-contour and both of the inputs are cropped into 227 × 227 tracked image blotches comprising tight-bounding boxes of the respective local information. Each input stream shares the same

L. Ghosh et al.

Table 1 Details of the participants.

| Gender   |   Number | Age           | Health  Issues   | Playing Game  Years   | Daily Playing  Time   |
|----------|----------|---------------|------------------|-----------------------|-----------------------|
| Male     |       20 | 16  ± 8  yrs. | No               | 8  ± 2 yrs.           | 6  ± 1 h.             |
| Female   |       15 | 18  ± 5  yrs. | No               | 6  ± 2 yrs.           | 5  ± 2 h.             |

network architecture of C3D network, proposed by Tran et al. (Tran et al., 2015), which includes 8 convolutional layers and 5 pooling layers. The global and local information from the three data-streams are then combined together by 2 fully connected layers. The output of the fully connected layer can be mathematically expressed as

ofl = f ( W T fl x + bfl ) (1)

where, x is  the  input  vector  of  the  fully-connected  layer, W fl is  the randomly initialized weight vector, T denotes the transpose operation and bfl is the bias term. f (.) is the activation function. Instead of using sigmoid non-linearity in the fully connected layer, we utilize Exponential Linear Sigmoid Squashing (ELiSH), defined by (2).

f ( a ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ( a 1 + e GLYPH<0> a ) , a ≥ 0 ( e a GLYPH<0> 1 1 + e GLYPH<0> a ) , a < 0 (2)

The  ELiSH  function  is  a  combination  of  Exponential  Linear  Unit (ELU) (Laukka et al., 1995) and sigmoid (Delorme & Makeig, 2004) (Maclinet al., 2011). The three-stream 3D-CNN is first pre-trained with the training dataset to fix all the weights of subsequent layers. Once the training  of  the  3D-CNN  is  complete,  the  last  fully-connected  layer  is discarded and truncated to the first fully connected layer during the test phase.

## 2.2. EEG signal processing using general Type-2 fuzzy reasoning

The EEG signal processing includes two progressive steps: feature extraction using Common Spatial Pattern technique and class-centroid computation using General Type-2 Fuzzy reasoning.

Let the EEG signals acquired from m channels be represented by a data-matrix of the form X l i =[ x → l i , 1 , x → l i , 2 , ..., x → l i , m ] having dimension n × m , where x → l i , c is a vector representing the instantaneous amplitudes of n EEG samples of cth channel, c /uni03B5 [1, m ] and l -th trial for class i . The phase information of the signal is extracted by evaluating the Discrete Hilbert transform (DHT) (Prucnal & Polak, 2018) of x → l i , c as follows

B.1 Phase-sensitive CSP Feature Extraction Algorithm: CSP has emerged as the most promising algorithm that extracts spatial filters encoding the most discriminative information. CSP provides a robust approach for learning spatial filters, capable of maximizing the discriminiability between two or more classes. CSP algorithm yields features by assigning spatial weights to each EEG electrode such that the variances of the EEG signals  associated  with  two  different  classes  are  maximally  discriminated. Here, the main aim is to discriminate the different emotions of android gamers into six different emotion classes: happiness, sadness, surprise, anger, disgust and neutral from the acquired EEG signals..

H { x [ n ]} = DFT GLYPH<0> 1 ( ̂ X ) , (3)

where, for even N,

̂ X = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ GLYPH<0> j ∑ N GLYPH<0> 1 n = 0 x [ n ] . e GLYPH<0> jnk 2 /uni03C0 N , k = 1 , N 2 GLYPH<0> 1 j ∑ N GLYPH<0> 1 n = 0 x [ n ] . e GLYPH<0> jnk 2 /uni03C0 N , k = N 2 + 1 , N GLYPH<0> 1 (4a)

and for odd N,

Fig. 3. Timing Diagram of the stimulus presented during offline game playing.

<!-- image -->

Fig. 4. Images depicting 6 different emotions.

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Happiness

<!-- image -->

Surprise

Sadness

Anger

Neutral

Disgust

L. Ghosh et al.

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Fig. 5. sLORETA activations (top and bottom views) for the emotion (a) happiness, (b) sadness, (c) surprise, (d) anger, (e) disgust and (f) neutral.

<!-- image -->

Fig. 6. Discriminating 6 distinct emotional states using (a) Standard CSP and (b) the Proposed CSP features.

<!-- image -->

L. Ghosh et al.

Table 2 Comparison of the feature extraction Algorithm.

| Feature Extraction methods                                                                                                                                |   CA  (%) |   True Positive  Rate (TPR) |   False Positive  Rate (FPR) |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------------------------|------------------------------|
| Proposed CSP                                                                                                                                              |     87.45 |                        0.89 |                         0.07 |
| Standard CSP                                                                                                                                              |     70.64 |                        0.8  |                         0.19 |
| Time-domain EEG Features (RMS  value, mean values, variance, 1st and                                                                                      |     76.43 |                        0.78 |                         0.19 |
| 2nd order derivatives, peak counts)  Frequency-domain Features (spectral  energy, power spectral density,  discrete fast Fourier transform  coefficients) |     80.53 |                        0.86 |                         0.1  |
| Time and Frequency domain Features  (Discrete Wavelet Transform)                                                                                          |     86.16 |                        0.89 |                         0.14 |
| Hybrid adaptive filtering (  Petrantonakis  &  Hadjileontiadis,  2010)                                                                                    |     82.29 |                        0.87 |                         0.13 |

̂ X = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ GLYPH<0> j ∑ N GLYPH<0> 1 n = 0 x [ n ] . e GLYPH<0> jnk 2 /uni03C0 N , k = 1 , N GLYPH<0> 1 2 j ∑ N GLYPH<0> 1 n = 0 x [ n ] . e GLYPH<0> jnk 2 /uni03C0 N , k = N + 1 2 , N GLYPH<0> 1 . (4b)

In (3), DFT GLYPH<0> 1 denotes inverse discrete Fourier transform. In (4a) and (4b), ∑ N GLYPH<0> 1 n = 0 x [ n ] . e GLYPH<0> jnk 2 /uni03C0 N = X [ k ] represents  the  discrete  Fourier  transform (DFT) of x [ n ]. When N is even, the samples X [ k ] for k = 1, 2, … , N 2 GLYPH<0> 1 , are called positive harmonics, whereas the samples X [ k ] for k = GLYPH<0> ( N 2 GLYPH<0> 1 ) , ( )

GLYPH<0> N 2 GLYPH<0> 2 , ..., GLYPH<0> 2 , GLYPH<0> 1 , are called negative harmonics (Todoran et al.,

2008). The negative harmonics { X [ GLYPH<0> ( N 2 GLYPH<0> 1 )] , X [ GLYPH<0> ( N 2 GLYPH<0> 2 )] , ..., X [ GLYPH<0> 2 ] , X [ GLYPH<0> 1 ] } are equivalent to { X [ N 2 + 1 ] , X [ N 2 + 2 ] , ..., X [ N GLYPH<0> 2 ] , }

X [ N GLYPH<0> 1 ] , as the sample X [ n-k ] = X [-k ] has a correspondent spectral

density sample with the negative frequency X ( GLYPH<0> k /uni03C9 0 ) . Similarly when N is odd, the samples X [ k ] for k = 1, 2, … , N GLYPH<0> 1 2 , are the positive harmonics and that for k = N + 1 2 , 2, … , N GLYPH<0> 1 , are called negative harmonics. The samples  against k = 0  and N 2 are  discarded  as  they  are  continuous components.

Now, H{ x [ n ]} can also be re-written as

H { x [ n ]} = Re [ n ] + j Im [ n ] . (5)

where, the phase angle of H { x [ n ]} is /uni03B8 = tan GLYPH<0> 1 Im [ n ] Re [ n ] . Now, the amplitude x l i , c and phase content /uni03B8 l i , c of the EEG signal acquired from c -th channel in l -th trial  is  integrated  such  that  a  complex  data  matrix  Zi  is  produced, defined as

Z l i , c = x l i , c × e GLYPH<0> j/uni03B8 l i , c . (6)

The CSP problem can be formulated to derive the co-efficient of the spatial  filters  ' S ' that  maximizes  the  following  Rayleigh  Quotient problem,

JZ = S T ∑ * 1 S S T ∑ * 2 S . (7)

In (7), the co-variance matrix ∑ * i of Z l i for class i can be obtained by

∑ * i = Cov [ Z i , Z i ] . (8)

Now, it is to be worth mentioning that in (7), the spatial filter S is a data-matrix of dimension m × m containing complex variables and S is its complex conjugate. To maximize Jz , the constraint S T ∑ * 2S = 1must be considered. Therefore, the Rayleigh Quotient problem, presented by Jz in  (7)  can  be  transformed  into  an  optimization  problem  after computing  the  co-variance  matrices ∑ * i for  class i .  The  constrained optimization can then be solved by Lagrange Multiplier technique. The Lagrange objective function is thus defined as

L ( /uni03BB , S ) = S T ∑ * 1 S GLYPH<0> /uni03BB ( S T ∑ * 2 S GLYPH<0> 1 ) (9)

To optimize the Lagrange function L , the first and foremost criteria is that the filter S should be such that,

/uni2202 L /uni2202 S = /uni2202 /uni2202 S ( S T ∑ * 1 S ) GLYPH<0> /uni03BB /uni2202 /uni2202 S ( S T ∑ * 2 S ) = 0 (10)

Equation  (10)  cab  solved  by  following  the  methods  adopted  in (Mendel, 2013) and thus we obtain

∑ * 2 GLYPH<0> 1 . ∑ * 1 S = /uni03BBS (11)

Therefore, the possible solutions to the specified problem would be the eigenvalues of ∑ * = ∑ * 2 GLYPH<0> 1 . ∑ * 1. The principal components evaluated against the corresponding largest and lowest eigen values of ∑ * are the required CSP filters. Thus, for an n × m dimension EEG data-matrix, CSP produces p such spatial filters, given by n × p matrix S with p Eigen values in decreasing order.

The fuzzification layer converts the crisp input into two fuzzified output values, namely Upper Membership Function (UMF) and Lower Membership Function (LMF) by the following strategy:

Fig. 7. Stimulus preparation for online emotion recognition.

<!-- image -->

B.2 Centroid Computation using General Type-2 Fuzzy Reasoning (GT2FR):  A  General  Type-2  (GT2)  Vertical  Slice  (Mendel,  2013) approach to fuzzy reasoning is adopted in the present application. Let f1, f2, … , fn  be the extracted n CSP features of the EEG signal and yj is a measure of emotional state j by a given subject. Let fi  is ˜ Ai be a fuzzy proposition used to build up the antecedent part of the fuzzy rule j, and yj is ˜ Bj is a fuzzy proposition to develop the consequent of the same rule. Here, ˜ Ai for i = 1 to n are vertical slice based GT2FS given by (( fi , u ) , /uni03BC ˜ A ( fi ) ( u )) , where /uni03BC ˜ A ( fi ) ( u ) is the vertical slice at given at a given fi for m discretizations u1,u2, … ,um  along the u-axis. Similarly, ˜ Bj is the consequent in the university of discourse yj. As shown in Fig. 1, the T2FR module computes the class-centroids into 3 major steps, demonstrated by 3 layers: i) fuzzification layer, ii) fuzzy rule layer and iii) centroid computation layer.

L. Ghosh et al.

<!-- image -->

Fig. 8. Flow diagram for (a ) offline data analysis and (b) online emotion recognition.

<!-- image -->

## and variance

/uni03C3 2 i , v = ∑ 15 l = 1 ( fi , l , v GLYPH<0> f i , v ) 2 15 , (13)

to construct one type-1 Gaussian MF: Ai , v , for v = 1 to 10.

- 2.  Then the footprint of uncertainty (FOU) is constructed using type-1 Ai , v , for v = 1 to 10 with LMF and UMF obtained by

/uni03BC ˜ Ai ( fi ) = UMF ( fi ) = Max 10 v = 1 ( /uni03BC ˜ Ai , v ( fi ) ) , (14)

/uni03BC ˜ Ai ( fi ) = LMF ( fi ) = Min 10 v = 1 ( /uni03BC ˜ Ai , v ( fi ) ) , (15)

In order to maintain the convexity criteria of the proposed GT2FS, the peaks of the constituent type-1 MFs are joined with a straight line of zero slope, resulting in a flat-top approximation.

## Parameters of FT2FDNN.

Table 3

| Modules of  FT2FDNN   | Parameters of the classifiers                               | Parameter values             |
|-----------------------|-------------------------------------------------------------|------------------------------|
| 3D-CNN                | 8                                                           | No. of convolution Layers    |
| 3D-CNN                | No. of max-pooling layers                                   | 5                            |
| 3D-CNN                | No. of filters for 8                                        | 64, 128, 256, 256, 512, 512, |
| 3D-CNN                | convolution layers                                          | 512, 512                     |
| 3D-CNN                | 3D convolution kernel size                                  | 3  × 3  × 3                  |
| 3D-CNN                | Pooling kernel size                                         | 2  × 2  × 2                  |
| 3D-CNN                | Initial learning rate                                       | 0.003                        |
| No. of neurons in     | 5 (no. of classes)                                          | GT2FS                        |
| No. of neurons in     | fuzzification layer  No. of neurons in  fuzzification layer | 5  × n                       |

- 1.  At first ˜ Ai is constructed by collecting 15 daily samples of feature fi over 10 days. Let the measurement l of fi on day v be denoted by fi , l , v . Thus for l = 1 to 15, we obtain the mean

f i , v = ∑ 15 l = 1 fi , l , v 15 , (12)

L. Ghosh et al.

Table 4 Comparative analysis of the FT2FDNN algorithm with state-of-the-art algorithms.

| Classifier Algorithms                                   | Training   | Training   | Training   | Validation   | Validation   | Validation   | Testing   | Testing   | Testing   |
|---------------------------------------------------------|------------|------------|------------|--------------|--------------|--------------|-----------|-----------|-----------|
|                                                         | CA (%)     | TPR        | FPR        | CA (%)       | TPR          | FPR          | CA (%)    | TPR       | FPR       |
| FT2FDNN                                                 | 88.26      | 0.87       | 0.16       | 86.05        | 0.87         | 0.0874       | 87.58     | 0.85      | 0.07      |
| Decision level fusion (Huang et al., 2017)              | 80.96      | 0.81       | 0.25       | 79.75        | 0.81         | 0.3295       | 81.34     | 0.79      | 0.2857    |
| Statistical features and LSVM in (Sokolov et al., 2017) | 82.72      | 0.81       | 0.30       | 81.24        | 0.7511       | 0.3297       | 82.95     | 0.78      | 0.6241    |
| Emotion detection in the loop (Savranet al., 2006)      | 78.42      | 0.79       | 0.58       | 79.47        | 0.78         | 0.53         | 78.41     | 0.76      | 0.61      |
| From Brain signals only                                 | 75.57      | 0.75       | 0.65       | 73.13        | 0.73         | 0.25         | 74.32     | 0.75      | 0.63      |
| From facial expressions only                            | 72.83      | 0.73       | 0.75       | 71.98        | 0.72         | 0.76         | 72.53     | 0.71      | 0.78      |

<!-- image -->

Fig. 9. Convergence analysis of FT2FDNN on (a) JU database and (b) MAKAUT database.

<!-- image -->

- 3.  Now, at the measurement points fi = f ' i , an isosceles triangle with peak = 1 is drawn to represent /uni03BC ˜ A ( f ' i ) ( u ) , the secondary plane.

In the fuzzy rule layer, by the given GT2 fuzzy rule j , ' if fi is ˜ Ai , then yj is ˜ Bj ' and the fuzzified  value  of  (14)  and  (15),  the  AND  fuzzy  logic operation using the well-known /uni03B1 -cut representation, is performed. For every pair of points, let ( f ' 1 , f ' 2 ) , such that f ' i ∈ dom /uni03BC ˜ Ai ( f ' i ) , we compute the meet operation as follows,

˜ A j 1 ∩ ˜ A j 2 = ∫ ∀ /uni03B1 ∈[ 0 , 1 ] sup [ /uni03B1 /{[ a 1 /uni03B1 GLYPH<0> f ' 1 ) ∧ a 2 /uni03B1 GLYPH<0> f ' 2 ) , b 1 /uni03B1 GLYPH<0> f ' 1 ) ∧ b 2 /uni03B1 GLYPH<0> f ' 2 )}] / f (16)

where,

˜ Ai = ∫ ∀ f ' i ∈ F [ sup ∀ /uni03B1 ∈[ 0 , 1 ] /uni03B1 / [ ai /uni03B1 ( f ' i ) , bi /uni03B1 ( f ' i )]] / f ' i (17)

and the /uni03B1 -cuts of the triangle secondary MFs are given by

⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ ˜ A j i ( f ) = [ ai /uni03B1 GLYPH<0> f ' i ) , bi /uni03B1 GLYPH<0> f ' i )] ai /uni03B1 GLYPH<0> f ' i ) = /uni03BC ˜ A GLYPH<0> f ' i ) + w [ /uni03BC ˜ A GLYPH<0> f ' i ) GLYPH<0> /uni03BC ˜ A GLYPH<0> f ' i ) ] /uni03B1 bi /uni03B1 GLYPH<0> f ' i ) = /uni03BC ˜ A GLYPH<0> f ' i ) GLYPH<0> ( 1 GLYPH<0> w ) [ /uni03BC ˜ A GLYPH<0> f ' i ) GLYPH<0> /uni03BC ˜ A GLYPH<0> f ' i ) ] /uni03B1 . (18)

Equ. (16) is the vertical slice of. ˜ A j 1 ∩ ˜ A j 2 .

The  centroid  computation  layer  then  performs  the  centroid  type reduction strategy to map the GT2FS into its corresponding type-1 fuzzy set. There are many type-reducer techniques available in the literature. We adopt the technique described by Mendel in (Abdi & Williams, 2010) and  obtain  the  centroid  type-reduction  of  consequent  GT2FS ˜ Bj as follows

Verification by alternative fusion approaches.

Table 5

| Fusion Approaches   |   CA (%) |   TPR |   FPR |
|---------------------|----------|-------|-------|
| 3DCNN  + GT2FS      |    87.58 |  0.85 |  0.07 |
| CNN  + GT2FS        |    85.84 |  0.83 |  0.23 |
| CNN  + SVM          |    83.75 |  0.78 |  0.39 |
| LSTM  + GT2FS       |    81.29 |  0.75 |  0.47 |
| RNN  + GT2FS        |    78.54 |  0.74 |  0.54 |
| 3DCNN  + IT         |    75.67 |  0.71 |  0.58 |
| 3DCNN  + IT2FS      |    74.31 |  0.69 |  0.65 |

C ˜ Bj = sup ∀ /uni03B1 ∈[ 0 , 1 ] C ˜ Bj /uni03B1 (19)

where, C ˜ Bj /uni03B1 is the centroid of ˜ Bj /uni03B1 at /uni03B1 -cut and completely specified by its left and right end-point centroids l ˜ Bj /uni03B1 and r ˜ Bj /uni03B1 respectively as follows

C ˜ Bj /uni03B1 = /uni03B1 / [ l ˜ Bj /uni03B1 , r ˜ Bj /uni03B1 ] (20)

l ˜ Bj /uni03B1 and r ˜ Bj /uni03B1 are obtained using extended Karnik-Mendel algorithm (Abdi & Williams, 2010). The centroid C ˜ Bj is then used as the output of the type-2 fuzzy represented block and is denoted by of .

## 2.3. Multimodal fusion

In the decision-level fusion, the outputs generated by the previous two modules of facial image processing and EEG signal processing are combined  together,  inspired  by  the  concept  of  multimodal  learning (Saha & Karia, 2019). The approach of multimodal fusion usually produces  multiple  features  from  different  traits  and  represents  them  by high-level feature values for achieving better performance of the classifier.  In  the  proposed  FT2FDNN,  we  employ  3D-CNN  and  GT2FS

L. Ghosh et al.

<!-- image -->

Fig. 10. Parametric sensitivity analysis of FT2FDNN with respect to the hyper-parameters (a) /uni03B2 and (b) /uni03B3

<!-- image -->

o fl and GT2FS module, defined by o f , are fused together with the weights w fl and w f , respectively.

Table 6 Statistical validation.

| Prediction  Algorithms   |   Average Rank over 7  datasets | Friedman Statistics  /uni03C7 2 F   |
|--------------------------|---------------------------------|-------------------------------------|
| 3DCNN  þ GT2FS           |                            1    | 28.19  (Null Hypothesis  rejected)  |
| CNN  + GT2FS             |                            2.06 | 28.19  (Null Hypothesis  rejected)  |
| CNN  + SVM               |                            6.77 | 28.19  (Null Hypothesis  rejected)  |
| LSTM  + GT2FS            |                            3.14 | 28.19  (Null Hypothesis  rejected)  |
| RNN  + GT2FS             |                            4.28 | 28.19  (Null Hypothesis  rejected)  |
| 3DCNN  + IT              |                            5.56 | 28.19  (Null Hypothesis  rejected)  |
| 3DCNN  + IT2FS           |                            6.21 | 28.19  (Null Hypothesis  rejected)  |

modules to provide a better representation of the respective input data by decreasing the uncertainty and noise level from the data. Here, we employ  the  commonly  used  multimodal  NN  structure  (Hasan  et  al., 2013) to combine the 3D-CNN and GT2FS outputs with dense-connected fusion layers, defined as follows

fo ( l ) i = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ ( p ( l ) i 1 + e GLYPH<0> p ( l ) i ) , p ( l ) i ≥ 0 ( e p l i GLYPH<0> 1 1 + e GLYPH<0> p l i ) , p ( l ) i < 0 (21)

where , p ( l ) i = GLYPH<0> w fl ) ( l ) i GLYPH<0> o fl ) ( l GLYPH<0> 1 ) + GLYPH<0> w f ) ( l ) i GLYPH<0> o f ) ( l GLYPH<0> 1 ) + b ( l ) i . (22)

In (22), the outputs obtained from the 3D-CNN module, defined by

## 2.4. Classification

Finally, the fused information foi is connected with a dense layer for classification  into  its  corresponding  emotion  classes.  In  this  layer,  a sparsemax function (Balters & Steinert, 2017) is used to get the desired class  labels:  happiness,  sadness,  surprise,  anger,  disgust  and  neutral based on probability measurements. We compute coupling coefficient using Sparsemax function, defined as follows

ci = argmin r ∈ /uni0394 G GLYPH<0> 1 || r GLYPH<0> foi || (23)

where, /uni0394 G GLYPH<0> 1 : = { r ∈ R G ⃒ ⃒ 1 T r = 1 , r ≥ 0 } is the G GLYPH<0> 1dimensional simplex. The computation of Sparsemax function is able to provide sparse distributions where very small probabilities are reduced to zero values. Finally, an Euclidean norm is taken on the output vector to predict the class label, ̂ y i .

## 2.5. Loss function and training of the proposed FT2FDNN

For a desired class label, yi , the mean square loss of the proposed FT2FDNN over l training samples can be defined as

L = 1 l ∑ l i = 1 ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ̂ y i GLYPH<0> yi ⃒ ⃒ ⃒ ⃒ | 2 2 . (24)

L. Ghosh et al.

<!-- image -->

Fig. 11. Changes in emotional states over 30 days of experiment for (a) Candy Crush Saga and (b) Stickman Archers.

<!-- image -->

The training goal of a network is to minimize the Loss function (LF) with a simultaneous maximization of the mutual information between the input data and the representation learned by the network (Torkkola, 2003). The mean square LF only can preserve the similarity measure between the input data vectors during the training phase of the network. As  the  1st  order  structural  proximity  describes  the  local  network configuration  and  the  2nd  order  structural  proximity  describes  the global network configuration, we here define the LF, which can preserve both the local and global structural features of the proposed network. To preserve  the  1st  order  structural  proximity  between  the  outputs  of directly connected n number of node-pairs ( i , j ) of the FT2FDNN, the following LF is defined

L 1 = ∑ n i , j = 1 sij ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ o ( K ) i GLYPH<0> o ( K ) j ⃒ ⃒ ⃒ | 2 2 , (25)

where, sij is the edge weight between the i th and j th node-pair, o ( K ) i and o ( K ) j are the output representation of a pair of node ( i , j )  and K is the number layers of the neural network. Similarly, to  preserve  the  2nd order proximity (Torkkola, 2003), we define the LF as

L 2 = ∑ n i = 1 ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ( ̂ s i GLYPH<0> s i ) /uni2218r s i ⃒ ⃒ ⃒ ⃒ ⃒ | 2 2 (26)

where, /uni2218 is the Schur product (Todoran et al., 2008) and r s i for i = 1 to n are  the  penalty  parameters  for  non-zero  adjacency  elements. r s i , j is assigned to 1, if sij = 0 and is assigned to a value greater than 1, if. sij / = 0 .

The final LF is then defined as the combination of the 1st and 2nd order structural proximities (Torkkola, 2003) as follows

LF = L 1 + /uni03B2 /uni22C5 L 2 + /uni03B3 /uni22C5 Lr (27)

where, /uni03B2 and /uni03B3 are the small positive weights and L r  is the ||L2|| regularization  term  in  the  LF  to  prevent  overfitting  (Torkkola,  2003),  as defined below

Lr = 1 2 ∑ K k = 1 ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ W ( k ) ⃒ ⃒ ⃒ | 2 F (28)

where, W (k) , k = 1, 2, … , K is the weight matrix of the k th layer. The hyper-parameters: /uni03B2 and /uni03B3 of the LF are fine-tuned using grid search algorithm and the respective sensitivity analysis is given in section 5.

## 3. Experimental framework

The  section  describes  the  experimental  protocol  undertaken  to perform the experiments.

## 3.1. Experimental framework

The data collection process has been carried in two distinct universities to get the essence of diversities of changes in emotional states in subjects while playing android games. Thus, two databases have been prepared named as JU database and MAKAUT database. For the former database,  a  21-channel  Nihon  Kohden  EEG  system  is  used  in  the

L. Ghosh et al.

Artificial Intelligence Laboratory of Jadavpur University. For the latter one, a 24- channel EEG sensor by Brain Tech is used in Human Computer Interaction  Laboratory  of  Maulana  Abul  Kalam  Azad  University  of Technology. For the current study EEG data is collected from pre-frontal, frontal,  motor  cortex,  parietal,  occipital  and  temporal  regions  by following the norms of international 10/20 electrode placement system. To capture the facial expressions, for both the databases front camera of a mobile phone with 16 frames and 227 × 227 configuration is implemented. The chosen two games are of different kind: Candy Crush saga is a leisure game, while Stickman Archers is a violent game.

## 3.2. Participants

The participants include 35 healthy volunteers (20 male and 15 female in 13 -25 years age-group), with no physical disabilities. Before selecting the participants, we have given them a questionnaire, where among other trivial questions, two vital questions were asked whether they are inclined to one or more video games and if yes then how much time they invest each day in those games. We have taken this survey from more than 100 subjects and among them only 35 is chosen who have answered 4 or more hours as an answer to the second question. The information regarding the age group, health issues, playing game years and  daily  playing  times  of  male  and  female  groups  are  enlisted  in Table 1.

## 3.3. Artifact removal

A Chebyshev band pass filter has been used to get rid of the interference  of  physiological  and  environmental  artifacts  from  the  EEG signals.

## 4. Offline and online analysis of image and EEG data

The experiments are conducted into two ways: offline data analysis and online data analysis .

## 4.1. Offline data analysis

During offline data analysis, the experiment is carried out over 30 days for each subject. On each day, 5 experimental trials are conducted, where in each trial the subject plays a given android game for 10 min duration  and  during  playing  the  game  his/her  facial  expressions  are captured using a video camera and the corresponding EEG signals are acquired from the subject ' s scalp. Therefore, we have altogether (30 × 5) = 150 experimental trials per subject for offline data analysis. The timing diagram of the stimulus presented in each day during offline data analysis is shown in Fig. 3. In each experimental trial, an audio-visual cue  is  presented  to  the  subject  indicating  the  cognitive  task  (here, android game playing) needed to be accomplished. At the beginning of the trial, a fixation cross of 2 s duration appears in the screen (see Fig. 3) informing the subject to concentrate on the given task. An audio-visual cue of playing game appears thereafter with duration of 10 min. A rest period of 5 min between consecutive trials is given to the subject. The facial images captured during game playing are then visually investigated to differentiate the 6 different emotions: happiness, sadness, surprise,  disgust,  anger  and  neutral,  and  are  used  to  train  the  3D-CNN. Similarly, the corresponding EEG signals are used to train the GT2FS classifier.

A.1 Offline Image Data Analysis : Six basic emotional states: happiness, sadness, surprise, disgust, anger and neutral are considered for this study as presented in Fig. 4. A notable change in the area covered by the eye opening, as well as structure of lip contour is present in the figure. The figure has been given as an example to show that how for 'sad ' emotion the  eye-opening  region  is  getting  smaller,  while  the  same  region  for 'surprise ' emotion is getting much wider. Whereas considering facial expression  related  to  'happiness ' emotion,  lip  contour  is  completely

different than that of 'anger ' . Therefore, we use the cropped images of eye contour and lip contour as the local detailed information for the 3DCNN and the full face image as global information of the 3D-CNN.

The first experiment aims at assessing the electrical activations of the intra-cortical distribution using sLORETA software from the EEG signal acquired during different emotional state changes while playing android games (Levenson, 2014). The top and bottom views of the sLORETA solutions for the 6 emotions: happiness, sadness, surprise, anger, disgust and neutral are portrayed in Fig. 5(a -f). In the color label, white color designates  deactivation,  whereas  yellow  indicates  highest  activation. Analyzing the sLORETA solutions, it is observed that pre-frontal, frontal and temporal brain region remains highly active during the arousal of all the 5 emotions (except neutral), whereas only occipital lobe activation is found during neutral (no cognitive task/neutral) condition, as shown in Fig. 5(f).

A.2 Offline EEG Data Analysis :  The filtered EEG signal is analyzed offline to i) identify the brain regions activated during different emotion arousals using sLORETA software and 2) validate the supremacy of the proposed CSP algorithm in extracting relevant EEG features.

The second experimental fact is reported in Fig. 6 and Table 2 in terms of overall performance analysis of the two variants of CSP algorithms:  standard  CSP  and  the  proposed  phase-sensitive  CSP  and  the other  state-of-the-art  EEG  feature  extraction  techniques  reported  in literature till date. The feature-level discrimination ability of the standard and the proposed CSP algorithms are examined by plotting the respective  feature  values  averaged  over  all  the  trials  for  6  distinct emotion classes as shown in Fig. 6 (a) and (b) respectively. The figure clearly explains that the introducing phase information in formulating CSP gives better discrimination among the 6 different emotional states.

For the proposed FT2FDNN classifier, the values of the three classifier parameters: percentage classification accuracy (%CA), True Positive Rate (TPR) and False Positive Rate (FPR) (Mauss & Robinson, 2009) obtained for each feature extraction technique are tabulated in Table 2. The proposed CSP achieves the best accuracy above 87% and higher TPR and  lower  FPR  values  indicating  the  supremacy  over  the  other algorithms.

## 4.2. Online data analysis

The online data analysis includes the performance evaluation of the proposed  approach  in  real-time,  thoroughly  explained  in  section  5. Similar to the offline experiment, same procedure is followed for each trial in the online experiment. The only difference is that during each trial of 10 min duration, after every 1 min epoch, the emotional state of the  player  is  recognized  by  capturing  facial  images  using  the  video camera and collecting the corresponding EEG signals of the intended 1 min duration. The proposed FT2FDNN, previously trained with offline training instances, is used to determine the overall emotional state of the player which helps understanding the impact (positive/negative) of the game onto the player. The stimulus diagram for online data analysis is depicted in Fig. 7. Data flow diagrams for offline and online data analysis are shown in Fig. 8.

## 5. Performance evaluation of the proposed FT2FDNN

The performance of the FT2FDNN classifier with the existing ones has been evaluated in this section.

## 5.1. Parameters of the FT2FDNN model and comparative study with the existing models

The proposed fused model combines the architectural features of both the GT2FS and 3D-CNN modules and thus the parameters along with  their  dimensions  of  individual  network  module  is  presented  in Table 3. Here, n is the number of input features of the GT2FS.

Performance evaluation of the proposed FT2FDNN algorithm against

L. Ghosh et al.

the existing state-of-the-art classifiers has been done with respect to the %CA, TPR and FPR values. The parametric values obtained by each of the 6 classifier algorithms are compared in Table 4. It is apparent from the  table  that  FT2FDNN  yields  best  results  in  all  the  three  phases: training, validation and testing phases for the present problem.

## 5.2. Convergence Analysis of the training

The experiment deals with investigating the convergence property of the FT2FDNN. To perform the experiment, the training iterations are varied from 0 to 100 and plotted against the respective LF values of the in-sample (training) and out-of-sample (testing) data for the two databases, as presented in Fig. 9. The figure clearly explains that the training curve  of  the  proposed  FT2FDNN  convergences  after  50  iterations whereas the testing curve shows significant oscillations at the end for both the datasets due to the over-fitting phenomenon. Therefore, we consider only 50 iterations in experiments to maintain the effectiveness and efficacy of the FT2FDNN.

## 5.3. Verifications by alterative fusion approcahes

The FT2FDNN is composed of two parts: image processing by 3DCNN  and  EEG  signal  processing  by  GT2FS.  In  this  experiment,  we employ alternative approaches for either or both of these two parts and compare the corresponding fused neural networks with the proposed one. Instead of 3D-CNN, we employ the other well-known image/video data  processing  algorithms  like  Long  Short-Term  Memory  (LSTM) network, Recurrent neural network (RNN) in fusion with the proposed GT2FS and compare their performances with the proposed one. On the other hand, the GT2FS plays the role of reducing the data ambiguity (uncertainty)  for  EEG  signal  processing.  There  exists  another  widely used  approach,  called  Information  Theoretic  (IT)  learning  for  uncertainty handling. Thereby, we examine the performance of a famous IT feature learning network (Torkkola, 2003) in comparison with GT2FS. Apart from this, Interval type 2 fuzzy set is also used to replace the GT2FS. Table 5 includes the results of comparison.

## 5.4. Parametric sensitivity study of the FT2FDNN

The parametric sensitivity of the FT2FDNN has been analyzed by investigating how the hyper-parameter values of the LF: /uni03B2 and /uni03B3 affect the  performance  of  the  FT2FDNN.  To  carry  out  the  experiment,  the parameters are initially set to /uni03B2 , /uni03B3 = {0.01, 0.1} and are tuned one by one in an iterative manner until one of the two converges and then is finetuned using grid search algorithm. The results of the sensitivity analysis of the two parameters with respect to the classification accuracy for the two datasets are given in Fig. 10. It follows from the results that the highest accuracy is obtained at their optimum values: /uni03B2 = 0.5 and /uni03B3 = 1.4.

## 5.5. Statistical test

We employ the Friedman 2-way statistical test (Shiota et al., 2011) to validate the comparative performances of the classifier algorithms. Each of the 7 algorithms, enlisted in Table 6, is ranked according to the median of the classifier accuracies obtained for the two datasets. Let the null hypothesis, H 0, here be that the medians of classifier accuracies, achieved  by  the  7  distinct  classifiers  for  both  the  datasets,  are  not significantly different. The Friedman ' s statistic score, denoted by /uni03C7 2 F , is given by

/uni03C7 2 F = 12 D z ( z + 1 ) [ ∑ z i = 1 ( 1 7 ∑ 7 j = 1 r j i ) 2 GLYPH<0> z ( z + 1 ) 2 7 ] (29)

where z is the no. of algorithms to be compared (here, 7), D represents

the no. of datasets (here, 2). Here r j i denotes the rank of ith algorithm for the jth  dataset.  The  algorithm  with  smallest  rank  indicates  the  best performer.

## 5.6. Understanding the impact of playing different android games on emotional changes of subject

This experiment is carried out to understand the difference in subject ' s emotional state changes for playing different android games. On each day of the experiment, the average values of each emotion for the two  different  games:  Candy  Crush  Saga  and  Stickman  Archers  are plotted using bar charts (Fig. 11). Based on the 30 days ' experiment for all the 35 participants, it can be easily observed that the two games have distinct effects on human emotions. From the graphical representation, it is very much clear that the positive emotions are generated more for the Candy Crush saga and negative emotions are overpowering day-byday for the Stickman Archers.

## 6. Discussion

The proposed work is dedicated to recognize 6 different emotional states for android game players. The advantages of the described work can be stated based on the below mentioned parameters.

- a) Robustness analysis: The classification of the emotional states is done using a multi-modal fusion model using GT2FS and 3D-CNN. If any classifier suffers from overfitting and/or underfitting, then that reflects in the performance of the classification accuracy. But, in the proposed work, 87.58% classification accuracy is  obtained  which negates the occurrence of overfitting and underfitting and validates the robustness of the proposed method.
- c) Benefits: The  main  inspiration  behind  this  work  is  to  tackle  the constantly increasing problem developing in the young individuals of addiction towards gadgets. If the 'unhealthy ' games can be identified, who have long term negative impacts on our mind then that could be beneficial to the whole society.
- b) Convenience: The proposed architecture has minimal hardware setup (a 21-channel EEG sensor and a mobile phone ' s camera is required). Further, the work is independent of the gender and age of the subjects. So, this makes the system very much convenient to deal with the present-day emerging problem of mobile phone addiction, which many times lead to depression in individuals.
- d) Productivity :  The  system  proposed  can  able  to  correctly  identify emotion classes based on EEG and facial expression data.
- f) Flexibility : The proposed algorithm is applied for a large number of participants who have different mind sets towards playing android games.
- e) Convergence : The computation time of 5 s on Intel Core i5 8th Generation processor @ 1.60 GHz and 4 GB RAM running on Matlab R2017b software is quite small fin respect to using CNN.
- g) Feasibility :  The  work  can  be  easily  applicable  to  a  larger  mass  of people, so feasibility is not an issue.
- i) Diversity : The proposed system has been implemented on a diverse group of subjects.
- h) Efficiency : The efficiency of the proposed method has been compared with a wide range of already available algorithms and every time the work proves its worth.
- j) Reliability :  The proposed approach is already realized on the realworld scenario.

## 7. Conclusion

The paper presents a novel method of real-time emotion recognition of android-gamers using the concept of multi-modal fusion. The proposed technique works automatically by fusing the results obtained by

L. Ghosh et al.

EEG classification using GT2FS and facial expression classification by 3D-CNN. To the best of the authors ' knowledge, this is a novel attempt where  the  brain  signals  as  well  as  facial  expressions  are  measured simultaneously  for  understanding  how  android  games  are  playing  a significant role in changing their moods. The work produces significant outcomes in comparison with state-of-the art literatures. Besides this, the proposed method is statistically validated by Friedman statistical test with a confidence level of 95%.

The  proposed  research  findings  have  incredible  applications  in demonstrating the psychosomatic changes in gamers, who used to spend quite a lot of their times in playing android games. A gamming application can be rated based on the experimental evaluation such that how it affects the emotional states of a gamer during the execution of the game. If negative emotions (such as sadness, disgust and anger) dominate over the positive ones (such as happiness and surprise), then it can be easily concluded that the game can impose adversarial effects on the gamers.

## Credit author statement

Lidia Ghosh: Methodology, Writing -original draft, Validation, Data curation,  Software;  Sriparna  Saha:  Visualization,  Writing -original draft, Software, Data curation, Investigation; Amit Konar: Conceptualization.

## Acknowledgment

The first and third authors thankfully acknowledged the fund provided by Ministry of Human Resource Development for RUSA-II project granted to Jadavpur University, India. The second author is thankful to the  University  for  providing  research  seed  money  and  UGC  Start-up Grant  under  the  scheme  of  Basic  Scientific  Research.  The  work  is approved by Institutional Ethics Committee.

## References

- Abdi, H., & Williams, L. J. (2010). Principal component analysis. Wiley Interdiscip. Rev. Comput. Stat., 2 (4), 433 -459.
- Anderson, C. A., et al. (2010). Violent video game effects on aggression, empathy, and prosocial behavior in eastern and western countries: A meta-analytic review. Psychological Bulletin, 136 (2), 151.
- Acharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., & Adeli, H. (2018). Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals. Computers in Biology and Medicine, 100 , 270 -278.
- Anttonen, J., & Surakka, V. (2005). Emotions and heart rate while sitting on a chair. In Proceedings of the SIGCHI conference on Human factors in computing systems (pp. 491 -499).
- Balters, S., & Steinert, M. (2017). Capturing emotion reactivity through physiology measurement as a foundation for affective engineering in engineering design science and engineering practices. Journal of Intelligent Manufacturing, 28 (7), 1585 -1607.
- Arriaga, P., Monteiro, M. B., & Esteves, F. (2011). Effects of playing violent computer games on emotional desensitization and aggressive behavior 1. Journal of Applied Social Psychology, 41 (8), 1900 -1925.
- Carnagey, N. L., Anderson, C. A., & Bushman, B. J. (2007). The effect of video game violence on physiological desensitization to real-life violence. Journal of Experimental Social Psychology, 43 (3), 489 -496.
- Cowie, R., & Cornelius, R. R. (2003). Describing the emotional states that are expressed in speech. Speech Communication, 40 (1 -2), 5 -32.
- Coulson, M. (2004). Attributing emotion to static body postures: Recognition accuracy, confusions, and viewpoint dependence. Journal of Nonverbal Behavior, 28 (2), 117 -139.
- Delorme, A., & Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of singletrial EEG dynamics including independent component analysis. Journal of Neuroscience Methods, 134 (1), 9 -21.
- Fredrickson, B. L., Tugade, M. M., Waugh, C. E., & Larkin, G. R. (2003). What good are positive emotions in crisis? A prospective study of resilience and emotions following the terrorist attacks on the United States on september 11th, 2001. Journal of Personality and Social Psychology, 84 (2), 365.
- FakhrHosseini, M., Jeon, M., & Bose, R. (2015). Estimation of drivers ' emotional states based on neuroergonmic equipment: An exploratory study using fNIRS. In Adjunct proceedings of the 7th international conference on automotive user interfaces and interactive vehicular applications (pp. 38 -43).
- Friesen, E., & Ekman, P. (1978). Facial action coding system: A technique for the measurement of facial movement. Palo Alto, 3 .

glyph<c=11,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=21,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=19,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=21,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=20,font=/YUSQVD+TimesNewRomanPS-ItalicMT>glyph<c=12,font=/YUSQVD+TimesNewRomanPS-ItalicMT>

- Ghosh, L., Konar, A., Rakshit, P., & Nagar, A. K. (2018). Hemodynamic analysis for cognitive load assessment and classification in motor learning tasks using type-2 fuzzy sets. IEEE Trans. Emerg. Top. Comput. Intell., 3 (3), 245 -260.
- Gottman, J. M., & Krokoff, L. J. (1989). Marital interaction and satisfaction: A longitudinal view. Journal of Consulting and Clinical Psychology, 57 (1), 47.
- Ghosh, L., Rakshit, P., & Konar, A. (2019). Working memory modeling using inverse fuzzy relational approach. Applied Soft Computing, 83 , 105591.
- Halder, A., Rakshit, P., Chakraborty, A., Konar, A., & Janarthanan, R. (2011). Emotion recognition from the lip-contour of a subject using artificial bee colony optimization algorithm. In Swarm, evolutionary, and memetic computing (pp. 610 -617). Springer.
- Homma, I., & Masaoka, Y. (2008). Breathing rhythms and emotions. Experimental Physiology, 93 (9), 1011 -1021.
- Hasan, Y., B ' egue, L., Scharkow, M., & Bushman, B. J. (2013). The more you play, the more aggressive you become: A long-term experimental study of cumulative violent video game effects on hostile expectations and aggressive behavior. Journal of Experimental Social Psychology, 49 (2), 224 -227.
- Huang, Y., Yang, J., Liao, P., & Pan, J. (2017). Fusion of facial expressions and EEG for
- Kesler, M. L., Andersen, A. H., Smith, C. D., Avison, M. J., Davis, C. E., Kryscio, R. J., & Blonder, L. X. (2001). Neural substrates of facial emotion processing using fMRI. Cognitive Brain Research, 11 (2), 213 -226.
- multimodal emotion recognition. Computational Intelligence and Neuroscience , 2017. Huang, J., Zhou, W., Zhang, Q., Li, H., & Li, W. (2018). Video-based sign language recognition without temporal segmentation. In Thirty-second AAAI conference on artificial intelligence .
- Kühn, S., Kugler, D. T., Schmalen, K., Weichenberger, M., Witt, C., & Gallinat, J. (2019). Does playing violent video games cause aggression? A longitudinal intervention study. Molecular Psychiatry, 24 (8), 1220 -1234.
- Lawrence, S., Giles, C. L., Tsoi, A. C., & Back, A. D. (1997). Face recognition: A convolutional neural-network approach. IEEE Transactions on Neural Networks, 8 (1), 98 -113.
- Laukka, S. J., J arvilehto, T., Alexandrov, Y. I., & Lindqvist, J. (1995). Frontal midline theta related to learning in a simulated driving task. Biological Psychology, 40 (3), 313 -320.
- Levenson, R. W. (2014). The autonomic nervous system and emotion. Emotion Review, 6 (2), 100 -112.
- Maclin, E. L., et al. (2011). Learning to multitask: Effects of video game practice on electrophysiological indices of attention and resource allocation. Psychophysiology, 48 (9), 1173 -1183.
- Liu, X., Cheung, Y., Li, M., & Liu, H. (2010). A lip contour extraction method using localized active contour model with automatic parameter selection. In 2010 20th international conference on pattern recognition (pp. 4332 -4335).
- Mauss, I. B., & Robinson, M. D. (2009). Measures of emotion: A review. Cognition & Emotion, 23 (2), 209 -237.
- Messias, E., Castro, J., Saini, A., Usman, M., & Peeples, D. (2011). Sadness, suicide, and their association with video game and internet overuse among teens: Results from the youth risk behavior survey 2007 and 2009. Suicide and Life-Threatening Behavior, 41 (3), 307 -315.
- Mendel, J. M. (2013). General type-2 fuzzy logic systems made simple: A tutorial. IEEE Transactions on Fuzzy Systems, 22 (5), 1162 -1182.
- Murugappan, M., Rizon, M., Nagarajan, R., Yaacob, S., Hazry, D., & Zunaidi, I. (2008). Time-frequency analysis of EEG signals for human emotion detection. In 4th Kuala Lumpur international conference on biomedical engineering 2008 (pp. 262 -265). Berlin, Heidelberg: Springer.
- Petrantonakis, P. C., & Hadjileontiadis, L. J. (2010). Emotion recognition from brain signals using hybrid adaptive filtering and higher order crossings analysis. IEEE Trans. Affect. Comput., 1 (2), 81 -97.
- Peng, K., Chen, L., Ruan, S., & Kukharev, G. (2005). A robust agorithm for eye detection on gray intensity face without spectacles. Journal of Computer Science and Technology, 5 .
- Pollatos, O., Herbert, B. M., Matthias, E., & Schandry, R. (2007). Heart rate response after emotional picture presentation is modulated by interoceptive awareness. International Journal of Psychophysiology, 63 (1), 117 -124.
- Prucnal, M. A., & Polak, A. G. (2018). Analysis of features extracted from EEG epochs by discrete wavelet decomposition and Hilbert transform for sleep apnea detection. In 2018 40th annual international conference of the IEEE engineering in medicine and biology society (pp. 287 -290). EMBC).
- Prescott, A. T., Sargent, J. D., & Hull, J. G. (2018). Metaanalysis of the relationship between violent video game play and physical aggression over time. Proceedings of the National Academy of Sciences, 115 (40), 9882 -9888.
- Przybylski, A. K., & Weinstein, N. (2019). ' Violent video game engagement is not associated with adolescents ' aggressive behaviour: Evidence from a registered report. R. Soc. open Sci., 6 (2), 171474.
- Rakshit, P., Saha, S., Konar, A., & Saha, S. (2016). A type-2 fuzzy classifier for gesture induced pathological disorder recognition. Fuzzy Sets and Systems, 305 . https://doi. org/10.1016/j.fss.2016.05.001
- Purves, D., Augustine, G., Fitzpatrick, D., William, W. C., Anthony-Samuel, L., & White, L. E. (2012). Neuroscience . Sunderland, MA: Sinauer Associates.
- Ryan, R. M., Rigby, C. S., & Przybylski, A. (2006). The motivational pull of video games:
- Saha, S., Konar, A., Saha, A., Sadhu, A. K., Banerjee, B., & Nagar, A. K. (2016). EEG based gesture mimicking by an artificial limb using cascade-correlation learning architecture. In Proceedings of the international joint conference on neural networks (Vol. 2016). https://doi.org/10.1109/IJCNN.2016.7727814. Octob.
- A self-determination theory approach. Motivation and Emotion, 30 (4), 344 -360. Saha, S., & Karia, J. (2019). Analyzing adverse gaming effects on emotions using neural networks based hybrid architecture. In 2019 IEEE 16th India council international conference (INDICON) (pp. 1 -4).

L. Ghosh et al.

Sarlo, M., Palomba, D., Buodo, G., Minghetti, R., & Stegagno, L. (2005). Blood pressure changes highlight gender differences in emotional reactivity to arousing pictures. Biological Psychology, 70 (3), 188 -196.

- Schneider, E. F., Lang, A., Shin, M., & Bradley, S. D. (2004). Death with a story: How story impacts emotional, motivational, and physiological responses to first-person shooter video games. Human Communication Research, 30 (3), 361 -375.

Savran, A., et al. (2006). Emotion detection in the loop from brain signals and facial images. In Proceedings of the eNTERFACE 2006 workshop .

Shiota, M. N., Neufeld, S. L., Yeung, W. H., Moser, S. E., & Perea, E. F. (2011). Feeling good: Autonomic nervous system responding in five positive emotions. Emotion, 11 , 1368-6.

- Sokolov, S., Velchev, Y., Radeva, S., & Radev, D. (2017). ' Human emotion estimation from EEG and face using statistical features and SVM. In Proc. Int. Conf. Comput. Sci. Inf. Technol. (pp. 37 -47).

- T Dasborough, M., Sinclair, M., Russell-Bennett, R., & Tombs, A. (2008). Measuring emotion: Methodological issues and alternatives .
- Torkkola, K. (2003). Feature extraction by non-parametric mutual information maximization. Journal of Machine Learning Research, 3 (Mar), 1415 -1438.
- Todoran, G., Holonec, R., & Iakab, C. (2008). Discrete hilbert transform. numeric algorithms. Acta Electrotehnica, 49 (4), 485 -490.
- Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision (pp. 4489 -4497).

Zheng, W.-L., Dong, B.-N., & Lu, B.-L. (2014). Multimodal emotion recognition using EEG and eye tracking data. In 2014 36th annual international conference of the IEEE engineering in medicine and biology society (pp. 5040 -5043).

- Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54 (6), 1063.