Future Generation Computer Systems 74 (2017) 76–85
ELSEVIER
Contents lists available at ScienceDirect
# Future Generation Computer Systems
journal homepage: www.elsevier.com/locate/fgcs

# Multi-key privacy-preserving deep learning in cloud computing

Ping Li a, Jin Li a,∗, Zhengan Huang a, Tong Li b, Chong-Zhi Gao a, Siu-Ming Yiu c, Kai Chen d
a School of Computational Science & Education Software, Guangzhou University, 510006, Guangzhou, PR China
- b College of Computer & Control Engineering, Nankai University, 300071, Tianjin, PR China
- c Department of Computer Science, The University of Hong Kong, Hong Kong, PR China
- d Institute of Information Engineering, Chinese Academy of Sciences, Beijing, PR China
h i g h l i g h t s
- In the basic scheme, we use M-FHE as our privacy-preserving technique. Only the decrypt operation needs the interaction among data owners.
- • In the advanced scheme, we propose a hybrid structure scheme by combining the double decryption mechanism and FHE.
- • In the advanced scheme, only the encrypt and decrypt algorithms are performed by data providers.
- • We prove that these two multi-key privacy-preserving deep learning schemes over encrypted data are secure.
a r t i c l e
i n f o
# ARTICLE INFO
a b s t r a c t
Article history: Received 21 December 2016 Received in revised form 28 January 2017 Accepted 6 February 2017 Available online 22 March 2017
# Keywords:
Cryptography Machine learning Fully homomorphic encryption Cloud computing
Deep learning has attracted a lot of attention and has been applied successfully in many areas such as bioinformatics, imaging processing, game playing and computer security etc. On the other hand, deep learning usually requires a lot of training data which may not be provided by a sole owner. As the volume of data gets huge, it is common for users to store their data in a third-party cloud. Due to the confidentiality of the data, data are usually stored in encrypted form. To apply deep learning to these datasets owned by multiple data owners on cloud, we need to tackle two challenges: (i) the data are encrypted with different keys, all operations including intermediate results must be secure; and (ii) the computational cost and the communication cost of the data owner(s) should be kept minimal. In our work, we propose two schemes to solve the above problems. We first present a basic scheme based on multi-key fully homomorphic encryption (MK-FHE), then we propose an advanced scheme based on a hybrid structure by combining the double decryption mechanism and fully homomorphic encryption (FHE). We also prove that these two multi-key privacy-preserving deep learning schemes over encrypted data are secure.
© 2017 Elsevier B.V. All rights reserved.
# 1. Introduction
Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storing, networking and analytical software. The application of these resources has fostered impressive Big Data advancements [1, 2] and Internet of Things (IoT). Because of the wide range of applications, people have paid more attention to cloud security for data management, data storing, and data processing in recent decades. Recently, deep learning has made impressive success in a wide range of applications, such as bioinformatics, image processing, game playing, natural language processing, computer
∗
Corresponding author.
E-mail address: jinli71@gmail.com (J. Li).
security etc. On the other hand, in order to get an accurate result without over-fitting, deep learning requires a lot of training records to determine tens of thousands of parameters. In many cases, the massive amount of training data cannot be provided by one single user, but collected from different users. The trend is to have these huge datasets stored in an untrusted third-party cloud system in encrypted form due to the confidentiality and sensitivity of the datasets.
To also leverage the computing ability provided by the cloud platform, more and more applications choose to conduct the deep learning on the cloud platform. In fact, if the amount of training records is huge, it is difficult, to download all records by a single user for processing. This poses a new challenge to the community, i.e., how to perform deep learning over outsourced encrypted data owned by multiple users in cloud. In this paper, we consider to develop a multi-key privacy-preserving deep neural network
http://dx.doi.org/10.1016/j.future.2017.02.006 0167-739X/© 2017 Elsevier B.V. All rights reserved.
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
in cloud over encrypted data. The key challenges include (1) Data are located in different places and encrypted with different keys. To protect data privacy, all computation (e.g. inner product and the approximation of nonlinear sigmoid function used in deep learning), intermediate results generated during the deep learning process and the learning results must be secure. (2) To improve the efficiency of the deep learning process, computation should be done by the cloud server so as to decrease the computation/communication cost of the data owner(s). Existing solutions such as secure multi-party computation (SMC) [3], encryption schemes, garbled circuit, and detective controls were designed for other scenarios and cannot be applied directly to tackle these two challenges.
Organization. The rest of this paper is organized as follows. In Section 2, we briefly discuss the related work. Some notations, including deep learning, stochastic gradient descent, BCP scheme, FHE, and MK-FHE will be described in Section 3. We give the system model definition and describe the details of our privacy-preserving deep learning system in Section 4 and Section 5, respectively. Section 6 shows the complexity and security analysis for the proposed system. And we give an application in our system in Section 7. Finally, we conclude the paper in Section 8.
# 2. Related work
2.1. Deep learning
Our Contributions. To solve the above challenges, this paper designs two schemes to support multi-key learning system. Both schemes allow multiple data owners with different datasets to collaboratively learn a neural network model securely in cloud computing. To protect the confidentiality, data owners encrypt their sensitive data with different public keys before uploading to cloud server.
We first propose a basic scheme which is based on multi-key fully homomorphic encryption (MK-FHE) [4–6]. In this scheme, multiple data owners send their data (encrypted with different public keys chosen by data owners independently of each other) to an untrusted cloud server. Cloud server computes the output of deep learning on this joint data and issues it back to all participating data owners. Finally, all of the data owners jointly perform a secure SMC protocol to decrypt and extract results from this encrypted deep learning results.
To avoid the interaction among multiple data owners, we further propose an advanced scheme which is based on a hybrid structure by combining the double decryption mechanism (BCP scheme [7]) and fully homomorphic encryption (FHE) [8]. If we only use BCP scheme to support the secure computation, in the training phase, both the computation of inner product of the inputs and weights, and the computation of the activation function require additional communication with the cloud server. To solve this challenge, we introduce FHE scheme directly by transforming BCP ciphertext into FHE ciphertext, such that the computations over FHE ciphertext can be realized without interaction. In this scheme, a cloud server C and an authorized center (a trusted third party) AU are queried, which is assumed to be non-colluding and honest-but-curious. The cloud server C keeps the encrypted datasets under different public keys uploaded by multiple data owners. The authorized center AU, on the other hand, only holds the master key of the master decryption of BCP scheme and the private key of FHE. In this paper, all participants are assumed to be honest-but-curious.
In summary, our contributions can be summarized as follows:
- • We address a multi-key privacy-preserving deep learning in cloud computing by proposing two schemes, which allow multiple data owners to conduct collaboratively privacy- preserving deep learning.
- • Our multi-key privacy-preserving deep learning schemes are able to preserve the privacy of sensitive data, intermediate results as well as the training model.
- • We provide a security analysis to guarantee the privacy- preserving of our proposed two schemes.
- • We give an application of our advanced scheme in face recognition. Note that our solutions are generic and can be applied to perform many other machine mining with the same setting over the same setting.
In cloud computing, deep learning has shown its success in many cases such as image recognition [9,10], speech recogni- tion [11], and biomedical data analysis [12]. Deep learning is able to transform the original data into a higher level and more ab- stract expression. It means that high-dimensional original data can be converted to low-dimensional data by training a multi- ple neural network with a small central layer to reconstruct high- dimensional input data. Through these transformations, compli- cated functions can be learned by composing many simple func- tions. Hinton et al. [13] showed that multiple hidden layers of arti- ficial neural network have excellent characteristics of learning abil- ity. The characteristic obtained by learning are more intrinsic char- acterization of the data that facilitates an improved visualization or classification of the data. They also showed that the difficulty to op- timize the weights in nonlinear auto-encoders can be overcome by layer-by-layer ‘‘pretraining’’ procedure. Usually, deep learning ar- chitectures are constructed as multi-layer neural networks. There are several different neural architectures, such as the feed-forward neural network, Recurrent Neural Network (RNN), and Deep Belief Network (DBN).
2.2. Privacy-preserving machine learning
With the advance of cloud computing, some related works have addressed some security problems in cloud, such as the security for the cloud framework [14,15], location privacy in mobile cloud [16, 17], security in cloud storage [18–20], data mining [21–24] and machine learning [25–27]. Existing privacy-preserving techniques are based mainly on data perturbation method and cryptographic methods (such as secure multi-party computation and secure function evaluation).
In the data perturbation method, differential privacy [28– 30] has been widely applied to protect privacy of statistical database. Generally speaking, differential privacy guarantees that the removing or adding one record (usually known as noise) does not (substantially) affect the outcome of any usefully analysis. Therefore no risk is incurred by joining the database, providing a mathematically rigorous means of coping with the fact that distributional information may be disclosive. For instance, Abadi et al. [31] proposed a new algorithms, which are based on differential privacy version of stochastic gradient descent (SGD) process. In their work, scaled noise is added to the computed gradient to prevent information leakage. They also implemented the model on several neural networks and analyzed the privacy leakage. In [32], the authors considered another approach, in which they proposed a distributed selective SGD by collecting computed gradients from different parties. Then the users update the parameters of deep learning model selectively according to the collected gradients. The selective SGD can ensure data privacy because the computation process is held locally and only gradients will be reported to the central server. However, it is more difficult for selective SGD to achieve the global/local optimal compared to
77
78
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
use conventional SGD with the entire dataset. In other words, the data are not fully utilized in selective SGD.
For the cryptographic methods, the proposed schemes based on privacy-preserving machine learning have been presented recently. In these existing schemes, there are two different settings: (1) training without the aid of cloud/third-party, and (2) training with the aid of cloud/third-party. In the first setting, the authors in [33,34] proposed a privacy-preserving two-party distributed algorithm for back-propagation training with vertically partitioned data and arbitrarily partitioned data, respectively. They used ElGamal scheme to support the secure computation operations. In the second setting, the authors used the SMC technique to train the horizontal partitioned data for multi- party case [35]. Graepel et al. [27] demonstrated that some basic machine learning algorithms, such as simple linear classifiers, can be performed efficiently over a small scale encrypted datasets. However, the efficiency will be degraded rapidly when the input size grows large. Yuan et al. [36] adopted a doubly homomorphic encryption scheme (BGN) [37] and proposed a system in which the training phase can be securely delegated to a cloud system for the multi-party scenario. In [38], the authors used BGV scheme [39] to support the secure computation operations and realized a high- order back-propagation algorithm efficiently for deep computation model training on the cloud.
Algorithm 1 Multi-layer Back-propagation network learning
Input: input sample x, target vector t, learning rate η, sigmoid
- function f (x), and network depth l;
(i), i ∈ {1, 2, · · · , l}
- Output: the weight matrices of the model: W
- 1: Feed Forward Stage:
(0) = x;
- 2: h
- 3: for k = 1 to l do
v(k) = W
(k)
(k−1)
- h 4: ;
(k) = f (v(k));
- h 5:
- 6: end for
(l)
7: y = h
8: E = E(t, y)//compute the cost function
- 9: Back-propagation Stage:
10: e ← ∇yE = ∇yE(t, y)//compute the gradients of E with
- parameter W
11: for k = l to 1 do
- e ← ∇v(k) E = ef ′(v(k))//convert the gradient on the layer’s 12: output into the gradient pre-nonlinearity activation
(k−1)τ
13: ∇W
- (k) E = eh //compute the gradients on weights
- e ← ∇(k−1) (k)τ E = W e//modify the next lower-level hidden 14: h layer’s activations
# 15: end for
Recently, researchers also proposed a CryptoML [40] frame- work for secure delegation of iterative machine learning to un- trusted cloud servers. This secure delegation protocol is based on Shamir’s secret sharing model. However, none of the existing crypto-based schemes are able to deal with data encrypted with different public keys. In our work, we propose two solutions to tackle this challenge. We show how to achieve privacy-preserving deep learning for training encrypted datasets under different public keys. In our two schemes, the data owners encrypt the date before uploading it to the cloud server. Most of the computation is per- formed by the cloud server and only the data owners are able to obtain the final results of the training model.
# 3. Preliminaries
# 3.1. Deep learning
Output Layer ( Hidden Layer Hidden Layer Input Layer
Deep learning can be viewed as a multi-layer neural network. The input data or variables that we are able to observe is presented at the input layer. There are also several hidden layers, which extract increasingly abstract features from the input layer. They are called ‘‘hidden’’ because the parameters for these layers are not given in the data. During the learning process, the model must determine which features are useful for explaining the relationships in the input data. Precisely, we take a vector of real-valued as input, calculate a linear combination of these vector and corresponding weight, and the output of the neuron by applying a nonlinear activation function to the total input value is defined as
Fig. 1. Configuration of neural network with two hidden layers and two output nodes.
exp(z)), which can be used to produce the β or σ parameter of a normal distribution because its range is (0, ∞).
Gradient-Based Optimization. Most deep learning algorithms involve optimization problem. Optimization problem aims to solve the task of either minimizing or maximizing some function f (x). Hence, this function f is called objective function. If we choose to minimize it, then this function is called the error function, loss function, or cost function.
xk = f (xk−1Wk + b)
where f is an activation function and Wk is a real-valued constant matrix in hidden layer k, or weight matrix, which determines the contribution of each input to the output. Parameter b is called bias, which guarantees that the input sum is greater than 0. Algorithm 1 describes the learning process of a multi-layer back-propagation neural network and Fig. 1 shows a neural network with two hidden layers and two output nodes.
One of the activation functions is the logistic sigmoid f (z) = 1 1+exp(−z) . Usually, the logistic sigmoid function is used to produce the φ parameter of a Bernoulli distribution due to its range is (0, 1). Another activation function is softplus function, ζ (z) = log(1 +
(1)
Assuming that there is a function y = f (x), where x is vector, ∂y and y is a real number. Let f ′(x) or ∂x denote the derivative of this function f , its Taylor series expansion is
f (x + ε) ≈ f (x) + ε · f ′(x). (2)
The above Eq. (2) shows how to scale a small change ε in the input data x, in order to make a small change in y. Therefore, we can use the derivative for minimizing a function. This technique is called gradient descent. If f ′(x) = 0, then the tangent orientation at point x is 0, it means that no information provided about what direction to move. Such points are called critical points, which can be classified as local minimum point and global minimum point.
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
Stochastic gradient descent (SGD). When we perform the gradient descent algorithm, there are two problems to be considered: the problem of the speed of the convergence to a local minimum and the problem in the error surface, there are many local minima error does not mean that the global minimum error is found. Hence, to save the computation per weight update step, a good idea for optimizing the algorithm is to use some training samples at a time, i.e., the weights are updated upon examining each individual training example. This optimization algorithm is called stochastic gradient descent (SGD), which can be seen as an extension of the gradient descent algorithm.
- • (pk, sk) ← KeyGen(pp): choose a random element a ∈ Z N 2 and compute the user’s public key h = g a mod N 2, where the user’s private key is sk = a. Finally the algorithm outputs (pk, sk).
- • (A, B) ← Enc(pp,pk)(m): given a message m ∈ ZN and pick a N 2 , and outputs the ciphertext (A, B), random element r ∈ Z where A = g r mod N 2 and B = hr (1 + mN) mod N 2.
- • m ← Dec(pp,sk)(A, B): given a ciphertext (A, B) and private key sk = a, it returns the message m as
m = Aa − 1 mod N 2 B N
In general, the gradient in stochastic gradient descent is viewed as an expectation. This expectation can be calculated by using a subset of training sample set. In more details, assume that n is the training sample set size, we choose a subset, denoted as X = {X1, . . . , Xn′ }, which is uniformly sampled from the training sample set. The size n′ is called mini-batch, and is smaller than n. Usually, n′ is fixed, and independent of n. Algorithm 2 shows the detail of the SGD by taking the average gradient on a mini-batch of n examples.
or the special message ‘‘reject’’ if it is invalid ciphertext.
- m ← mDec(pp,pk,mk)(A, B): given a ciphertext (A, B), pubic key
- pk = h and master private key mk, the user’s private key (sk = a) can be computed as
a mod N = hp ′ q ′ − 1 mod N 2 N · k −1 mod N.
In order to remove the random element r ∈ Z N 2 , it is necessary to compute
Algorithm 2 Stochastic gradient descent (SGD)
Input: Learning rate η
# Output: Initial weight parameter w
while stopping criterion not met do
Sample a mini-batch of n′ examples from the training sample set {X1, . . . , Xn′ } with the corresponding target outputs ti. for i = 1 to n′ do
n′ ▽w i E(f (Xi; w); ti); Calculate gradient estimate: y ← 1 end for
# Apply update: w ← wk − η · y
# end while
r mod N = Ap ′ q ′ − 1 mod N 2 N · k −1 mod N
then compute τ = ar mod N. Finally, it returns the message m as
m = B τ g p ′ q ′ − 1 mod N 2 N · ζ −1 mod N
or the special message ‘‘reject’’ if it is invalid ciphertext, where k−1 and ζ −1 denote the inverse of k mod N 2 and p′q′ mod N 2, respectively.
Here, learning rate η is a small positive scalar, which is used to moderate the step size in the gradient descent search. E is the cost function or error function, which is essentially the difference between the target output of the network and output of the objective function. Notice that ▽w E(·) is a vector, and its components are the partial derivatives of E with respect to each of the wj, where wj is the component of weight vector w.
To simplify the notation, we use Enc pk(m) instead of Enc pp,pk(m) and use Add(·) to denote the addition-gate, i.e., (A, B) := (A1 · A2 mod N 2, B1 · B2 mod N 2) = Add(C1, C2), where {Ci}2 {(Ai, Bi)}2 i=1 is a ciphertext set. i=1 =
3.3. Fully homomorphic encryption
3.2. Double decryption mechanism
In a public-key encryption schemes with a double decryption mechanism, there exists two independent decryption algorithms. These two decryption algorithms are called user decryption algorithm (which has the general private key as input) and master entity decryption algorithm (which has the master private key as input), respectively. Taking the master private key as input, the master decryption procedure can decrypt any given ciphertext successfully. We formally define the BCP scheme as follows.
Fully homomorphic encryption (FHE) allows one to compute the encrypted results of addition and multiplication of two plaintexts using the corresponding ciphertexts directly without decryption. Generally speaking, a FHE system E F consists of four algorithms: key generation algorithm F.KeyGen, encryption algorithm F.Enc, decryption algorithm F.Dec, and evaluation algorithm F.Eval. For this evaluation algorithm F.Eval, given a circuit C , a public key pkF, and any ciphertexts ci is generated by F.EncpkF (mi), outputs a refreshed ciphertext c∗ such that F.DecskF (c∗) = C(m1, . . . , mn).
Definition 3.1 (BCP Scheme [7]). There are five algorithms in the BCP scheme, including setup algorithm Setup, key-generation algorithm KeyGen, encryption algorithm Enc, user decryption algorithm Dec and master decryption algorithm mDec, which is defined as E = {Setup, KeyGen, Enc, Dec, mDec}.
- • (pp, mk) ← Setup(1 κ ): given a security parameter κ, let p, q, p′, q′ be distinct odd primes and p = 2p′ + 1 and q = 2q′+1. Set the bit-length as |N| = |pq| = κ. For a multiplication group Z∗ N 2 , the algorithm Setup chooses a random element g ∈ ′ ′ ≡ 1 + kN (mod N 2) for Z∗ N 2 of order pp′qq′ such that g p q k ∈ {1, 2, ·, N − 1}. After this step, the algorithm’s outputs are
public parameter pp = (N, k, g) and
master private key mk = (p ′, q ′).
Suppose that circuit DAddF can handle addition gate, denoted by AddF. If c1 and c2 are two ciphertexts that encrypt m1 and m2, respectively, under pkF, then we can compute
c ← F.Eval(pkF
# , DAddF
, c1, c2)
c < F.Eval(pke, Daddgs C1, C2)
which is a ciphertext under pkF of m1 + m2. Similarly, circuit DMultiF can handle multiplication gate, denoted as MultiF. For ciphertexts c1 and c2 as defined before,
c ← F.Eval(pkF , DMultiF , c1, c2)
is a ciphertext under pkF of m1 × m2. Assume [m] = F.EncpkF [y] = [x + y] = AddF([x], [y]) and [x] ×F[y] = then [x] + F ([x], [y]) denote the addition and multiplication [xy] = MultiF (m), computation of FHE, respectively.
We formally define the multi-key fully homomorphic encryp- tion [4,5] as follows.
79
80
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
Definition 3.2 (Multi-Key Fully Homomorphic Encryption, MK- FHE). For arbitrary circuit class C, a family of encryption schemes {E n = (MF.KeyGen, MF.Enc, MF.Dec, MF.Eval)}n>0 is said to be a multi-key fully homomorphic encryption, if for every n > 0, E n satisfies the following properties:
κ ): given a security
- • (pkMF , skMF, ekMF) ← MF.KeyGen(1 parameter κ, outputs a public key pkMF, a private key skMF and a public evaluation key ekMF.
- • c ← MF.Enc(pkMF , x): for a message x and public key pkMF, this algorithm outputs a ciphertext c.
- • x′ ← MF.Dec(skMF1 , skMF2 , . . . , skMFn , c): given a ciphertext c , . . . , skMFn, this algorithm outputs a and n private keys skMF1 message x′.
and set up the target vector t = {ti}n i=1 and weight matrix W (i) (i) (i) in advance, where ti = (t , t , . . . , t ), j = 1, 2. 1 2 Ii
Because these datasets are highly sensitive, to preserve confidentiality of data, each data owner Pi (i ∈ [1, n]) has to encrypt his/hers data before uploading them to a cloud server. The cloud server will train a model over these encrypted datasets which are encrypted under different public keys. To realize a multi- key privacy-preserving deep learning system, we consider two schemes, i.e., basic scheme and advance scheme. Both schemes ensure the inputs, intermediate results generated during the learning process and final output are secure and no information will be leaked during the whole learning process. In addition, we assume that each data owner stays online with broadband access to the cloud server.
- • c∗ MF.Eval((c1, pkMF1 , ekMF1 ), . . . , (cm, pkMFm ← ), C): taken any boolean circuit C ∈ C, any valid m key ekMFm pairs (pkMF1 , ekMF1 ), . . . , (pkMFm , ekMFm ), and any ciphertexts c1, . . . , cm as input, this algorithm outputs a refreshed cipher- text c∗. ,
The scheme should satisfied the following properties: the cor- rectness of decryption and compactness of ciphertexts. That is to say, for any circuit C ∈ C, the support of MF.KeyGen(1 κ ): n key pairs {(pk′ , sk′ , ek′ )}t∈[1,n] and its any subset of m key MFt MFt MFt pairs {(pkMFi , skMFi , ekMFi )}i∈[1,m], and any valid ciphertext ci ← MF.Enc(pkMFi , xi).
(i ∈ [1, m]), the algorithm MF.Eval holds the properties:
Correctness of decryption: given a tuple of private key ′ ′ , . . . , skMF n, a refreshed ciphertext c∗, the correct decryption skMF 1 is
MF.Dec(skMF ′ 1 , . . . , skMF ′ n , c ∗) = C(x1, . . . , xm),
4.2. Adversary model
In this paper, we assume that the data owners and servers are honest-but-curious, sometimes called semi-honest. It means that all the entities (i.e., all the data owners and the servers) will honestly follow the protocol, and try to gather or discover information about the intermediate results of the learning process by observing the transcripts. And we also assume that all the data owners’ dataset are sensitive and required to be fully protected against the cloud server. Based on this assumption, we consider three kinds of attacks (1) online attack by compromised active data owners, who aim to learn or infer sensitive information on the dataset from other data owners and cloud server; (2) online attack by a compromised active server, which is the cloud server can pretend as participants execute the learning process for every guess; (3) outside attack whose goal is to obtain private information from the data owners and cloud server.
where c∗ ← MF.Eval((c1, pkMF1 ), C). ekMFm , ekMF1 ), . . . , (cm, pkMFm
# Compactness of ciphertexts: let
∗ ← MF.Eval((c1, pkMF1 , ekMF1 ), . . . ,
# c
# (cm, pkMFm
, ekMFm ), C)
be a refreshed ciphertext, the size of c∗ is independent of the parameter m and the size of C , i.e., there is a polynomial f holds |c∗| ≤ f (κ, n), where |c∗| is denoted as the size of c∗.
If n = 1, the Definition 3.2 is the standard definition of FHE scheme. We use AddMF to denote the secure addition gate, which is given the ciphertext c1 and c2 of plaintext m1 and m2 under public key pkMF1 and pkMF2 respectively, the server calculates the MF c2 = AddMF(c1, c2). Similarly, MultiMF denotes the sum as c1 + secure multiplication gate, the server calculates the products as MF c2 = MultiMF(c1, c2). c1 ×
,
In our work, we try to address the privacy-preserving problem of deep learning in cloud computing. The security and privacy of the data owners’ sensitive data, model’s outputs, and intermediate results should be protected during the deep learning process.
Security goals. There are two aspects of the security goals for data privacy and model privacy. The detailed security goals for these two notions are as follows.
- • Privacy of Data. The data privacy should be kept secret even if a subset of data owners and cloud server are corrupted. In more details, no information about the data will be leaked to the adversary.
- • Privacy of training model. The training model should be kept secret and can be only known by data owners. The adversary even with all the information of cloud server, is not able to get the information of the training model.
# 5. Multi-key privacy-preserving deep learning system
# 4. System model
- 5.1. The basic scheme
4.1. Multi-key privacy-preserving deep learning system
In a deep learning system, each data owner has a sensitive dataset DBi in which local resources are fully administrated by the data owner. In our multi-key system, we consider n such data owners, denoted by P1, . . . , Pn. Each data owner Pi(i ∈ [1, n]) has own pair of public and private keys (pki, ski), local sensitive (i) (i) (i) , X , . . . , X dataset DBi has Ii attributes {X } and DB1 ∩DB2 · · ·∩ 1 2 Ii DBn = Φ, where i ∈ [1, n]. These data owners want to perform collaborative deep learning with the other data owners. Due to the computational ability is limited, data owners need to outsource the data to an untrusted cloud server for collaboratively deep learning. Before running a neural network, data owners jointly negotiate
In this subsection, we give a basic scheme (refer to Fig. 2) to realize a scenario that multiple data owners want to collaboratively (1), W (2) learn the parameters W with their partitioned data without leaking the information of their sensitive datasets.
Main idea. Generally speaking, SMC cannot handle the data encrypted with different public keys, and it can only deal with the ciphertext under the same public key. In our basic scheme, to preserve the data privacy when multiple parties are involved in deep learning model, we use MK-FHE [4] E n to encrypt the data before uploading it to a cloud. Assume there are n data owners P1, . . . , Pn, who hold their respect mutually disjoint, sensitive and vertically partitioned dataset DB1, . . . , DBn, and corresponding target vectors t1, . . . , tn. Each
(j)
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
Uploaded Encrypted Data Ph VE ye eB & Encrypted Result
Algorithm 4 Securely outsourcing computation of activation function
- Input: Ciphertexts [x], [a0], [a1], [a3], where a0, a1, a3 are constants.
# Output: [y]
# 1: Compute c0 with F.Enc(pkF
), i.e., c0 = [a0];
- 1: Compute cp with F.Enc(pkp), i.e., co = [do];
- 2: Compute c1 with secure multiplication MultiF, i.e., c1 = [a1] × [x];
# Data Owners
Fig. 2. The basic model.
# Cloud Server
- 3: Compute c3 with secure multiplication MultiF, i.e., c3 = [a3] × [x] × [x] × [x]; F
# F
# F
4: Compute [y] with secure addition AddF, i.e., [y] = c0 +
# F c1 +
data owner Pi (i ∈ [1, n]) encrypts his dataset with MK.Enc and uploads the ciphertexts MK.Enc(pkMFi , DBi), MK.Enc(pkMFi , W and MF.Enc(pkMFi , ti) and its public key pkMFi to an untrusted cloud (j) i ), server C for secure deep learning.
Algorithm 3 Overall scheme of multi-key privacy-preserving deep learning based on MK-FHE
- Input: {DB1, DB2, . . . , DBn}, initial W ing rate η (1), W (2) ; iterationmax, Learn-
(1), W
(2)
- Output: W
1: Data owner Pi(i ∈ [1, n]) does:
- 2: Initialize the parameters randomly;
# 3: Sample a key tuple (pkMFi
# , skMFi
# , ekMFi
# ) ← MF.KeyGen(1
- 3: Sample a key tuple (pkye;, Skwr; . ekwr,) — MF.KeyGen(1*);
(j)
# encrypts data DBi, W
- ci : 4: Each data owner i (j) ), gi ← MF.Enc(pkMFi , DBi), di ← MK.Enc(pkMFi , W i , ti); MK.Enc(pkMFi
# 5: Upload (pkMFi
# , ekMFi
# , ci, di, gi) to the cloud;
- 5: Upload (pkyr;, ekme; Ci, di, i) to the cloud;
κ );
←
5: return [y];
Cloud Server ? Data Owners PK, authorized Center
Fig. 3. The advanced model.
- 6: Cloud server dose:
- 7: Execute Algorithm 1 over the ciphertext domain;
(j)
- 8: Update the ciphertext of W
;
- 9: Send the learning results τ ∗ to the data owners;
10: Data owners P1, · · · , Pn do:
- 11: Data owners P1, · · · , Pn jointly run a SMC protocol to calculate MF.Dec(skMF1 , · · · , skMFn , τ ∗);
To realize a multi-key privacy-preserving deep learning system, the concrete operation is shown in Algorithm 3. It is worth noting that the activation function f (x) = 1+exp(−x) ∈ (0, 1) in Algorithm 1 1 is nonlinear. To support effective computation of the sigmoid function, we use Taylor series and approximate which is defined as:
key. Assume each data owner has uploaded the ciphertext (encrypted under its own public key of BCP scheme) to cloud server C. C will perform a neural network with (α − β − γ ) configuration over multi-key encrypted domain. However, for any valid ciphertext, authorized center AU (holds master key mk) can decrypt it by using the second decryption algorithm mDec(·) of BCP scheme. Hence, C needs to blind the ciphertexts before sending them to the authorized center AU. After receiving the blinded ciphertexts from the cloud C, AU decrypts it by using the mDec(·) and re-encrypts the blinded plaintext by using FHE F.EncpkF E F. Finally, AU sends the fully homomorphic ciphertext to the (·) of cloud C. After that, deep learning can be performed over the re- encrypted data. We assume C and AU are semi-honest and do not collude with each other.
1 1 + e(−x) = 1 2 + x 4 − x3 48 + o(x4). (3)
According to the property of Taylor series, the number of terms in the expansion can be decided according to the accuracy requirement. The secure computation of the activation function Eq. (3) is listed in Algorithm 4.
5.2. The advanced scheme
In this subsection, we describe another more practical method for multi-key privacy-preserving deep learning system without interaction among data owners. These types of entities are involved in the advanced scheme, including a cloud server C, an authorized center AU and n data owners P1, P2, . . . , Pn (illustrated in Fig. 3).
We explain the more details of the advanced scheme as follows.
Initialization. To protect the security and privacy of the data, each data owner encrypts his/her dataset before uploading to C. We choose a hybrid scheme which is a combination of double decryption mechanism (BCP scheme) and FHE as our encryption technique. In the initialization process, AU sets up BCP scheme and FHE, uses Setup of BCP scheme to generate a public parameter pp = (N, k, g) and a master key mk = (p′, q′). Then AU sends (1), W pp to C while keeping mk. The initialized parameters W (2) and target vector {Ti}n i=1 should be jointly negotiated in advance (1) (1) n )τ (1) = (W , . . . , W and by the data owners. Note that W 1 (2) = (w(1) (2) (1) , n n )τ (2) = (W , . . . , W )Ii×γ . W i=1 Ii = α; W ij i 1 = (w(2) (2) )Ji×β , n i=1 Ji = γ ; T = {Ti}n W i=1 is target vector and ij i ), Ii and Ji is the number of input and output of Ti = (t1, . . . , tIi Pi (i ∈ [1, n]), respectively.
Main idea. To decrease the computation/communication, we propose a hybrid multi-key deep learning training system model. By using a double decryption mechanism and FHE, the training process can be performed over ciphertexts under different public
Data uploading. In this phase, data owner Pi(i ∈ [1, n]) uses the received public parameter pp = (N, k, g) to generate it own pair of public and private keys {(pki, ski)}n i=1. Let us assume each data (i) (i) (i) owner Pi(i ∈ [1, n]) has a set of vectors DBi = (X , X · · · , X 1 2 Ii ).
81
# F
# F c3;
82
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
Data owner Pi (i ∈ [1, n]) uses BCP scheme to encrypt its private (i) (i) (i) , B = {(A )}Ii (X )}Ii (DBi) = {Enc pki dataset (Enc pki j=1) and j=1 j j j (2) (1) (W (W ), Enc pki ). weight parameter Enc pki i i
After that, data owner Pi (ı ∈ [1, n]) uploads the encrypted
data, the target vector Ti and the public key pki to the cloud server C. Here, we assume the communication channel for uploading is secure, which means nobody can obtain the uploaded data from C. For simplicity, we use (Ai, Bi) to denote BCP ciphertext of the DBi for the data owner Pi (i ∈ [1, n]).
Training. In this phase, we describe how the cloud server C han- dles α request from data owners in each learning round. After re- ceiving the data encrypted with different public keys, cloud C needs to perform an α-input deep learning algorithm over encrypted do- (k) {(Enc pk1 (DB1), Enc pk1 (W ), T1), . . . , (Enc pkn main 1 (k) n ), Tn)}, where k = 1, 2; α = n (W Enc pkn s=1 Is. (DBn),
Secure back propagation: The stochastic gradient descent (SGD) over the plaintext domain is described in Algorithm 2,
δ(3)
k = o (3) k (1 − o (3) k )(tk − o (3) k ), (4)
δ(2) h = o (2) k (1 − o (2) k ) Wkhδ(3) k (5) k∈D
# where δ(3)
# and δ(2)
k h are the error terms for each network output unit k and for each hidden unit h, respectively. D denotes all of units whose immediate inputs including the output of unit h.
(1), W
Finally, update each network weight W
(2)
W (2) := W (2) + ηδ(3) k xkh (6)
(1) := W (1) + ηδ(2) h xhi. (7)
# W
However, the learning algorithm cannot process the ciphertext under the same public keys directly. Therefore, cloud server C first runs Algorithm 5 with authorized center AU to transform the ciphertexts under different public keys into ciphertext under the same public key. Since AU holds the master key mk, it can decrypt any given valid ciphertext by using the master decryption of BCP scheme. Hence, C needs to blind ciphertexts {(Ai, Bi)}n i=1 with a (ri ∈ ZIi ) before sending the ciphertexts random message {ri}n i=1 N to AU. After receiving the blinded ciphertexts {(A′ , B′ )}n i=1, AU i i decrypts the blinded ciphertexts and re-encrypts the blinded plaintext zi with FHE F.EncpkF (zi) of E F, which is denoted by Zi, and sends this new ciphertext Zi to C. By removing the blinding factor ri, C can get the ciphertext under the public key pkF of F.Enc(·) without knowing the underlying plaintext.
From the Eqs. (4) and (5) over plaintext domain, the ci- phertext can be securely computed by AddF and MultiF as [δ(3) (3) ]) and [δ(2) (3) (3) ]) ×F([tk] +F[o ] ×F([1] +F[o ] = ] = [o k k k h k (2) (2) [Wkh] ×F[δ(3) ]) ×F( ] ×F([1] +F[o ]) respectively. Since [o k∈D k k k ] and [δ(2) the ciphertexts [xhi], [xkh] and [η] are known, [δ(3) ] are k h securely computed, the Eqs. (6) and (7) can be performed over en- crypted domain.
Algorithm 6 Transformation of the FHE ciphertext under pkF to BCP ciphertext under different public keys
Input: A ciphertext C under pkF and pk1 Output: {(Ai, Bi)}n i=1 , pk2 , · · · , pkn
1: Cloud server C does:
Algorithm 5 Transformation of BCP , pk2 , · · · , pkn to FHE ciphertexts under pkF pk1 ciphertexts under
# Input: ((Ai, Bi), pki
# Output: (A′′
, B′′
)
(4/,B/)
# i
# i
) for i ∈ [1, n]
- 1: Cloud server C does:
- 2: choose randomness value ri ∈ ZIi N , and compute the blinded ciphertexts (A′ , B′ ) ← Add((Ai, Bi), Encpki (ri)); i i
- 3: send these blinded ciphertexts and pki to the authorized center AU;
- 4: Authorized center AU does:
- ,mk(A′ , B′ ) //AU holds the mk of BCP scheme; 5: zi ← mDecpki i i
- 6: Zi ← F.EncpkF (zi)//AU encrypts zi with fully homomorphic encryption E F;
- 7: send Zi to the authorized center AU;
- 8: Cloud server C does:
- 9: C ′ ← AddF((Zi), F.EncpkF i (−ri));
After performing the Algorithm 5, cloud server C holds the data encrypted with the same public key pkF of F.Enc. Later C can realize deep learning over the encrypted domain. We give some (2) notations as follows: DB denotes the set of {DBi}n i=1, net (3) net represents the input and output values of the hidden layer, (2) (3) o and o represents the activation and output layer values, and respectively.
Secure feed forward: Cloud server C needs to compute the values (2) (3) (2) (3) of net , net , o and o over the encrypted domain. In the plaintext domain, we have
o (2) = f (net (2)) = f (W (1) · DB), o (3) = f (net (3)) = f (W (2) · o (2)),
then we use Algorithm 4 to compute the activation function over the encrypted domain as follows.
[o (2)] = f ([net (2)]) = f ([W (1)] ×F[DB]), [o (3)] = f ([net (3)]) = f ([W (2)] ×F[o (2)]).
- 2: randomly choose r AddF(C, F.EncpkF (r)); ∈ ZN , and compute D ←
- 3: send blinded ciphertext D to the authorized center AU.;
# SURA
- 4: Authorized center AU does:
# 5: z ← F.DecskF
(D);
- 6: (Zi, Di) ← Encpki (z) for all i ∈ [1, n];//Encrypting ciphertexts by using Enc of BCP scheme
# 7: send (Zi, Di) to the cloud server C;
- 8: Cloud server C does:
- 9: (Ai, Bi) ← Add((Zi, Di), Encpki (−r));//Removing the blinding factor
Extraction. After running the deep learning model, cloud server C obtains a set of learning results [τ ]. Then C runs Algorithm 6 with AU to transform the data encrypted with pkF into ciphertexts , . . . , pkn. Finally, C sends under the data owners’ public key pk1 these ciphertexts to the data owner P1, . . . , Pn, who can decrypt this ciphertext with its private key ski.
# 6. Security analysis
Our system aims to achieve the privacy of input data, the intermediate results security and output results security of the deep learning under the semi-honest model, i.e., all participants in our two schemes are assumed to be semi-honest. Now, we describe the definition of the semantic security [41], i.e., security against polynomially indistinguishable chosen-plaintext attack (referred as IND-CPA security).
Definition 6.1 (Semantic Security(SS), IND-CPA). A public-key en- cryption scheme E = (KeyGen, Enc, Dec) is semantically secure, if for any stateful PPT adversary A = (A1, A2), its advantage (k) := |Pr[ExpSS (k) = 1] − 1 | is negligible, where the ex- AdvSS E,A E,A 2 (k) is defined as follows: periment ExpSS
# E,A
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
(k): E,A b ← {0, 1} (pk, sk) ← KeyGen(1k) (m0, m1) ← A1(pk) c ← Enc pk(mb) b′ ← A2(c) If b′ = b, return 1; else, return 0
# ExpSS
# 7. Application
In this section, we show an application of our advanced scheme in face recognition.
Privacy-Preserving Face Recognition. As a typical biometric authentication technique, face recognition is increasingly applied in real life. The widespread use of this technique arouse people’s many privacy concerns, especially outsourcing computing in an untrusted cloud server.
Here, we require that the two plaintexts have the same length. If they are not, padding message can be applied.
Privacy of Data. To protect the data privacy of the data owners, in basic scheme and advanced scheme, we use MK-FHE and hybrid encryption scheme to support the secure computation, respectively. According to the definition of semantic security, we can obtain the following conclusions.
Corollary 6.2 (MK-FHE Semantic Security). If the underlying encryption scheme is semantically secure, then the multi-key fully homomorphic encryption is semantically secure.
Proof (Sketch). Let us assume the public key encryption scheme & — {KeyGen, Enc, Dec} is semantically secure. Based on this scheme &, the challenger constructs a evaluate algorithm Eval, such that the new public key encryption scheme &’ = {KeyGen, Enc, Dec, Eval} keeps homomorphic of addition and multiplication operations. If the evaluation key ek is public, then the adversary can compute Eval directly according to the public key pk, the ciphertext c and the evaluation key ek. Therefore, the MK- FHE scheme is semantically secure. O
Recall that in our basic scheme, data owners do not communi- cate with each other until the decryption phase. Each data owner Pi(i ∈ [1, n]) generates its own key tuple (pkMFi , skMFi , ekMFi ) and encrypts its input DBi under the public key pkMFi of MK-FHE. A semi-honest data owner P may collude with some data owners, and wants to reveal a sample vector DB uploaded by other data owners. However, data owners do not need to coin-flip for each other’s ran- dom coins. From the Definition 3.2, it guarantees our basic scheme is secure against corrupt data owners. Therefore, the privacy of the data owners is confidential.
In our advanced scheme, because the BCP scheme and FHE are semantically secure (the detailed proof [7], Theorem 11), the cloud server C should be probabilistic polynomially bounded, and sends the blinded ciphertexts to AU for computation. The computing power of AU, on the other hand, dose not have to be bounded, since it only receives the blinded ciphertexts and only be able to see the blinded messages, which are decrypted by master decryption algorithm of BCP scheme. Hence, the cloud server C and authorized center AU cannot obtain the learning results. Therefore, the privacy of the learning results is confidential and we obtain the following lemma.
Lemma 6.3. Without any collusion, Algorithm 5, 6 is privacy- (1), W (2) preserving for the weight W .
Privacy of Training Model. A semi-honest cloud server C can train a deep learning model privately. Because AddF and MultiF are both semantically secure, there is no information leakage for C. (1), W (2) Hence, for the weights W in the training process of In the feed work stage and In the back propagation stage, cloud server C performs AddF and MultiF operations, therefore privacy for the whole training process is guaranteed.
Assume there exists an image sample set, which collect n × m grayscale images, n (e.g. n = 20) person P1, . . . , Pn in various poses, and each person has m (e.g. m = 30) images (each image pixel p1 × p2, p1 = p2 = constant, n, m ≪ p1, p2 and p1, p2 ∈ (0, 255)). These m images include person’s expression (such as sad, dismay, happy, angry), which direction they are watching (e.g. straight, right, left, up), and whether they are wearing mask or not. The main task is to learn a target function from this image sample set securely. Suppose we choose a network with a (α − β − γ ) configuration, i.e., one input layer with α nodes, one hidden layer with β nodes and one output layer with γ nodes. The system includes three phases, including input encoding, learned hidden representation, and output encoding. The details are described as follows.
Input encoding . Before uploading the image to cloud server C for collaborative deep learning, P1, . . . , Pn should preprocess the image data to feature extraction, then execute deep learning network with these features (e.g. edges) as input. Using this feature as input can reduce the number of input and corresponding weights, and cut down the computation and keep the classification correctly. Since the deep learning network has a fixed number of input units, then encoding the image has a fixed pixel intensity values, i.e., n × m, which can be seen as the abstract expression of the original p1 × p2 image. For example, Pi (i ∈ [1, n]) has m original images Γ1, . . . , Γm, each image Γi(i ∈ [1, m]) with p1×p2 pixel. After feature extraction, the original image is replaced by a feature set of n × m pixel, while each pixel value as one network input. However, in order to efficiently compute network of hidden and output activation functions, the input feature sample range space (0, 255) should be converted to range space (0, 1), which is represented as (A1, . . . , As), where Ai ∈ (0, 1)n×m, s < n. Finally, each Pi (i ∈ [1, n]) runs Data Uploading protocol (which is described in Section 5.2), and uploads the ciphertexts (1)), Enc pki (2)) to the cloud server C. (Ai), Enc pki (W (W Enc pki
Learned Hidden Representations. Right now, cloud server C has obtained the data encrypted with different public keys, so cloud server C and authorized center AU run Training protocol (which is described in Section 5.2) to training a neural network securely.
Out encoding . Let us assume the deep learning network has four output units, and each output unit represents one of the four face directions. These four outputs can be viewed as a four-dimension vector, i.e., (straight, right, left, up) and we use four real numbers to represent the possibility of directions. If some component value is the highest, then the corresponding direction can be seen as the deep learning network prediction. For instance, (0.3, 0.3, 0.3, 0.7) is target output vector and 0.7 is the highest value, this result indicates the person is looking at his up. Since all the computation is performed in encrypted domain, the output results are also encrypted. After comparing these four components of the output vector over the encrypted domain, cloud server C chooses the highest one. Once the network output is given, C runs Algorithm 6 and sends the result encrypted with pki to person Pi, where i ∈ [1, n].
83
84
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
# 8. Conclusions and future work
In this paper, we focused on the privacy issues of collaborative deep learning in cloud computing, and proposed two schemes, i.e., basic scheme and advanced scheme, to protect the privacy in deep learning. The basic scheme is based on a Mk-FHE scheme, and the advanced scheme is based on a hybrid structure, which combines the double decryption mechanism with FHE scheme. Both schemes are able to tackle the problem of privacy-preserving collaborative deep learning ciphertext with different public keys. Compared with the basic scheme, the advanced scheme does not need the interaction among the data owners during the decryption of the learning result. Our future work will be focused on two open problems: how to implement the FHE scheme in practical machine learning, and how to reduce the cost of computation and communication.
- [21] C.C. Aggarwal, S.Y. Philip, A general survey of privacy-preserving data mining models and algorithms, in: Privacy-Preserving Data Mining, Springer, US, 2008, pp. 11–52.
- [22] S.Q. Ren, B.H.M. Tan, S. Sundaram, et al., Secure searching on cloud storage enhanced by homomorphic indexing, Future Gener. Comput. Syst. (2016).
- [23] A. Evfimievski, T. Grandison, Privacy Preserving Data Mining, IGI Global, 2009, pp. 1–8.
- [24] P. Vijayakumar, V. Chang, L.J. Deborah, et al., Computationally efficient privacy preserving anonymous mutual and batch authentication schemes for vehicular ad hoc networks, Future Gener. Comput. Syst. (2016).
- [25] W. Du, Y. Han, S. Chen, Privacy-preserving multivariate statistical analysis: Linear regression and classification, in: SDM, 2004, Vol. 4, pp. 222–233.
- [26] G. Jagannathan, R.N. Wright, Privacy-preserving distributed k-means cluster- ing over arbitrarily partitioned data, in: Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, ACM, 2005, pp. 593–599.
- [27] T. Graepel, K. Lauter, M. Naehrig, ML confidential: Machine learning on encrypted data, in: Information Security and Cryptology, (ICISC), 2012, pp. 1–21.
# Acknowledgments
This work was supported by National Natural Science Foun- dation of China (No. 61472091), Natural Science Foundation of Guangdong Province for Distinguished Young Scholars (2014A030306020), Science and Technology Planning Project of Guangdong Province, China (2015B010129015) and the Innovation Team Project of Guangdong Universities (No. 2015KCXTD014).
- [28] D. Agrawal, R. Srikant, Privacy-preserving data mining, in: Proc. ACM Conf. Manage. Data, 2000, pp. 439–450.
- [29] N. Li, M. Lyu, D. Su, et al., Differential privacy: From theory to practice, Synth. Lect. Inf. Secur. Privacy Trust 8 (4) (2016) 1–138.
- [30] T. Zhang, Q. Zhu, Dynamic differential privacy for ADMM-based distributed classification learning, IEEE Trans. Inf. Forensics Secur. 12 (1) (2017).
- [31] M. Abadi, A. Chu, I. Goodfellow, et al., Deep learning with differential privacy, in: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016, pp. 308–318.
- [32] R. Shokri, V. Shmatikov, Privacy-preserving deep learning, in: Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, ACM, 2015, pp. 1310–1321.
# References
- [33] T.T. Chen, S. Zhong, Privacy-preserving back-propagation neural network learning, IEEE Trans. Neural Netw. 20 (10) (2009) 1554–1564.
- [1] V. Chang, Towards a big data system disaster recovery in a private cloud, Ad Hoc Networks 35 (2015) 65–82.
- [2] Z.W. Wang, C. Cao, N.H. Yang, V. Chang, ABE with improved auxiliary input for big data security, J. Comput. System Sci. (2016).
- [3] O. Goldreich, Secure multi-party computation. Manuscript. Preliminary version, 1998, pp. 86–97.
- [4] A. López-Alt, E. Tromer, V. Vaikuntanathan, On-the-fly multiparty computa- tion on the cloud via multikey fully homomorphic encryption, in: Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing, ACM, 2012, pp. 1219–1234.
- [5] P. Mukherjee, D. Wichs, Two round multiparty computation via multi-key FHE, in: Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer Berlin, Heidelberg, 2016, pp. 735–763.
- [6] P. Mukherjee, P, D. Wichs, Two Round MPC from LWE via Multi-Key FHE. IACR Cryptology ePrint Archive, 2015, p. 345.
- [34] A. Bansal, T. Chen, S. Zhong, Privacy preserving back-propagation neural network learning over arbitrarily partitioned data, Neural Comput. Appl. 20 (1) (2011) 143–150.
- [35] N. Schlitter, A protocol for privacy preserving neural network learning on horizontally partitioned data, in: PSD, 2008.
- [36] J.W. Yuan, S.C. Yu, Privacy preserving back-propagation neural network learning made practical with cloud computing, IEEE Trans. Parallel Distrib. Syst. 25 (1) (2015) 212–221.
- [37] D. Boneh, E.J. Goh, K. Nissim, Evaluating 2-dnf formulas on ciphertexts, in: Proceedings of the Second International Conference on Theory of Cryptography, TCC05, Berlin, Heidelberg, 2005, pp. 325–341.
- [38] Q. Zhang, L.T. Yang, Z. Chen, Privacy preserving deep computation model on cloud for big data feature learning, IEEE Trans. Comput. 65 (5) (2016) 1351–1362.
- [7] E. Bresson, D. Catalano, D. Pointcheval, A simple public-key cryptosystem with a double trapdoor decryption mechanism and its applications, in: Advances in Cryptology-ASIACRYPT 2003, 2003, pp. 37–54.
- [39] Z. Brakerski, C. Gentry, V. Vaikuntanathan, (Leveled) fully homomorphic encryption without bootstrapping, in: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ACM, 2012, pp. 309–325.
- [8] C. Gentry, Fully homomorphic encryption using ideal lattices, in: Symposium on the Theory of Computing, 2009.
- [40] A. Mirhoseini, A.R. Sadeghi, F. Koushanfar, CryptoML: Secure outsourcing of big data machine learning applications. 2016.
- [9] T.H. Chan, K. Jia, S. Gao, J. Liu, et al., PCANet: A simple deep learning baseline for image classification? IEEE Trans. Image Process. 24 (12) (2015) 5017–5032.
- [10] A. Graves, A.R. Mohamed, G. Hinton, Speech recognition with deep recurrent neural networks. in: ICASSP, 2013.
- [41] S. Goldwasser, S. Micali, Probabilistic encryption, J. Comput. System Sci. 28 (2) (1984) 270–299.
- [11] G. Hinton, L. Deng, D. Yu, et al., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, Signal Process. Mag. 29 (6) (2012) 82–97.
- [12] M. Liang, Z. Li, T. Chen, J. Zeng, Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach, IEEE/ACM Trans. Comput. Biol. Bioinf. (TCBB) 12 (4) (2015) 928–937.
- [13] G.E. Hinton, R.R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science 313 (5786) (2006) 504–507.
- [14] V. Chang, M. Ramachandran, Towards achieving data security with the cloud computing adoption framework, IEEE Trans. Serv. Comput. 9 (1) (2016) 138–151.

Ping Li received the M.S. and Ph.D. degree in mathemat- ics from Sun Yat-sen University in 2010 and 2016, re- spectively. Currently, she works at Guangzhou University as postdoctoral. And hers main research interest include cryptography, privacy-preserving and cloud computing.
- [15] V. Chang, Y.H. Kuo, M. Ramachandran, Cloud computing adoption framework: A security framework for business clouds, Future Gener. Comput. Syst. 57 (2016) 24–41.
- [16] G. Sun, Y. Xie, D. Liao, et al., User-defined privacy location-sharing system in mobile online social networks, J. Netw. Comput. Appl. (2016).
- [17] G. Sun, D. Liao, H. Li, et al., L2P2: A location-label based approach for privacy preserving in LBS, Future Gener. Comput. Syst. (2016).
- [18] J. Li, X.F. Chen, M.Q. Li, et al., Secure deduplication with efficient and reliable convergent key management, IEEE Trans. Parallel Distrib. Syst. 25 (6) (2014) 1615–1625.
- [19] J. Li, Y.K. Li, X.F. Chen, et al., A hybrid cloud approach for secure authorized deduplication, IEEE Trans. Parallel Distrib. Syst. 26 (5) (2015) 1206–1216.
- [20] J. Li, X.F. Chen, X.Y. Huang, et al., Secure distributed deduplication systems with improved reliability, IEEE Trans. Comput. 64 (12) (2015) 3569–3579.
y ‘i A i - -
conferences.
Jin Li received the B.S. degree in mathematics from Southwest University in 2002 and the Ph.D. degree in information security from Sun Yat-sen University in 2007. Currently, he works at Guangzhou University as a professor. He has been selected as one of science and technology new star in Guangdong province. His research interests include applied cryptography and security in cloud computing. He has published more than 70 research papers in refereed international conferences and journals and has served as the program chair or program committee member in many international


— =
P. Li et al. / Future Generation Computer Systems 74 (2017) 76–85
Zhengan Huang received his B.S. and M.S. degrees from Department of Mathematics, Sun Yat-sen University in 2009 and 2011, respectively, and his Ph.D. degree from Department of Computer Science and Engineering, Shanghai Jiao Tong University in 2015. He served as a security engineer in Huawei Technologies Co. Ltd. from 2015 to 2016. Currently, he is a postdoctoral researcher in Guangzhou University. His research interests include public-key cryptography and information security.

Tong Li received his B.S. (2011) and M.S. (2014) from Taiyuan University of Technology and Beijing University of Technology, respectively, both in Computer Science Tech- nology. Currently, he is a Ph.D. candidate at Nankai Univer- sity. His research interests include applied cryptography and data privacy protection in cloud computing.
Chong-Zhi Gao received his Ph.D. (2004) in applied mathematics from Sun Yat-sen University. Currently, he is a professor at the School of Computer Science of Guangzhou University. His research interests include cryptography and privacy in machine learning.

Siu-Ming Yiu received a B.S. in Computer Science from the Chinese University of Hong Kong, a M.S. in Computer and Information Science from Temple University, and a Ph.D. in Computer Science from The University of Hong Kong. Currently, he is a associate professor of the University of Hong Kong. His research interest include bioinformatics, computer security and cryptography.
Kai Chen received the B.S. degree from Nanjing University, China, in 2004, and the Ph.D. degree from University of Chinese Academy of Sciences in 2010. He is an Associate Professor in the Institute of Information Engineering, Chinese Academy of Sciences. He was also a postdoc at Pennsylvania State University, State College, PA USA. His research interests include software security, security testing on smartphones, and privacy protection in social networks.
85
