Neurocomputing 396 (2020) 587–598
ELSEVIER
Contents lists available at ScienceDirect
# Neurocomputing
journal homepage: www.elsevier.com/locate/neucom

# A data-eﬃcient deep learning approach for deployable multimodal social robots
MU) Check for updates
# Heriberto Cuayáhuitl
School of Computer Science, University of Lincoln, Lincoln Centre for Autonomous Systems (L-CAS), Brayford Pool, Lincoln LN6 7TS, United Kingdom
a r t i c l e
i n f o
# ARTICLE INFO
a b s t r a c t
# ABSTRACT
Article history: Received 1 April 2018 Revised 2 August 2018 Accepted 11 September 2018 Available online 24 April 2019
Keywords: Deep reinforcement learning Deep supervised learning Interactive robots Multimodal perception and interaction Board games
The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for eﬃciently training a humanoid robot to play multimodal games—and use the game of ‘Noughts and Crosses’ with two variants as a case study. Its minimum re- quirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic sim- ulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot conﬁrms that highly accurate visual perception is required for successful game play.
© 2019 Elsevier B.V. All rights reserved.
# 1. Introduction
In a not so distant future, we will be able to buy purposeful and socially-aware humanoid robots that can be delivered home much like buying personal computers nowadays. While robots may come with a pre-deﬁned or pre-trained set of skills—arguably and ideally—they should be able to self-adapt or self-extend for carry- ing out new useful tasks relevant to their individual user(s). A new task can be one that is entirely distinct from pre-deﬁned skills or one that is similar but not the same. We will refer to both types as ‘new tasks’ and present two examples of new tasks in our case study below. Deploying robots with pre-built skills is still challeng- ing assuming that they have to adapt to different spatial and social environments each time an adaptation of existing knowledge oc- curs. Deploying robots with the ability to acquire new skills has the potential to be even more challenging. This latter form of de- ployment is of great interest to AI because it requires advanced multimodal communication (via human-like verbal and non-verbal commands) in order to achieve a task or set of tasks successfully. A concrete scenario e.g. is as follows: Your new robot has arrived and you want to teach it to play a game (that is at least par- tially unknown to the robot) so it can play with you, your family and friends whenever you want. Having said that ... How can the
robot be equipped and/or trained to play such a game with a reduced amount of human intervention? What are the basic building blocks required to make that happen? This article makes a step towards an- swering some of these challenging questions and discusses future work towards purposeful and socially-aware humanoid robots.
As a case study, the multimodal game that we focus on is Noughts and Crosses also known as ‘Tic-Tac-Toe’—with two variants. In both games players alternate turns, and each player is repre- sented by either noughts or crosses.
- • Its standard version uses a 3 × 3 grid, where a game is won if and only if three noughts or crosses are in line or diag- onal (a draw otherwise)—see Fig. 1 (left). One player adopts noughts and the other crosses, alternating turns until the game is over. In this game, the more expertise the players acquire, the more likely it is to end up in a draw. This has motivated the development of variants of the game with higher degrees of complexity.
- • A substantially more diﬃcult variant, called Ultimate Noughts and Crosses 1 , uses 3 × 3 subgrids each of 3 × 3 squares (81 squares in total), where the goal is to win three subgrids (each of 3 × 3 squares) in line or diagonal. In this latter game, while the ﬁrst game move is allowed to take any of the 81 squares, a subsequent game move is restricted to take a square in the
E-mail address: h.cuayahuitl@gmail.com
1
https://en.wikipedia.org/wiki/Ultimate _ tic- tac- toe .
https://doi.org/10.1016/j.neucom.2018.09.104 0925-2312/© 2019 Elsevier B.V. All rights reserved.
588
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
(a) Nougths & Crosses (b) Ultimate Nougths & Crosses
tion (policy learning) is important for training an autonomous robot that learns—in a scalable way—its competitive behaviour from trial and error. Our newly proposed algorithms win sub- stantially more than the well-known DQN method [2] .
- 3. We carried out a near real world evaluation of our deep learning-based humanoid robot, who played the game of Noughts and Crosses against 130 different individuals in the wild. While most previous work has carried out evaluations using simulations only or controlled experiments in lab envi- ronments, we believe it is important and timely to show that newly developed approaches or algorithms work (to a large ex- tent) in real or near-real world settings – out of lab conditions. Robots interacting in the wild have to be able to deal with un- structured interactions, partially known environments, unseen human behaviour, etc. This is a big challenge for multimodal robots, and this work reports a step in this direction.
# 2. Related work
# So far the topic of deep learning-based conversational and/or
Fig. 1. A humanoid robot playing the game of noughts and crosses with two vari- ants using multiple modalities and learnt behaviour.
subgrid that mirrors the previous game move of the opponent player. For example, a player taking the middle right square of any subgrid would restrict the opponent to take a move anywhere in the middle right subgrid—as shown in Fig. 2 (a) and (b). Similarly, a player taking for example the bottom-right square of any subgrid would restrict the opponent to take a move anywhere in the bottom-right subgrid—as in Fig. 2 (b) and (c), and so on. There is one exception to these restrictions: a player can take an empty square anywhere in the entire board if and only if the target subgrid has been won/lost/draw al- ready. This game is more advanced than its standard counter- part and strategically challenging—see Fig. 1 (right).
The challenging task for the robot is to successfully play either game against unknown humans and partially familiar physical en- vironments. This article describes a machine intelligence approach for eﬃciently training such a deployable robot, and makes the fol- lowing contributions:
- 1. We propose a deep learning-based approach for training a mul- timodal robot with low data requirements. This is demonstrated by a scenario with two variants of different com plexity and the following data requirements: a few hundred example images, a dozen example multimodal dialogues (see example in [1] – Appendix A), a few example physical demonstrations of hand- writing, and automatically generated simulated games. Apply- ing our approach to other games or tasks would require similar resources (though with further training examples depending on task complexity), plus a mechanism to let the robot know about valid actions and when a task has been achieved or not (e.g. game won/lost). The latter together with more reﬁned manip- ulation or locomotion would require additional programming, which future work should try to automate.
- 2. We propose two novel learning algorithms, one for visual per- ception and the other one for policy learning. The former is useful for tracking the game state, i.e., what moves have been made so far by each player. Accurate recognition is important for playing the game so that the robot’s view of the world is as accurate as possible, as opposed to a blurry view that would lead to unexpected or non-human-like behaviours. Deep learn- ing can be used to provide the robot with game moves (in our case), and it can also provide the interaction agent with learnt internal representations. The latter algorithm for interac-
multimodal social robots is in many respects unexplored. Some ex- ceptions include the following. Noda et al. [3] train a humanoid robot to carry out the following object manipulation behaviours: ball lift, ball roll, bell right (left and right), ball roll on a plate, and ropeway. To train this multimodal robot three neural networks are used: ﬁrst, a deep autoencoder is used for feature learning from audio signals in the form of spectrograms; second, a deep au- toencoder is used for feature learning from 2D images; and third, a deep autoencoder is also used for multimodal feature learn- ing from audio and visual features generated by the previous two autoencoders. The latter learnt features are given as input to a multiclass Support Vector Machine classiﬁer in order to predict the object manipulation task to carry out. Focusing more on so- cial skills, Qureshi et al. [4] train a humanoid robot with social skills whose goal is to choose one of four actions: wait, look to- wards human, wave hand, and handshake. The authors use the DQN method [2] and a two-stage approach. While the ﬁrst stage collects grayscale and depth images from the environment, the sec- ond stage trains two Convolutional neural nets with fused features. The robot receives a reward of +1 for a successful handshake, −0.1 for an unsuccessful handshake, and 0 otherwise. Combining social and action learning, [1,5] train a robot to play games also using the DQN method and a variant of it. In this work a Convolutional neu- ral net is used to predict game moves, and a fully-connected neural net is used to learn multimodal actions (18 in total) based on game rewards. Other previous works have addressed multimodal deep learning but in non-conversational settings [6–9] . From all these works it can be observed that learning agents use small sets of ac- tions in single-task scenarios. Thus, humanoid social robots with more complex behaviours including larger sets of actions remain to be investigated.
There is a similarly limited amount of previous work on hu- manoid robots playing games against human opponents. Notable exceptions include [10] , where the DB humanoid robot learns to play air hockey using a Nearest Neighbour classiﬁer; [11] , where the Nico humanoid torso robot plays the game of rock-paper- scissors using a ‘Wizzard of Oz’ setting; [12] , where the Sky hu- manoid robot plays catch and juggling using inverse kinematics and induced parameters with least squares linear regression; [13] , where the Nao robot plays a quiz game, an arm imitation game, and a dance game using tabular reinforcement learning; [14] , where the Genie humanoid robot plays the poker game using a ‘Wizard of Oz’ setting; and [15] , where the NAO robot plays Check- ers using a MinMax search tree. Most of these robots only exhibit non-verbal abilities and are either teleoperated or based on heuris- tic methods, which suggests that verbal abilities in autonomous
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
inimiz |— a mitt. | 1 t- 1 | i a 4 (a) Robot game movel (b) Human game movel (c) Robot game move2 (d) Human game move2
Fig. 2. Example robot and user game moves — robot’s ﬁeld of view from bottom to top.
trainable robots playing games are underdeveloped. Apart from [1,5] , we are not aware of any other previous work in humanoid robots playing social games against human opponents and trained with deep learning methods.
- 2. Train a deep supervised learner for visual perception to keep track of the environment dynamics as described in Section 3.1 .
Previous work on multimodal robots trained to carry out spe- ciﬁc tasks and that have been deployed in the wild are almost absent as pointed out by [16–18] —perhaps due to the complex- ity involved. Most previous multimodal trainable robots are either trained and tested in simulation, or trained (usually oﬄine) and tested in controlled conditions and/or using recruited participants. For the sake of clarity, we refer to robots deployed in the wild as those robots interacting with non-recruited participants in a non- controlled manner and with rather spontaneous, unrestricted and untimed interactions. The closest previous work is the Minerva robot [19] , which gave 620 tours to people through the exhibitions of a museum. Another related work is the Nao robot [20] , which gave route instructions to employees and visitors of a company building. Lessons learnt by these works include the application of probabilistic approaches to deal with uncertainty in the interac- tion, and challenges in starting and ﬁnishing conversational en- gagements with out-of-domain responses. Our work complements previous work by showcasing a robot that carries out a joint ac- tivity with people, namely playing multimodal games, in a sponta- neous and uncontrolled setting.
In the remainder of the article we describe a deep learning- based approach for eﬃciently training a robot with the ability of behaving with reasonable performance in a near real world de- ployment. In particular, we measure the effectiveness of neural- based game move interpretation and the effectiveness of Deep Q- Networks (DQN) [2] for interactive social robots. Field trial results show that the proposed approach can induce reasonable and com- petitive behaviours, especially when they are not affected by un- seen noisy conditions.
- 3. Write a set of example dialogues (e.g. a dozen or as needed) as in the Appendix of [1] , which can be used for generating simulated interactions as in [21] .
- 4. Use the outputs of the previous two steps for training a deep reinforcement learner using simulations as described in Section 3.2 .
- 5. Collect a set of pre-recorded motor motions or train a component to carry out commands such as ‘write cross or circle in a particular location’.
- 6. Test the robot using the previous resources, and iterate from step 1 if needed.
- 7. Deploy the robot subject to successful interactions in the previous step.
- 3.1. Visual-based perception of game moves
We use the 2D camera in the robot’s mouth to recognise draw- ings on the game grid using Algorithm 1 . The robot extracts video frames, locates the game grid using colour-based detection of the largest contour—see Fig. 3 , and splits the game grid into a set of
Algorithm 1 Game Move Recogniser.
- 1: Input: video stream from 2D camera, n × n =grid size (e.g. 3 ×3, 9 ×9), noise threshold τ , ρ=resolution of subimages, γ =pause between recognition events, C=statistical classiﬁer, initialise (i ) L t =labels at time t for grid square i
- 2: Output: set G of recognized game moves
# 3. Proposed learning approach
# 3: repeat
4:
# F ← extract video frame from video stream
Our proposed approach uses two deep learning tasks in cas- cade with low data requirements, which is useful to enable robots with new skills and where training data is either absent or scarce. While the ﬁrst learning task predicts what is going on in the environment—game moves and who said what, the second learn- ing task inherits such predictions in order to decide what to do or say next. Our approach is motivated by the fact that once a robot system is trained, it is expected to operate not only in known en- vironments but also in partially-known environments. The latter may include unseen rooms, unseen furniture, and unseen human opponents, among others. This approach can be applied to other tasks beyond the case study in this article through the following methodology:
- 5: P ← extract page region from F
P' < get projected transformation from P
6:
if dist (Pf,
P’_,)+dist(P/_,,
P{_,)
t−2 ) > τ then
- 7:
- continue (detect and omit hands in handwriting)
- 9: end if
G < remove grid and convert image P” to grayscale
10: M1:
G’ < divide G into n xn images of p pixels
(i )
(i )
- 12: t−2 ← L L
t−1
(i )
(i )
t−1 ← L
- 13: L t
(i )
Lo < predict labels for each image in G’ using C
14:
(i )
(i )
(i )
1
# t )
- 15: i ∗ = arg max Z P r(L t−2 ) + P r(L t−1 ) + P r(L i
if label of grid square i ∗ is not ‘nothing’ then
16:
7.
Update G with game move of grid square i ∗
- 18: break
- 1. Collect a modest set of example images (e.g. a few hun- dred or as needed) and label them.
- 19: end if
# sleep γ milliseconds
- 20:
- 21: until end of game turn
589
590
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598

Fig. 3. Raw input image in colour space BGR-Gray (left), largest contour with projected transformation (middle), and game space used for game move recognition (right). Our image pre-processing is based on OpenCV ( http://opencv.org/ ).
(a) Before game move (b) During game move X IR 6/0 | 0 sim bak (c) After game move
Fig. 4. Illustration of a game move before and after handwriting.
subimages (one for each grid square) in order to pass them to a probabilistic classiﬁer for game move recognition.
a set of user and robot game moves 3 in the following format:
[ who = usr ∧ what = draw ∧ where = i ∗] .
Although a game move recogniser can be trained in an online fashion, our algorithm below assumes the existence of a supervised learner trained oﬄine. Thus, we leave the topic of online train- ing, during the course of the interaction, as future work. In our case we use a Convolutional Neural Network (CNN) [22] C trained from data set D = { ( x 1 , y 1 ) , . . . , (x N , y N ) } , where x i are n × n ma- trices of pixels and y j are class labels. This classiﬁer maps images to labels—in our case {‘nought’, ‘cross’, ‘nothing’}. Our CNN uses the following architecture: input layer of 40 × 40 pixels, convolu- tional layer with 8 ﬁlters, ReLU, pooling layer of size 2 × 2 with stride 2, convolutional layer with 16 ﬁlters, ReLU, pooling layer of size 3 × 3 with stride 3, and the output layer used a linear Support Vector Machine with 3 labels. This CNN is used multiple times— once per grid cell—to predict the state of each game grid, in each game turn, for detecting drawing events. Note that the larger the grid the larger the number of recognition events needed for pre- dicting the state of the entire game grid. In addition, rather than using only the most recent video frame for game move recogni- tion, we use a history of user and robot game moves (denoted as G) to focus recognition on valid game moves, i.e., newly recognised moves from empty grid squares to non-empty grid squares. This process needs to be done in (near) real-time for exhibiting smooth human-robot interactions.
3.2. Learning to interact given multimodal inputs
The visual perceptions above plus words raised in the interac- tion by both conversants 4 are given as input to a reinforcement learning agent to induce its behaviour, where such multimodal per- ceptions are mapped to multimodal actions by maximizing a long- term reward signal. The goal of a reinforcement learner is to ﬁnd an optimal policy denoted as π ∗(s ) = arg max a ∈ A Q ∗(s, a ) , where Q ∗ represents the maximum sum of rewards r t discounted by fac- tor γ at each time step, after taking action a in state s [24,25] . While reinforcement learners take stochastic actions during train- ing, they select the best actions a ∗ = π ∗(s ) at test time.
Our reinforcement learning agents approximate Q ∗ using a mul-
Our reinforcement learning agents approximate Q* using a mul- tilayer neural network as in (2). The O function is parameterised as Q(s, a; 6;), where 6; are the parameters (weights) of the neural network at iteration i. Furthermore, training a deep reinforcement learner requires a dataset of experiences D = {e;,...ey} (also re- ferred to as ‘experience replay memory’), where every experience is described as a tuple e, = (Sr, Gr, Tr, S¢41). Inducing 05 consists in iteratively applying Q-learning updates over minibatches of expe- rience MB = {(s,a,r,s’) ~ U(D)} drawn uniformly at random from the full data set D. A learning update at iteration i is thus defined
Algorithm 1 formalises the description above for game move recognition, which can be used at each game turn in a game. Lines 7–9 2 are particularly useful for ignoring video frames with hu- man hands, which can be a source of misrecognitions—see Fig. 4 . In this way the game move recogniser is responsible for main- taining, as accurately as possible, the state of the game based on
3
While 18 game moves, i.e., 9 grid squares × 2 players, are used to describe the state of the standard noughts and crosses game grid, 81 × 2 = 162 game moves are used to describe the state of the ultimate noughts and crosses game grid. Consid- ering a window history of 3 time steps, 18 × 3 = 54 and 162 × 3 = 486 classiﬁcation events are needed at each time step for both games, respectively. In our case, the number of window histories is indeﬁnite because human players can take a turn in their own time.
2
Function dist ( · , · ) in Algorithm 1 is based on the Euclidean distance.
4
This work assumes that words raised by the human opponent are derived from the top recognition hypothesis of a speech recogniser. An alternative representa- tion would be the use of word embeddings to deal with unseen words and similar meanings [23] .
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
Algorithm 2 Competitive DQN Learning.
# 4. Experimental setting
- 1: Initialise Deep Q-Networks with replay memory D , action-value function Q with random weights θ , and target action-value functions ˆ Q with weights ˆ θ = θ
functions Ö with weights 6 = 0 2: repeat 3: s < initial environment state in S 4: repeat 5: Ae actions with min(r(s,a) < 0 Va & A) ' ~ |g otherwise ge A= action(s) leading to win (legally) the game ' ~ [all available actions in state s otherwise 7 randgca if random number < € di a= a pe Il: i max, -4\A Q(s’,a’;@) otherwise 8: Execute action a and observe reward r and next state s’ 9: Append transition (s, a, r,s’) to D 10: Sample random minibatch (sj, ağ, Tp.) from D __ Jr; if final step of episode [rj + y maxg-aQ(s',0/;6) otherwise 12: Set err = (y;— 0(s',a”; 0) 13: Gradient descent step on err with respect to 0 14: Reset Ö = O every C steps 15: sas! 16: until s is a goal state 17: until convergence
In contrast to previous studies that require millions of video frames for training visually-aware game-based policies [2] , we train our game move recogniser from a few hundred example im- ages and our reinforcement learners using simulations due to the large amount of training examples required.
4.1. Characterisation of deep reinforcement learners
4.1.1. Multimodal states
Our environment states include 73 and 289 features (for N&C 3 × 3 and N&C 9 × 9, respectively) that describe the game moves, executed commands, and words raised in the interactions. While words derived from system responses are treated as binary vari- ables (i.e., present or absent), words derived from user responses are treated as continuous variables. We treat game moves as binary features and future work can consider treating them as continu- ous variables to mirror the recognition conﬁdence. In addition, we consider a robot that cannot distinguish the order of game moves versus a robot that can distinguish the sequence of game moves. The latter is addressed by features that describe when a game move happened, where an earlier move has a lower value than a later move. We calculate such values according to T emporalIn f o = T imeStep , which is a value between 0 and 1. Fig. 6 shows | RobotGameMov es | an example set of features and their values in a particular game, which can have different values in other games due to different sequences of game moves. Table 1 summarises the features given
according to the following loss function
as inputs to our reinforcement learning agents 5 .
16) = Eval (r + y max Q(s’, a’; 9;) — Q(s, a; ol a
4.1.2. Multimodal actions
# where θ
i are the parameters of the network at iteration i , and θ
are the target parameters of the network at iteration i . The latter i are held ﬁxed between individual updates. This process is known as Deep Q-Learning with Experience Replay [2] .
We extend the learning algorithm described in [1] by reﬁning the action set at each time step so that agents gain access to look- ahead information and can learn to make inferences over the ef- fects of their actions. In this way, our agents anticipate the ef- fects of their decision making better than during pure naive explo- ration/exploitaion. An agent may for example have the winning or loosing move at some point during the game together with other available actions. But this raises the question ‘Why should agents learn what to do if they have the ability to infer that a game is about to be won or lost?’ In the former case (win), it could omit all actions that are not winning moves—unless winning is not the objective, see line 6 in Algorithm 2 . In the latter case (lose), it could avoid all actions that lead to loosing the game—see line 5 in Algorithm 2 . This algorithm requires taking actions temporar- ily in the environment to observe the consequent rewards (with 1 look-ahead time step), and then undo such temporary actions to remain in the original environment state s —before looking for the worst negative actions and/or winning actions. The main changes in contrast to the original DQN algorithm require the identiﬁcation of worst actions ˆ A as well as best actions (if any) so that decision making can be made based on actions in A not in ˆ A , also denoted as A \ ˆ A . Our agents thus select actions according to
Multimodal actions include 18 or 90 dialogue acts (for N&C 3 × 3 and N&C 9 × 9, respectively), where grid square i = { topLe f t, . . . , bottomRight} or i = { a 1 , . . . , i 9 } . Rather than training agents with all actions in every environment state, the actions per state were automatically restricted in two ways. First, dialogue acts are derived from the most likely actions, Pr ( a | s ) > 0.001, with prob- abilities derived from a Naive Bayes classiﬁer trained from example dialogues—see [1] . Second, all game moves were allowed from the subset of those not taken yet (to the robot’s knowledge). Table 2 illustrates the set(s) of outputs of reinforcement learning agents.
4.1.3. State transitions
The features in every environment state are based on numerical vectors representing the last system and user responses, and game history. The language generator used template-based responses similar to those provided in Table 2 .
# 4.1.4. Rewards
The game-based rewards are as follows:
⎧
+5 for winning or about to win the game +1 for a draw or about to draw in the game —5 for a repeated (already taken) action —5 for loosing or about to loose the game 0 otherwise. r(s,a,s’) =
# π ∗
θ (s ) = arg max A \ ˆ A ∗(s, a, θ ) , Q
where both s and a exhibit multimodal aspects. While states s in- clude verbal and visual observations (i.e., words and game moves), actions a include verbalisations and motor commands—see videos in Section 4.2 .
5
Implementation wise, our states are maintained using a dictionary of key-value pairs (also known as ‘Hash Table’)—which can be seen as a memory. From this data structure and at each time step, a vector of numerical values is generated based on a sorted list of keys for consistency purposes. In other words, every value i in a different state ( s i t ) refers to the same key. In this way, observing a new state consists of generating a vector of numerical values from such a data structure, which is given as input to our neural network.
591
592
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
a poo P | sm \\ DA toe robot and Why human YANA ords YK YAN i Pay Mal ‘ ) = \etalsations and | motor commands |
Fig. 5. Illustration of our multimodal deep reinforcement learning agent.
# Table 1
Feature sets for the standard and ultimate noughts and crosses games corresponding to 9 + 9 + 7 + 39 + 9 = 73 and 81 + 81 + 7 + 39 + 81 = 289 features, respectively. While words and temporal information are continuous features [0 . . . 1] , the remaining ones are binary features [0,1]. For the sake of clarity, assuming that all features are binary, the sizes of state spaces would correspond to 2 73 and 2 289 , respectively—these sizes justify the use of a neural-based approach to scale up to such large state spaces.
Num. Feature Description 9 or 81 [ who = rob ∧ what = draw ∧ where = i ∗] Robot game moves, one for each grid square 9 or 81 [ who = usr ∧ what = draw ∧ where = j] 7 [ who = usr ∧ what = command] Robot commands, e.g. gestures 39 Words Presence or absence of uttered words with recognition conﬁdence for human responses 9 or 81 Temporal Information Game moves with time-based occurrence
# Human game moves, one for each grid square
# Table 2
Action set for the standard Noughts and Crosses (N&C) game, where squared brackets denote robot commands such as gestures. A similar set is used for the ultimate N&C game but with a larger set of moves.
# Dialogue Act
# Multimodal verbalisation
Salutation(greeting) “Hello! [who = rob ∧ what = hello]” Provide(name) “I am Pepper [who = rob ∧ what = please]” Provide(feedback = win) “Yes, I won! [who = rob ∧ what = happy]” Provide(feedback = loose) “No, I lost. [who = rob ∧ what = no]” Provide(feedback = draw) “It’s a draw. [who = rob ∧ what = think]” GameMove(gridloc = Middle) “I take this one [who = rob ∧ what = draw ∧ where = middle]” GameMove(gridloc = UpperMiddle) “I take this one [who = rob ∧ what = draw ∧ where = uppermiddle]” GameMove(gridloc = LowerMiddle) “I take this one [who = rob ∧ what = draw ∧ where = lowermiddle]” GameMove(gridloc = MiddleRight) “I take this one [who = rob ∧ what = draw ∧ where = middleright]” GameMove(gridloc = MiddleLeft) “I take this one [who = rob ∧ what = draw ∧ where = middleleft]” GameMove(gridloc = UpperRight) “I take this one [who = rob ∧ what = draw ∧ where = upperright]” GameMove(gridloc = LowerRight) “I take this one [who = rob ∧ what = draw ∧ where = lowerright]” GameMove(gridloc = UpperLeft) “I take this one [who = rob ∧ what = draw ∧ where = upperleft]” GameMove(gridloc = LowerLeft) “I take this one [who = rob ∧ what = draw ∧ where = lowerleft]” Request(playGame) “Would you like to play a game? [who = rob ∧ what = asr]” Request(userGameMove) “your turn [who = rob ∧ what = read]” Reply(playGame = yes) “Nice. Let me start.” Salutation(closing) “Good bye!”
# 4.1.5. Model architectures
The neural networks consist of fully-connected multilayer neu- ral nets with 5 layers organised as follows: 62 or 207 nodes in the input layer, 100 nodes in each hidden layer, and 18 or 90 nodes in the output layer. The hidden layers use ReLU (Rectiﬁed Linear Units) activation functions to normalise their weights. The same learning parameters are used for both games including experi- ence replay size = 10,0 0 0, burning steps = 10 0 0, discount factor = 0.7, minimum epsilon = 0.005, batch size = 2, learning steps = 200 K, and maximum number of actions per dialogue = 100.
moves. While system actions are chosen by the learnt policies, system responses are sampled from templates (verbalisations seen in demonstration dialogues). In addition, while non-game user ac- tions are sampled from observed interactions in the demonstration dialogues, game user actions are generated randomly from avail- able legal actions in order to explore all possible game strategies.
4.2. Integrated system
4.1.6. Simulated interactions
The humanoid robot ‘Pepper’ 6 was equipped with the compo- nents below running concurrently, via multi-threading. This robot system illustrated in Fig. 5 uses the Naoqi API version 2.5, and has
In our simulated dialogues (one per game) and for practical purposes, the behaviour of the simulated opponent is driven by semi-random user behaviour, i.e., from random but legal game
6
http://www.aldebaran.com/en/a-robots/who-is-pepper .
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
333333333333 SSG OOODODOO FEFREREREFEREERERE EEE ER ERER 002000000000000, 333333333 teteteteiriririni YY 12020000000000200007. 7000000000000} See eee DOOOCOCOOBOOODO, YYÜ YUA EEEEEEEEEEREEEEEREİ SOYDU 333333333; foloterotetetotercy 0: < Li 0.56 0.48 0.40 0.32 0.24 0.16 0.08 0.00 o 10 20 30 40 50 Time Steps
Fig. 6. Illustration of input features describing temporal information, i.e. when game moves occur – the higher the value the later the game move occurred in a dia- logue/game.
been fully implemented and tested. Example interactions can be seen in the following videos: https://www.youtube.com/watch?v= 8MqBdkfNl4c and https://www.youtube.com/watch?v=377tVIvd67I . While the former uses handwriting, the latter does not use it due to higher complexity (future work)—instead, the human opponent does the handwriting on behalf of the robot.
4.2.1. Interaction manager
The interaction manager, based on the publicly available tools SimpleDS 7 [26] and ConvNetJS 8 , can be seen as the robot’s brain due to orchestrating modalities by continuously receiving vi- sual and verbal perceptions from the environment, and decid- ing what to do next and when based on the learning agents described above. Most actions are multimodal; for example, ac- tion GameMov e (gridloc = a 1) can be unfolded as “I take this one [ who = rob ∧ what = draw ∧ where = a 1] ”, where the square brackets denote a physical action (drawing a circle or cross at the given location).
4.2.2. Speech recognition
This component activates the Nuance Speech Recogniser once the robot ﬁnishes speaking. Although the targeted games are mostly based on non-verbal interaction, speech recognition results
(words with conﬁdence scores) are used as features in the state space of the deep reinforcement learning agents.
4.2.3. Game move recognition
This component receives video frames as input in order to out- put interpreted game moves as described in Section 3.1 . It gets active as soon as previous verbal and non-verbal commands are executed, and inactive as soon as a game move is recognised. In other words, our robot uses an automatic turn-taking mechanism based on recognised game moves. These vision-based perceptions are used as features in the state space of the deep reinforcement learners.
4.2.4. Speech synthesis
The verbalisations in English, translations of high-level actions from the interaction manager, used a template-based approach and the built-in Acapela speech synthesizer. They were synchronised with arm movements, where the next verbalisation waited until the previous verbalisation and arm movements completed their execution.
4.2.5. Arm movements and wheel-based locomotion
7
# https://github.com/cuayahuitl/SimpleDS .
8
http://cs.stanford.edu/people/karpathy/convnetjs .
This component receives commands from the interaction man- ager for carrying out gestures. We used both built-in arm move- ments for non-game moves and pre-recorded arm movements
593
594
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
corde 12009. Görde 32909 carce.78 909 Greesopng ice. #png irc 855m9) er ros 62909 cross Tipng cross.S0.png Ges ng ceo x cross 102909 cress_98png Cress. 1ODpng Cross dop cross 402909 (a) Training Görde 41png Görde 439m9 m Görde, 90 png, Görde 92.png oss 91909 cross. 98.009 cross102.9n9 ross 105 png r055.117909 rors 1369n9 rors 139.909 cross Müpna cross 149.png Cross 150.png cross 151.9ng corcle_s49n9 nothing png corde 28909 Görde 38.909 n g 9 ress 105png Gross 10Spng_——_—Cross_107 png examples without noise Görde T4png cele 95.909 3 cross_1089ng cross 128.909 cross 132.909 cross 542909 rose 388.909 rors 14599 nothing png nothing nothing Apng eti Spng eee (b) Representative examples with noise
Fig. 7. Example images for training the game move recogniser.
from human demonstrations to indicate game moves. In addition and due to the robot’s short arms, it used its omnidirectional wheels to move from an initial location (right in front of the game grid) to the left/right/front/back in order to reach a targeted grid cell in the game—with return to the initial location. While our robot used locomotion for the standard N&C game, it only indi- cated verbally its game moves in the case of the ultimate N&C game. The latter was due to higher complexity of motor commands and interaction eﬃciency without locomotion. This game setting required the human player to do the drawings on behalf of the robot.
# Table 3
Confusion matrix of test results in character recognition.
Nought Cross Nothing Nought 95.1% 0 0 Cross 4.9% 100% 0 Nothing 0 0 100%
# 5. Automatic evaluation
5.1. Deep supervised learner for character recognition
This classiﬁer labels grayscale images into three classes (nought, cross, nothing) as described in Section 3.1 . The classiﬁer used two sets of images: one set without noise (109 images as shown see Fig. 7 (a)), and the other set with noise (201 images from human
writings with partially included grid lines as shown in Fig. 7 (b)). First, the classiﬁcation accuracy in the data set without noise was 99.9% according to a leave-one-out cross validation. Second, the classiﬁer trained without the noisy data set obtained 74% of clas- siﬁcation accuracy when tested in the noisy data set. Third, the classiﬁer trained with the noisy data set obtained 98.4% of clas- siﬁcation accuracy when tested in the non-noisy data set—see more details in Table 3 . This is an indication of accurate clas- siﬁcation of human handwriting for the targeted game. At the same time though these results suggest that a vision-based clas-
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
# Table 4
Test results averaged over 30 0 0 games (N&C = Noughts and Crosses) using the baseline and proposed algorithms. 1
# and 2
used 100 and 150 nodes per hidden layer, respectively.
Game ModelArch . Learning Average Task Dialogue Algorithm Reward Success Length Standard N&C 1 DQN-Original (Baseline 1) [2] 0.0658 0.8258 13.93 DQN-Variant (Baseline 2) [1] 0.5530 0.9720 13.99 Proposed without Temporal Info. 0.4710 0.9868 15.31 Ultimate N&C 1 Proposed with Temporal Info. DQN-Original (Baseline 1) [2] 0.6300 −0.6900 0.9980 0.7873 14.06 63.54 DQN-Variant (Baseline 2) [1] 0.0177 0.9074 64.82 Proposed without Temporal Info. 0.0693 0.9377 63.62 Ultimate N&C 2 Proposed with Temporal Info. DQN-Original (Baseline 1) [2] 0.1440 −0.1120 0.9753 0.7290 52.03 65.49 DQN-Variant (Baseline 2) [1] −0.0310 0.8663 70.57 Proposed without Temporal Info. 0.0997 0.9473 59.69 Proposed with Temporal Info. 0.1640 0.9846 52.09
Average Reward 30000 100000 150000 200000 O 250000 300000 Learning Steps (no. experiences) Learning Time in hours) 0000 100000 150000 200000 250000 300000 Learning Stops (n0. experiences) 08 r T T mA 5 p g > : 2 8 ozs | ğ a 2 ol 3 i 3 ; | ; goss şi we 5 ? £ : ‘tia 5 £ ; : 0s 16 = = ‘30000 190000 150000 290000 250000 ‘30000 100000. 160000 200000 250000 300000 Learning Steps (no. experiences) LLeaming Steps (no. experiences) (a) DQN-Original (baseline 1) (b) DQN-Variant (baseline 2) 1 2. Ni os 3 § ei 12 : 245 os = : 2 3 3 25 “ “ o 25 o ‘30000 100000 150000 200000 250000 300000 #0000 100000 150000200000 250000 00000 Learning Steps (vo. experiences) Leaming Steps (no. experiences) 4 08 al = 3 2 os ag ş ğ 00s 20 a A % 0s 19 8 g 078 sö 8 B07 78 3 2 065 16 $ 5 ae 2 i i a 30000 100000 150000 200000 250000 300000 Leaming Steps (no. experiences) (c) Proposed without Temporal Info. 15 ‘50000100000 150000 O 200000 O 250000 300000 Leaming Steps (no. experiences) (d) Proposed with Temporal Info.
Fig. 8. Learning curves of DQN-based agents for playing Standard Noughts and Crosses.
siﬁer should be retrained in case substantially different images are observed.
the better in avg. reward and avg. task success, and the lower the better in training time and dialogue length.
5.2. Deep reinforcement learners for game playing
We compare our proposed algorithm described in Section 3.2 against two baselines [1,2] in the domain of Noughts and Crosses (N&C) with two variants. Figs. 8 and 9 show learning curves for the baseline agents (see top plots (a) and (b)), and agents using the proposed algorithm (see bottom plots (c) and (d)). All agents report results over 400 K learning steps or 20 K games—whatever occurred ﬁrst. We use four metrics to measure system performance: average reward, learning time 9 , average task success [0 . . . 1] (win/draw rate), and avg. dialogue length (avg. number of actions per game). Results can be seen as the higher
Our results show that our proposed algorithm can train more successful agents than previous work. This is evidenced by higher task success in plots (c) and (d) vs. (a) and (b), and lower dialogue length in plots (c) and (d) vs. (a) and (b).
We tested the performance of the learnt policies over 30 0 0 games for each of the three agents per game and per architecture (100 vs. 150 nodes per hidden layer), obtaining the results shown in Table 4 . It can be noted that indeed the proposed learning algo- rithm performs better than the baseline algorithms, across games and model architectures. These results also suggest that there is room for hyperparameter optimisation in future work. Nonetheless, these results suggest that our proposed algorithm can be used for training agents with competitive behaviour in social games.
9
Ran on Intel Core i5-3210M CPU@2.5 GHzx4; 8 GB RAM@2.4 GHz.
595
596
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
a7 2 z hae H st : se % E 2 Ae. : 2 18 0 T0000 200000 300000 20000 Leaming Steps (no. experiences) Average Reward Learning Time (in hours) i 100000 200000 00000 Leatning Steps (no. experiences) ‘Average Task Suncess Average Dialogue Length ‘Average Dialogue Length 100000 200000 ‘300000 “400000 Lamina Steps (no. experiences) (a) DQN-Original (baseline 1) Average Reward Leaming Time in hours) 700000 200000 300000 400000 gi 100000 200000 300000 Leaming Steps (no. experiences) (b) DQN-Variant (baseline 2) Average Reward 100000 200000 00000 400000 Learning Steps (no. experiences) ‘Average Dialogue Length 3 Leaming Steps (no. experiences) (c) Proposed without Temporal Info. ‘Average Dialogue Length 100000 300000 i 200000 Leaming Steps (no. experiences) (d) Proposed with Temporal Info.
Fig. 9. Learning curves of DQN-based agents for playing Ultimate Noughts and Crosses.
(a) Training and test environment (b) Deployment environment.
Fig. 10. Robot’s training, test, and deployment environments.
# 6. Human evaluation
We trained and tested our robot system in an oﬃce environ- ment, and deployed it in a partially-known environment (atrium of a University building)—see Fig. 10 . This evaluation ran for four non- consecutive days where the robot played against 29, 64, 27 and 10 human opponents, respectively. The ﬁrst three days involved only the Standard N&C game, and the latter involved both games (Stan- dard N&C and Ultimate N&C). These human-robot games included primary and secondary school children, prospective university stu- dents, and parents—they were visitors to the building rather than traditional experiment participants (no questionnaires involved). While our best interaction policies (but without temporal informa- tion) were used over all games across days, the game move recog- niser evolved due to improvements after each deployment day. The improvements consisted in better game grid detection and hand detection, which reduced misrecognised game moves as follows: 31% on day one, 25% on day two, 22%, and 10% on day four. In games without misrecognitions the robot ended up winning or in
a draw. Algorithm 1 describes our game move recogniser after such improvements, which has been used in both games (Standard and Ultimate Noughts and Crosses) and can be applied to other social games beyond those in this article. Algorithm 2 has also been used in both games and can be applied to other social games and do- mains beyond those in this article. The reason for this is because this algorithm is general enough, not only applicable to games, as long as there is a notion of success and failure (in our case win or loose) at the end of a dialogue, where the same reward function can be applied or extended.
Although the partially-known conditions exhibited in the de- ployment environment were challenging for our interactive mul- timodal robot (e.g., different light conditions, multiple visual backgrounds, two human opponents instead of one, among oth- ers), its human opponents continuously expressed to be im- pressed – presumably due to the fact that the robot was speaking, moving, writing, listening, and seeing—all of these au- tonomously, with near real-time execution, and in a coordinated way.
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
# 7. Conclusion and future work
This article presents a deep learning-based approach for ef- ﬁciently training deployable robots that can carry out joint multimodal activities together with humans that involve speaking, listening, gesturing, and learning. Although the proposed approach assumes no initial training data, it is bootstrapped with relatively small datasets, and it learns interactive behaviour from trial and error using simulations. This approach is demonstrated using the game of Noughts and Crosses (N&C) with two variants. Even when these two variants exhibit different degrees of complexity, the data requirements remained equivalent. In other words, the more com- plex task (Ultimate N&C) did not require more data than the sim- pler task (Standard N&C). Given the generality of the approach and proposed algorithms, they can also be applied to other tasks be- yond N&C. An automatic evaluation shows that our deep super- vised and reinforcement learners achieve high performance in both game move recognition and task success. In the latter, anticipat- ing the effects of the decision making and temporal information proved essential for improved performance. Our experimental re- sults with 130 human participants showed that when the vision- based perception works as expected, successful human-robot inter- actions can be obtained from the induced skills using the proposed data-eﬃcient approach.
haviour across games) and also keeping track of emotion-based signals, which can be taken into the reward function for policy retraining.
- • The proposed algorithms can be applied to other social games (possibly more complex) and also other domains. A robot at home for example should not only be expected to be able to play games, but also to carry out other tasks—potentially trained with the same algorithms across tasks.
- • More real-world evaluations are needed across the ﬁeld to truly and thoroughly assess the performance of human-robot inter- actions in the wild, out of the lab [16–18] . The robot system described in this article would not have been possible without the participation of unseen humans playing against the robot.
# Conﬂict of interest
None
# Acknowledgement
The robot used in this paper was donated by the Engineering and Physical Sciences Research Council ( EPSRC ), U.K. This work was carried out under ethical approval by the University of Lincoln with reference UID CoSREC396 .
Example avenues for future related works are as follows.
- • Extending the robot’s language skills for larger-scale language interpretation [27] and language generation [28] coupled with visual perception, multimodal interaction and motor commands remains to be investigated. In addition, the conversational be- haviour of the robot can be framed within a chatbot approach [23] in order to deal with the out-of-domain responses pointed out by Bohus et al. [20] .
# References
- [1] H. Cuayáhuitl , Deep reinforcement learning for conversational robots playing games, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2017 .
- [2] V.E.A. Mnih , Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529–533 .
- [3] K. Noda , H. Arie , Y. Suga , T. Ogata , Multimodal integration learning of robot be- havior using deep neural networks, Robot. Auton. Syst. 62 (6) (2014) 721–736 .
- • A comparison of the temporal information approach used in this article versus an approach based on recurrent neural net- works (as in [29] ) remains to be investigated, identifying pros and cons of each approach.
- • Another interesting extension to this work is online training [30,31] , which can be investigated for improving the perfor- mance of both supervised and reinforcement learning by for example retraining them after each game. For this, it should be taken into account that the reinforcement learner requires longer training times than its supervised counterpart, where faster training algorithms can be investigated by combining ideas from [32–38] .
- [4] A.H. Qureshi , Y. Nakamura , Y. Yoshikawa , H. Ishiguro , Robot gains social in- telligence through multimodal deep reinforcement learning, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2016, pp. 745–751 .
- [5] H. Cuayáhuitl , G. Couly , C. Olalainty , Training an interactive humanoid robot using multimodal deep reinforcement learning, in: Proceedings of the NIPS Workshop on the Future of Interactive Learning Machines, 2016 .
- [6] S. Wermter , C. Weber , M. Elshaw , C. Panchev , H.R. Erwin , F. Pulvermüller , To- wards multimodal neural robot learning, Robot. Auton. Syst. 47 (2–3) (2004) .
- [7] J. Ngiam , A. Khosla , M. Kim , J. Nam , H. Lee , A.Y. Ng , Multimodal deep learn- ing, in: Proceedings of the International Conference on Machine Learning ICML, 2011 .
- [8] N. Srivastava , R. Salakhutdinov , Multimodal learning with deep Boltzmann ma- chines, J. Mach. Learn. Res. 15 (1) (2014) .
- [9] S. Levine, C. Finn, T. Darrell, P. Abbeel, End-to-end training of deep visuomotor policies, CoRR (2015) arXiv: 1504.00702 .
- • Another interesting extension is learning to write as in [39,40] . While handwriting was relatively straightforward for the stan- dard Nought and Crosses game, handwriting for the ultimate Noughts and Crosses game was a challenge due to more ﬁne grained motions of smaller characters in smaller grid squares on the game board. This extension requires visuomotor learn- ing in order to achieve human-like handwriting.
- [10] D.C. Bentivegna, C.G. Atkeson, A. Ude, G. Cheng, Learning to act from obser- vation and practice, Int. J. Hum. Robot. 01 (04) (2004) 585–611, doi: 10.1142/ S02198436040 0 0307 .
- [11] E. Short , J. Hart , M. Vu , B. Scassellati , No fair!!: an interaction with a cheating robot, in: Proceedings of the International Conference on Human-Robot Inter- action (HRI), ACM, 2010, pp. 219–226 .
- [12] J. Kober , M. Glisson , M. Mistry , Playing catch and juggling with a humanoid robot, in: Proceedings of the IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2012 .
- • Our policy learning algorithm with 1 step look-ahead infor- mation could be explored with multiple time steps, leading to combinations of deep reinforcement learning policies with MinMax [41] or Monte Carlo tree search methods [42] . But there is a trade-off because the more look-ahead information is used, the more computational expense is involved. The training times of our proposed algorithm vs. the original DQN method are equivalent, and the quality of policies are substantially different—ours much more competitive. In addition, our pol- icy learning algorithm can be applied for optimising robot be- haviour that goes beyond always winning. Instead, it could be used to train multimodal robots that keep players as happy as possible. This would require adding further multimodal actions (for varied behaviour rather than repetitive or monotonous be-
- [13] T.E.A. Belpaeme, Multimodal child-robot interaction: building social bonds, J. Hum.-Robot Interact. 1 (2013) 33–53, doi: 10.5898/JHRI.1.2.Belpaeme .
- [14] M. Kim , K. Suzuki , Comparative study of human behavior in card playing with a humanoid playmate, Int. J. Soc. Robot. 6 (1) (2014) 5–15 .
- [15] E.I. Barakova, M.D. Haas, W. Kuijpers, N. Irigoyen, A. Betancourt, Socially grounded game strategy enhances bonding and perceived smartness of a hu- manoid robot, Connect. Sci. 30 (1) (2018) 81–98, doi: 10.1080/09540091.2017. 1350938 .
- [16] M. Jung, P. Hinds, Robots in the wild: a time for more robust theories of human–robot interaction, ACM Trans. Hum.-Robot Interact. 7 (1) (2018) 2:1– 2:5, doi: 10.1145/3208975 .
- [17] H. Cuayáhuitl , Robot learning from verbal interaction: a brief survey, in: Pro- ceedings of the Fourth International Symposium on New Frontiers in HRI, 2015 .
- [18] H. Cuayáhuitl, K. Komatani, G. Skantze, Introduction for speech and language for interactive robots, Comput. Speech Lang. 34 (1) (2015) 83–86, doi: 10.1016/ j.csl.2015.05.006 .
- [19] S. Thrun , et al. , Probabilistic algorithms and the interactive museum tour-guide robot minerva, Int. J. Robot. Res. 19 (11) (20 0 0) 972–999 .
597
598
H. Cuayáhuitl / Neurocomputing 396 (2020) 587–598
- [20] D. Bohus , C.W. Saw , E. Horvitz , Directions robot: In-the-wild experiences and lessons learned, in: Proceedings of the 2014 International Conference on Au- tonomous Agents and Multi-agent Systems, AAMAS ’14, International Foun- dation for Autonomous Agents and Multiagent Systems, Richland, SC, 2014, pp. 637–644 .
- [21] H. Cuayáhuitl , SimpleDS: A simple deep reinforcement learning dialogue sys- tem, in: Proceedings of the International Workshop on Spoken Dialogue Sys- tems (IWSDS), 2016 .
- [22] Y. LeCun , L. Bottou , Y. Bengio , P. Haffner , Gradient-based learning applied to document recognition, in: Proceedings of the IEEE, 86, 1998, pp. 2278–2324 .
- [23] H. Cuayáhuitl , D. Lee , S. Ryu , S. Choi , I. Hwang , J. Kim , Deep reinforcement learning for chatbots using clustered actions and human-likeness rewards, To appear in proc. of the International Joint Conference on Neural Networks (IJCNN), 2019 .
- [36] T.D. Kulkarni , K. Narasimhan , A. Saeedi , J. Tenenbaum , Hierarchical deep re- inforcement learning: Integrating temporal abstraction and intrinsic motiva- tion, in: Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2016 .
- [37] H. Cuayáhuitl , S. Yu , A. Williamson , J. Carse , Scaling up deep reinforcement learning for multi-domain dialogue systems, in: Proceedings of the Interna- tional Joint Conference on Neural Networks (IJCNN), 2017 .
- [38] H. Cuayáhuitl , S. Yu , Deep reinforcement learning for multidomain dialogue systems using less weight updates, in: Proceedings of the INTERSPEECH, 2017 .
- [39] K. Gregor , I. Danihelka , A. Graves , D.J. Rezende , D. Wierstra , DRAW: a recurrent neural network for image generation, in: F.R. Bach, D.M. Blei (Eds.), Proceed- ings of the Thirty-second International Conference on Machine Learning, ICML 2015, Lille, France, 6–11 July 2015, JMLR Workshop and Conference Proceed- ings, 37, JMLR.org, 2015, pp. 1462–1471 .
- [24] R.S. Sutton , A.G. Barto , Reinforcement learning – an introduction, Adaptive computation and machine learning, 2nd ed., MIT Press, 2018 .
- [40] D. Bullock, S. Grossberg, C. Mannes, A neural network model for cursive script production, Biol. Cybern. 70 (1) (1993) 15–28, doi: 10.10 07/BF0 0202562 .
- [25] C. Szepesvári , Algorithms for Reinforcement Learning, Morgan and Claypool Publishers, 2010 .
- [26] H. Cuayáhuitl, SimpleDS: A Simple Deep Reinforcement Learning Dialogue Sys- tem, Springer, Singapore, pp. 109–118, doi: 10.1007/978- 981- 10- 2585- 3 _ 8 .
- [27] L. Deng , Y. Liu , Deep Learning in Natural Language Processing, Springer Singa- pore, 2018 .
- [28] N. Dethlefs, Domain transfer for deep natural language generation from ab- stract meaning representations, IEEE Comp. Int. Mag. 12 (3) (2017) 18–28, doi: 10.1109/MCI.2017.2708558 .
- [41] D. Kalles , P. Kanellopoulos , A minimax tutor for learning to play a board game, in: Proceedings of the ECAI Workshop on Artiﬁcial Intelligence in Games (AIG@ECAI), 2008 .
- [42] D. Silver , A. Huang , C.J. Maddison , A. Guez , L. Sifre , G. van den Driess- che , J. Schrittwieser , I. Antonoglou , V. Panneershelvam , M. Lanctot , S. Diele- man , D. Grewe , J. Nham , N. Kalchbrenner , I. Sutskever , T. Lillicrap , M. Leach , K. Kavukcuoglu , T. Graepel , D. Hassabis , Mastering the game of go with deep neural networks and tree search, Nature 529 (7587) (2016) 4 84–4 89 .
- [29] A.H. Qureshi, Y. Nakamura, Y. Yoshikawa, H. Ishiguro, Show, attend and in- teract: perceivable human-robot social interaction through neural attention q- network, CoRR (2017) arXiv: 1702.08626 .
# [30] B. Settles , Active Learning, Morgan & Claypool Publishers, 2012 .
- [31] H. Cuayáhuitl , N. Dethlefs , Dialogue systems using online learning: beyond em- pirical methods, in: Proceedings of the NAACL-HLT Workshop on Future direc- tions and needs in the Spoken Dialog Community: Tools and Data, 2012 .
- [32] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A.D. Maria, V. Panneer- shelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V. Mnih, K. Kavukcuoglu, D. Silver, Massively parallel methods for deep reinforcement learning, CoRR (2015) arXiv: 1507.04296 .
- [33] V. Mnih, A.P. Badia, M. Mirza, A. Graves, T.P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, CoRR (2016) arXiv: 1602.01783 .
- [34] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized experience replay, CoRR (2015) arXiv: 1511.05952 .

Dr. Heriberto Cuayáhuitl is a Senior Lecturer in Com- puter Science in the College of Science at the University of Lincoln, and member of the Lincoln Centre for Au- tonomous Systems (L-CAS). He received a Ph.D. from the University of Edinburgh in 2009, and has an international research proﬁle in academia and industry in the discipline of machine intelligence including (spoken) language pro- cessing, (deep) machine learning, (multimodal) dialogue systems and robotics. He has published over 70 research papers in these areas, has been led organiser of the in- ternational workshop on Machine Learning for Interac- tive Systems (MLIS), and guest editor of the journals ACM Transactions on Interactive Interactive Systems and Else-
vier Computer Speech and Language. His work in industry has been carried out at SpeechWorks International Inc. (now Nuance Communications Inc.), the German Research Center for Artiﬁcial Intelligence (DFKI), and Samsung Research.
- [35] F.S. He, Y. Liu, A.G. Schwing, J. Peng, Learning to play in a day: faster deep reinforcement learning by optimality tightening, CoRR (2016) arXiv: 1611.01606 .
