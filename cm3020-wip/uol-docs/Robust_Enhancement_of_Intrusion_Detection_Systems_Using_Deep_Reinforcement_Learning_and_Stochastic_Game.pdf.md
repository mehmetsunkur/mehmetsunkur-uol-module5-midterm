IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
# Robust Enhancement of Intrusion Detection Systems Using Deep Reinforcement Learning and Stochastic Game
Hafsa Benaddi , Khalil Ibrahimi , Senior Member, IEEE, Abderrahim Benslimane , Senior Member, IEEE, Mohammed Jouhari , and Junaid Qadir , Senior Member, IEEE
# Abstract— The incorporation of advanced networking technolo-
gies makes modern systems vulnerable to cyber-attacks that can result in a number of harmful outcomes. Due to the increase of security incidents and massive activities on networks, exist- ing works have mainly focused on designing Intrusion Detection Systems (IDSs) based on traditional machine learning and deep learning models. In recent times, state of the art performance has been achieved in various ﬁelds through Deep Reinforcement Learning (DRL), which combines deep learning with reinforcement learning. In this paper, we propose a new DRL-based IDS for network trafﬁcs using Markov decision process (MDP) to improve the IDS decision-making performance. In addition, an extensive analysis of the IDS behavior is provided through modeling the interaction between the well-behaving IDS and attacker players using Stochastic Game Theory. Speciﬁcally, we used a non-zero- sum stochastic game, where the transitions between states depend on both the IDS and the attacker’s actions at each stage of the game. We show that our game reaches a Nash Equilibrium upon convergence to seek the optimal solution, which corresponds to the optimal decision policy where both players maximize their proﬁts. We compared the performance of our proposed DRL-IDS to the baseline benchmark of standard reinforcement learning (RL) and several machine learning algorithms using NSL-KDD dataset. As a result, our proposed DRL-IDS outperforms the existing models by improving both the detection rate and the accuracy while reducing false alarms. Results were provided to demonstrate the convergence of the game theory-based IDS under various settings toward equi- librium. This equilibrium corresponds to the safe state where both players are playing their respective best strategies.
Index Terms—Security, Network trafﬁc, Intrusion Detection System, Deep Reinforcement Learning, Stochastic Game, Q-learning, Performance Evaluation, NSL-KDD.
Manuscript received 31 January 2022; revised 25 April 2022; accepted 12 June 2022. Date of publication 28 June 2022; date of current version 17 October 2022. The review of this article was coordinated by Dr. Zubair Fadlullah. (Corresponding author: Abderrahim Benslimane.)
Hafsa Benaddi and Khalil Ibrahimi are with the Ibn Tofail University, Faculty of Sciences, LaRI Laboratory, 6CW7+CV Kenitra, Morocco (e-mail: hafsa.benaddi@uit.ac.ma; ibrahimi.khalil@uit.ac.ma).
Abderrahim Benslimane is with the University of Avignon, CERI/LIA, 84029 Avignon, France (e-mail: abderrahim.benslimane@univ-avignon.fr).
Mohammed Jouhari is with the School of Computer Science, Mo- hammed VI Polytechnic University, Ben Guerir 43150, Morocco (e-mail: mohammed.jouhari@uit.ac.ma).
# I. INTRODUCTION
A CTUALLY, the overall worldwide IP trafﬁc is expected
to reach 400 exabytes per month while it was only about 120 exabytes per month in 2017 [1]. Thus, effective techniques and tools should be designed to deal with network trafﬁc se- curity such as the threats of critical information disclosure, unauthorized access as well as the disruption of transmitted data. Many worldwide organizations adopted the use of Network Intrusion Detection Systems (NIDS) as a key part of their infor- mation system. This is due to the power of NIDS on securing the information communicated over the target system through processing the network trafﬁc and classifying the normal and abnormal trafﬁcs that can be seen as cyber-attacks. Intrusion Detection System (IDS) is an important process in network security that aims to detect and monitor the network from abnormal activities and the threat of intrusions in the network trafﬁc. This is performed by identifying normal and abnormal network activities. In general, we can classify the IDS systems into two types. The ﬁrst one is called misuse-based IDS (also called signature-based IDS), where its role consists of detecting the intrusion by observing activities that are similar to those used in known attacks. The ability of the ﬁrst method to predict new and unknown attacks is limited due to its dependency on a database of general attack activities. The second type (anomaly detection based IDS) works by creating a proﬁle of normal network behavior and then by identifying any strange behavior that substantially differs from the pattern of ordinary trafﬁc. With such a strategy, the IDS can identify previously unseen attacks. Various works in the literature have adopted standard Deep Learning (DL) techniques such as Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), and Deep Neural Networks (DNNs). Such classiﬁcation schemes enhance the performance of the IDS by ensuring high detection precision of normal and abnormal activities [2], [3], [4]. However, several research works have recommended the use of Deep Learning (DL) for IDS to enable the classiﬁcation of the high dimensional complex features by classifying the critical examples and giving the optimal activation function to get the best performance of the model [5]–[7], [8]. The advantage of DL arises in complex large-scale settings where the number of states is enormous. The DL models can be used in such settings to efﬁciently estimate the
Junaid Qadir is with the Department of Computer Science, Engineering, College of Engineering, Qatar University, 2713 Doha, Qatar (e-mail: junaid. qadir@itu.edu.pk).
Digital Object Identiﬁer 10.1109/TVT.2022.3186834
action values by offering the ability to perform classiﬁcation of
0018-9545 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11089
11090
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
# an incident, object monitoring, image captioning, and semantic
segmentation in real-time. However, including the concept of Q-Learning of Reinforcement Learning (RL) on the IDS has been comprehensively explored for auditing and monitoring the sensor networks. This is used for building a fast decision mechanism where the IDS learns the optimal policy of actions over the set states depending on the considered environment. The interaction between control systems and cyber threats is impor- tant in the network traffics [9]-[12]. To date, several approaches have been proposed to keep the networks safe and credible [13]. Especially, game-theoretic approaches [9], [14], [15] have been considered to solve security problems such as jamming attacks and replay attack. In this regard, a non-cooperative evolutionary game is proposed by Shi ef al. [16] for enhancing the security levels of the systems while considering the honeypot configu- ration dynamically and iteratively against malicious activities. Likewise, the concept of modeling the interaction between the hackers and the technology as a game is introduced by Mishra and Smirnova in [17] where the game results in providing an efficient configuration of the IDS that ensure either a high detection rate and low false alarm rate. We note that this paper is a substantial extension of our earlier work [18], which focused on designing a Deep Reinforcement Learning-based (DRL)-IDS. In this paper, we extend our previous work by improving the IDS that provides more analysis of our proposed DRL-IDS and additionally model the interaction between the IDS and attacker using a non zero-sum-game with incomplete information where both players can dynamically change behavior based on their strategies and their predicted rewards. Moreover, we obtained an optimal strategy to achieve an e-Nash equilibrium game using a value iteration concept. The main contributions of this paper are summarized next:
1) A hybrid RL and DL based IDS scheme (DRL-IDS) is pro- posed to improve the performance of the IDS on detecting cyber threats for network trafﬁcs in real-time and identifying new and existing malware and threats—e.g., Man-In-the Middle attack, and Denial-of-Service (DoS)—with high precision and gaining maximum rewards. DRL-IDS can identify attacks according to their risk level with high accuracy and precision while ensuring a low false alarm rate. The Deep Q-Network is used to improve the Q-function estimation given by RL incorporated with the IDS. The RL algorithm deﬁnes the error target of the estimated Q-function to aid Deep Q-Network decision on estimated state and action.
2) A non zero-sum stochastic game is developed to reach the equilibrium point among players, where each participant tried to take action for achieving an objective of maximizing their proﬁts in the predictability of the subsequent actions. The game theory as proved in the related work, it becomes a new respective of many systems, because, we didn’t need the global information than the classical approach (MDP) to react in real time and ﬁnd the optimal decisions as shown in our numerical results.
obtained from our extensive simulation to study the interaction between players based on our proposed stochastic game in which a safe state of the system is achieved by the game.
The rest of the paper is organized as follows: In Section II, we investigate the related work that has focused on devel- oping approaches-based IDS. In Section III, we describe the formulation of DRL-IDS, a detailed description of NSL-KDD dataset and we model the interaction between two-player a using non-zero-sum stochastic game. In Section IV, we provide the performance evaluation of our proposed IDS models based on the DRL and the stochastic game. Finally, Section V concludes this article.
# II. RELATED WORK
Researchers seeking a much more elaborate discussion on
state-of-the-art intrusion detection systems and open research problems rely on various detailed works published in the liter- ature. Therefore, in this section, we summarize the most recent related work in the ﬁeld. Several traditional Deep Learning (DL) approaches have been widely applied to ensure efﬁcient and accurate IDS. Yin et al. [19] suggested a DNN for intrusion detection in the network. They used RNN to ﬁnd the intrusion. The model is made up of two-stage forward propagation and backward propagation. Their suggested approach was designed for binary and multiclass classiﬁcation and improved jointly the accuracy and the capability to identify the kind of known attacks. Yang et al. [20] proposed a hybrid intrusion detection model using a Modiﬁed Density Peak Clustering Algorithm (MDPCA) and Deep Belief Networks (DBNs), which uses MD- PCA for feature extraction and DBNS for classiﬁcation. They assessed the model on NSL-KDD and UNSW-NB15 datasets. The experimental results showed that the combined MDPCA with the DBNs model achieved better accuracy, detection rate, and false-positive rate. However, the model does not have a strong modeling ability to detect U2R and R2L attacks. Ahsan et al. [21] proposed a hybrid intrusion detection network using CNN and Long Short-Term Memory (LSTM). CNN is used for extracting the feature and LSTM for merging of features. The model showed the highest accuracy on the NSL KDD dataset. Wisanwanichthan et al. [22] suggested a Double-Layered Hy- brid Approach (DLHA) in network intrusion detection, was employed to detect anomalies and unknown attacks. The DLHA model deployed the ﬁrst layer to detect DoS and Probe using the Naive Bayes classiﬁer. The second layer to distinguish R2L and U2R from normal instances using SVM conﬁrmed their high detection rate of 93.11% with over 96.67% detection rate of R2L, and 100% of U2R for the NSL-KDD dataset in comparison with several existing IDS techniques. Liu et al. [23] proposed a hybrid intrusion detection K-Means, Random Forest (RF), and DL, where k-Means and RF were used for binary classiﬁcation, and DL approaches such as LSTM and CNN for multiclass classiﬁcation. The model classiﬁcation was evalu- ated through the NSL-KDD and CIC-IDS2017 datasets. The accuracy of 5 types of data in NSL-KDD reached 85.24%. Min et al. [24] proposed a network intrusion detection method
3) We provide two distinct analyses. First, the non game- theoretic IDS using NSL-KDD as the incoming trafﬁc data to monitor the real-time network streams in which the performance evaluation of the proposed model-IDS is given with several tests on the proposed environments. Second, we discuss the results
using a memory-augmented deep auto-encoder (MemAE). The
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
MemAE model consists of an encoder, a decoder, and a memory
module. They reported experimental results on several datasets, such as the UNSW-NB15 and the NSL-KDD datasets. Kunang et al. [25] proposed a DL IDS using a pretraining approach with deep autoencoder (PTDAE) combined with DNN. They used DAE as feature extraction and ﬁne-tuning phase using DNN architecture. Their technique provides the best attack detection rate performance. Reinforcement learning (RL) illuminates an ample space of learning problems characteristic of autonomous agents performing various interactions in an environment, such as sequential decision-making problems with gaining rewards. Lately, a few studies have likewise incorporated the utilization of RL for IDS improvement. Sengupta et al. [26] proposed a method by altering the Q-learning calculation to learn ideal cut incentive for an alternate property of network trafﬁc. Their model can get high accuracy of 98% and has a quicker handling speed for real-time prediction. Moreover, other few works apply RL-IDS for dispersed network frameworks. Caminero et al. [27] incorporated the supervised learning process with a modiﬁed RL algorithm based on interaction with a real environment behavior based on an adversarial strategy. They replaced the environment by using AWID and NSL-KDD datasets. Their model aims to optimize the agent’s policy function and not deﬁne the reward function. Their suggested approach was achieved accuracy 80.16% and F1-score 79.40%. Lopez et al. [28] con- ducted an application of RL for detecting intrusions in the network. Through its trial after effects of utilizing AWID and NSL-KDD datasets. The best outcomes are acquired for the Double Deep Q-Network (DDQN) with an accuracy of 95.70 % and F1 93.94% measurements. Similarly, Suwannalai et al. [29] proposed the Adversarial/Multi-Agent Reinforcement Learning utilizing Deep Q-network (AE-DQN) for network IDS prob- lems. Likewise, Sethi et al. [30] proposed an RL-based IDS that utilizes Deep Q-Network in numerous disseminated agents and employs an attention process to characterize the network behaviors. Alavizadeh et al. [31] collaborated Q-learning-based reinforcement learning with a deep feed-forward neural net- work approach to avoid malicious behaviors in the network trafﬁc. Moreover, game-theoretical approaches based on trafﬁc anomaly detection are mainly used to study decision drawbacks such as the interaction between decision-makers. It is also used to resolve the issue of the resolution of the equilibrium [32]. Sangeetha et al. [33] designed an intrusion detection system for mobile ad hoc networks based on the Bayesian Hybrid Detection model that reduces malicious nodes as a result of an improved reliability detection. Xia et al. [34] proposed collaborative in-
trusion detection schemes based on sharing strategies in smart
grids, where two-layer collaborative IDSs were designed. The
helped in the training step. Deep reinforcement learning was deployed instead of the traditional machine learning models used for IDS due to its power on providing an accurate prediction of attacks from observations (dataset rows) and its ability to handle high-dimensional states space. Furthermore, we used the stochastic game to model the interaction between players to distinguish between the malicious and honest players such as the players are the trafﬁc generator and the IDS as motivated by the above related work. Therefore, we considered the classiﬁcation of attack to reach a Nash equilibrium where the system is safe under the monitoring of our robust IDS. To the best of our knowledge, this is the ﬁrst time to classify the attack in order to obtain an optimized solution of the game where the IDS maximizes its reward.
# III. PROPOSED MODEL
A. Problem Formulation
We start by deﬁning the concept of RL, another extension of ML-based on a Markov decision process (MDP). The main idea concerning how to take action a to exploit reward R environment by giving a state s to the attacker and IDS. We deﬁne two quintuples: Detection (Sd, Ad, Pd, Rd, γ) and Attack (Sa, Ba, Pa, Ra, ˜γ).
Space of system states: The set of states captured by the IDS is denoted as Sd = {v0 = normal, v1 = detection, v2 = noDetection} respectively. Where v0 means the normal trafﬁc record on the network trafﬁc, v1 which means detection of attacks by IDS in the trafﬁc, and v2 indicates that the IDS cannot detect the attacks because these attacks do not exist in the list of known attacks or new attacks that have not yet exploited. Further- more, The set of states captured by the attacker is represented as Sa = {u0 = U nattack, u1 = Attack} respectively, where u0 means the attacker ﬁnd the system safe, u1 which means the attacker successfully attacked the system.
Space of actions: Set of possible actions that can be taken by the IDS, it is expressed: Ad = {a0, a1, a2, a3, . . ., am}, where ak indicates the type of reaction of the IDS in the kth attacks class and k = 0, 1, 2, 3, . . .m. Moreover, the actions are ordered ([35]) by the risk level of the estimated attacks with a0 < a1 < a2 < a3 < . . . < am. The action set of the IDS is ordered starting from the action with the lowest cost to the action with the highest cost. Knowing that the IDS can take either software or hardware defense strategy to face the attacker action. The set of possible actions that can be taken by the attacker are given by: Ba = {b0, b1, b2, b3, . . ., bn}, where bl indicate the type of reaction of the attacker in the f th level of protection and f = 0, 1, 2, 3, . . .n. Likewise, the actions are ordered by the type of these attacks with b0 < b1 < b2 < b3 < . . . < bn.
ﬁrst layer is used to model the interaction between the IDS
and the attackers. The second layer allows efﬁcient resource
# allocation under harsh resource restriction conditions. Their
proposed method uses a stochastic game that quickly reaches
the Nash Equilibrium where the IDS maximizes its payoff and
results in optimal detection strategies. Motivated by the existing
model suggested previously to enhance the IDS performance,
Transition of states probability: Matrix of transition proba- bilities observing at time t for a ∈ Ad is deﬁne as:
⎛
βa βa βa 1,2 1,3 1,1 ⎠ ⎝ βa βa βa Pd = 2,1 2,2 2,3 βa βa βa , 3,2 3,3 3,1
⎞
we propose a robust IDS model that relies on classifying the
attacks based on their risk and impact on the system which
# where βa
is the probability to change the state of the i,j IDS depending on the impact of actions. Given by βa i,j =
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11091
11092
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
# βa
p(vt+1/vt) = p(vj|vi, a) for i, j = 1, 2, 3 and: 3 i,j = j=1 1 , i = 1, 2, 3 and a ∈ A. The matrix of transition probabilities observing at time t for b ∈ Ba is expressed as:
Pa = ψb 1,1 ψb 2,1 ψb 1,2 ψb , 2,2
Action (Ay), State (Sy), Target Networking Traffics. $ | p ze] ww Vi D a ? — m z = = Gali m .— Cyber-Attacks GL Environment
# where ψb
where we. ; the probability to change the state of the at- tacker according to the impact of actions. Denoted by: 7 ; = q(ui4i/se) = q(uj|ui,b) for i,j = 1,2 and: an os 1 ,i=1,2and be By.
Improving the protection of network data trafﬁcs using DRL-IDS.
TABLE I CLASSES DISTRIBUTION OF NSL-KDD DATASET
Reward functions: We aim to optimize the system’s objective function, which allows us to represent the IDS returns and to feat an action immediately with the location of reward received in the state s and the action a. This reward takes two forms, a negative value Rn when the IDS takes the best action to protect the system even if the investment against the action is too expensive and positive value Rp when the IDS decides the right action. We consider the value of the reward of the IDS as:
Attack Type Training Set Testing Set Normal 67,343 9710 Denial of Service (DoS) 45,927 7458 Probe 11,656 2422 Remote to Local (R2L) 995 2887 User to Root (U2R) 52 67 Total 125,973 22,544
# rd
# t (vt, at)
⎧
Rp For v; = vo anda; = ao; (1 —aj(az))Rp Forv, = vo anda; € {a),.., 4m}; Rp For v; = v; and a; = ag; ¢ (1—Aj;(a,))R, Foru, = v; anda, € {ao, .., 4-1}; Rn For vy = vı anda; € (aşı... am); Rp For uv; = v2 anda; = dm; (1 — 0;(a,))R, Forw, = vz anda; # ao,
=
where 0 < αj(at) < 1, 0 < λj(at) < 1 and 0 < θj(at) < 1 de- scribed the penalty coefﬁcients. Let ra t (ut, bt) be the value of the reward of the attacker which denoted as:
# ra
# t (ut, bt)
⎧
eRp For uş = uo and b; = bo; (1 — 6;(b:)) Rp For uy, = uo and by € {bj,.., bi}; = 4 «Ry For u,; = uş and b; = bp; (1 = pj(b))Rp Foru, = uş and by € {bo, .., bp ik Rp For uy = uş andb, € (bp şı, .., bi}, 2)
=
where 0< ¢;(b:) <1 and 0<p;(b,) <1 denoted the penalty coefficients. The action in each state can be either a reward if detected or a cost if undetected. So these equations (1),(2) represent the gains obtained R, (which means reward positive) in each state this is when the attack is detected and the costs R,,(reward negative) when the attack is undetected. We can consider the average reward of the IDS and the attacker respectively in each time step ¢t as expressed by the following expressions: Ralsı =v, a = 4) = Mesa Pulur, Ariva). Ralsı—u,bı = b) = Mes, ül, bre (ueq1, b).
(1)
(2)
B. Deep Reinforcement Learning-Based IDS (DRL-IDS) Model
Several types of attacks may occur during the deployment of network trafﬁc, data exchanged among these networks are analyzed by a DRL-based IDS [18] which is needed to provide the best decision in order to secure the network as shown in Fig. 1. In following we will explain the different steps to build this proposal:
Dataset Description: Most state-of-the-art IDS approaches utilized the NSL-KDD dataset as a frequently available dataset on NIDS research [36], [37] for the performance evaluation. This dataset represents a reduced version of the KDD Cup 99 dataset, proposed in 2009 by experts in the ﬁeld of network in- trusion detection to solve some issues appearing in the KDD’99 database [38]. The CSV format of the latter has a reasonable number of training records 125973 and testing records 22544 and possesses TCP/IP connection records (each record consists of 41 attributes characterizing the connection (each attribute can be normal or abnormal records). Table I shows the distribution of training and testing sets for the behaviors of network trafﬁc. The training dataset comprises 23 trafﬁc classes that incorporate 22 abnormal classes and one normal class. The test dataset incorpo- rates 38 abnormal classes, out of which 16 are novel abnormal classes and one normal classThe dataset includes four attack categories which are further divided into 39 subcategories. A full explanation of the testbed conﬁguration and attacks is available in the referenced article [39]. We note that the NSL-KDD dataset is accessible for download on the website [40].
Preprocessing: During this step, the recorded dataset col- lected from the network trafﬁc is registered with a different type that can represent a challenge for training the model. However, this dataset consists of redundant and invalid records that can be removed in the input [41]. Typically, the preprocessing of the NSL-KDD dataset, including the defects as mentioned earlier, is outlined in detail as follow:
Discount factors: γ, ˜γ ∈ [0, 1] be discount factor for the IDS
and the attacker respectively.
Data labeling: We categorized the NSL-KDD dataset into 4 types of attacks, including DoS, Probe, R2L, U2R, and one
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
normal data type. Then we mapped all the attribute values of these data types to clear the data and train our agent.
One-hot encoding: The features are most likely to be numeric or boolean, not strings of characters. Therefore, the categorical features are represented by a one-hot encoding, which requires all symbolic data to be transformed into a real vector that enhances the set of features.
Normalisation: Numerical data were mapped to the inter- val [0, 1] with a min-max normalization without warping the differences in the value ranges. This step is useful only when the features differ. This step has the beneﬁt of speeding up some deep learning applications. Following these preprocessing steps, the NSL-KDD dataset was ﬁnally transformed into 42 features, which helped us improve the outcome of the simulation, especially the training process.
Classiﬁcation of Attacks: Traditional IDSs proposed in the literature do not provide sufﬁcient security to detect new types of attacks because they are not adaptive and scalable. With the appearance of new kinds of attacks and the massive rise in net- work trafﬁc, traditional IDS offer limited protection in terms of security and privacy. Critical infrastructure faces both classical and unknown cyber-intrusions, where the classical attacks work to collect, retrieve, and maintain access to the target data by using various techniques; the most popular attacks exploit software vulnerabilities and ﬂaws. On the other hand, unknown attacks can be deﬁned as newly discovered malicious. However, we classiﬁed the risk of attacks based on their appearance on the dataset, such as the most frequent attack is considered as the critical attack (DoS: critical, Prob: high, R2L: medium, U2R: low). Note that in the NSL-KDD dataset there is no feature indicating the risk level of the attacks.
Reinforcement Learning based IDS: In our case we modeled the IDS in such way to select randomly an a,, then the environment take samples of reward r/(v;, a,) depending on the state of arrival v;41; consequently, the IDS obtains its reward on the next state v,.,. Furthermore, 7 is a given policy from v; to v;+1 that specifies the action a, that will be taken in each state v,. Then, the strategy will be updated in each modification of the observation of the IDS in the environment, it produces samples path (vo, a0, r@), (v1, a1, 72), .... We define 7 = (771, 772,...) as the vector of the optimal policies, the target of each data stream is to obtain 7; that represents the best policy. The state of the system v is a sufficient input to select the best action; thus, the maximum expected sum of the IDS rewards at t¢ is given by: 7” = argmargea{rf(v1, a0) + Ces Plt a YP, ¢(v')}. To choose the best state, we define the optimal value function VW, of our IDS in-state v as the expected cumulative reward from the policy 7* which can be calculated in each step i by: V2,(v) = argmax, ca (rf şen ae) + Dyes Pr-1-i(w'|v. Ve). Using the concept of Q-Learning, we will determine a time step size to define the action a, performed at the next time step. The Q-learning algorithm performs the updates according to the optimal policy 7* corresponding to the best action a in each state v, even if it is not these optimal actions that the IDS realizes, with the learning rate 0<a<1. This latter defines how quickly or slowly our
agent learns to take accurate action during the learning process and influences its learning behavior. Q(v, a1) = Qe, aş) + alır + ymaxa,ca{Q(ve1, a) — Ova). The IDS predicts the state value function V;?, to update the pair (v,a) in each iteration to determine which step has the best reward. Then, Q-Learning constructs a Q-table whose lines represent the states v, and the columns represent the actions a. In each vy, the IDS realizes an action a;, observes the reward rd of this action as well as the next state as (v,;41), and updates the estimated value of Q following Bellman equation: Q(vi41, aşı) = (1 = a)Q(ur, ar) + afr? + ymaraea{Q(v',a’)}}.
Deep Q-Network based IDS: We incorporated the Deep Q- Network (DQN) to our model using the Q-function that we estimated previously so that we can ﬁnd the best decision of attack prediction and also improve the effectiveness by estimat- ing the action values over the set of states by the non-linear function Q(vt, at; θ) ≈ ˆQ(vt+1, at+1). The parameter θ refers to the neuron’s weights, which is updated in each iteration step i to train the Q-Network. We present below the improvement provided by this implementation:
- 1. Use a feedforward pass for the current state s to get predict
Q-values for all actions;
2. Apply the experiences replay like an historical of the inter- action process of the IDS in over-time t as ft = (vt, at, rd t , vt+1) into the replay dataset Ht = {h(1), h(2), . . ., h(t)} which can help the network to learn the various transitions of the ancient experiences.
3. Update Deep Q-Network on the records from the training data (v,a,r¢,v’) around the target Q-value by optimizing the loss function at each iteration i denoted as follows: L;(0;) = E[(z; — Q(v, a; 0;))21, which 2; = ré + yargmax,Q(v’, a’; 0; 1). Where 6;_; network parameters of the previous network.
4. Update the weights using back-propagation using the gra- dient of the loss function with respect to the parameters θ as shown above:
# ∇θi
Q(v, a; θi)]. (3)
Li(θi) = E[(zi − Q(v, a; θi))∇θi
The system performance metrics are based on the Algorithm 1.
C. Stochastic Game With Incomplete Information Model
A stochastic game process can be considered as several static non-cooperative games that are repeated over time. Each of them is called a “state,” making stochastic “transitions” between the game states. In this type of game, the participants can switch their actions strategies based on the other players’ past actions and transitions state. We can use this game efﬁciently to eval- uate and analyze the IDS reaction to monitor the system from malicious activities regarding the transitions of the attackers. Formally, we deﬁne the stochastic game as a segment of tuples G =< N, S, M, U, P , γ, ˜γ >, each parameter described as fol- lows: N = {Nd, Na} is the set of players, Nd and Na represent the IDS and the attacker participants. S is the set of the game states. Let Sd describes the IDS behavior from its constrained
0 < α < 1. This latter deﬁnes how quickly or slowly our
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11093
11094
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
Algorithm 1: Pseudo Code of the Non-Game-Theroretic-Based DRL-IDS.
DRL-IDS. Data: Collected dataset Y Input: Initialize State, Action, environment, parameters 4 Initialize the target Q-network Initialize Replay memory H space Output: return vector Q(vz, a+; 0) 1 while |Qi+1 — Qi| <o do 2 for steps 1 to M do 3 for features X <1 to N do 4 v = vo (starting of State v) for t— 0 to T—1 do 5 - Select a random action a; with the random prob- ability p based on e-strategy as: a, = argmax, Q(v, ak; 0) - Apply a; and the IDS observe the reward rf and the next state - Observe chosen reward r, and Store the tuple (ve, ae, rf, ves) in H - Choose a Mini Batch arbitrary with this selected features (vr, a1,71, vışı) from H if v4.1 is a terminal State then 6 m= Tt 7 end 8 else 9 | =r + dargmax,, Q(v’, a’; 0) 10 end i - Calculate the gradient of the loss function based on (3). 2 end 13 end 4 end
The behaviors of the IDS depends on the value of F (a|b, s)
that describes the success prediction of the IDS when the attacker varies the type of attack, and D(b|a, s)) is the damage incurred by the attacker: ⎧
1 ifa = a3; l-e ifa = ay; F(alba, 8) = I—lxe ifa= a, it ifa = ag.
With e depicted the sensibility of thelDSand0<e<l
⎧
0 ifa— az; 1 ifa = a; F(alb1,s) = 1—e ifa — a, 1-5xe ifa = ao.
When the attacker is not doing the attacks, we will have the lack of effectiveness of the IDS, such as F (a|b0, s) = 0.
However, the attacker applied a damage D(b|a, s)) if the IDS
behave following F (a|b, s), so, the damage can be calculated as follow:
⎧
⎨ 0 if b = b0; D(b|a, s)) = ⎩ 1 − ξ if b = b1; 1 if b = b2.
Wherein ξ denoted the level of damage of the attacker and 0 <
ξ < 1.
15
# end
actions set Ad, and the attacker chooses a type of attack inde-
pendently from its actions set Ba at time t. Where S = Sd × Sa the aggregated states space where the actions taken by each participant result in the next state transition probability s(t+1) such as s ∈ S. M = {Ad, Ba} is the set of actions such as Ad = {a0, . . ., ak−1} is a set of IDS actions where k = 4, such as a0 means low level of monitoring, a1 means medium level of monitoring, a2 high level of monitoring and a3 critical level of monitoring. Ba = {b0, . . ., bf −1} is a set of attacker actions space where f = 3, such as b0 stands for not attacks, b1 a passive attacks, and b2 an active attacks. U = {Ud, Ua} is the set of payoff functions matrices corresponding to the strategy space G. Where Ud depict the payoffs of the IDS. Ua represents the expected payoffs of the attacker corresponding to their strategy actions spaces Ad and Ba, respectively. P is the state transition probability set at t, where the transitions between the IDS and attacker is independent of each other. Therefore, each player controls its state separately P from S × M .
Utilities Function: The utilities of the IDS and the attacker at time t depend on the state of the game and the actions taken by each player. Let U (t) d the utility function of the IDS at stage t. The goal of the IDS is to maximise the model’s effectiveness by minimizing the damages of malicious activities. However, the utility can be described U (t) as follow: U (t) d (a(t), b(t), s(t)) = d F (a(t)|b(t), s(t)) × Rd(v(t), a(t)) − D(b(t)|a(t), s(t)) × Cd. Let U (t) a be the utility of the attacker at time t. U (t) a (a(t), b(t), s(t)) = D(b(t)|a(t), s(t))) × Ra(u(t), b(t)) − F (a(t)|b(t), s(t)) × Ca.
State Transition Probability: According to transition proba- bilities, the stochastic game moves from one state to another, simultaneously controlled by the players’ actions. Furthermore, each initialization of time steps depends on the current state and the subsequent potential state. The transition probability is given by the probability P: Pe Jal), 0, 5) = Mes, Pav) at) ONE sO)TL,. Paul) a, vo), st 0) This transition probability relies on the performance of the IDS and the success damage of the attacker, which can be learned directly from experience or indirectly in an empiric way.
Expected Payoffs: Let x“) (s) and y“ (s) be the probabilities in time step ¢ and state s of players Ng and N, to choose respectively the actions a € Aq and b € By. We consider x and y as distribution vectors of strategies of the IDS Oe Ni attacker at t: x = (9 (s), a (s), x 0s), Ye lela 0; Dio: mls) = 1. ¥ = (WC). (8). 21s)) yf!" > 0; Xi yi Ms) =1. Where player Ng takes action ag € Ag with probability a) and player N, takes action ba € B, with probability y‘. Hence, the utility of IDS Ng is the sum of discounted payoffs for endless in- stant: w}(s,x,¥) = oo ES x UP (a, b, s). Furthermore, the utility of the attacker N,, can be expressed as: wi (s,x,y) = “ETE xyUa {(a,b,s8), Where 7,7 € [0,1] are the dis- count factors that models respectively the level of the IDS to monitor against any attack strictly for its own benefit and the attacker to take action separately.
Nash Equilibrium of Game: In this part, we focus our effort on ﬁnding a solution to our proposed game while respecting the requirement of our system in terms of security level. The Nash equilibrium can be described in general for arbitrary strategies as a strategy proﬁle such as no player has an incentive to
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
{DS agent Actions ry a a Unlar) Reinforcement Learning Values (Q-Values) Environment Reinforcement Learning Values (0-Values)
TABLE II EVALUATION INDICATORS
Situations Actual Record | Predicted Record True positive (Tp) anomalous anomalous True negative (Tn) | normal normal False positive (Fp) | normal anomalous False negative (Fn) | anomalous normal
Fig. 2. The Attacker-IDS Stochastic Game Model.
diverge unilaterally from his chosen strategy. We formulate our deﬁnition as follow:
# Deﬁnition 1: Let wγ
# d = [wγ
d (s1), . . ., wγ
# d (sN )] and w˜γ
Definition 1: Let w) = [w}(s1),--.,w}(sn)] and wi = [w7 (s1),--.,w2(sy)]. The Nash equilibrium action is a pair of strategies (x*,y*), that can be expressed as : w}(s,x*,y”) > wi(8,X},--.,Xn,-- XN ¥*), O< Xn < lw (s,x*,y*) > wi (5,X" Yi, -- Yms Vis), 0 < Ym < 1. For all strategies x,y and state s, no participants have the interest in modifying its profiles in the balance. Within the e-Nash equilibrium is a stationary strategy that can verify the following formula: wi (s,x*,y*) — Ww) (8, X},---.Xns-- XN") 260 < Xn < 1. wi(s,x*,y*) — wİ(S.XİYİ, ymm ya) >6,0< Ym <1.
Let η, ˜η be an equilibrium of the game ˙Γ(V t+1, ˜V t+1) for
0 < t < T , The value functions for every s ∈ S given by the following formula, wherein Qt(a, b, s) is updated by:
Qi (a,b, 8) = Ent nt ge +% >, ses Ua(a, b, s) P(s'la,b, EY), (7)
Q4(a, b,s) = Ey at at, U,(a, b, s) +4; YL P(s'|a,b, ay), (8) s'eS
# Deﬁnition 2: Nash equilibrium of the auxiliary game ˙Γ
Definition 2: Nash equilibrium of the auxiliary game I with the equilibrium payoffs (w}(s,x*,y*),w7(s,x*,y*)) is the strategy profile (x*(s),y*(s)). For each s€5, the payoffs of the players N, and N, are defined respectively as follows: T'(s,a,b) = Ua +7 Yves P(s'la,b, 8) uye x,y) F(s, a,b) = UN es P(s'la, b, s)wd(s,'x,y).
In order to ﬁnd the Nash equilibrium solutions, we use the a, w˜γ solutions of the ﬁxed-point equations (4),(5), then wγ b the unique solutions are given by solving the next systems:
a = N E{ ˙Γ(a, b, wγ wγ a)}, (4)
b = N E{ ˙˜Γ(a, b, wγ w˜γ b )}, (5)
where N E is the Nash payoff function that yields non-zero-sum
game.
Theorem 1: Any non-zero-sum stochastic game with ﬁnite discounting has at least one equilibrium point in stationary strategies (As proved in [42]).
Optimal Decision Making Policy: In the previous section, we modeled the two-player non-zero-sum stochastic game. In such a game, the IDS aims to maximize its expected payoff by minimizing the cost of damage incurred by the attacker. In contrast, the attacker aims to maximize his expected payoff. Consequently, the Q-value is maximized. Hence, we give details on how to solve this game from the side of the IDS and attacker to get optimal decision policy by a value iteration scheme. We also described the expected total value of a stochastic game by V t(s) at a state s. At a given stage t, we denote a Qt(a, b, s) as an expected discount cost when the IDS takes action a, and the attacker takes action b in the current state s as shown in Fig. 2.
However, (7, 7) is the e-Nash equilibrium of every s € S. Then,
η, ˜η are optimal stable strategies for the IDS and the attacker respectively.
# IV. EXPERIMENTAL RESULTS AND ANALYSIS
A. Performance evaluation metrics: In order to study the efﬁciency of our proposed DRL-IDS to predict the system’s state and estimate the quality of attack detection in the network environment, a set of performance metrics is applied such as ac- curacy, detection rate, false negative rate, recall, and F-measure which relies on the following metrics: Tp (True positives), Tn (True negatives), Fp (False positives), Fn (False negatives) that constructed the confusion matrix in Table II.These metrics are deﬁned as follow:
Accuracy (AC): The aptness to predict accurately and to detect all known and newly malicious activities.
Precision (PR): Conﬁgurable hyper-parameter used to detect correctly the intrusive actions of nodes trying to attack the system which is expressed as follow: P R = T p T p+F p ∗ 100%.
False positive rate (FPR): The normal behaviors that are detected as abnormal activity, described in the above formula: F P R = F p F p+T n ∗ 100%.
Recall (RC): The ratio of accurate prediction over the overall anomaly detected in the analyzed records, equivalent to the Detection Rate (DR) which can be calculated as: RC = DR = T p T p+F n ∗ 100%.
F-measure (FM): A measurement of the precision calculated as the weighted harmonic mean of the detection rate and pre- cision of the model. FM formulated by: F M = 2 ∗ P R∗RC P R+RC ∗ 100%.
The optimal value in this game can be obtained as follow:
Vi(s) = max max IE { > y! Q:(a, b, o.) : (6) x acAg
B. Experimental setup: The simulation results provided in the following sections are obtained using an Intel(R) Core(TM) i5-5200 U CPU machine with 8 GB memory. In the ﬁrst part of our study concerning DRL, we performed our simulation using
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11095
11096
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
# TABLE III
PARAMETERS INPUTS SETTINGS
Parameters Value Learning rate 0.4, 0.04, 0.004, 0.0004, 0.00004 batch size 32 Episodes 37790 (305), 88180 (70%), 125973 (100%) Episode length 1 timestep buffer size Discount factor 0.999 Type of features Normal, DoS, R2L, U2R, PROBE. Tested dataset NSL-KDD Number of actions of the IDS k 4 Number of actions of the attacker f | 3 Profit of both 1 players (%) high-90,low-50 Cost of IDS Cg (%) 10 Cost of the attacker Ca (%) 15 Trust rang (0,1) Tteration 500 Sensibility € 0.1 Discount Factor for the IDS + level Discount Factor for the attacker 7 Damage € high-0.3, low-0.05 high-0.3, low-0.05 0.1
Average reward © & 0.3 —© —Leaming rate = det 0.24 —*— Learning rate = 4e2 —8— Learning rate = 4e3 | 0.1 —A— Learning rate = 4e4 —— Learning rate = 4e5 o a 0 05 1 15 2 25 3 35 4 Episode x109
Fig. 3. DRL convergence for different Learning rate value.
the Stable Baselines framework, composed of a set of RL algo- rithms stable implementation on the top of OpenAI Baselines. Using this framework gives a credential of our results since we worked on a structured environment that respects RL properties and OpenAI Baselines requirements. Using this framework, we deﬁned our customized environment to handle the NSL-KDD dataset as an observation and modeled our reward function for efﬁcient agent learning. The agent gets a reward for the current episode only if his action corresponds to the label, knowing that we considered in our case each episode include only one timestep. We used the Google Colab platform to implement our game model and analyze the numerical results in the second part. This part consists of implementing our mathematical formula to study our proposed modulation’s performance. The parameters values used in our contribution are listed in Table III.
DoS 8000 Probe = 6000 5 2 ri R2L E 4000 U2R 2000 normal 0 DoS Probe R2L UZR normal Predicted label
Fig. 4. Confusion matrix of our detection model on NSL-KDD dataset.
C. Proposed DRL-IDS evaluation: We analyzed and discussed the impact of the proposed DRL-IDS and its performance com- pared to the existing IDSs in the literature. We analyzed our proposed IDS model based on DRL by simulating its conver- gence using different learning rates. Furthermore, we trained our model on different portions of the NSL-KDD dataset. Our sim- ulation considers a DRL agent with two-layer fully connected and a Deep RL with seven hidden layers. We then compared the obtained results with the existing model in the literature. Through this study, we have the detection of data streams cover three states: normal, detection, and noDetection. We consider the transition probability matrix between states Pd1 which is sym- metric probability, then we obtain similar probability of Pd2 and ⎛ ⎞
0.4 0.3 0.3
⎝ ⎠ . To ﬁnd the best optimal actions 0.3 0.4 0.3 Pd3 as: Pd1 = 0.3 0.3 0.4
that can improve the performance of the proposed scheme, we adopted the technique of deep Q-network. The complexity of the Q-learning algorithm depends on many possible aspects, such as the high number of actions and the state transition probability. Hence, deep Q-learning uses DL to approximate the Q-function. Consequently, the convergence time is affected by many factors, including the learning rate, the mini-batch size, and the number of convolutional layers. However, the evaluation of the target
Q-network is the only one trained relying on the gradient de- scent method, and we substitute the target Q-network for every mini-batch size by the updated trained Q-value. Among the tests that we performed for our implementation, we considered simulating the convergence of the model. Initially, we trained the model using a learning rate of α=4e-1 that is depicted in Fig 3. by the red line, and we can easily see that this case is the worst since the model converges at the lowest reward value corresponding to the lowest accuracy. The blue line in this ﬁgure depicts the training using the learning rate 4e-2; in this value, the model is not stable, as shown in the ﬁgure. The reward converges at 0.9 and unexpectedly goes down to 0.5 after 15000 episodes. Later, we trained the model on a learning rate of 4e-3 corresponding to the best accuracy, as shown in the following ﬁgure. The remaining simulations converge to the maximum, but they don’t reach 1 and are unstable. Consequently, they ensure good accuracy about 0.8 and 0.9 but not the best as obtained by the learning rate of 4e-3. Fig 4 shows the outcome of our best simulation test using a learning rate of 4e3. It shows the confusion matrix for testing on KDD test+ of our proposed agent on detecting different attacks. Our agent detects all probe attacks, which are 2421 attacks in the test dataset. While 2863 R2L attacks can be detected by our agent and fail to identify the remaining R2L attack of the test dataset. This ﬁgure shows that
our agent fails in detecting U2R attacks. In addition, the majority
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
Tue label 8000 6000 4000 2000 0 DoS Probe R2L OUZR normal Predicted label (a) 8000 6000 4000 2000 0 normal Tue label DoS Probe R2L UZR Predicted label (c) Tue label 8000 6000 4000 2000 0 DoS Probe R2L OUZR normal Predicted label (b) 8000 6000 4000 2000 0 normal Tue label DoS Probe R2L UZR Predicted label (dy
Fig. 5. Confusion matrix of DRL agent trained using different learning rates.
of Dos attacks and normal activities can be identiﬁed using the
DRL-IDS. All these results will be supported by showing the accuracy, the precision, the F1-score, and the recall in Fig 6. As we can see, our proposed DRL-IDS cannot detect U2R attacks due to the inefﬁcient U2R sample in the dataset used for training the agent. This means that our agent requires sufﬁcient samples of the speciﬁc attack for training to identify this type of attack accurately. Our trained agent can identify the most dangerous attack, which is, in our case, the DoS attack. Moreover, our DRL-IDS agent outperforms the existing model in literature by providing the most accurate classiﬁcation of DoS attack and identifying well the normal trafﬁc. Fig 5. shows confusion matrices of different tests conducted toward ﬁnding the best DRL training parameters. In this case, we varied the learning rate on the training process of our agent as shown in the previous Fig 3. each learning corresponds to a different learning outcome. As we mentioned previously, the write learning rate in our case is equal to 4e-3 while the other tentative either converge in lower reward or gives unstable convergence, which is the case of the learning rates 4e-1, 4e-2, 4e-4, and 4e5 depicted respectively in Fig 5(a), (b), (c), (d), (a) shows the training results for a learning rate of 4e-1. Our agent in this experiment can only identify the DoS attacks, such as 7349 were identiﬁed while failed on identiﬁed only 201 DoS attacks. The other attacks’ classes can not be identiﬁed using our DRL agent trained with this learning rate. Fig 5. (b) shows the confusion matrix of agent training using 4e-2 as the learning rate. In this scenario, our agent detects normal trafﬁc well while completely failing to detect the other classes of attack. In these two latter cases, the behavior of our model will be explained by the low accuracy resulting from training with
rate can detect the majority of DoS and Probe attacks as well
as the normal trafﬁc as shown in Fig. 5(c) Where fewer DoS attacks and fewer Probe attacks are detected using agent trained on 4e-5 while normal trafﬁc can be easily identiﬁed, this outcome can be easily explained when considering the model’s accuracy- such accuracy will be less than that of the agent trained on 4e-4 learning rate Fig. 5(d). To adjust the training parameters of our DRL-IDS agent, we split the NSL-KDD dataset into different portions used for training, knowing that we have a separate dataset for testing. We should note that each row of the dataset corresponds to an episode. In general, each episode is composed of a set of time steps where the agent takes action to win a reward at the end of the episode. Although in our case, the agent plays one time and gets a reward which means that the episode is composed of a single time step. Fig 6. shows results of training using different dataset portions, accuracy, precision, F1 score, recall are all used to identify the best dataset portion to train our agent efﬁciently and get the best results. Green, orange, and gray bars depict the results obtained for training on 30%, 70%, and full dataset. The green bar shows the best results compared to the others, which means that training our agent using only 30% of the dataset results in a DRL-IDS agent that can provide the most accurate classiﬁcation. We can explain this by the number of experience required to atteint the maximum convergence of the DRL agent. Experience is the raw data received by the agent as an observation to take action at each episode, which is the decision if the entered raw data corresponds to a speciﬁc attack or is normal data. We observed in our study that adding more data raws for training only destabilizes the agent and reduces its performance; as shown in our simulation results, this is due to the unbalanced
the imperfect parameters. DRL agent trained using 4e-4 learning
NSL-KDD dataset. Knowing that learning rate, batch size, and
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11097
11098
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
M03 10751 0.993657 0.972674 0.990728 0.973023 0.992185 0.971587 0.993657 0.972674 0.897684 0.864437 0.871219 0.864437 ACCURACY PRECISION Fi SCORE RECALL
# TABLE IV
DRL-IDS PERFORMANCE EVALUATION METRICS IN DIFFERENT ATTACK CATEGORIES ON NSL-KDD TEST SET
Class PR RC FM DoS 1.00 1.00 1.00 Probe 0.99 1.00 1.00 U2R 0.00 0.00 0.00 R2L 0.98 0.99 0.99 Macro average 0.79 0.80 0.80 Weighted average 0.99 0.99 0.99
TABLE V COMPARISON OF ATTACK DETECTION OF EXPERIMENTAL RESULTS WITH OTHER EXISTING STUDIES (%)
Fig. 6. Performance metrics of DRL-IDS trained on different NSL-KDD portion.
# Ir=
# lr=
0.004 slr=0.0004 0.00004 & bi a 6 | . | 5 RECALL 1-04 8lr-004 2 0.993657 0.992185 0.992657 0.865413 0.726726 0.732733 0.430511 o.g13451 0.236407 Fi SCORE MM 0.162821 BE 0326044 ACCURACY PRECISION
Performance evaluation of DRL-IDS using different learning rates.
Approaches DoS Probe R2L UZR RNN [19] 83.49 83.40 24.69 11.50 MDPCA-DBN [20] 81.09 73.94 17.25 6.50 CNN-LSTM [21] 98.8 0.0 94.6 99,4 DLHA [22] 92.4 90.87 96.67 100 Hybrid K-means+RF [23] 90.42 91.53 73.84 25.79 MemAE [24] 94.23 95.70 85.86 95.79 PTDAE [25] 99.36 100 48.25 18.92 Siam-IDS [43] 85.37 48.66 33.25 56.72 Our Proposed DRL-IDS 100 99.0 98.0 0.0
model for the attack classes like DoS, Probe, and R2L. But, the model considers the attack of the type U2R as missing/neglected. Because the number of existing attack of this class is very low as depicted in Table I) and the model use 30% of training data including this class. This means that our agent requires more samples of the speciﬁc attack for training to accurately identify this type of attack. Our trained agent can identify the most dangerous attack, which is, in our case, the DoS attack.
the number of episodes is the main factors that should be tuned to obtain the perfect convergence of our agent in the training process. On the other hand, Fig 7. shows different performance parameters of testing our proposed DRL-based IDS, such as the accuracy, precision, F1-score, and recall. This ﬁgure shows the outcome of testing our pre-trained agent using different learning rates in the same direction as Fig 5. to explain the behavior of the system. The blue bar depicts the agent trained using a learning rate of 0.4, the worst one, as shown in the confusion matrix. Agent trained using 0.04 learning rate presented by orange bar achieve an accuracy of 43.05% and precision of 31.34%. Otherwise, the best results are depicted by the green bar corresponding to the pre-trained agent using a learning rate of 0.004; an accuracy of 99.36 is ensured, which is the highest one. While this agent can provide a decision with a precision of 99.07%, these results support the previous ones, meaning that 0.004 is the best learning rate to train our DRL agent on 30% of the dataset. Other ﬁndings show promising results in terms of accuracy and F1-score but still under the performance of the previous agent.
1) Impact of Attack Scenarios: We benchmarked our proposed DRL-IDS by evaluating its performance metrics in various at- tack scenarios as described in the table IV. The results demon- strated that DRL-IDS could accurately detect different kinds of attacks. However, its performance is better for DoS attacks, which its high occurrence can explain in the NSL-KDD dataset. The results in this table show the good performance of our
2) Comparison with state-of-the-art: The outcomes of the performance evaluation of our proposed DRL-IDS model are compared with the state-of-the-art applied for intrusion detection system using the NSL-KDD dataset. We compared the results of the detection rate (DR) of each attack in the dataset with hybrid approaches such as MDPCA-DBN, Siam-IDS, CNN-LSTM, PTDE, MemAE, and hybrid k-means and Random Forest. Furthermore, Reinforcement learning (RL) approaches such as Adversarial Environment Reinforcement Learning(AE-RL), Double Deep-Q-Network(DDQN), Adversarial/Multi-Agent Reinforcement Learning using Deep Q-Learning (AE-DQN), Attention mechanism-based Deep-Q-Network (A-DQN), Deep Q-Learning (DQL) as provided in [27], [28], [29], [30], [31]. The remaining comparison is based on the Accuracy (AC), False Positive Rate (FPR), Precision (PR), Recall (RC), and F1-score (FM). In Table V the comparison reports that our proposed DRL- IDS is not the best model to detect U2R, as our ﬁndings are about 0%, while others show higher scores. In contrast, our model can successfully identify any attack compared to others with low detection ratings on DoS, Probe, and R2L. Our model signif- icantly exceeds all other approaches by achieving the highest detection rates of 100% in DoS, 99% in Probe, and 98% in R2L. As demonstrated in Table VI, our model is also more accurate compared to other methods, whereas it has less FPR. However, the lowest AC achieved by the DQL model is approximately 78.07%. Both DDQN and A-DQN models have an accuracy of 89.78% and 97.2%, respectively. In addition, it was observed
that our DRL-IDS model offered the best performance based
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
# TABLE VI
PERFORMANCE EVALUATION METRICS WITH EXISTING STUDIES ON RL APPROACHES
Approaches AC FPR PR RC FM AE-RL [27] 0.8016 - - - 0.794 DDQN [28] 0.8978 - 0.8944 0.9303 0.912 AE-DQN [29] 0.80 - - - 0.79 A-DQN [30] 0.972 0.0142 0.965 0.991 0.978 DQL [31] 0.7807 - 0.7784 0.7676 0.8141 Our proposed DRL-IDS O 0.9936 0,0022 0.9907 0.9936 0.9921
TABLE VII COMPUTATIONAL TIME FOR TRAINING AND PREDICTION OF APPROACHES
Approaches Training time Predicting time (sec) (sec) AE-RL [27] 1090.13 0.50 DDQN [28] 507.01 0.55 DQN [28] 290.5 0.54 Policy Gradient [28] 352.48 0.65 Actor Critic [28] 1725 0.83 DQL [31] 1260 Our proposed DRL-IDS (90.402 0.32
Utility (value) 0 200 400 600 (b) Utility (value) Iteration (c) (d) Iteration
Fig. 8. The Q-value of the IDS against the opponent actions of the attacker such as ﬁgures (Fig. (a), (b), (c), and (d)) correspond respectively to the IDS taking the actions a0, a1, a2, and a3.
on FM(F1-score). This is because of the efﬁcient modulation that allowed the DRL agent to learn well during the training process and make accurate decisions to protect the system from attacks during the inference step. During the training process of our DRL-IDS agent, we identiﬁed that once the RL agent is trained, the decision-maker gives the accurate intrusion label (which is the agent action) for the considered state (depicted as the intrusion features). This can be viewed only as a simple neural network model that performs fast inference to provide the most accurate prediction and be used in an industrial production environment. This pattern can be conﬁrmed in Table VII, which denotes the training and predicting times for our model with such RL approaches. It can be seen that our model has a smaller training and predicting time than other approaches. Such as 90.402 seconds spent on training, and 0.32 seconds on testing overall KDDT est+.
D. Stochastic Game Impact on the System Safe: We evaluated the IDS performance using the stochastic game, where this later consists of modeling the interaction between the IDS and the attacker. To obtain an efﬁcient solution for our system, we used a value iteration algorithm to solve the equations (7) and (8) con- sequently to ﬁnd the game value and obtain optimal stationary strategies. We recall the actions taken by the IDS are a0 = 0, a1 = 1, a2 = 2, a3 = 3 correspond to the level of protection low, medium, high, and critical respectively. The attacker actions b0 = 0, b1 = 1, and b2 = 2 denote respectively that the attacker does not attack, attacks passively, and attack actively. We assume in our case that the transition probabilities between states of the game are unknown to the system. In the following, we split several scenarios to illustrate the convergence of our proposed model. Fig. 8 shows the Q-value of the IDS using his actions against the attacker’s actions corresponding to b0, b1, and b2. We can see that the IDS’s Q-value converges after the ﬁrst 10 rounds for the four scenarios, such as Fig. 8(a) shows the result of the IDS playing action low protection in the function of the attacker’s actions. The IDS will lose against the attacker when the latter plays the action b2 while the IDS wins the game by maximizing
performed by the attacker is not dangerous for our system while
on the other side the IDS kept their resources for upcoming dangerous attacks. Fig. 8(b) shows that the IDS maximizes its Q-value and wins the game by playing the action a1 against b2. At the same time, the IDS loses the game when playing b0. This is due to the energy spent in detecting the attack while the attacker plays b0 corresponding to no attacker, which is called a true negative rate. From Fig. 8(c) corresponding to the IDS playing the action a2, we can see that the IDS wins the game against the attacker when the latter plays the action b1 while losing the game against b0 corresponding to a false-negative rate. Fig. 8(d) shows that the IDS only maximizes its Q-value when the attacker is performing an active attack depicted by b2 (Detection Rate). Similarly, to the previous results Fig. 8(c), the IDS loses the game against the attacker playing either b0 or b1. This is because the IDS takes high-risk and critical actions against passive and low risque attacks which results in a negative reward for the IDS that fails in predicting the risque of the attacker’s action. In Fig. 9 we plot the Q-value of the attacker regarding the actions of the IDS such as Fig. 9(a), (b), (c), and (d) correspond respectively to the IDS taking the action low detection, medium, high and critical detection. In general, the attacker can only maximize its Q-value when performing an active attack (b2) regardless of the IDS action. In contrast, the attacker obtains the best Q-value when the IDS takes action a0 corresponding to the low detection Fig. 9(a) after converging to the max Q-value. The optimal Q-value decreases regarding the IDS actions as shown in ﬁgures Fig. 9(b), (c), and (d). In this case, the action b2 against the action a0 of the IDS presents the best scenario of the attacker’s gain. In Fig. 9(b) the attacker gain less with the action b2 due to the detection level of the IDS playing a1. The max Q-value of the attacker keeps decreasing regarding IDS’ actions, such as the lowest Q-value correspond to the critical detection of this later. Fig. 10(a) conﬁrms the existence of the Nash equilibrium in the game (a∗ , b∗ ) and (a∗ , b∗ ), where both points correspond 0 1 3 2 respectively to compromised system s4 and safe system s1, such
his Q-value when the attacker plays b1 since the passive attack
as the ﬁrst point present the scenario where the attacker perform
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11099
11100
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022
oo 3 Rİ = 2 5 © 3 3 = 2 —S— bya, 5 os —a—b,a, —A—b,a, “o 200 400 600 Iteration Iteration (c) (d)
Fig. 9. The Q-value of the attacker against the opponent actions of the IDS such as ﬁgures (a), (b), (c), and (d) correspond respectively to the IDS taking the actions a0, a1, a2, and a3 against attacker taking the actions b0, b1, and b2.
(b)
si and y∗(si) is the attacker actions probability at state si.
Such as the mixed strategies at the equilibrium are given by (0.03343351, 0.06995044, 0.15010229, 0.74651377), (0.529 44866, 0.30220828, 0.08202081, 0.08632225), (0.17142309, 0.18300888, 0.26373177, 0.38183626), (0.5737554, 0.357 35559, 0.05597631, 0.0129127) for s1, s2, s3 and s4 respectively, for the IDS and (0.02211347, 0.10832202, 0.86956452), (0.75766459, 0.12885015, 0.11348526), (0.732 69738, 0.19175167, 0.07555095), (0.08460271, 0.3667609, 0.54863639) for the attacker. Fig. 11(b) shows the probability that the system is at the safe s2 and not safe s4 states corresponding respectively to the best strategy of the IDS and the attacker. This later represents the probability of the attacker attaining the equilibrium (blue line). We can see that the probability of both players reaches the value one after 40 iterations; this is due to the convergence of our system as shown in ﬁgures Fig. 8(d) and Fig. 9(a), where attacker scheme converge after 40 iterations. In addition, we remark that the IDS probability is more signiﬁcant than the attacker and attain one faster. Anomaly activity is successfully detected in the system corresponds to the state s1 (true positive), the state s2 means the system successfully detected acceptable activity (true negative). A false positive state s3 is when normal activity identiﬁed as an anomaly. The last state, false-negative s4, is the most dangerous one. This is when malicious activity is identiﬁed as normal.
# V. CONCLUSION
Fig. 10. (a) Best reply of IDS (player I) and the best reply of attacker (player II) curves show the existing Nash Equilibrium solutions namely (a∗ = a0, b∗ = b1) and (a∗ = a3, b∗ = b2). (b) The Q-values of both players at the Nash equilib- rium. (a∗ , b∗ ).
3
2

Fig. 11. (a) The IDS-game value is plotted in function of the iteration times at different states of the game. (b) Probability of system being safe.
passive attack and the IDS apply a low detection strategy. The second one corresponds to the attacker performing an active attack and the IDS performs a critical detection which allows them to protect the system against the attacker. These results conﬁrm the expected system protection. At the equilibrium, each player gets its maximum Q-value depending on others’ actions, as depicted in Fig. 10(b). This is attained only if both players kept unchanged their best strategy.
This paper presents a robust IDS based on DRL and the analyses of the interaction between the IDS and attacker using a non zero-sum stochastic game model. The aim is to detect and identify known and unknown malicious behaviors from the network trafﬁc based on the NSL-KDD dataset. We have implemented the IDS with and without the game where the proposed DRL-based single IDS approach built based on MDP has shown signiﬁcant improvements in terms of accuracy, pre- cision, recall, and F1-score. Likewise, using the concept of a value iteration in the game, we have obtained better protection of the network trafﬁcs to deal with the active attacker. The obtained results show that our proposed model can provide a high-security level of the system compared to the existing ap- proaches and handle the network’s time-variant property under any type of attack. Therefore, our future work will focus on evaluating the model on the other datasets such as BoT-IoT and IoT-23 intrusion detection and developing an extended model to improve the network security against large-scale cyber-attacks and show the impact of heterogeneity on the IDS performance and computational complexity. Furthermore, it can be interesting to extend this work to a general non-zero-sum stochastic game by considering multiple attackers and IDS scenarios and ﬁnding the safe situation of the system.
In Fig. 11(a) we plot the game value V (s) for s = s1, s2, s3, s4 for the stationary mixed strategies. After 500 iterations the game value of each state are given by V (s1) = 1.5311, V (s2) = 1.5967, V (s3) = 0.5030, V (s4) = 0.0373 which correspond to the mixed equilibrium points (x∗(si), y∗(si)) where x∗(si) present the IDS actions probability at state
# REFERENCES
- [1] T. Barnett, S. Jain, U. Andra, and T. Khurana, “Cisco visual networking index (VNI): Complete forecast update, 2017–2022,” Americas/EMEAR Cisco Knowl. Netw. Presentation, 2018. Accessed: Jan. 5, 2022. [Online]. Available: https://www.cisco.com/c/dam/m/en_us/network- intelligence/service-provider/digital-transformation/knowledge- network-webinars/pdfs/1213-business-services-ckn.pdf
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
BENADDI et al.: ROBUST ENHANCEMENT OF INTRUSION DETECTION SYSTEMS USING DEEP REINFORCEMENT LEARNING
11101
- [2] G. Singh and N. Khare, “A survey of intrusion detection from the per- spective of intrusion datasets and machine learning techniques,” Int. J. Comput. Appl., pp. 1–11, Feb. 2021, https://doi.org/10.1080/1206212X. 2021.1885150
- [3] Y. Zhao, Y. Xun, and J. Liu, “ClockIDS: A real-time vehicle intru- sion detection system based on clock skew,” IEEE Internet Things J., doi: 10.1109/JIOT.2022.3151377.
- [4] Y. Xun, J. Liu, and Y. Zhang, “Side-channel analysis for intelligent and connected vehicle security: A new perspective,” IEEE Netw., vol. 34, no. 2, pp. 150–157, Mar./Apr. 2020.
- [5] I. H. Sarker, “Deep cybersecurity: A comprehensive overview from neural network and deep learning perspective,” SN Comput. Sci., vol. 2, no. 3, pp. 1–16, 2021.
- [6] Y. Xun, Y. Zhao, and J. Liu, “VehicleEIDS: A novel external intrusion detection system based on vehicle voltage signals,” IEEE Internet Things J., vol. 9, no. 3, pp. 2124–2133, Feb. 2022.
- [7] N. Marchang, R. Datta, and S. K. Das, “A novel approach for efﬁcient usage of intrusion detection system in mobile ad hoc networks,” IEEE Trans. Veh. Technol., vol. 66, no. 2, pp. 1684–1695, Feb. 2017.
- [8] Y. Xun, J. Liu, N. Kato, Y. Fang, and Y. Zhang, “Automobile driver ﬁngerprinting: A new machine learning based authentication scheme,” IEEE Trans. Ind. Informat., vol. 16, no. 2, pp. 1417–1426, Feb. 2020.
- [9] N. Haddadou, A. Rachedi, and Y. Ghamri-Doudane, “A job market sig- naling scheme for incentive and trust management in vehicular ad hoc networks,” IEEE Trans. Veh. Technol., vol. 64, no. 8, pp. 3657–3674, Aug. 2015.
- [10] A. Kaci and A. Rachedi, “Toward a machine learning and software deﬁned network approaches to manage miners’ reputation in blockchain,” J. Netw. Syst. Manage., vol. 28, no. 3, pp. 478–501, 2020.
- [11] A. Rachedi and A. Hasnaoui, “Advanced quality of services with secu- rity integration in wireless sensor networks,” Wireless Commun. Mobile Comput., vol. 15, no. 6, pp. 1106–1116, 2015.
- [12] A. Kaci, T. Bouabana-Tebibel, A. Rachedi, and C. Yahiaoui, “Toward a Big Data approach for indexing encrypted data in cloud computing,” Secur. Privacy, vol. 2, no. 3, 2019, Art. no. e65.
- [13] N. S. Bhati and M. Khari, “A survey on hybrid intrusion detection tech- niques,” in Research in Intelligent and Computing in Engineering. Berlin, Germany: Springer, 2021, pp. 815–825.
- [14] Y. Du, J. Xia, J. Ma, and W. Zhang, “An optimal decision method for intrusion detection system in wireless sensor networks with enhanced cooperation mechanism,” IEEE Access, vol. 9, pp. 69498–69512, 2021.
- [15] B. Subba, S. Biswas, and S. Karmakar, “A game theory based multi layered intrusion detection framework for wireless sensor networks,” Int. J. Wireless Inf. Netw., vol. 25, no. 4, pp. 399–421, 2018.
- [16] L. Shi, X. Wang, and H. Hou, “Research on optimization of array honeypot defense strategies based on evolutionary game theory,” Mathematics, vol. 9, no. 8, 2021, Art. no. 805.
- [17] B. Mishra and I. Smirnova, “Optimal conﬁguration of intrusion detection systems,” Inf. Technol. Manage., vol. 22, pp. 231–244, 2021.
- [18] H. Benaddi, K. Ibrahimi, A. Benslimane, and J. Qadir, “A deep reinforce- ment learning based intrusion detection system (DRL-IDS) for securing wireless sensor networks and Internet of Things,” in Proc. Int. Wireless Internet Conf., Springer, 2019, pp. 73–87.
- [19] C. Yin, Y. Zhu, J. Fei, and X. He, “A deep learning approach for in- trusion detection using recurrent neural networks,” IEEE Access, vol. 5, pp. 21954–21961, 2017.
- [20] Y. Yang, K. Zheng, C. Wu, X. Niu, and Y. Yang, “Building an effective intrusion detection system using the modiﬁed density peak clustering algorithm and deep belief networks,” Appl. Sci., vol. 9, no. 2, 2019, Art. no. 238.
- [26] N. Sengupta, J. Sen, J. Sil, and M. Saha, “Designing of on line intru- sion detection system using rough set theory and q-learning algorithm,” Neurocomputing, vol. 111, pp. 161–168, 2013.
- [27] G. Caminero, M. Lopez-Martin, and B. Carro, “Adversarial environment reinforcement learning algorithm for intrusion detection,” Comput. Netw., vol. 159, pp. 96–109, 2019.
- [28] M. Lopez-Martin, B. Carro, and A. Sanchez-Esguevillas, “Application of deep reinforcement learning to intrusion detection for supervised prob- lems,” Expert Syst. with Appl., vol. 141, 2020, Art. no. 112963.
- [29] E. Suwannalai and C. Polprasert, “Network intrusion detection systems using adversarial reinforcement learning with deep q-network,” in Proc. IEEE 18th Int. Conf. ICT Knowl. Eng. (ICTKE), 2020, pp. 1–7.
- [30] K. Sethi, Y. V. Madhav, R. Kumar, and P. Bera, “Attention based multi- agent intrusion detection systems using reinforcement learning,” J. Inf. Secur. Appl., vol. 61, 2021, Art. no. 102923.
- [31] H. Alavizadeh, H. Alavizadeh, and J. Jang-Jaccard, “Deep q-learning based reinforcement learning approach for network intrusion detection,” Computers, vol. 11, no. 3, p. 41, 2022.
- [32] Z. Wang, S. Xu, G. Xu, Y. Yin, M. Zhang, and D. Sun, “Game theoretical method for anomaly-based intrusion detection,” Secur. Commun. Netw., vol. 2020, p. 10, 2020, Art. no. 8824163, https://doi.org/10.1155/2020/ 8824163
- [33] V. Sangeetha, M. Vaneeta, S. S. Kumar, P. K. Pareek, and S. Dixit, “Efﬁcient intrusion detection of malicious node using bayesian hybrid detection in MANET,” in Proc. IOP Conf. Series: Mater. Sci. Eng., IOP Publishing, 2021, vol. 1022, Art. no. 012077.
- [34] Z. Xia, J. Tan, K. Gu, and W. Jia, “Detection resource allocation scheme for two-layer cooperative IDSs in smart grids,” J. Parallel Distrib. Comput., vol. 147, pp. 236–247, 2021.
- [35] P. Maillé, P. Reichl, and B. Tufﬁn, “Of threats and costs: A game-theoretic approach to security risk management,” in Performance Models and Risk Management in Communications Systems. ser. Springer Optimization and Its Applications, vol. 46, New York, NY: Springer, 2011, pp. 33–54, https: //doi.org/10.1007/978-1-4419-0534-5_2
- [36] Z. Ahmad, A. Shahid Khan, C. Wai Shiang, J. Abdullah, and F. Ahmad, “Network intrusion detection system: A systematic study of machine learn- ing and deep learning approaches,” Trans. Emerg. Telecommun. Technol., vol. 32, no. 1, 2021, Art. no. e4150.
- [37] M. Ring, S. Wunderlich, D. Scheuring, D. Landes, and A. Hotho, “A survey of network-based intrusion detection data sets,” Comput. Secur., vol. 86, pp. 147–167, 2019.
- [38] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A detailed analysis of the KDD CUP 99 data set,” in Proc. IEEE Symp. Comput. Intell. Secur. defense Appl., 2009, pp. 1–6.
- [39] M. Ghurab, G. Gaphari, F. Alshami, R. Alshamy, and S. Othman, “A detailed analysis of benchmark datasets for network intrusion detection system,” Asian J. Res. Comput. Sci., vol. 7, no. 4, pp. 14–33, 2021.
- [40] “NSL-KDD dataset,” Accessed: Dec. 15, 2021. [Online]. Available: https: //www.unb.ca/cic/datasets/nsl.html
- [41] H. S. Obaid, S. A. Dheyab, and S. S. Sabry, “The impact of data pre- processing techniques and dimensionality reduction on the accuracy of machine learning,” in Proc. 9th Annu. Inf. Technol., Electromechanical Eng. Microelectronics Conf., 2019, pp. 279–283.
- [42] A. M. Fink, “Equilibrium in a stochastic n-person game,” J. Sci. Hiroshima Univ., series ai (mathematics), vol. 28, no. 1, pp. 89–93, 1964.
- [43] P. Bedi, N. Gupta, and V. Jindal, “Siam-IDS: Handling class imbalance problem in intrusion detection systems using siamese neural network,” Procedia Comput. Sci., vol. 171, pp. 780–789, 2020.
- [21] M. Ahsan and K. E. Nygard, “Convolutional neural networks with LSTM for intrusion detection,” in Proc. CATA, 2020, pp. 69–79.
- [22] T. Wisanwanichthan and M. Thammawichai, “A double-layered hybrid approach for network intrusion detection system using combined naive bayes and SVM,” IEEE Access, vol. 9, pp. 138432–138450, 2021.
- [23] C. Liu, Z. Gu, and J. Wang, “A hybrid intrusion detection system based on scalable k-means random forest and deep learning,” IEEE Access, vol. 9, pp. 75729–75740, 2021.
- [24] B. Min, J. Yoo, S. Kim, D. Shin, and D. Shin, “Network anomaly detec- tion using memory-augmented deep autoencoder,” IEEE Access, vol. 9, pp. 104695–104706, 2021.

Hafsa Benaddi received the bachelor’s degree in software engineering from the Faculty of Sciences and Techniques, Moulay Ismail University, Er- rachidia, Morocco, in 2015, and the Master’s of Sci- ence degree in cryptography and information secu- rity from the Faculty of Sciences, Mohammed V University, Rabat, Morocco, in 2017. She is cur- rently working toward the Ph.D. degree in mathe- matics, computer science, and applications with the Faculty of Sciences, Ibn Tofail University, Kenitra, Morocco. She was the recipient of the Best Paper
- [25] Y. N. Kunang, S. Nurmaini, D. Stiawan, and B. Y. Suprapto, “Attack classi- ﬁcation of an intrusion detection system using deep learning and hyperpa- rameter optimization,” J. Inf. Secur. Appl., vol. 58, 2021, Art. no. 102804.
Award from Wicon’19 Conference in Taiwan. Her research interests include performance evaluation of intrusion detection systems, wireless sensor networks, and blockchain solutions.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
11102
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 71, NO. 10, OCTOBER 2022

Khalil Ibrahimi (Senior Member, IEEE) was born in Kenitra, Morocco. He received the B.Sc. degree in mathematical sciences and the M.Sc. degree in engi- neering, telecommunications, and multimedia from the Faculty of Sciences, Mohammed V University, Rabat, Morocco, in September 2003 and December 2005, respectively, and the Ph.D. degree in computer sciences from the University of Avignon, Avignon, France, and Mohammed V University, in November 2009. In 2010, he was an Assistant Professor (ATER part-time) with the CERI/LIA of University of Avi-
e
Mohammed Jouhari received the B.Sc. degree in physics and the M.Sc. degree in signals processing and telecommunication from the Faculty of Sciences, Mohammed V University, Rabat, Morocco, in 2011 and 2013, respectively, and the Ph.D. degree from Ibn Tofail University, Kenitra, Morocco, in 2019. He was a Postdoctoral Fellow with Computer Science and Engineering Department, Qatar University, Qatar. He is currently a Postdoctoral Researcher with the School of Computer Science, Mohammed 6 Polytechnic Uni- versity, Ben Guerir, Morocco. His research interests
gnon. From 2010 to 2015, he was an Assistant Professor with the Faculty of Sciences, LaRIT Laboratory, IBN-TOFAIL University, Kenitra, Morocco. From 2015 to 2021, he was an Associate Professor (HdR) with the Modeling of Information and Communication Systems (MISC) Laboratory, IBN-TOFAIL University, where he was responsible of Research Team (SMCS): Security and Modeling of Communication Systems. He is currently a Full Professor with LaRI Laboratory (Research in Informatics), IBN-TOFAIL University, and responsi- ble of Research Team (DaWNet): Data Science, Intelligent System, Wireless Network and Security. His research interests particularly include performance evaluation and resources allocation of next generation networks (3G, beyond 3G and 4G), social network, underwater sensor network (UWSN), Security and Blockchains. Dr. Ibrahimi was the Chair of RAWSN 2016 Workshop, Local Chair of WINCOM’17, Organizer of WINCOM 2020. He was the recipient of the Best POSTER Award from MSWIM 13 Conference and the Best PAPER Award from the WiCON-2019 International Conference. He was a reviewer of many international journals, such as Computer Communications, Cybernetics and Information Technologies, International Journal of Conservation Science, IEEE ACCESS, comcom, and MDP, and conferences Globecom, ICC, and MSWIM.
include wireless communication, underwater acoustic sensor networks, dis- tributed machine learning, Internet of Things, and deep reinforcement learning.

Junaid Qadir (Senior Member, IEEE) is currently a Professor with Information Technology University (ITU), Lahore, Pakistan, where he is also the Director of the IHSAN Lab that focuses on deploying ICT for development, and is engaged in systems and network- ing research. His research interests include the appli- cation of algorithmic, machine learning, optimization techniques in networks, wireless networks, cognitive networking, software-deﬁned networks, and cloud computing. He was the recipient of the Highest Na- tional Teaching Award in Pakistan and the Higher
Education Commission’s (HEC) Best University Teacher Award (during 2012– 2013). He is a Senior Member at ACM.
-
Abderrahim Benslimane (Senior Member, IEEE) received the B.S. degree in computer science from the University of Nancy, Nancy, France, in 1987, the DEA (M.S. degree) and the Ph.D. degree in computer science from the Franche-Comte University of Besanon, Besanon, France, in 1989 and 1993, respectively. Since 2001, he has been a Full Professor of computer-science with the Avignon University, Avignon, France. He is currently the Vice Dean of the Faculty of Sciences and Technology and the Head of the Master Degree SICOM, Communicating Sys-
tems. He has been nominated in 2020 as IEEE VTS Distinguished Lecturer. He has the French award for Doctoral supervision and Research during 2017–2021. Since September 1994, he has been as an Associate Professor with the University of Technology of Belfort-Montbliard, Belfort, France. He has more than 220 refereed international publications, such as books, conference proceedings, journals and conferences, and more than 20 Special issues. All publications are in his research topics. He supervised more than 20 Ph.D thesis and more than 40 M.Sc. research thesis. He was the recipient of the title to supervise researches (HDR 2000) from the University of Cergy-Pontoise, Cergy, France. He has been nominated IEEE ComSoc Steering Chair of Multimedia Commu- nications TC during 2022–2024 and was the Vice Chair during 2020–2022. During 2017–2019, he was the past Chair of the ComSoc Technical Committee of Communication and Information Security. He is the EiC of Inderscience International Journal of Multimedia Intelligence and Security (IJMIS), the Area Editor of Security in IEEE INTERNET OF THINGS JOURNAL, the Editorial Member of IEEE TRANSACTION ON MULTIMEDIA, IEEE Wireless Communication Mag- azine, IEEE SYSTEM JOURNAL, Elsevier Ad Hoc Networks, Springer Wireless Network Journal, and the Past Area Editor of Wiley Security and Privacy journal during 2017–2019. He has been the Co-Founder and is the General-Chair of the IEEE WiMob since 2005 and iCOST and MoWNet international conference since 2011.
Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:15:34 UTC from IEEE Xplore. Restrictions apply.
