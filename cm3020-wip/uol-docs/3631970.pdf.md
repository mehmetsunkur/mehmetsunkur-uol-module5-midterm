®
# Testing of Deep Reinforcement Learning Agents with Surrogate Models
MATTEO BIAGIOLA and PAOLO TONELLA, Università della Svizzera italiana, Switzerland
Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents.
In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called Indago, trains a classifier on failure and non-failure environment (i.e., pass) configurations resulting from the DRL training process. The classifier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment configuration induces a failure of the DRL agent under test. The failure prediction acts as a fitness function, guiding the generation towards failure environment configurations, while saving computation time by deferring the execution of the DRL agent in the environment to those configurations that are more likely to expose failures.
Experimental results show that our search-based approach finds 50% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78% more diverse; similarly, the behaviors of the DRL agent induced by failure configurations are 74% more diverse.
# CCS Concepts: • Software and its engineering → Software verification and validation;
Additional Key Words and Phrases: Software testing, reinforcement learning
# ACM Reference format:
Matteo Biagiola and Paolo Tonella. 2024. Testing of Deep Reinforcement Learning Agents with Surrogate Models. ACM Trans. Softw. Eng. Methodol. 33, 3, Article 73 (March 2024), 33 pages. https://doi.org/10.1145/3631970
# 1 INTRODUCTION
Reinforcement Learning (RL) is a learning paradigm in which an agent interacts with the en- vironment to complete a given task. Learning is driven by a reward signal returned to the agent by the environment. The ultimate goal of an agent is to learn a policy, i.e., a way to act in each environment state, that maximizes the amount of reward the agent earns in its lifetime. The first RL algorithms were tabular [58] and could handle tasks with finite and small environment states. With the advent of Deep Learning (DL), new algorithms (called Deep RL, or DRL for short) were proposed, which could deal with complex environment states (e.g., images) [42].
This work was partially supported by the H2020 project PRECRIME, funded under the ERC Advanced Grant 2017 Program (ERC Grant Agreement n. 787703).
Authors’ address: M. Biagiola and P. Tonella, Università della Svizzera italiana, 6900 Via Buffi, 13 – Lugano, Switzerland; e-mails: matteo.biagiola@usi.ch, paolo.tonella@usi.ch.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
1049-331X/2024/03-ART73 $15.00
https://doi.org/10.1145/3631970
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73
73:2
M. Biagiola and P. Tonella
DRL has recently been applied in many practical contexts. An example is personalization, i.e., the problem of customizing a service to the needs of a particular user. For instance, Netflix uses DRL to choose which movie artwork to show to a user in order to maximize engagement [28]. Similarly, Microsoft developed Personalizer [35], a service developers can use for content recommendation and ad placement. Meta proposed Horizon [29, 30] (also called ReAgent), an open source applied DRL platform, employed to deliver personalized notifications to their users, replacing the previous system based on supervised learning.
Another practical context in which the DRL paradigm is applied is continuous control. For in- stance, the automaker Audi [71] showcased that their 1:8 scale car could, using DRL, search for a parking place in an area of 9 square meters and park autonomously. Indeed, advancements in state representation learning and smooth exploration [36, 48, 68] made it possible to train DRL agents directly on real robots. Moreover, in recent years, simulators have become more realistic for a variety of tasks, besides achieving high parallelization thanks to GPU acceleration [26, 39]. In addition, domain randomization and learned actuator dynamics are reducing the sim-to-real gap in robotics research [25, 41, 52].
Despite the growing prevalence of DRL agents in the real world, methodologies for testing such agents are still largely unexplored. On the other hand, DRL agents present some peculiar character- istics. Indeed, DRL agents are trained online since they interact with the environment to learn the optimal actions to perform the task. Specifically, in order to increase generalization, DRL agents are usually trained on randomized environment configurations [15, 63] to prevent agents from memorizing how to behave in a particular instance of the environment (e.g., a self-driving car that drives only on a specific track). Therefore, during training the DRL agent is presented with dif- ferent environment configurations (e.g., different tracks) and it fails in some while it succeeds in others.
The current state-of-the-practice to test DRL agents is to run them on a set of environment configurations generated at random [43, 63]. However, testing a DRL agent on randomly generated environment configurations has two shortcomings. First, random generation [43] is unlikely to expose failures. As a consequence, their absence might lead the developer to overestimate the capabilities of the DRL agent and to the deployment of an unsafe agent. Secondly, even finding challenging environment configurations by random exploration is difficult and computationally expensive, since many executions are needed and each execution requires running the DRL agent in a simulator or in the real world.
On the other hand, the interactions of the DRL agent with the environment during training provide clues about the weaknesses of the DRL agent that result from the training process. The intuition is that training failures are representative of critical environment configurations even for the DRL agent once it has been trained, and could be used as guidance for the generation of new environment configurations that will likely challenge it.
Our approach, implemented in a tool called Indago, considers the interaction data produced dur- ing the DRL training process as a labeled dataset to train a surrogate model—i.e., a classifier—on failure and non-failure (i.e., pass) environment configurations. Then, Indago uses such surrogate model as a proxy for the execution of the DRL agent in an environment with a newly generated con- figuration. Indago uses a search-based approach to maximize the failure prediction for an environ- ment configuration given by the surrogate model, instead of a random search [63]. Indeed, random search needs to generate a large set of environment configurations in order to be effective, which makes it computationally expensive. On the other hand, Indago uses a mutation operator guided by saliency-based input attribution [55] to identify mutations that have the maximum influence on the failure prediction. This way, Indago turns a non-promising environment configuration into a critical one for the DRL agent under test, making the search more efficient. Indago executes the
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
DRL agent under test only on the most promising environment configurations, i.e., those with the highest failure predictions, hence saving computation time while maximizing failure exposures. Our article makes the following contributions:
- (1) Failure Search with Surrogate Models: in this article, we propose an approach that makes use of a surrogate model of the environment to guide failure search while automatically generating environment configurations for a DRL agent. In particular, we train a classifier on the training interaction data and use its output as a fitness function to maximize the failure prediction of a given environment configuration. Moreover, we use the saliency method to efficiently identify the most critical mutations for the given environment configurations.
- (2) The Indago Tool: a practical tool, that implements the aforementioned approach, which we make publicly available [7]. We also release three DRL agents trained on as many complex environments, as well as the required infrastructure to test them.
- (3) Experimental Evaluation: we systematically compare different configurations of Indago with the state-of-the-art sampling approach [63] that maximizes failure prediction by generating a large set of environment configurations. On three complex case studies, i.e., a parking task [37], a walking humanoid [62], and a self-driving car [61], our experiments show that, overall, Indago is able to find 50% more failure environment configurations than sampling. Moreover, we introduce a clustering-based technique to measure the diversity of failure environment configurations triggered by the competing approaches. Experimental results show that the failure environment configurations found by Indago are 77% more diverse than those generated by sampling. Moreover, the behaviors of the DRL agent induced by such environment configurations are 74% more diverse.
# 2 RELATED WORK
Testing DRL agents is a rather unexplored area of research. Works that generate adversarial at- tacks [59] have been proposed [23, 38], showing that such DRL agents can be vulnerable to adver- sarial attacks, similarly to DL agents (i.e., agents trained using supervised learning). However, our approach is substantially different since it is not focused on perturbing the raw inputs of the DRL agent sensors but rather on generating configurations for the whole environment the DRL agent runs into. The most similar work to ours is that by Uesato et al. [63], who proposed the sampling ap- proach we used as a baseline in our experiments. Our results show that our search-based approach outperforms it both in terms of number of failures triggered and their diversity in all case studies.
Biagiola et al. [8] proposed an approach to test the adaptation capabilities of DRL agents. In particular, the training of a DRL agent is resumed with environment configurations that are dif- ferent from those experienced by the DRL agent during the previous training phase. Then, the proposed approach builds the adaptation frontier of the DRL agent, separating the configurations in which the DRL agent under test is able to adapt from those where adaptation is unsuccessful. Our approach is similar, in the sense that we also generate environment configurations. However, we are interested in testing the DRL agent to find its weaknesses at testing time rather than its adaptation capabilities (i.e., we do not resume training).
Also related to our work is that by Ruderman et al. [15], who studied how to train and test agents in procedurally generated environments. In particular, they trained DRL agents on a set of procedurally generated mazes for a 3D navigation task. Then, at testing time, they adopt a local search process to modify generated mazes guided by the performance of the DRL agent. This process generates out-of-distribution mazes, i.e., environment configurations that are not possible under the training distribution. In our work, we minimize the computational cost of the search by using a surrogate model of the environment (i.e., a failure predictor), rather than executing the DRL
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:3
73:4
M. Biagiola and P. Tonella
agent under test in the environment to measure its performance. Indeed, executing the DRL agent in the environment at each search iteration becomes prohibitively expensive in environments more complex than mazes. Moreover, our approach does not generate out-of-distribution environment configurations since all environment configurations generated at testing time are subject to the same validity constraints as the environment configurations generated during the DRL training process.
More recently, Tappler et al. [60] proposed a search approach to assess the quality of DRL agents. Their approach consists of searching for a reference trace that solves the RL task by sampling the environment. Such trace is built using a depth-first search algorithm and it is composed of all the states not part of the backtracking branches of the search. In particular, the search backtracks when it encounters an unsafe state, which is a state where the environment terminates the episode unsuccessfully. The states in the search graph preceding both an unsafe state and at least a succes- sor non-terminal state, are called boundary states. The prefixes of the reference trace that end up in a boundary state are safety tests since they are sequences of actions designed to bring the DRL agent under test into safety-critical situations. Our approach is complementary since our goal is to generate new environment configurations to test the DRL agent, rather than evaluating it in the same environment configuration.
The literature in testing DL agents is quite rich and includes a multitude of works summarized in different surveys on the topic [9, 24, 74]. Particularly relevant to our work are those that use search-based methods to generate test inputs [1, 2, 6, 19, 49]. However, our work is different since it specifically targets DRL agents by exploiting the interaction data a DRL agent produces during training, which is not possible in DL testing since DL agents are trained offline.
# 3 BACKGROUND AND MOTIVATION
# 3.1 Reinforcement Learning
Fundamentals and Notations. RL aims at learning a policy, which is a mapping from states to actions, in order to optimize a numerical reward signal [58]. The agent that acts in the environ- ment needs to discover what actions result in a high reward through trial and error without the presence of a supervisor. The main assumption of this learning paradigm is the so-called reward hypothesis [58], stating that training goals can be expressed as the maximization of the cumulative reward.
More formally, at each timestep t an RL agent receives a state s as input from the environment, and it has to decide the action a to take. The executed action triggers a change of state and results in a reward value r given to the agent by the environment. Assuming that the task we formalize as an RL problem is episodic, i.e., it terminates once certain conditions hold, the goal of an RL agent is to maximize the cumulative reward (i.e., often called return) of the episode. In the general case, however, the RL objective is expressed as the maximization of the expected return since both the environment in which the agent operates and its policy can be stochastic. The main reason for the stochastic nature of a policy is due to a fundamental dilemma in RL, which is the exploration- exploitation dilemma. In fact, on the one hand, the agent has to exploit the actions already known to be rewarding but, on the other hand, it has to explore unknown actions that might result in even more reward.
The most important component of an RL agent is its policy π : S → A, where A is a set of actions and S is a set of states. The optimal policy π ∗ tells the RL agent how to act in each state in order to maximize the expected return. The optimal policy can be learned directly or can be extracted from value functions, namely the state-value function vπ (s) and the action-value function qπ (s, a). The former quantifies the value of the state s , i.e., the expected return in s ,
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
{ | | | | | | | “env_configuration": { 5 "goal lane": 20, "head ego": 0.0, “pvehicles": { 3, 5, 6, 8, 13 ty "pos, ego": (0.0, 0.0) head_ego 3 3 i e Kİ 3 & A B
Fig. 1. An initial configuration of the Parking environment in the HighwayEnv simulator [37]. The left-hand side (A) shows how the configuration on the right-hand side (B) is rendered in the environment.
whereas the latter quantifies the value of the state-action pair (s, a). The difference between the two functions is that vπ provides the value of a state s by considering each possible action the agent can take in s (in other words, the average of the expected return in s for all the actions), while qπ considers the value of a state s for a particular action a . Both functions satisfy the Bellman equations [58] which are recursive consistency equations relating the values of a state (or state- action pair) to the values of all the possible successor states (or state-action pairs). In particular, by solving the Bellman equation for qπ , we obtain q∗ π from which we can extract π ∗ by choosing in each state s the action a that maximizes q∗ π .
Deep RL Algorithms. Before DL, the RL problem was addressed using dynamic programming and approximate tabular methods, such as Monte Carlo methods and temporal difference learn- ing [58]. However, such methods are not applicable to problems where the state dimensionality is high (e.g., images) and/or the action space is continuous (e.g., the throttle in a self-driving car). The advent of DL made it possible to create Deep RL (DRL) algorithms that work on such complex practical scenarios [42]. In particular, Deep RL (DRL) algorithms use non-linear function approxi- mators, such as neural networks, to approximate high dimensional state and action spaces besides modeling the dynamics of the environment. Therefore, instead of having exact representations of policies s and value functions (vπ and qπ ), neural networks can be used to approximate such quantities as well as to model the environment in which the agent operates.
In our experiments, we consider model-free DRL algorithms which do not use a model of the environment. There exist different categories of model-free algorithms, based on how the RL problem is addressed. Specifically, policy gradient algorithms, of which PPO is a notable example [54], directly solve the RL objective by representing the policy explicitly (i.e., with a neural network). Value-based algorithms, on the other hand, extract the policy by solving the Bellman equations, hence representing value functions explicitly. Examples of algorithms in such category are DQN and its improvements [14, 21, 22, 42, 53, 70]. Hybrid methods represent both policy and value functions to incorporate, the benefits of both policy gradients and value-based methods. SAC [20] and TQC [34] are the state-of-the-art algorithms in this category. Belonging to a special category is the algorithm HER [3], which was proposed as a wrapper on top of traditional DRL algorithms, to speed up learning in the context of goal-based tasks, e.g., parking a car from a starting position to a target parking spot.
# 3.2 Motivating Example
Figure 1 shows the Parking environment, created by Leurent et al. [37], in a particular configuration where the DRL agent needs to control the ego vehicle positioned at the center of the parking place.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:5
73:6
M. Biagiola and P. Tonella
The left-hand side of the figure (i.e., Figure 1(A)) shows how the environment configuration on the right-hand side (i.e., Figure 1(B)) is rendered in the HighwayEnv simulator. In such environment configuration, the ego vehicle is at the center of the parking place, i.e., its position is (0.0, 0.0), and it has a heading of 0.0 (this parameter ranges in the interval [0.0, 1.0), representing a complete rotation). In the parking place, there are 20 parking spots, 5 cars parked at lanes 3, 5, 6, 8, and 13 and the target parking spot is at lane 20. The task of the DRL agent in this environment is to park the ego vehicle inside the target parking spot, with the proper heading.
The action space of the DRL agent is composed of two actions, namely, throttle and steering, both of which are continuous. The DRL agent receives a negative reward at each timestep, propor- tional to the Euclidean distance of the ego vehicle from the target. Moreover, it receives a constant positive reward when the target is reached and a big constant negative reward when it collides with a parked vehicle. The task is episodic with an episode finishing when either the ego vehicle is parked in the right target spot with the right heading, or it collides with a parked vehicle, or the timeout, measured in number of timesteps, expires.
We made the environment configurable, such that the parameters of the configuration (i.e., the initial conditions at the beginning of each episode) can be changed programmatically. Correspond- ingly, an environment configuration needs to be valid, i.e., it has to respect the constraints imposed by the environment. Such constraints are designed by the developers of the environment to ensure that valid configurations are solvable by the DRL agent. In other words, any agent would be able to solve the task when starting from a valid initial configuration, since the environment does not contain any physically insurmountable obstacle or impediment.
The constraints defined for the Parking environment are the following: there cannot be a parked vehicle in the goal lane (goal_lane ¢ pvehicles) since, otherwise, it would be impossible for the DRL agent to successfully park the vehicle in the target spot. Moreover, goal_lane and pvehicles elements can vary in the interval [1, 20], ie., the target cannot be out of the parking place and there cannot be vehicles outside the parking spots. The head_ego parameter can vary in the inter- val (0.0, 1.0). The parameters pos_ego.x and pos_ego.y can vary in the intervals [—10, 10] and [-5, 5], respectively; in the former case the constraint ensures that the ego vehicle is not too far from the parking place while the latter constraint avoids the ego vehicle to be too close to parked vehicles that would make any maneuver impossible and, as a consequence, the task unsolvable.
# 4 APPROACH
The goal of our approach is to exploit the data resulting from the interaction between the DRL agent and the environment during training in order to discover the weaknesses of the DRL agent at testing time. The interaction data we consider is in the form of pairs (ei , ci ) where ei is the environment configuration at episode i during training and ci is a class label, i.e., a boolean value indicating whether the DRL agent failed (ci = 1) or not (ci = 0) at the task in episode i.
Our approach exploits the information on the failures that happened during training with the objective of generating new critical test cases, i.e., environment configurations, in which the DRL agent under test (i.e., the DRL agent at the end of training) is likely to fail. Since the execution of the DRL agent in the environment is computationally expensive we avoid the execution of candi- date new environment configurations that are not promising, i.e., that are unlikely to lead to the exposure of a failure. The training interaction data can be leveraged to predict which, among the newly generated environment configurations at testing time, are more likely to produce a failure and the DRL agent can be executed only in environments with such promising configurations.
The current state-of-the-practice to test DRL agents is to evaluate them for a certain number of episodes, each with an environment configuration generated at random [43, 63]. However, en- vironment configurations generated at random are unlikely to expose failures, although specific
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
DRL Agent Training Surrogate Model Training © Failure Search © DRL Agent Execution Envi Q e (e) DRL => | Interaction RS OO => Ew Em LO OO Agent Data OO Generator) "LE? OO O ö öğ 0.30 Surrogate Ens Surrogate Failure Model Model Predictions A B Cc
Fig. 2. Overall approach for testing DRL agents. Our approach takes as input a trained DRL agent that produced the interaction data (A) we use in the first step of our approach (B), i.e., Surrogate Model Training ❶. The second step of our approach is Failure Search ❷, that selects the environment configuration in which the DRL agent is more likely to fail. In the evaluation (C), we execute the selected environment configuration on the real environment to check whether the execution results in a failure or not.
environment configurations may exist that are challenging for the DRL agent under test. On the other hand, the DRL training process offers a valuable source of information exploitable at testing time to efficiently expose failures of the DRL agent under test. The intuition is that the failures experienced during training by weaker versions of the DRL agent under test are representative of critical environment configurations of the DRL agent at the end of training, and can be used to guide the generation of new environment configurations that can challenge it.
Figure 2 summarizes the overall approach to use interaction data produced during the DRL training process to test a DRL agent. Our approach (i.e., Figure 2(B)) takes as input a trained DRL agent (i.e., Figure 2(A)) as well as the output of the DRL training process (i.e., pairs of interaction data (ei , ci )). The first step of our approach ❶ consists of training a surrogate model of the environ- ment on the set of interaction data. The surrogate model is used in the next step (i.e., failure search) to predict whether unseen environment configurations are likely to be failures. The failure search step ❷ uses the surrogate model to generate environment configurations in which the DRL agent is more likely to fail, by acting as a proxy for the execution of the DRL agent in the environment with such configurations. The output of step ❷ is the environment configuration that, considering the surrogate model a classifier, has the highest failure prediction among the candidates (in Figure 2 the selected environment configuration is Env1 and it is encircled with a dashed line). Finally, we evaluate the effectiveness of our approach by executing the DRL agent (i.e., Figure 2(C)) in an environment with such configuration, to check whether the execution results in an actual failure or not.
# 4.1 Surrogate Model Training
4.1.1 Classifier. The simplest implementation for the surrogate model is to train a classifier to predict whether, given an environment configuration, the DRL agent under test will fail in it or not. The interaction dataset D = {(e1, c1), . . . , (eN , cN )} is used to train such classifier. In particular, we train a softmax classifier (i.e., a neural network) to minimize the cross-entropy loss.
The classification problem to address in step ❶ presents some peculiar characteristics. In partic- ular, the dataset D might be unbalanced, i.e., the number of environment configurations in which the agent fails (c = 1) is much lower than the number of environment configurations in which the agent does not fail (c = 0). The reason is that at the beginning of the training process the DRL agent fails in most of the environment configurations while, as training goes on, the DRL agent fails less and less until the training process converges and failures become rare. One of the strate- gies to train a classifier in situations of class unbalance is to introduce a weight vectorW in the loss function [12]. Such vector has two components, i.e., one for each class, in order to scale the loss
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:7
73:8
M. Biagiola and P. Tonella
function for each ith datapoint w.r.t. the class the datapoint belongs to. The idea is to give more weight to the datapoints of the underrepresented class (i.e., the failure class) such that the classifier can learn to classify datapoints of both classes equally. In the literature [13, 32], there are various proposals for the computation of W . Our implementation choice is described in Section 5.2.
4.1.2 Regressor. When the number of failures in interaction dataset D is too small to train a reasonable classifier, another implementation choice for the surrogate model is to train a regressor. Contrary to the classifier, where it is straightforward to log when episodes are success- ful/unsuccessful during training, training a regressor requires defining a function that computes a continuous value for each episode. Such value should be small when the agent is close to a failure in a certain episode while it should be increasingly larger as long as the agent is far from a failure. The labels c1, . . . , cN in the interaction dataset D are continuous values and the training objective is to minimize the Mean Squared Error between the prediction of the regressor and the ground-truth value in the dataset.
Similarly to the classifier, the interaction dataset D can be unbalanced, with the majority of the data having a continuous value which represents far-from-failure situations. Also in this case, we use weighted training to cope with the unbalanced dataset (see Section 5.2).
In the following sections (i.e., Sections 4.2 and 4.3), we describe our approach considering the classifier as an implementation choice for the surrogate model. Without loss of generality, the same applies when the surrogate model is implemented as a regressor.
# 4.2 Failure Prediction
The classifier is used to predict the class of any environment configuration that is not present in the dataset D of interaction data. Therefore, it is a proxy for the actual execution of the DRL agent in the environment with a specific configuration. More formally, it is a function f : E → [0, 1] ∈ R that takes as input an environment configuration e ∈ E and outputs a failure prediction.
Given the classifier, our objective is to generate an environment configuration that maximizes the failure prediction, i.e., to solve ˆe = arg maxe f (e), and then execute the DRL agent in an environment with configuration ˆe. It should be noticed that the real failure prediction function f ∗ is only approximated by our classifier, i.e., f ≈ f ∗. The reason for the approximation is two-fold: (1) the dataset D of interaction data might not be large and diverse enough to best represent all real- world conditions; (2) during training the DRL agent performs non-deterministic actions to better explore the state space. As a consequence, a failure that happens during training might be due to some non-deterministic actions carried out in a specific episode. Despite the mismatch between f and f ∗, failure search (see Figure 2) can still effectively use the classifier that implements f , provided its feedback on the failure prediction of a new environment configuration can be reliably used to converge toward inputs that challenge the DRL agent. In fact, failure search does not re- quire perfect guidance toward optimal inputs, it just needs a direction where the search should be directed [31].
# 4.3 Search-based Failure Search with Surrogate Models
For the failure search step ❷, we use the output of the classifier as a fitness function to be op- timized by a search-based algorithm. In particular, we consider two search-based algorithms, i.e., Hill Climbing and Genetic Algorithm, to generate the environment configurations where to execute the DRL agent.
Hill climbing is a local search algorithm that, starting from an arbitrary candidate solution to the problem (i.e., an environment configuration), incrementally changes such solution to create new ones. If a better solution is found the process is repeated until the current solution can no
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
ALGORITHM 1: Hill climbing algorithm for the generation of environment configurations  Input  : f , classifier;  N S , neighborhood size;  ef , environment configuration in which the DRL agent failed during training.  Output : ˆe, environment configuration to execute the DRL agent on.  1 /* Set the seed environment configuration */  2 if ef = null then  e ← generateRndEnvConfig()  3  4 else  5  e ← ef  6 end  7 /* Main loop that mutates an environment configuration e guided by the classifier f until the timeout expires */  8 repeat  9  /* Assign the initial set of environment configurations and associated failure predictions */  10  E ← {e }  11  F P ← {f (e ) }  12  /* Generate the neighborhood of the current best environment configuration */  13  for each i ∈ N S do  14  /* Mutate the current best environment configuration ensuring its validity */  15  ei ← mutateEnvConfig(e)  16  f pi ← f (ei )  17  E ← E ∪ {ei }  18  F P ← F P ∪ {f pi }  19  end  20  /* Assign the current best environment configuration based on the results of the search */  21  j ← arg max F P  22  e ← E[j]  23 until ¬ timeout()  24 ˆe ← e  25 return ˆe 
longer be improved or a timeout expires. Genetic algorithm is a population-based algorithm that combines global and local search to avoid getting stuck in local optima while improving existing solutions.
Algorithm 1 shows the pseudocode of the hill climbing algorithm for the generation of envi- ronment configurations. It takes as input the classifier f , the size of the neighborhood N S of the current solution e and an optional environment configuration in which the DRL agent failed dur- ing training ef . If such environment configuration is not provided, the initial solution is generated at random (see if statement at Lines 2–6). The for loop at Lines 13–19 computes, at each ith itera- tion, the neighbors of the current solution e. Indeed, function mutateEnvConfig at Line 15 takes care of mutating the current solution e and ensuring the validity of the result, i.e., the mutation is applied to e only if the new environment configuration ei is valid. The mutated solution ei is then evaluated by the classifier to compute its failure prediction at Line 16. At Lines 17–18 each mutation of the current solution, i.e., ei , is stored as well as its failure prediction f pi . At the end of the loop, the index j of the neighboring solution with the maximum failure prediction is com- puted (Line 21) which is used to retrieve the corresponding neighboring solution e (Line 22). The outermost loop at Lines 8–23 is repeated until there is a search budget (i.e., the timeout) that has not expired. Finally, the best solution e is assigned to ˆe and returned.
The Genetic Algorithm shown in Algorithm 2 takes as input the classifier f , the population size PS, the crossover rate cr , and an optional set of environment configurations in which the DRL agent failed during training Ef . Such a set can be used to fill the initial population when it is available; otherwise, the initial population is generated randomly (Line 2). The computeFitness
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:9
73:10
M. Biagiola and P. Tonella
# ALGORITHM 2: Genetic algorithm for the generation of environment configurations
Input : f , classifier; P S , population size; cr , crossover rate; Ef , set of environment configurations in which the DRL agent failed during training/ Output : ˆe, environment configuration to execute the agent on. 1 /* Generate the initial population of environment configurations and compute the corresponding fitness */ 2 population ← generatePopulation(P S , Ef ) 3 computeFitness(population, f ) 4 currentIteration ← 0 5 /* Main loop that changes the population guided by the classifier f until the timeout expires */ 6 repeat 7 /* Build the new population by extracting a certain percentage of the best environment configurations */ 8 newPop ← elitism(population) 9 /* Fill the rest of the population by evolving the environment configurations */ 10 while | newPop | < P S do 11 /* Select the best environment configurations according to their fitness */ 12 pe1 ← selection(population) 13 pe2 ← selection(population) 14 /* Copy the environment configurations (offspring) to avoid changing the original ones (parents) */ 15 oe1 ← copy(pe1) 16 oe2 ← copy(pe1) 17 /* Crossover two environment configurations with a certain probability cr , ensuring their validity */ 18 19 if getRandomFloat() < cr then oe1, oe2 ← crossover(oe1, oe2) 20 end 21 /* Mutate the offsprings ensuring their validity */ 22 oe1 ← mutateEnvConfig(oe1) 23 oe2 ← mutateEnvConfig(oe2) 24 /* Add the best environment configurations to the population according to their fitness */ 25 addBestIndividuals(newPop, pe1, pe2, oe1, oe2) 26 end 27 /* Compute the fitness of environment configurations in the new population */ 28 population ← newPop 29 computeFitness(population, f ) 30 /* Replace the worst environment configurations in the population to avoid stagnation */ 31 32 reseedPopulation(population, currentIteration, Ef ) currentIteration ← currentIteration +1 33 until ¬ timeout() 34 /* Extract the environment configuration with the best fitness */ 35 ˆe ← getIndividualWithBestFitness(population, f ) 36 return ˆe
procedure at Line 3 computes the fitness value for each solution in the population, which is its failure prediction as computed by the classifier f . At Line 8 a new population with the best so- lutions from the current population is instantiated (elitism). The while loop at Lines 10–26 is the evolution part of the algorithm which terminates when the size of the new population reaches the target population size PS. At the beginning of the loop, two solutions are selected based on their fitness (Lines 12–13) and are crossed over according to the crossover probability cr (Lines 18–20); the solutions oe1 and oe2 are modified only if the crossover results are two valid configurations. Afterward, the solutions are mutated (Lines 22–23, also in this case oe1 and oe2 are modified only if the changes result in valid configurations) and finally, the best solutions (either the parents pe1, pe2 or the offsprings oe1, oe2) are stored in the new population (Line 25). At the end of the while
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
loop the current population is assigned the new population and the fitness value of each solution is computed (Lines 28–29). The reseedPopulation procedure at Line 31 takes care of avoiding stagnation by reseeding the population according to the currentIteration variable. If the set of fail- ure environment configurations Ef is available, the solutions with the worst fitness in the current population are replaced by randomly sampled solutions from the set Ef ; otherwise, the worst so- lutions are replaced by randomly generated individuals. The main loop (Lines 6–33) terminates when the search budget expires. Finally, the solution with the best fitness ˆe is taken at Line 35 and returned.
Both search algorithms support seeding from existing environment configurations that caused a failure of the agent during training. In particular, let us suppose that the parameter ef is not null in Algorithm 1 (similarly, Ef is not empty in Algorithm 2). The DRL agent may not fail in such environment configurations because it may have encountered similar environment configurations later during training, and it might have adapted to them. However, despite adaptation, the DRL agent might not perform well and a proper change in such environment configuration has the potential to trigger a failure. For example, let us suppose that the environment configuration ef = [20, 0.0, {3, 5, 6, 8, 13, 19}, (0.0, 0.0)] causes a failure of the DRL agent during training in the Parking environment (it is the same environment configuration shown in Figure 1, except that there is a parked vehicle beside the target spot), but the DRL agent does not fail in ef at testing time. However, changing the heading of the ego vehicle from 0.0 to 0.5, i.e., the opposite direction w.r.t. the target spot, might result in the DRL agent not being able to turn the vehicle and park it with the right heading, since the mutation has succeeded in making the environment more challenging, eventually exposing a failure.
4.3.1 Mutation Function. Knowing what to change in the environment configuration is impor- tant for finding failures. The function that makes this decision is the mutateEnvConfig function (Line 13 in Algorithm 1 and Lines 14–15 in Algorithm 2). We propose two implementations of the mutateEnvConfig function: the first one randomly changes a parameter of the given environment configuration, choosing among all parameters with equal probability. The second one only changes the parameter of an environment configuration that contributes the most to the failure prediction. The idea is that some parameters are more critical than others when consid- ering how they affect failure prediction. Hence, changing them is more beneficial for generating failure environment configurations than changing the other parameters. In particular, we propose to use the saliency method to compute input attribution [55], i.e., to determine how much an input influences a prediction made by a neural network (our classifier). Given an environment configuration and the classifier, the saliency method computes the input gradient, i.e., the partial derivatives over all the individual parameters of the environment configuration. The absolute value of each gradient indicates which input parameter is more critical to the failure prediction and its sign indicates the direction (i.e., positive or negative) of change. In our saliency-guided implementation of the mutateEnvConfig function we take the output computed by saliency and change the parameter in the environment configuration whose corresponding gradient is the highest one; the direction of change, either positive or negative, depends on the sign of the gradient.
For instance, let us consider the environment configuration: e = [20, 0.0, {3, 5, 6, 8, 13, 19}, (0.0, 0.0)], i.e., an environment configuration of the Parking environment. The input attribution for this environment configuration is an array of the same size as the input, as it contains the partial derivatives over each input. Let us suppose that the highest value in the array is in the second position, i.e., the position corresponding to the parameter head_ego, and that such value is positive. This means that the parameter head_ego is the most critical one affecting the failure
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:11
73:12
M. Biagiola and P. Tonella
prediction of the classifier and that changing it in the positive direction, i.e., increasing it will also increase the failure prediction for the resulting environment configuration.
4.3.2 Crossover Function. The crossover function is specific to the genetic algorithm (Line 12 in Algorithm 2). We propose a single-point crossover implementation where, given two environment configurations, the cut point is determined randomly; then the elements in the two configurations before and after the cut point are swapped. We chose this simple implementation for crossover because it can be applied to configurations of any case study with little to no modifications. Custom implementations that take into account the specific features of each case study remain possible in our implementation.
For instance, let us suppose that e1 = [20, 0.0, {3, 5, 6, 8, 13, 19}, (0.0, 0.0)] and e2 = [15, 0.5, {1, 3, 9}, (−1.0, 7.5)] are two Parking environment configurations. The cut point is computed based on the number of parameters in the environment configuration, which in the case of Parking is equal to four (i.e., goal_lane, head_ego, pvehicles, and pos_ego). Let us suppose that the cut point is at first position. The two environment configurations after crossover will be as follows: ce1 = [20, 0.5, {1, 3, 9}, (−1.0, 7.5)] and ce2 = [15, 0.0, {3, 5, 6, 8, 13, 19}, (0.0, 0.0)].
# 4.4 Implementation
We implemented our approach in a Python tool called Indago (Latin for “search”) which is publicly available [7]. The DRL agents are implemented by the stable-baselines [47] library and we use Pytorch [44] and scikit-learn [45] to implement the training and inference of the classifier. The saliency method is implemented by the captum library [33].
# 5 EMPIRICAL EVALUATION
We consider the following research questions:
RQ1 (Effectiveness): What is the effectiveness of the proposed approach? Can it generate failure environment configurations for the DRL agent under test?
RQ2 (Comparison): How does Indago compare against the random baseline? How does it compare against the state-of-the-art sampling approach?
RQ3 (Hyperparameters): What is the impact of the key hyperparameters of Indago? RQ4 (Ineffective Failure Seeds): How does the effectiveness of Indago change when failure seeds are ineffective at testing time?
RQ1 evaluates the effectiveness of Indago, i.e., its capability to generate failure environment configurations for the DRL agent under test. This research question acts as a sanity check to make sure that our approach is able to generate failures that we can analyze and study in the next research question.
RQ2 compares Indago with the random baseline and the state-of-the-art sampling ap- proach [63], both w.r.t. the number of generated failure environment configurations and their diversity.
RQ3 investigates the key hyperparameters of Indago both w.r.t. the number of failure environ- ment configurations that are generated by each hyperparameter setting and their diversity. We first analyze the choice of the search algorithm, i.e., hill climbing vs. genetic algorithm. Second, since our approach can work both with and without the provisioning of failure seeds (ef and Ef are optional parameters in Algorithms 1 and 2, respectively), we want to investigate which of the two seed strategies is more convenient to use. Third, when mutating the environment configurations Indago can either choose them randomly or it can focus on those that have the highest influence on the failure prediction, as determined by the saliency method. Hence, we want to evaluate the impact of the mutation strategy when using Indago.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
In RQ4, we study how Indago performs when failure seeds, i.e., failure configurations causing a failure of the agent during training, are not effective for the DRL agent at testing time. In this research question, we want to study whether Indago is still effective at finding failures when there is no guidance from the failure seeds.
# 5.1 Case Studies
Parking. We already introduced the first environment, i.e., Parking [37], in Section 3, where we also describe the representation of the environment configuration (see Figure 1). Such environment has been used in several studies, especially to evaluate the capabilities of DRL agents [10, 11, 16, 17, 57, 72, 76].
The encoding adopted to train a classifier for failure prediction consists of an array of 24 el- ements where the first two values are for the two single-value parameters of the environment configuration (i.e., goal_lane and head_ego), followed by 20 values corresponding to the one-hot encoding of the pvehicles parameter (i.e., each position has a value of 1 only when there is a vehicle in the respective spot; a value of 0 otherwise) and finally the two values of the pos_ego parameter.
Regarding the mutation operators, the goal_lane parameter value is increased or decreased, with equal probability, and the change amount is an integer in the interval [1, 20]. The heading_ego parameter value is increased or decreased, with equal probability, and the change amount is a random floating point number in the interval [0, 1). The coordinates of the pos_ego parameter tuple are increased or decreased, with equal probability, by a random float in the inter- val [0, 1]. For what concerns the pvehicles parameter, with equal probability parked cars in the pvehicles set are added/removed or the parked car occupancy indexes are mutated. In the former case, a certain number of indices is selected at random for adding or removing parked cars with equal probability. In the latter case, a certain number of parked car indices is selected at random to be mutated, i.e., increased or decreased by a random integer in the interval [1, 20]. Moreover, the crossover operator considers the parameters of the Parking environment configurations as unique features. An example of crossover for the Parking environment is in Section 4.3.2.
Humanoid. The second environment we consider is Humanoid, built using the Mujoco simula- tor [62]. Mujoco is a very popular simulator in the DRL community, especially to benchmark DRL algorithms in continuous control tasks. The 3D physics simulator has different pre-built environ- ments, with Humanoid being one of the most challenging environments for DRL algorithms [69].
In Humanoid the DRL agent needs to control a bipedal robot in a 3D space and make it walk on a smooth surface. Each environment configuration is composed of two arrays, i.e., joints_pos and joints_vel. The former has size 24 and consists of the positions and rotations of the joints of the robot while the latter has size 23 and consists of the linear and angular velocities of those joints (more information are available in the Gym Library wiki [18]). All the angles are in radians, represented as floating point numbers. Figure 3 shows two configurations of the Humanoid environment; we can see that the initial joints positions can be altered, forming different initial configurations for the Humanoid robot (Figure 3(A) and (B) shows the rendering of two different environment configurations, while Figure 3(C) shows how an environment configuration is encoded.).
The state space of the DRL agent is an array of size 376, composed of joint positions, angles, and relative velocities, plus other components as center of mass inertia and velocity. The action space is composed of 17 elements that are the degrees of freedom of the robot, i.e., the actuated joints of the robot. The reward function encourages the DRL agent to walk as fast as possible plus a bonus for each timestep. An episode terminates when the abdomen y coordinate of the robot goes out of the range (1, 2), which indicates that the robot has fallen down, or when a timeout expires (we
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:13
73:14
M. Biagiola and P. Tonella
"env_configuration": { oints_pos": [-0.03, 0.02, 1.42, 0.98,.., 0.03], joints_vel": (0.02, -0.01, 0.01, -0.03,.., -0.02] A B Cc
Fig. 3. Two initial configurations (A and B) of the Humanoid environment in the Mujoco simulator [62]. The right-hand side (C), shows an example of environment configuration.
set such timeout to 300 timesteps). When the robot falls we deem the episode unsuccessful; on the other hand, when the timeout expires we consider the episode successful. We changed the environment interface to be configurable such that the parameters joints_pos and joints_vel can be set programmatically at the beginning of each episode. The original implementation ini- tializes joints_pos to [0, 0, 1.4, 1, 0, . . . , 0] and joints_vel to all zeros; then, to generate a new environment configuration, each value in both arrays can be changed by adding or subtracting a small quantity m (which is set to 0.03). We followed the original implementation to define validity, i.e., we consider an environment configuration to be valid only if the values of its parameters, i.e., joints_pos and joints_vel, are within the interval [−m, m] w.r.t. the initial values of such parameters. For instance, a valid environment configuration can have the third value of its joints_pos array within the interval [1.4 − m, 1.4 + m], since that value is initialized to 1.4.
The encoding we adopt as input to the classifier is the concatenation of the two arrays that define the environment configuration, i.e., joints_pos and joints_vel. Regarding the mutation operators, they are defined in the same way for both parameters, i.e., once an index of either joints_pos or joints_vel is chosen at random, the value at that index is either decreased or increased, with equal probability, by a random floating point quantity in the interval [−m, m]. Moreover, the crossover operator is based on the parameters of the Humanoid environment, as in Parking. In Humanoid there are only two parameters, i.e., joints_pos and joints_vel; there- fore, during crossover, the entire joints_pos and joints_vel vectors are swapped between two different environment configurations.
Driving. The third environment we consider is Driving, built using the DonkeyCar simula- tor [61]. This platform has been used in previous works to train and test self-driving car software both based on supervised learning and RL [66, 67, 75, 77].
In this environment, the DRL agent controls a car which drives along a track. The configuration determines the shape of the track in which the car drives (see Figure 4(A)). The track is represented as a list of 12 pairs, where each pair consists of two elements, i.e., a command and a value (see Figure 4(C)). The possible commands are S which indicates a straight line, R which indicates a right curve, L which indicates a left curve and DY which signals the beginning of a curve. The value associated with each command represents the number of road units, except for the DY command, where the associated value represents the individual angle of rotation for each road unit. For example, the sequence of commands [(S,2), (DY,15.2), (L,3)] (i.e., the first curve of the track represented by the environment configuration in Figure 4(B)) instructs the road engine to build a straight road for two road units followed by a left curve which is three road units long, for a total of 45.6 degrees (i.e., 3 × 15.2).
The state space of the DRL agent is an RGB image of size (160, 120) taken by the front camera of the car, while the action space is two-dimensional, i.e., steering angle and throttle. The DRL
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
1 “env.configuration": { "track": [ (S,18), (S,12), (S,2), (DY,15.2), (L,3), (DY,30.5), (R,3), (DY,2.9), (L,18), (DY,8.4), (R,17), (S,2) A B Cc
Fig. 4. An initial configuration of the Driving environment in the DonkeyCar simulator [61]. The left-hand side shows both the plot (A) of the environment configuration on the right-hand side (C), and how the configuration is rendered in the DonkeyCar simulator (B).
agent receives a small negative reward per timestep plus a positive reward every time it crosses a waypoint in the track, minus a penalty related to the cross track error, i.e., the distance between the center of mass of the car and the center of the track. The first component of the reward encourages the DRL agent to go as fast as possible, the second privileges the progress on the track, and the third forces the DRL agent to drive as close to the center of the track as possible. Moreover, the DRL agent receives a large negative reward when it goes off-road. In such cases, the episode ter- minates unsuccessfully, whereas when the DRL agent crosses the end line of the track, the episode is deemed successful.
We modified the DonkeyCar simulator to make it configurable, such that at the beginning of each episode the track passed as input can be instantiated in the simulator. Regarding validity, there are a number of constraints that need to be respected in order for the track to be valid. First, each track should start and end with an S command. Moreover, after a DY command there must be either an L or R command. Afterward, the track must not contain loops (i.e., self-intersections), it must not have very sharp turns (i.e., with a rotation angle > 170◦) and it must have at least 3 curves, one of which must be with a rotation angle of at least 120◦. The last two constraints ensure that the generated tracks are non-trivial.
The encoding we adopt as input to the classifier is the concatenation of two arrays, i.e., the array of all commands, where each command is given a unique integer identifier, and the array of all values. Regarding the mutation operators, we define one for commands and one for values and, when analyzing a command-value pair, we either change the command or the value with equal probability. The change command operator can only change an L command to an R command or vice-versa. The change value operator first analyzes the associated command and, if it is a DY command, it either increases or decreases, with equal probability, the current value by a random floating point number in the interval [0, 50]. Otherwise, it increases or decreases, with equal probability, the current value by a random integer number in the interval [1, 20]. Moreover, the crossover operator considers the Driving configuration as a list of command-value pairs. The cut point is a value in the interval [1, 12] such that after crossover the resulting Driving environment configurations have command-value pairs coming from both parent environment configurations. We customized the crossover operator for this environment by retrying crossover a certain number of times, until either both resulting configurations are valid or the maximum counter is reached.
# 5.2 Experimental Setup
5.2.1 Procedure. We trained the DRL agents in each environment using the hyperparameters recommended by Raffin et al. [46, 47]. For RQ4, we simplified the training environments in order to make the resulting agents more robust against training failure configurations. In particular, we
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:15
73:16
M. Biagiola and P. Tonella
Table 1. Case Studies Training Metrics
RQ1—RQ3 RQ4 s s p e t s e m i t s e d o s i p e s e r u l i a f p e t s e m i t s e d o s i p e s e r u l i a f . r . r . r . r . r . r DRL algo t # t # t # t # t # t # Parking HER [3] + TQC [34] 200 k 8,789 206 80 k 3,159 17 Humanoid TQC [34] 1,500 k 6,965 270 300 k 1,141 318 Driving SAC [20] 10,00 k 11,274 194 500 k 1,445 13
removed the parked vehicles in Parking, we decreased the variation of the two parameter vectors determining the initial configurations in Humanoid (i.e., we set m to 0.01 instead of 0.03, see the respective paragraph in Section 5.1), and we constrained the number of curves in Driving to be ≥ 2 instead of ≥ 3, of which the hardest one must be ≤ 130◦ instead of ≤ 170◦. Table 1 shows in Column 1 the algorithms we used for each environment. Column 2 (respectively, Column 5 for RQ4) shows the number of training timesteps which corresponds to the number of episodes shown in Column 3 (respectively, Column 6 for RQ4). In each setting we trained each DRL agent until convergence, i.e., until the success rate in the last 100 episodes was 100%. Columns 4 and 7 show the number of training failures in the two settings after filtering the initial 30% of the data. We can observe that, in the first setting (i.e., Column 4), the number of failures remaining after filtering is comparable and close to 200 on average. In the second setting (i.e., Column 7), the number of failures is much lower in Parking and Driving (i.e., ≈20 vs. ≈200), while in Humanoid the DRL agent experiences a drop in performance after the initial training phase in both settings, making the number of training failures after filtering comparable.
Given the low number of failures in the second setting, we resort to a regressor-based surrogate model. Indeed, a regressor, as opposed to a classifier, can learn from near-failure interactions during training, i.e., where the agent got close to failing but ultimately such interactions were successful. For the Parking environment, we considered the length of the episode as a continuous value. In Parking, an episode finishes unsuccessfully when the agent is not able to park the vehicle in a certain amount of time. A long episode indicates a challenging environment configuration where the agent struggled to complete the parking task.
In Humanoid, we consider the abdomen latitudinal coordinate of the robot, which determines when the robot has fallen down; for a given episode we consider the minimum distance between the current coordinate and the respective lower and upper bounds. In Driving, we use the maxi- mum cross-track error in an episode as a continuous value for the regressor.
In all case studies, we normalize the continuous values between 0 and 1, where 0 indicates a failure. When using the regressor-based surrogate model for failure search, our objective is to generate environment configurations that minimize the prediction of the regressor.
Classifier Training. To compensate for the unbalanced dataset when training the failure clas- sifier, we compute a weight vector W as follows: given N datapoints, the array of class targets Y , with |C | being the number of classes:
W = N |C | · hist(Y ) , (1)
where hist(Y ) is a function that outputs an array of size |C | indicating the number of datapoints
for each class. Such formula, gives a higher weight to the underrepresented classes [32].
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
We chose a multi-layer perceptron as the classifier architecture. The reason is twofold: first, the size of the available training data is small (at most 10k examples for both failure and non-failure classes, see Table 1). Secondly, our environment configurations are small size one-dimensional feature vectors (24 for Parking, 47 for Humanoid, and 24 for Driving). Furthermore, since the output of the classifier is used as a fitness function during testing, adopting complex models to process simple inputs would be inefficient and prone to overfitting.
When training the failure predictor, there are two important hyperparameters to consider. The first one is the amount of initial interaction data to filter out. Indeed, at the beginning of training the agent carries out random actions and, as a consequence, it often fails regardless of the environment configuration. This means that the earliest failures are not useful to predict the failures of the DRL agent under test. The other hyperparameter that depends on the case study is the number of hidden layers of the multi-layer perceptron.
In our experiments, we considered nine levels of filtering (i.e., 5, 10–80), where, for instance, 5% filtering means that the first 5% of the environment configurations are not considered for train- ing the classifier. Moreover, we chose four different numbers of hidden layers, i.e., from 1 to 4, where each hidden layer is composed of 32 units followed by a batch normalization layer [27] and a dropout layer [56] with the probability of dropout of 0.5 (except when the network has only one hidden layer). For each pair filter-layers we trained the classifier 10 times (with all the other training hyperparameters, e.g., learning rate, fixed), every time with a different random seed. We used a validation set formed using 20% of the data to save the best model during training which we evaluated on a held-out test set. We built such a test set by choosing a filtering level of 5% for each case study and by selecting 10% of the data at random. During hyperparameter tuning, we measured precision and recall of the classifier considering the failure class as the positive class. We deem precision more important than recall since in order to guide failure search it is more important for the classifier to be as precise as possible (i.e., few false positives) even at the cost of missing some failures (i.e., false negatives). We chose the best classifier model by looking at the results of the 10 training runs for each filter-layers pair, and we selected the model with the highest precision and with a recall of at least 10%, in order not to miss too many failures.
The best classifier for Parking has four hidden layers and a filtering level of 50%, reaching a precision of 24% and a recall of 17% on the held-out test set. The best classifier for Humanoid has one hidden layer and a filtering level of 10%, reaching a precision of 62% and a recall of 48% on the held-out test set. Finally, the best classifier for Driving has four hidden layers and a filtering level of 30%, reaching a precision of 25% and a recall of 12% on the held-out test set. Such classifiers are used in the failure search step for each case study.
Regressor Training. In order to deal with the unbalanced regression problem, we use the Label Distribution Smoothing (LDS) approach proposed by Yuzhe et al. [73]. The approach starts by convolving a symmetric kernel with the empirical distribution of continuous values (e.g., obtained by binning the continuous values). The output is a smoothed version of the distribution that we use in the loss function to proportionally weight each continuous value based on their frequency.
The best regressor for Parking has two hidden layers and a filtering level of 10%; the best regres- sor for Humanoid has two hidden layers and a filtering level of 30%; the best regressor for Driving has two hidden layers and a filtering level of 5%. Such regressors are used in the failure search step for each case study when training failures are ineffective at testing time and correspondingly a regressor is potentially a better surrogate model than a classifier.
Baselines. We compared Indago with two approaches. The first approach is the random base- line, where environment configurations are generated at random. Such baseline is useful to un- derstand whether the proposed approach is able to outperform the state-of-the-practice in testing DRL agents [43, 63]. The second approach is the state-of-the-art sampling approach by Uesato
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:17
73:18
M. Biagiola and P. Tonella
et al. [63]. This simple approach consists of generating a large initial set of T environment config- urations at random and choosing the one that, according to the classifier f , has the highest failure prediction.
Indago. We considered hill climbing and genetic algorithm each with four different settings. In the first one, the seed environment configurations to evolve are generated at random (hcrnd and garnd). In the second, we used hill climbing and genetic algorithm to evolve failure environment configurations (hcfail and gafail). Regarding the settings with the failure seeds, we always filtered out the initial 30% of the environment configurations, in order not to include failures that are likely not representative of the failures of the DRL agent under test. Furthermore, we considered hill climbing and genetic algorithm guided by saliency, evolving both random (hcsal+rnd and gasal+rnd) and failure (hcsal+fail and gasal+fail) environment configurations. For Indago and the sampling approach we considered a fixed search budget B per environment configuration. In particular, we chose B = 5 seconds for Parking and Humanoid, while B = 30 seconds for Driving, as from preliminary experiments we observed that such budget was enough to reach fitness convergence for all approaches.
During failure search, we generated T = 100 environment configurations for each case study and each approach, and we evaluated the respective DRL agent in each environment configuration. Since Humanoid and Driving simulators are non-deterministic we evaluated the respective DRL agents in each environment configuration 10 times. Moreover, we repeated the experiments 10 times for each failure search approach in order to cope with the intrinsic randomness of the approaches. Overall, we have three case studies, 10 techniques (eight settings of Indago, including hill climbing vs. genetic algorithm, random vs. failure seed, two mutation strategies (i.e., random and saliency), plus the random and the sampling approaches), each generating 100 environment configurations (each repeated 10 times in Humanoid and Driving). Finally, each technique is executed 10 times for a total of 210 k simulations.
Hardware Resources. Due to the high number of simulations, we resorted to the university cluster with 20 CPU nodes, and parallelized the execution of the experiments to obtain the results in a reasonable amount of time. We did not make use of GPU nodes in the experiments, as DRL models are typically small and simulators could be executed headless on CPUs.
5.2.2 Metrics. In order to assess the effectiveness of our failure search approach (RQ1) and to compare it with the baselines (RQ2), we measured the number of failures each approach triggers given the number of environment configurations to generate (i.e., T ) and the fixed search budget B per environment configuration. In non-deterministic environments, such as Humanoid and Driv- ing, we measured the failure probability of each environment configuration out of 10 runs and we considered an environment configuration to cause a failure if its failure probability is > 0.5. As each failure search approach is executed 10 times, we determined whether there is a statistical difference between the failures triggered by each pair of failure search approaches (including the 8 settings of Indago) by computing the Mann–Whitney U Test [4, 40]. To measure the effect size, we computed the Vargha–Delaney metric ˆA12 [65].
To compare the competing approaches (RQ2) we also considered two types of diversity regarding the failures generated by each approach, namely, input and output diversity. For input diversity, we first clustered all the environment configurations that caused a failure, across all considered failure search techniques, obtaining a single partition of all such failing environment configurations. Similarly, for output diversity, we clustered the trajectories (i.e., positions over time) of the DRL agents on the failure inducing environment configurations. In Parking and Driving, we considered the trajectories of the vehicle while performing the task, i.e., parking the vehicle in the former case and driving along the track in the latter. In Humanoid, we considered
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
the trajectory of the height of the robot, which determines whether the robot falls or not. Since trajectories can have different lengths, we extended them with zero-padding to the maximum observed length. For both input and output diversity, we used the k-means clustering algorithm [5] and we determined the optimal number of clusters k ∗ by performing silhouette analysis [51], optimizing the balance between density and separation of the clusters. In our experiments, we varied the number of clusters k between two and the number of inputs (i.e., environment con- figurations or trajectories) to be clustered, and computed the silhouette score for each candidate. We increased k ∗ to a higher value only if the new silhouette was at least 20% greater than the best silhouette score found so far, in order to filter out random fluctuations of the silhouette score.
After applying clustering, we computed two diversity metrics for each failure search approach, namely coverage and entropy. Given a failure search approach A and the optimal number of clusters k ∗, the coverage of the clusters for the approach A is defined as
# k ∗
i=1 γA (i) CA = , k ∗ (2)
where the function γA (i) : Z+ → 0|1 determines whether a certain cluster labeled by i is covered
by the failure search approach A, i.e., whether at least one failure generated by the approach A belongs to the cluster with label i.
The second metric, entropy, measures how uniformly the different failures are distributed across the clusters. Given the number of failures triggered by the failure search approach A in the ith cluster, FA (i), the probability of finding a failure generated by the approach A in cluster i is given by
Fal) A A = 4 3 = FFD ®
and entropy is defined as
# ke
HA = ηA i · log2(ηA i ). i=1 (4)
In particular, entropy is zero when all failures are concentrated in one cluster, while it is maxi- mum and equal to log2(k ∗) when failures are distributed uniformly across all the clusters. Hence, we can use the following formula as a normalized measure of entropy ranging between 0 and 1:
H A = HA log2(k ∗) . (5)
We ran clustering 10 times to account for randomness and took the average of coverage and entropy across the 10 runs. Then, we compared statistically the coverage and entropy averages for each failure search approach across the respective 10 runs by computing the Mann–Whitney U Test and the Vargha–Delaney effect size.
To evaluate the impact of algorithm, seed, and mutation strategies on Indago (RQ3), we used the same metrics: number of failures and input/output diversity, the latter quantified with coverage and entropy.
# 5.3 Results
Effectiveness (RQ1). Column # Failures in Table 2 shows the average number of failure envi- ronment configurations triggered by each failure search approach out of 10 runs for each case study. Considering Indago in all its settings, such number is between 4 and 27 in the Parking en- vironment, between 1 and 4 in the Humanoid environment and between 2 and 42 in the Driving environment.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:19
73:20
M. Biagiola and P. Tonella
Table 2. Number of Failures and Input/Output Diversity Measured by Cluster Coverage or Entropy
Parking Humanoid Driving Diversity Diversity Diversity Input Output Input Output Input Output s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E random 1 55.00 18.37 31.94 12.13 1 16.08 0.00 30.00 0.00 1 10.00 0.00 10.00 0.00 sampling 13 50.00 0.00 41.10 22.91 1 14.72 12.62 25.00 0.00 5 80.00 54.97 75.00 hcrnd 4 75.00 41.54 44.72 26.90 1 29.57 19.03 40.00 0.00 3 65.00 23.80 56.00 hcfail 5 85.00 64.87 45.45 33.95 2 27.37 18.00 50.00 16.23 4 90.00 73.61 95.00 77.20 hcsal+rnd 6 90.00 57.37 48.01 31.59 4 59.73 61.32 95.00 77.75 2 60.00 28.37 60.00 hcsal+fail 13 100.00 93.23 66.03 58.91 3 56.98 55.66 80.00 53.70 11 100.00 83.80 100.00 86.69 garnd 6 55.00 5.44 43.39 25.68 2 35.95 31.21 55.00 29.18 6 90.00 72.00 80.00 gafail 11 50.00 0.00 46.55 29.64 2 24.99 12.90 63.50 43.48 22 90.00 85.63 70.00 gasal+rnd 8 60.00 10.31 47.73 29.77 1 23.87 17.45 40.00 19.52 5 83.00 59.70 60.00 gasal+fail 27 55.00 2.01 51.95 29.22 4 38.11 24.99 75.00 44.04 42 100.00 97.61 70.00
# Baselines
# Indago
37.50
10.11
26.48
52.86
19.16
17.22
12.73
Values represent the average among ten runs. Bold faced values indicate a statistically significant difference between Indago and sampling. Underlined values indicate a large effect size.
RQ1: Overall, Indago successfully challenged the DRL agent under test in all case studies, by generating a significant number of failure environment configurations. On average, the sampling approach generated 6 failures while Indago generated from 3 to 24 failures.
Comparison (RQ2). Table 2 shows the results for the first setting: bold values indicate a statistically significant difference (at level α = 0.05) between Indago and sampling; values are underlined when the effect size is large. In particular, in Parking the best approach is gasal+fail that is able to generate more failures than other approaches (i.e., 27 on average) and the difference w.r.t. the sampling approach (i.e., 13 failures on average) is statistically significant with a large effect size. In Humanoid, several settings of Indago are significantly better than sampling (which exposes only 1 failure on average). In Driving, gasal+fail and hcsal+fail are the best approaches with, on average, 42 and 11 failures, respectively. Their difference w.r.t. the sampling approach (which exposes 5 failures on average) is statistically significant, and the effect size is large.
Regarding the comparison with the random approach, in all case studies, classifier-based ap- proaches (i.e., sampling and Indago) found significantly more failures with a large effect size. Hence, our empirical results show that the failures experienced during training are indeed related to and informative of the failures of the DRL agent under test.
The macro-columns Diversity in Table 2 shows the average out of 10 runs of the Coverage and Entropy metrics regarding input and output diversity for each failure search approach. Although in Parking the gasal+fail approach generates more failures than sampling, the two approaches are comparable in terms of input and output diversity (both considering coverage and entropy). On the other hand, the hcsal+fail approach is able to generate inputs that are both significantly different from those of sampling (i.e., higher coverage) and better distributed among clusters (i.e., higher entropy). Output coverage and entropy values are higher for hcsal+fail than those of sampling (66.03% and 58.91% vs. 41.10% and 22.91%, respectively), with output coverage being
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
comparable and output entropy being statistically better with a large effect size. This means that, although hcsal+fail and sampling generated the same number of failures (i.e., 13 on average), the failures produced by hcsal+fail exercise more diverse behaviors of the DRL agent under test.
For what concerns Humanoid, the two best approaches in terms of input and output diversity are settings of Indago (hcsal+rnd and hcsal+fail) both considering coverage and entropy (the gasal+fail setting is comparable to them). All of them are significantly better than sampling with a large effect size, except for gasal+fail, whose input entropy (24.99%) is not significantly different from that of sampling (12.62%). In Driving, the best approaches in terms of input and output diversity are hcsal+fail and gasal+fail, the former significantly better than sampling on input coverage and output diversity and the latter on input diversity.
When comparing Indago with the random approach in terms of diversity, most of the Indago settings, especially those with the saliency-based mutations, are significantly better than random with a large effect size in all case studies.
We identify two reasons for the higher diversity achieved by Indago: (1) the search-based approach of Indago implicitly favors diversity by locally exploring the environment configuration space until failures are discovered. In this way, it is more likely for Indago to obtain failure configurations starting from diverse seeds. This is more difficult when failure configurations are to be randomly generated from scratch (e.g., by the sampling approach). (2) The failure environment configurations generated at training time affect different versions of the agent under test, which is evolving during its training process. Hence, these configurations tend to be quite diverse. Indago effectively uses such seed configurations by retaining their diversity while at the same time increasing their failure exposure.
RQ2: Overall, both considering the number of failures triggered and their input and output diversity, hcsal+fail is the best setting of Indago, generating 50% more failures than the sampling approach as well as failures being 78% more input-diverse and 74% more output- diverse.
Hyperparameters (RQ3). On average and considering all case studies, hcsal+fail generates fail- ures that are the most diverse both in terms of input and output diversity. Specifically, hcsal+fail has the best input coverage (85.66% vs. 69.91% of the second best hcsal+rnd), the best input entropy (77.56% vs. 52.16% of the second best hcfail), the best output coverage (82.01% vs. 67.67% of the second best hcsal+rnd) and the best output entropy (66.43% vs. 45.27% of the second best hcsal+rnd).
On the other hand, gasal+fail is the setting of Indago that generates the highest number of failures (i.e., 24 on average). However, such failures cover fewer clusters and have a lower entropy than the failures generated by hcsal+fail. Therefore, considering the number of failures and their diversity, hcsal+fail is the preferable Indago setting.
Across all case studies, the settings of Indago that use the saliency-based mutations are more effective than their counterparts, i.e., the difference is always statistically significant with a large effect size, except for Humanoid, where gasal+fail is comparable to garnd. Overall, this shows that the guidance offered by the saliency-based mutation operator is effective at finding failure envi- ronment configurations.
In Table 3, we report a further comparison between the different settings of Indago, focused on the impact of the seed strategy (random, rnd, vs. failure, fail). In each cell of the table the three case studies are separated by a forward slash “/” symbol. The symbol “F” (respectively, “R”) indicates that failure seeding (respectively, random seeding) is statistically better than random (respectively, failure) seeding. A dash symbol indicates no statistically significant difference. For instance, for the raw hcrnd vs. hcfail and the macro-column # Failures, the hcrnd and the hcfail settings are equivalent
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:21
73:22
M. Biagiola and P. Tonella
Table 3. Impact of the Seed Strategy
# Parking / Humanoid / Driving
Diversity Input Output s e r u l i a F # e g a r e v o C y p o r t n E e g a r e v o C y p o r t n E hcrnd vs. hcfail - / - / F - / - / F - / - / F - / - / F - / - / F garnd vs. gafail F / - / F - / - / - - / - / - - / - / - - / - / - hcsal+rnd vs. hcsal+fail F / - / F - / - / F F / - / F - / - / F F / - / F gasal+rnd vs. gasal+fail F / F / F - / - / F - / - / F - / F / - - / - / - rnd vs. fail 0 vs. 8 0 vs. 3 0 vs. 4 0 vs. 3 0 vs. 3
In each row, the three case studies (Parking / Humanoid / Driving) are separated by a forward slash “/” symbol. An “F” symbol indicates a statistically significant difference in favor of the approach with the failure seed; a dash “-” symbol indicates that the two approaches are indistinguishable and an “R” symbol (missing in the table) would indicate a statistically significant difference in favor of the approach with the random seed. As for the comparisons in the first two research questions, we compute statistical significance using the Mann–Whitney U Test.
in the first two case studies, i.e., Parking and Humanoid (hence the two “-” symbols separated by the “/” symbol), while the hcfail setting is statistically better than hcrnd in the last case study, i.e., Driving (hence the last symbol is “F”). In terms of number of failures, we can see that, in most cases (i.e., 8/12) the settings with the failure seeds are significantly better than their random seeds counterparts. In particular, the gasal+fail setting is significantly better than the gasal+rnd setting in all case studies (while, for instance, the hcsal+fail is significantly better than hcsal+rnd in Parking and Driving but not in Humanoid). From the point of view of diversity, the settings of Indago with the failure seeds are mostly comparable to the settings with the random seeds, except for Driving, where the failure seeds are critical to generate more diverse failures. Random seeding is never a better choice (indeed the “R” symbol is completely missing in the table), across all settings of Indago and across all three case studies.
RQ3: Overall, the settings of Indago with the saliency mutation strategy and failure seeds are either comparable or significantly better than their random counterparts. In particular, failure seeds are significantly better than random seeds in 21 comparisons out of 60. Be- tween hill climbing and genetic algorithm, the former is preferable because it generates more diverse failure scenarios (i.e., on average hill climbing failures are 60% more input- diverse and 80% more output-diverse than genetic algorithm failures). The latter might be considered when the number of exposed failures is important, regardless of their diversity.
Ineffective Failure Seeds (RQ4). RQ3 shows that the saliency mutation operator is effective at guiding the search towards failure-inducing configurations and that hill climbing generates more diverse failures w.r.t. genetic algorithm. To test the effectiveness of Indago in the presence of ineffective failure seeds, we consider its best configuration, i.e., hill climbing with the saliency mutation operator and random seeds. We chose to use random seeds to test whether the surrogate model is able to guide the search without relying on ineffective failure seeds. In Table 4, we indicate
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
Table 4. Number of Failures and Input/Output Diversity Measured by Cluster Coverage or Entropy
Diversity Diversity Diversity Input Output Input Output Input Output s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E ) % ( e g a r e v o C ) % ( y p o r t n E random 1 46.67 12.10 55.00 19.18 1 30.83 0.00 37.50 5.00 0 23.72 0.00 20.00 0.00 train. fail. 1 45.00 0.00 50.00 0.00 1 25.83 0.00 35.00 10.00 2 33.34 9.70 60.00 20.00 hcsal+rnd 3 71.67 57.22 80.00 62.42 3 43.42 7.29 75.00 48.08 3 57.97 48.61 80.00 56.23 hcsal+rnd 3 80.00 62.06 80.00 52.88 12 81.92 57.35 100.00 86.02 10 91.16 88.10 100.00 80.48
# Parking
# Humanoid
# Driving
# Baselines
# Indago
Values represent the average among ten runs. Bold faced values indicate a statistically significant difference between Indago and the best of the two baselines. Underlined values indicate a large effect size. We indicate with a bar the setting of Indago using the fitness-based surrogate model, i.e., hcsal+rnd.
with hcsal+rnd the version of Indago using the classifier, while we indicate with hcsal+rnd the version of Indago that uses the regressor-based surrogate model.
Table 4 shows that replaying training failures (row train. fail.) is ineffective at testing time, being indistinguishable from random in Parking and Humanoid. In Driving, training failure replay generates significantly more failures than random (i.e., on average 2 vs. 0, respectively), although regarding diversity, only the output coverage of training failures is statistically different than the output coverage of failures generated by random.
Regarding Indago, the version with the classifier-based surrogate model (i.e., hcsal+rnd) generates significantly more failures than random and training failure replay in Parking and Humanoid. Such failures are also significantly more diverse than the two baselines in Parking and Humanoid, although in the latter there is statistical significance only for output coverage. Regarding Driving, hcsal+rnd failures have a significantly higher input entropy, while for output coverage there is no statistical significance, although the trend is in favor of hcsal+rnd (i.e., 80 vs. 60 for output coverage and 56 vs. 20 for output entropy).
The version of Indago with the regressor-based surrogate model (i.e., hcsal+rnd) is equally effec- tive at finding failures as the classifier-based one in Parking. In Humanoid and Driving, it shows statistical significance of the differences w.r.t. the two baselines, while the classifier-based one does not (e.g., on input diversity in Humanoid; number of failures, input and output coverage in Driving).
RQ4: Overall, when failure seeds are ineffective, Indago is able to trigger more failures of the DRL agent under test than replaying the training configurations at testing time in all three case studies (i.e., on average 3 vs. 1 in Parking, 12 vs. 1 in Humanoid and 10 vs. 2 in Driving), with the differences being statistically significant with a large effect size.
# 6 DISCUSSION
# 6.1 Solvability of the Failures
For each approach, we resumed training of the three DRL agents under test by feeding all the failure environment configurations found by each approach in every run of the environment generation
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:23
73:24
M. Biagiola and P. Tonella
Parking Humanoid Driving “ ® Oe i ® - . a KY A hosakiraii cae tr. failures e e e a al [aaa © oo 3 Ona A Ai, . Os * Ase A he sasta oe A hesasta oca @ tr. failures Re a Ni @ tr. failures ks A B Cc
Fig. 5. t-SNE projection in 2D of trajectories associated with hcsal+fail ( ) and training failures ( ) config- urations. The C symbol indicates the centroid of a cluster. The leftmost plot (A) shows trajectories for the Parking environment, the center plot (B) for the Humanoid environment, and the rightmost plot (C) for the Driving environment.
process. In every case study and for each approach, we found that the given DRL agent could learn how to successfully terminate the respective episodes by performing some additional training. This shows that the failure environment configurations generated by all approaches, and indeed by Indago, are solvable by the DRL agents under test.
This also indicates that, although a generated environment configuration can be challenging for the DRL agent under test, there exists a sequence of actions that let the DRL agent solve the task successfully. For instance, after additional training, the DRL agent in the Parking environment is always able to find trajectories for the vehicle to reach the target spot, which is instead missed by the original DRL agent under test. Similarly, in the Driving environment, the generated failure environment configurations do not induce track shapes that are beyond the mechanical capabilities of the vehicle. Regarding Humanoid, the initial positions of the joints as well as their velocities, resulting from the generated failure environment configurations, do not prevent the DRL agent to control and regain the balance of the robot.
Solvability of the failure environment configurations generated by Indago shows that such environment configurations represent real weaknesses of the DRL agent under test, which could realistically occur during the operation of the DRL agent in production.
# 6.2 Training Failures
6.2.1 Qualitative Analysis. A simple way to test a DRL agent is to replay the training failures except for the earliest ones (e.g., by filtering out the first 30% of the failure environment configu- rations). However, this has two major downsides: (1) the DRL agent under test may have adapted to failure environment configurations in which weaker versions of itself failed, despite the exclu- sion of early failures; (2) such an approach would not be generative and by design it can only replay existing failure environment configurations. Generative approaches like hcsal+fail produce diverse and potentially unlimited challenging inputs exposing the limitations of the DRL agent, and potentially improving it through retraining [15].
For the sake of completeness, we replayed at testing time the failure environment configurations that happened during the DRL training process, and we compared them with the failures generated by hcsal+fail. In particular, we clustered the DRL agents trajectories associated with those failures, following the same process described in Section 5.2.2. Figure 5 shows a 2D t-SNE [64] projection of the failure trajectories of the three DRL agents under all failure environment configurations in each case study. Cluster centroids are indicated with the letter C. In Humanoid (Figure 5(B)) the trajectories associated with the failure environment configurations discovered by Indago, cover
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
Table 5. Comparison between Training Failures (i.e., Train. Fail.) and Failures Found by the Best Indago Settings (i.e., hcsal+fail and gasal+fail)
Parking Humanoid Driving Diversity Diversity Diversity Input Output Input Output Input Output ) ) ) ) ) s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E % ( y t i r u P i n i G ) % ( e g a r e v o C ) % ( y p o r t n E % ( y t i r u P i n i G s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E % ( y t i r u P i n i G ) % ( e g a r e v o C ) % ( y p o r t n E % ( y t i r u P i n i G s e r u l i a F # ) % ( e g a r e v o C ) % ( y p o r t n E % ( y t i r u P i n i G ) % ( e g a r e v o C ) % ( y p o r t n E 31 100.00 98.10 13 100.00 93.53 60.48 95.00 40.62 85.00 42.24 66.93 1 35.00 10.00 3 100.00 66.19 83.42 35.00 10.00 100.00 63.70 87.40 13 100.00 78.32 11 100.00 81.95 53.84 100.00 74.66 100.00 83.40 31 100.00 85.32 27 50.00 0.00 81.55 89.83 51.49 48.84 25.17 81.54 1 4 28.33 10.00 91.67 54.02 89.84 30.00 10.00 90.00 50.51 89.41 13 95.00 66.99 42 100.00 97.61 72.76 100.00 75.57 64.50 6.78 ) % ( y t i r u P i n i G 53.23 69.63
# train. fail.
# hcsal+fail
# train. fail.
# gasal+fail
The comparison is in terms of number of failures, input/output diversity measured by cluster coverage or entropy and Gini purity coefficient. Values represent the average among ten runs. Bold faced values indicate a statistically significant difference. Underlined values indicate a large effect size.
more clusters than the trajectories associated with training failure environment configurations. In Parking (Figure 5(A)) and Driving (Figure 5(C)) the two classes of trajectories are complementary since they cover the same clusters but with a different intra-cluster distribution, showing that the generative approach of Indago can explore new clusters or new regions within a cluster.
6.2.2 Quantitative Analysis. We also carried out a quantitative analysis by measuring the num- ber of failures triggered when replaying the failure environment configurations that happened during training and their diversity w.r.t. the failure configurations found by the two best settings of Indago, i.e., hcsal+fail and gasal+fail. Table 5 shows the results for each case study.
In Parking, the training failure configurations trigger significantly more failures than both Indago’s settings (i.e., 31 vs. 13 and 27, respectively). Such failures are equivalent to those found by hcsal+fail in terms of both input and output diversity, but significantly more diverse than those found by gasal+fail. In Humanoid, both Indago’s settings are better than replaying failure configu- rations in all dimensions, i.e., in terms of number of failures and their diversity. In Driving, hcsal+fail and the training failures are equivalent, while gasal+fail is significantly better in terms of number of failures (i.e., 42 vs. 13) and their input entropy (i.e., 97.61 vs. 66.99), but significantly worse in terms of output diversity.
Since the comparison between failure replay and Indago is not conclusive on input and output diversity (i.e., none of the two is superior w.r.t. the other in all case studies), we measured the complementarity between the two competing approaches. In fact, high diversity could be achieved by the two approaches either with similar or with complementary distribution of the inputs/out- puts among the same clusters. To measure such complementarity between training failures and the failures that Indago generates, we use the Gini purity coefficient [50] for each cluster, defined as follows:
IC|
G (i) = cf =1 p2 cf , (6)
where i indicates the ith cluster, |C | is the number of classes and pcf represents the probability of finding a data point of class cf in cluster i. The number of classes is |C | = 2, since we are measuring the complementarity of two approaches. Equation (6) gives the probability of having a failure in a cluster belonging to one specific class. In particular, G (i) = 1 means that cluster i is pure, i.e., all the data points in the cluster belong to a single class.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:25
73:26
M. Biagiola and P. Tonella
Parking Humanoid Driving 2.0 | pa ai Sana |3* çe 7 $ 16 > ; 14 * 424 i wna ay 101 — — — — — — 0 10 20 30 40 50 60 70 # Timesteps 20 al HANA |, 5 8 16 2 “ec | i 12 ONY bed | TAN 0 50 100 150 200 # Timesteps A B c
Fig. 6. Representation of different trajectories associated with hcsal+fail failure configurations. The trajectory of the vehicle is shown in blue for the Parking environment (A) and in red for the Driving environment (C). In Humanoid (B), the trajectory of the robot is the latitudinal position over time.
The Gini Purity columns in Table 5 show the average Gini purity coefficients for each case study. Overall, we can observe that, in both settings of Indago, i.e., hcsal+fail and gasal+fail, the majority of the clusters are highly pure (i.e., the Gini purity is always greater than 50%). In particular, the setting gasal+fail shows a higher degree of complementarity with the training failures than hcsal+fail (across all the case studies, the Gini purity coefficients are 80.79% and 67.71%, respectively).
Hence, we conclude that replayed failures and new failures generated by Indago are highly complementary. Developers should use both during RL testing.
# 6.3 Qualitative Analysis of Indago Failures
Figure 6 shows the failure trajectories associated to failure environment configurations generated by Indago. For all case studies, we selected two failure trajectories belonging to different clusters representing two different failure modes.
In Parking (Figure 6(A)), the first failure mode (at the top) consists of the DRL agent not being able to park the vehicle in the target spot with the right heading given a certain amount of time; the second failure mode (at the bottom) is when the DRL agent does not complete the parking maneuver due to a crash with another vehicle parked beside the target spot.
In Humanoid (Figure 6(B)), the abdomen latitudinal coordinate of the robot should be among the two horizontal lines shown in Figure 6 called healthy range [18], otherwise, the robot falls and the episode terminates unsuccessfully. The two failure trajectories associated with Indago failure environment configurations are different: the one at the top shows a trajectory that monotonically goes down in a relatively short amount of time (i.e., just above 70 timesteps). The trajectory at the bottom is more noisy indicating that the DRL agent under test is more uncertain. Indeed, the DRL
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
agent is able to recover the partial fall around the middle of the episode (i.e., around 120 timesteps) while eventually failing after ≈ 200 timesteps.
In Driving (Figure 6(C)), the first failure mode shows the DRL agent failing to drive the last curve of the track, while the trajectory at the bottom represents a track with a difficult left curve at the beginning.
# 6.4 Discrete vs. Continuous Configurations
In the Parking environment, the number of failures found by the sampling approach is equivalent to the failures found by the best Indago setting, i.e., hcsal+fail (i.e., 13 on average), although the fail- ures generated by hcsal+fail have a significantly higher input diversity and output entropy. Parking environment configurations are composed of several discrete and only a few continuous parame- ters, and they are subject to a few constraints. As a consequence, sampling Parking environment configurations at random is efficient, as a large number of candidate environment configurations can be generated within the given search budget, among which it is more likely to find environ- ment configurations where the failure prediction is high.
Also in the Humanoid environment, there are few validity constraints, hence it is efficient to sample environment configurations at random. However, the space of environment configurations is larger than in Parking (almost twice as much, i.e., 47 vs. 24 parameters) and all parameters are in the continuous domain. As a consequence, finding challenging environment configurations by sampling at random is not effective. Indeed, our results show that the sampling approach gener- ates, on average, 1 failure, while hcsal+fail generates 3 failures (a 200% increase), with a significantly higher input and output diversity. This shows that Indago works much better than random sam- pling when environment configurations have continuous parameters.
The Driving environment offers yet another perspective. Indeed, similarly to Parking, Driving environment configurations are mostly composed of discrete parameters but, on the other hand, such environment configurations are subject to complex constraints. Therefore, sampling Driving environment configurations at random is not efficient, contrary to modifying existing environment configurations. Indeed, the sampling approach generates, on average, 5 failures, while hcsal+fail generates 11 failures (i.e., a 120% increase), with a significantly higher input and output diversity. Although, the Driving environment configurations have mostly discrete parameters our search-based approach is more effective than sampling since it is able to use the search budget more efficiently by modifying existing environment configurations which satisfy the complex constraints that hold in this environment.
# 6.5 Ablation Study on the Driving Environment
When generating roads for the driving environment, we make sure that they are challenging enough for the agent under test. Indeed, we constrain the generation to roads that have at least three curves, one of which must have a rotation angle of at least 120◦. However, the driving agent under test might experience failures also when roads do not obey such constraints. To test this hypothesis, we executed Indago with random seeds instead of failure seeds (as failure seeds obey the constraints by construction) and the sampling approach. Table 6 shows the number of failures found when executing the failure search with constraints enabled (i.e., column w/ Constr.) and disabled (i.e., column w/o Constr.). Results show that for sampling, hcsal+rnd and gasal+rnd there is no statistical significant difference between the number of failures generated by the respective techniques with and without road constraints. However, hcrnd and garnd find significantly more failures when constraints are enabled, showing that restricting the inputs during the search can be beneficial to discover more failures.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:27
73:28
M. Biagiola and P. Tonella
Table 6. Comparison between Failures Found by Indago Settings on the Driving Environment with and without Enabling the Road Constraints when Generating New Configurations
Driving . r t s n o C . r t s n o C o / / w w # Failures Indago sampling 5 4 hcrnd 3 1 hcsal+rnd 2 1 garnd 6 3 gasal+rnd 5 3
# 6.6 Regressor-based vs. Classifier-based Surrogate Model
Due to the low number of failures in Parking and Driving in the setting for RQ4, we resorted to a regressor-based surrogate model to guide the search toward failure configurations. We observe that in Parking, the regressor-based surrogate model is as effective as the classifier-based surrogate model. On the other hand, the former is significantly better than the latter in both Humanoid (in terms of higher number of failures, better input diversity and output coverage) and Driving (in terms of higher number of failures and better input diversity).
One explanation of this phenomenon is in the function we use to compute the continuous values for each episode in the different case studies. In Parking, we use the episode length as guidance towards failures, which is not more effective than using a binary classifier. Indeed, the episode length only gives indirect guidance towards a failure; a long episode might point to configurations where the vehicle is simply far from the parking target spot. On the other hand, in Humanoid, we track the height of the robot throughout the episode; the lower the minimum height the higher the chance that the initial configuration is challenging for the DRL agent under test. Likewise, in Driving we use the minimum cross track error in an episode, which guides the regressor-based surrogate model toward configurations with challenging curves.
In conclusion, the regressor-based surrogate model outperforms the classifier-based one in two out of three case studies, while being equivalent to the classifier-based surrogate model in Parking. In such contexts (see the RQ4 macro-column in Table 1), the number of failures is low (Parking and Driving) or training failures are ineffective (Humanoid), and a continuous-valued function provides a finer-grained signal when training the surrogate model w.r.t. boolean values. However, it requires domain knowledge of the task at hand to define a continuous-valued function that measures how challenging a certain environment configuration is for the agent.
# 6.7 Threats to Validity
6.7.1 Internal Validity. A threat to internal validity may come from an unfair comparison of the considered approaches. We gave the same search budget to all approaches, and we generated the same number of environment configurations. Moreover, we considered the same DRL agents for each approach and executed the tests on the same environments.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
6.7.2 External Validity. Using a limited number of subjects poses an external validity threat, in terms of generalizability of our results. To mitigate such threat, we chose three environments which are widely used in the DRL community and have different characteristics that challenge the capabilities of each failure search approach.
6.7.3 Conclusion Validity. A conclusion validity threat may come from the wrong interpreta- tion of the results due to random variations and inappropriate use of statistical tests. We mitigated this threat by executing each failure search approach multiple times, as well as repeating multiple times the execution of environment configurations on non-deterministic environments. When computing diversity, we executed clustering multiple times to account for the randomness of the k-means algorithm. Moreover, we compared the different approaches, both in terms of number of failures and in terms of their diversity, using rigorous statistical tests such as the Mann– Whitney U Test for computing the p-value and the Vargha–Delaney metric to measure the effect size.
6.7.4 Reproducibility. In terms of reproducibility, we publish our replication package [7], mak- ing our evaluation repeatable and our results reproducible.
# 7 CONCLUSION AND FUTURE WORK
Our approach to test DRL agents uses the interaction data produced by the DRL agents during training to train a surrogate model—i.e., a classifier—on failure and non-failure (i.e., pass) envi- ronment configurations. Then, it uses the failure prediction output of the surrogate model, as a fitness function to be maximized, to achieve high failure-exposure capabilities of the generated environment configurations while saving computation time. Our empirical results show that our search-based approach is able to generate 50% more failures than the state-of-the-art sampling approach and that such failures are more diverse in all case studies (on average, 78% more diverse regarding input diversity, and 74% more diverse regarding output diversity). When training failures are effective at testing time, our results show that Indago generates failures that are complemen- tary. On the other hand, when training failures are ineffective at testing time, Indago triggers significantly more, and more diverse failures. In our future work, we plan to increase the diversity of the generated failures by incorporating it into the fitness function.
# REFERENCES
- [1] Raja Ben Abdessalem, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. 2016. Testing advanced driver assistance systems using multi-objective search and neural networks. In Proceedings of the 31st IEEE/ACM International Confer- ence on Automated Software Engineering, ASE. 63–74.
- [2] Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. 2018. Testing au- tonomous cars for feature interaction failures using many-objective search. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE 2018). ACM, 143–154. DOI:https://doi.org/10.1145/ 3238147.3238192
- [3] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in Neural In- formation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fer- gus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5048–5058. https://proceedings.neurips.cc/paper/2017/hash/ 453fadbd8a1a3af50a9df4df899537b5-Abstract.html
- [4] Andrea Arcuri and Lionel Briand. 2014. A hitchhiker’s guide to statistical tests for assessing randomized algorithms in software engineering. Software Testing, Verification and Reliability 24, 3 (2014), 219–250.
- [5] David Arthur and Sergei Vassilvitskii. 2006. k-means++: The Advantages of Careful Seeding. Technical Report. Stanford.
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:29
# 73:30
M. Biagiola and P. Tonella
- [6] R. Ben Abdessalem, S. Nejati, L. C. Briand, and T. Stifter. 2018. Testing vision-based control systems using learnable evolutionary algorithms. In IEEE/ACM 40th International Conference on Software Engineering (ICSE’18). 1016–1026. https://doi.org/10.1145/3180155.3180160
- [7] Matteo Biagiola. 2023. Replication package. https://www.github.com/matteobiagiola/drl-testing-experiments. On- line; accessed November 2023.
- [8] Matteo Biagiola and Paolo Tonella. 2022. Testing the plasticity of reinforcement learning-based systems. ACM Trans. Softw. Eng. Methodol. 31, 4 (2022), 80:1–80:46. https://doi.org/10.1145/3511701
- [9] Houssem Ben Braiek and Foutse Khomh. 2020. On testing machine learning programs. Journal of Systems and Soft- ware 164 (2020), 110542.
- [10] Bruno Brito, Achin Agarwal, and Javier Alonso-Mora. 2022. Learning interaction-aware guidance for trajectory opti- mization in dense traffic scenarios. IEEE Transactions on Intelligent Transportation Systems 23, 10 (2022), 18808–18821. https://doi.org/10.1109/TITS.2022.3160936
- [11] Dong Chen, Mohammad R. Hajidavalloo, Zhaojian Li, Kaian Chen, Yongqiang Wang, Longsheng Jiang, and Yue Wang. 2023. Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic. IEEE Trans. Intell. Transp. Syst. 24, 11 (2023), 11623–11638. https://doi.org/10.1109/TITS.2023.3285442
- [12] Torch Contributors. 2021. Torch CrossEntropyLoss class documentation. https://pytorch.org/docs/stable/generated/ torch.nn.CrossEntropyLoss.html. Online; accessed November 2023.
- [13] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9268– 9277.
- [14] Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2018. Distributional reinforcement learning with quantile regression. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.
- [15] Richard Everett. 2019. Strategically Training and Evaluating Agents in Procedurally Generated Environments. Ph.D. Dissertation. University of Oxford.
- [16] Ben Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. 2021. Robust predictable control. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27813–27825. https://proceedings.neurips.cc/paper/2021/hash/ e9f85782949743dcc42079e629332b5f-Abstract.html
- [17] Shuo Feng, Xintao Yan, Haowei Sun, Yiheng Feng, and Henry X. Liu. 2021. Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment. Nature Communications 12, 1 (2021), 1–14.
- [18] Farama Foundation. 2022. Humanoid. https://www.gymlibrary.dev/environments/mujoco/humanoid/. Online; ac- cessed November 2023.
- [19] Alessio Gambi, Marc Müller, and Gordon Fraser. 2019. Automatically testing self-driving cars with search-based procedural content generation. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA. 318–328.
- [20] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum en- tropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learn- ing Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 1856–1865. http://proceedings.mlr.press/v80/ haarnoja18b.html
- [21] Hado van Hasselt. 2010. Double Q-learning. In Advances in Neural Information Processing Systems 23: 24th An- nual Conference on Neural Information Processing Systems 2010. Proceedings of a Meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta (Eds.). Curran Associates, Inc., 2613–2621. https://proceedings.neurips.cc/paper/2010/hash/ 091d584fced301b442654dd8c23b3fc9-Abstract.html
- [22] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2018. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.
- [23] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial attacks on neural network policies. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=ryvlRyBKl
- [24] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning systems. In Proceedings of the 42nd International Conference on Software Engineering (ICSE’20). ACM, 12 pages.
- [25] Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. 2019. Learning agile and dynamic motor skills for legged robots. Sci. Robotics 4, 26 (2019). https://doi.org/10.1126/SCIROBOTICS. AAU5872
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
- [26] Jemin Hwangbo, Joonho Lee, and Marco Hutter. 2018. Per-contact iteration method for solving contact dynamics. IEEE Robotics and Automation Letters 3, 2 (2018), 895–902.
- [27] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning. PMLR, 448–456.
- [28] Justin Basilico J. Ashok Chandrashekar, Fernando Amat and Tony Jebara. 2017. Artwork Personalization at Netflix. https://netflixtechblog.com/artwork-personalization-c589f074ad76. Online; accessed November 2023.
- [29] Kittipat Virochsiri Jason Gauci and Edoardo Conti. 2018. Horizon: The first open source reinforcement learning plat- form for large-scale products and services. https://engineering.fb.com/2018/11/01/ml-applications/horizon/. Online; accessed November 2023.
- [30] Kittipat Virochsiri Jason Gauci and Edoardo Conti. 2021. A platform for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.). https://github.com/facebookresearch/ReAgent. Online; accessed November 2023.
- [31] Yaochu Jin. 2005. A comprehensive survey of fitness approximation in evolutionary computation. Soft Computing 9, 1 (2005), 3–12.
- [32] Gary King and Langche Zeng. 2001. Logistic regression in rare events data. Political Analysis 9, 2 (2001), 137–163.
- [33] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A unified and generic model interpretability library for PyTorch. CoRR abs/2009.07896 (2020). arXiv:2009.07896 https://arxiv.org/ abs/2009.07896
- [34] Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. 2020. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In Proceedings of the International Conference on Machine Learning. PMLR, 5556–5566.
- [35] Jennifer Langston. 2020. With reinforcement learning, Microsoft brings a new class of AI solutions to customers. https://blogs.microsoft.com/ai/reinforcement-learning/. Online; accessed November 2023.
- [36] Timothée Lesort, Natalia Díaz Rodríguez, Jean-François Goudou, and David Filliat. 2018. State representation learn- ing for control: An overview. Neural Networks 108 (2018), 379–392. https://doi.org/10.1016/J.NEUNET.2018.07.006
- [37] Edouard Leurent. 2018. An Environment for Autonomous Driving Decision-Making. https://github.com/eleurent/ highway-env. Online; accessed November 2023.
- [38] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. 2017. Tactics of ad- versarial attack on deep reinforcement learning agents. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, Carles Sierra (Ed.). ijcai.org, 3756–3762. https://doi.org/10.24963/IJCAI.2017/525
- [39] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. 2021. Isaac Gym: High performance GPU based physics simulation for robot learning. In Proceedings of the Neural Information Processing Sys- tems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 28dd2c7955ce926456240b2ff0100bde-Abstract-round2.html
- [40] Henry B. Mann and Donald R. Whitney. 1947. On a test of whether one of two random variables is stochasti- cally larger than the other. The Annals of Mathematical Statistics 18, 1 (1947), 50–60. https://doi.org/10.1214/aoms/ 1177730491
- [41] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. 2022. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics 7, 62 (2022), eabk2822.
- [42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nat. 518, 7540 (2015), 529–533. https://doi.org/10.1038/NATURE14236
- [43] OpenAI and collaborators. 2021. Gym Leaderboard. https://github.com/openai/gym/wiki/Leaderboard. Online; ac- cessed November 2023.
- [44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Ro- man Garnett (Eds.). 8024–8035. https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740- Abstract.html
- [45] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:31
73:32
M. Biagiola and P. Tonella
Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12 (2011), 2825–2830. https://doi.org/10.5555/1953048.2078195
- [46] Antonin Raffin. 2020. RL Baselines3 Zoo. https://github.com/DLR-RM/rl-baselines3-zoo. Online; accessed November 2023.
- [47] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. 2021. Stable- Baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research 22, 268 (2021), 1–8. Retrieved from http://jmlr.org/papers/v22/20-1364.html
- [48] Antonin Raffin, Jens Kober, and Freek Stulp. 2022. Smooth exploration for robotic reinforcement learning. In Pro- ceedings of the Conference on Robot Learning. PMLR, 1634–1644.
- [49] Vincenzo Riccio and Paolo Tonella. 2020. Model-based exploration of the frontier of behaviours for deep learning system testing. In Proceedings of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE’20).
- [50] Lior Rokach and Oded Maimon. 2005. Top-down induction of decision trees classifiers-a survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 35, 4 (2005), 476–487.
- [51] Peter J. Rousseeuw. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math. 20 (1987), 53–65. https://doi.org/10.1016/0377-0427(87)90125-7
- [52] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. 2022. Learning to walk in minutes using massively parallel deep reinforcement learning. In Proceedings of the Conference on Robot Learning. PMLR, 91–100.
- [53] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2016. Prioritized experience replay. In 4th Inter- national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.05952
- [54] John Schulman, FilipWolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017). arXiv:1707.06347 http://arxiv.org/abs/1707.06347
- [55] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http: //arxiv.org/abs/1312.6034
- [56] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.
- [57] Haowei Sun, Shuo Feng, Xintao Yan, and Henry X. Liu. 2021. Corner case generation and analysis for safety assess- ment of autonomous vehicles. Transportation Research Record 2675, 11 (2021), 587–600.
- [58] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.
- [59] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http: //arxiv.org/abs/1312.6199
- [60] Martin Tappler, Filip Cano Córdoba, Bernhard K. Aichernig, and Bettina Könighofer. 2022. Search-based testing of reinforcement learning. In Proceedings of the 31st International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 503–510. https://doi.org/10.24963/IJCAI.2022/72
- [61] Maxime Ellerbach Tawn Kramer and contributors. 2021. Self Driving Car Sandbox. https://github.com/tawnkramer/ sdsandbox. Online; accessed November 2023.
- [62] Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine for model-based control. In Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 5026–5033.
- [63] Jonathan Uesato, Ananya Kumar, Csaba Szepesvári, Tom Erez, Avraham Ruderman, Keith Anderson, Krishnamurthy (Dj) Dvijotham, Nicolas Heess, and Pushmeet Kohli. 2019. Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=B1xhQhRcK7
- [64] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research 9, 86 (2008), 2579–2605. http://jmlr.org/papers/v9/vandermaaten08a.html
- [65] András Vargha and Harold D. Delaney. 2000. A critique and improvement of the CL common language effect size statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics 25, 2 (2000), 101–132.
- [66] Ankit Verma, Siddhesh Bagkar, Naga Venkata SaiTeja Allam, Adhiti Raman, Matthias Schmid, and Venkat N. Krovi. 2021. Implementation and validation of behavior cloning using scaled vehicles. In Proceedings of the SAE WCX Digital Summit. SAE International. DOI:https://doi.org/10.4271/2021-01-0248
- [67] Ari Viitala, Rinu Boney, and Juho Kannala. 2020. Learning to drive small scale cars from scratch. CoRR abs/2008.00715 (2020). arXiv:2008.00715 https://arxiv.org/abs/2008.00715
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
Testing of Deep Reinforcement Learning Agents with Surrogate Models
- [68] Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, and Juho Kannala. 2021. Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning. In 20th International Conference on Advanced Robotics, ICAR 2021, Ljubljana, Slovenia, December 6-10, 2021. IEEE, 275–281. https://doi.org/10.1109/ICAR53236.2021.9659342
- [69] Che Wang, Yanqiu Wu, Quan Vuong, and Keith W. Ross. 2019. Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning. CoRR abs/1910.02208 (2019). arXiv:1910.02208 http://arxiv.org/abs/1910.02208
- [70] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016. Dueling network architectures for deep reinforcement learning. In Proceedings of the International Conference on Machine Learning. PMLR, 1995–2003.
- [71] Automotive World. 2016. Automatic Intelligent Parking: Audi at NIPS in Barcelona. Retrieved July 05, 2023 from https://www.automotiveworld.com/news-releases/automatic-intelligent-parking-audi-nips-barcelona/
- [72] Mengdi Xu, Peide Huang, Fengpei Li, Jiacheng Zhu, Xuewei Qi, Kentaro Oguchi, Zhiyuan Huang, Henry Lam, and Ding Zhao. 2021. Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sam- pling. CoRR abs/2106.10566 (2021). arXiv:2106.10566 https://arxiv.org/abs/2106.10566
- [73] Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. 2021. Delving into deep imbalanced regres- sion. In Proceedings of the International Conference on Machine Learning. PMLR, 11842–11851.
- [74] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering (2020).
- [75] Qi Zhang and Tao Du. 2019. Self-driving scale car trained by Deep reinforcement Learning. CoRR abs/1909.03467 (2019). arXiv:1909.03467 http://arxiv.org/abs/1909.03467
- [76] Songan Zhang, Lu Wen, Huei Peng, and H. Eric Tseng. 2021. Quick learner automated vehicle adapting its road- manship to varying traffic cultures with meta reinforcement learning. In Proceedings of the 2021 IEEE International Intelligent Transportation Systems Conference (ITSC’21). IEEE, 1745–1752.
- [77] Hongli Zhou, Xiaolei Chen, Guanwen Zhang, and Wei Zhou. 2021. Deep reinforcement learning for autonomous driving by transferring visual features. In Proceedings of the 2020 25th International Conference on Pattern Recognition (ICPR’21). DOI:https://doi.org/10.1109/ICPR48806.2021.9412011
Received 26 September 2022; revised 30 October 2023; accepted 31 October 2023
ACM Transactions on Software Engineering and Methodology, Vol. 33, No. 3, Article 73. Pub. date: March 2024.
73:33
