[
    {
        "element_id": "3de4b982b2b6174e1fbfcbd68cf2c8c0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.3
                    ],
                    [
                        136.0,
                        104.9
                    ],
                    [
                        1125.7,
                        104.9
                    ],
                    [
                        1125.7,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82836,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "1758d1a522ce5739b38535a09a14a5a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        255.7,
                        163.2
                    ],
                    [
                        255.7,
                        307.2
                    ],
                    [
                        1448.2,
                        307.2
                    ],
                    [
                        1448.2,
                        163.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73814,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "3de4b982b2b6174e1fbfcbd68cf2c8c0"
        },
        "text": "AlphaSeq: Sequence Discovery With Deep Reinforcement Learning",
        "type": "Title"
    },
    {
        "element_id": "48fe265c04247b0bd529c83029b351ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.3,
                        345.3
                    ],
                    [
                        144.3,
                        376.4
                    ],
                    [
                        1562.6,
                        376.4
                    ],
                    [
                        1562.6,
                        345.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76471,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "Yulin Shao , Student Member, IEEE, Soung Chang Liew , Fellow, IEEE, and Taotao Wang , Member, IEEE",
        "type": "NarrativeText"
    },
    {
        "element_id": "a83031172067e3ecf0b79a2f32a96d34",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.8,
                        499.0
                    ],
                    [
                        133.8,
                        1215.9
                    ],
                    [
                        836.2,
                        1215.9
                    ],
                    [
                        836.2,
                        499.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94733,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "Abstract\u2014 Sequences play an important role in many applica- tions and systems. Discovering sequences with desired properties has long been an interesting intellectual pursuit. This article puts forth a new paradigm, AlphaSeq, to discover desired sequences algorithmically using deep reinforcement learning (DRL) tech- niques. AlphaSeq treats the sequence discovery problem as an episodic symbol-\ufb01lling game, in which a player \ufb01lls symbols in the vacant positions of a sequence set sequentially during an episode of the game. Each episode ends with a completely \ufb01lled sequence set, upon which a reward is given based on the desirability of the sequence set. AlphaSeq models the game as a Markov decision process (MDP) and adapts the DRL framework of AlphaGo to solve the MDP. Sequences discovered improve progressively as AlphaSeq, starting as a novice, and learns to become an expert game player through many episodes of game playing. Compared with traditional sequence construction by mathematical tools, AlphaSeq is particularly suitable for problems with complex objectives intractable to mathematical analysis. We demonstrate the searching capabilities of AlphaSeq in two applications: 1) AlphaSeq successfully rediscovers a set of ideal complemen- tary codes that can zero-force all potential interferences in multi-carrier code-division multiple access (CDMA) systems and 2) AlphaSeq discovers new sequences that triple the signal-to- interference ratio\u2014benchmarked against the well-known Legen- dre sequence\u2014of a mismatched \ufb01lter (MMF) estimator in pulse compression radar systems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "07de64627dcd2f38ffc0db49bcb74497",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1234.0
                    ],
                    [
                        136.0,
                        1314.3
                    ],
                    [
                        836.9,
                        1314.3
                    ],
                    [
                        836.9,
                        1234.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93191,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "Index Terms\u2014 AlphaGo, deep reinforcement learning (DRL), Monte Carlo tree search (MCTS), multi-carrier code-division multiple access (MC-CDMA), pulse compression radar.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2175a9f7da7df93c46457cd350c0fa81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.6,
                        497.2
                    ],
                    [
                        866.6,
                        724.2
                    ],
                    [
                        1567.4,
                        724.2
                    ],
                    [
                        1567.4,
                        497.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95513,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "sequences are critical components in many informa- tion systems. For example, cellular code-division multi- ple access (CDMA) systems make use of spread spectrum sequences to distinguish signals from different users [3]; pulse compression radar systems make use of probe pulses modu- lated by phase-coded sequences [4] to enable high-resolution detection of objects at a large distance.",
        "type": "NarrativeText"
    },
    {
        "element_id": "50a0b3204e24904234a4d946d7fdaaf6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        729.8
                    ],
                    [
                        866.7,
                        989.8
                    ],
                    [
                        1566.8,
                        989.8
                    ],
                    [
                        1566.8,
                        729.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9543,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "Sequences in information systems are commonly designed by algebraists and information theorists using mathematical tools such as \ufb01nite \ufb01eld theory, algebraic number theory, and character theory. However, the design criterion for a good sequence may be complex and cannot be put into a clean mathematical expression for a solution by the available math- ematical tools. Faced with this problem, sequence designers may do the following two things.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9fab69f7895fecb66f6e610085e3a7ca",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.7,
                        994.9
                    ],
                    [
                        881.7,
                        1122.8
                    ],
                    [
                        1564.6,
                        1122.8
                    ],
                    [
                        1564.6,
                        994.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92449,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "1) Overlook the practical criterion and simplify the require- ments to make the problems analytically tractable. In so doing, a disconnect between reality and theory may be created.",
        "type": "ListItem"
    },
    {
        "element_id": "5da83d60289cd7ab5a3412b54fd9a08a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        1127.8
                    ],
                    [
                        885.0,
                        1355.2
                    ],
                    [
                        1567.9,
                        1355.2
                    ],
                    [
                        1567.9,
                        1127.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94393,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "1758d1a522ce5739b38535a09a14a5a7"
        },
        "text": "2) Introduce additional but arti\ufb01cial constraints absent in the original practical problem. In this case, the analytical solution is only valid for a subset of sequences of interest. For example, the protocol sequences in [5] are constructed by means of the Chinese remainder theorem (CRT); hence, the number of supported users is restricted to a prime number.",
        "type": "ListItem"
    },
    {
        "element_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        375.3,
                        1342.5
                    ],
                    [
                        375.3,
                        1370.2
                    ],
                    [
                        595.6,
                        1370.2
                    ],
                    [
                        595.6,
                        1342.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66106,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "3de4b982b2b6174e1fbfcbd68cf2c8c0"
        },
        "text": "I. INTRODUCTION",
        "type": "Title"
    },
    {
        "element_id": "c8af0ddfd7ab656d2356184b58ef2e98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1370.1
                    ],
                    [
                        136.0,
                        1503.5
                    ],
                    [
                        833.9,
                        1503.5
                    ],
                    [
                        833.9,
                        1370.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90072,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "A SEQUENCE is a list of elements arranged in a certain order. Prime numbers arranged in ascending order, for example, are a sequence [1]. The arrangements of nucleic acids in DNA polynucleotide chains are also sequences [2].",
        "type": "NarrativeText"
    },
    {
        "element_id": "7307d5cb8ff86f3a1094799bc9d62c86",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1509.2
                    ],
                    [
                        136.0,
                        1569.8
                    ],
                    [
                        833.5,
                        1569.8
                    ],
                    [
                        833.5,
                        1509.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92051,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Discovering sequences with desired properties is an intel- lectual pursuit with important applications [1]. In particular,",
        "type": "NarrativeText"
    },
    {
        "element_id": "83be15d4d00f49663da17bc40173f87b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1596.6
                    ],
                    [
                        136.0,
                        1768.7
                    ],
                    [
                        835.1,
                        1768.7
                    ],
                    [
                        835.1,
                        1596.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95076,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Manuscript received October 3, 2018; revised January 14, 2019, May 1, 2019, and August 7, 2019; accepted September 16, 2019. Date of publication October 21, 2019; date of current version September 1, 2020. This work was supported in part by the General Research Funds established under the University Grant Committee of the Hong Kong Special Administrative Region, China, under Project 14200417. (Corresponding author: Soung Chang Liew.)",
        "type": "NarrativeText"
    },
    {
        "element_id": "cd40c42cc096f5fafec875588c519b33",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1771.8
                    ],
                    [
                        136.0,
                        1844.0
                    ],
                    [
                        835.1,
                        1844.0
                    ],
                    [
                        835.1,
                        1771.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9291,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Y. Shao and S. C. Liew are with the Department of Information Engi- neering, The Chinese University of Hong Kong, Hong Kong (e-mail: sy016@ie.cuhk.edu.hk; soung@ie.cuhk.edu.hk).",
        "type": "NarrativeText"
    },
    {
        "element_id": "e9b49d0985df535710099e92debe967f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1846.4
                    ],
                    [
                        136.0,
                        1943.3
                    ],
                    [
                        833.3,
                        1943.3
                    ],
                    [
                        833.3,
                        1846.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93134,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "T. Wang was with the Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong. He is now with the College of Information Engineering, Shenzhen University, Shenzhen 518061, China (e-mail: ttwang@szu.edu.cn).",
        "type": "NarrativeText"
    },
    {
        "element_id": "6d28980cc6495b8226af3a3b0d6e898e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1946.2
                    ],
                    [
                        136.0,
                        1993.3
                    ],
                    [
                        832.2,
                        1993.3
                    ],
                    [
                        832.2,
                        1946.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9018,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Color versions of one or more of the \ufb01gures in this article are available online at http://ieeexplore.ieee.org.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7d9d917e66e59c4875694e29d0572de1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1360.8
                    ],
                    [
                        866.7,
                        1687.2
                    ],
                    [
                        1567.9,
                        1687.2
                    ],
                    [
                        1567.9,
                        1360.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95221,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Yet, a third approach is to \ufb01nd the desired sequences algorithmically. This approach rids us of the con\ufb01nes imposed by analytical mathematical tools. On the other hand, the issue becomes whether good sequences can be found within a reasonable time by algorithms. Certainly, to the extent that desired sequences can be found by a random search algorithm within a reasonable time, then the problem is solved. Most desired sequences, however, cannot be found so easily, and algorithms with complexity polynomial in the length of the sequences are not available.",
        "type": "NarrativeText"
    },
    {
        "element_id": "684f9d2da24dabaf810db177b89ab9ad",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1692.8
                    ],
                    [
                        866.7,
                        2022.1
                    ],
                    [
                        1568.4,
                        2022.1
                    ],
                    [
                        1568.4,
                        1692.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95008,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Reinforcement learning (RL) is an important branch of machine learning [6] known for its ability to derive solutions for Markov decision processes (MDPs) [7] through a learning process. A salient feature of RL is \u201clearning from interac- tions.\u201d Fig. 1 illustrates a framework of RL. In the framework, an agent interacts with an environment in a sequence of discrete-time steps. At time step t, the agent observes that the environment is in state st . Based on the observation of st , the agent then takes an action at , which results in the agent receiving a reward R(st +1) and the environment moving to",
        "type": "NarrativeText"
    },
    {
        "element_id": "81dcf0ba521dae469a69ffb1fec24ff8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        153.4,
                        1996.2
                    ],
                    [
                        153.4,
                        2018.3
                    ],
                    [
                        661.5,
                        2018.3
                    ],
                    [
                        661.5,
                        1996.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75034,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Digital Object Identi\ufb01er 10.1109/TNNLS.2019.2942951",
        "type": "NarrativeText"
    },
    {
        "element_id": "121b34b7e864c3cb5c56a936f830bb1c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        339.7,
                        2036.5
                    ],
                    [
                        339.7,
                        2083.6
                    ],
                    [
                        1355.0,
                        2083.6
                    ],
                    [
                        1355.0,
                        2036.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67067,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "2162-237X \u00a9 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.",
        "type": "NarrativeText"
    },
    {
        "element_id": "83b95e1159b9e1ec052053fca39abf7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.9
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1553.0,
                        2143.2
                    ],
                    [
                        1553.0,
                        2122.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83401,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1,
            "parent_id": "af7eaec2cd7414bd77b2a2d1f4ecfd60"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7d8a5f144a0622e09ef572951419283c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.0,
                        82.0
                    ],
                    [
                        1521.0,
                        105.9
                    ],
                    [
                        1567.5,
                        105.9
                    ],
                    [
                        1567.5,
                        82.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81084,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 1
        },
        "text": "3319",
        "type": "Header"
    },
    {
        "element_id": "b669ad6efbf69f25859595a732066b1e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.7,
                        82.2
                    ],
                    [
                        133.7,
                        105.9
                    ],
                    [
                        178.2,
                        105.9
                    ],
                    [
                        178.2,
                        82.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79485,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "3320",
        "type": "Header"
    },
    {
        "element_id": "99cdad785bd6e67005894ca0346167a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        574.0,
                        83.7
                    ],
                    [
                        574.0,
                        104.8
                    ],
                    [
                        1564.1,
                        104.8
                    ],
                    [
                        1564.1,
                        83.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82479,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "10b33536daed0d980bf506a1930a44c8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        223.4,
                        161.2
                    ],
                    [
                        223.4,
                        321.2
                    ],
                    [
                        723.1,
                        321.2
                    ],
                    [
                        723.1,
                        161.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85326,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-2-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "action a, \u00a2 & state 5, 1 Sesi ao - reward R(S;41) Agent Environment",
        "type": "Image"
    },
    {
        "element_id": "3cbc40021c1246d0df0d5051f2e80955",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        352.2
                    ],
                    [
                        136.0,
                        452.6
                    ],
                    [
                        833.2,
                        452.6
                    ],
                    [
                        833.2,
                        352.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9437,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "Fig. 1. Agent-environment interactions in RL. Given the observation that the environment is in state st , the agent follows its current policy and takes action at . The environment then moves to state st+1 and feedbacks a reward R(st+1) [6].",
        "type": "FigureCaption"
    },
    {
        "element_id": "af5062acd400d6be0dab2b09bf834c87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        524.2
                    ],
                    [
                        136.0,
                        1017.2
                    ],
                    [
                        836.6,
                        1017.2
                    ],
                    [
                        836.6,
                        524.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95246,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "state st +1. RL framework can also be episodic, in which the agent-environment interactions are broken into sessions called episodes. The environment will feedback a reward R(sT ) only at a terminal state, i.e., at the end of one episode. The mapping from st to at is referred to as a policy function. The aim of the policy is to maximize the expected reward received at the end of the episode. This policy function could be deterministic, in which case a speci\ufb01c action at is always taken upon a given state st . The policy could also be probabilistic, in which case the action taken upon a given state is described by a conditional probability P(at |st ). The objective of the agent is to learn an expected reward maximizing policy after going through multiple episodes.1 The agent may begin with bad policies early on, but as it gathers experiences from successive episodes, the policy gets better and better.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4bebeec6b10b72b3b15537af3f27952c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1022.8
                    ],
                    [
                        136.0,
                        1415.5
                    ],
                    [
                        837.9,
                        1415.5
                    ],
                    [
                        837.9,
                        1022.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95397,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "The latest trend in RL research is to integrate the recent advances of deep learning [8] into the RL framework [9]\u2013[11]. RL that makes use of deep neural networks (DNNs) to approx- imate the optimal policy function\u2014-directly or indirectly\u2014is referred to as deep RL (DRL). DRL allows RL algorithms to be applied when the number of possible state-action pairs is enormous and that traditional function approximators cannot approximate the policy function accurately. The recent success of DRL in game playing, natural language processing, and autonomous vehicle steering (see the excellent survey in [11]) has demonstrated its power in solving complex problems that thwart conventional approaches.",
        "type": "NarrativeText"
    },
    {
        "element_id": "63f034590a540c1eb0b1f982f2297c4d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1421.2
                    ],
                    [
                        136.0,
                        1548.5
                    ],
                    [
                        837.1,
                        1548.5
                    ],
                    [
                        837.1,
                        1421.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93526,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "This article puts forth a DRL-based paradigm, referred to as AlphaSeq, to discover a set of sequences with desired properties algorithmically. The essence of AlphaSeq is as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a668a70dde7a2a41755759fb569fbf0f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        1553.8
                    ],
                    [
                        154.7,
                        1814.7
                    ],
                    [
                        836.8,
                        1814.7
                    ],
                    [
                        836.8,
                        1553.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94017,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "1) AlphaSeq treats sequence-set discovery\u2014a sequence set consists of one or more sequences\u2014as an episodic symbol-\ufb01lling game. In each episode of the game, AlphaSeq \ufb01lls symbols into vacant sequence positions in a consecutive manner until the sequence set is completely \ufb01lled, whereupon a reward with a value between \u22121 and 1 is returned. The reward is a nonlinear function of a metric that quanti\ufb01es the desirability of the",
        "type": "ListItem"
    },
    {
        "element_id": "3181d9a56820d31e0070ad836c74cb16",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1854.4
                    ],
                    [
                        136.0,
                        2081.3
                    ],
                    [
                        833.2,
                        2081.3
                    ],
                    [
                        833.2,
                        1854.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94951,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "1RL shares the same mathematical principle as that of dynamic program- ming (DP). To learn the optimal policy, RL algorithms typically contain two interacting processes: policy evaluation and policy improvement. We refer the reader to the exposition in [6], in which Section IV.1 explains how policy evaluation predicts the state-value function for an arbitrary policy, and Section IV.2 explains how policy improvement improves the policy with respect to the current state-value function. Overall, these two processes interact with each other as a generalized policy iteration (Section IV.6), enabling the convergence to the optimal value function and an optimal policy.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bd726d5674b75dd81837c5891fe7f2b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        920.3,
                        161.8
                    ],
                    [
                        920.3,
                        255.8
                    ],
                    [
                        1571.8,
                        255.8
                    ],
                    [
                        1571.8,
                        161.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92405,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "sequence set. AlphaSeq aims to maximize the reward. It learns to do by playing many episodes of the game, improving itself along the way.",
        "type": "NarrativeText"
    },
    {
        "element_id": "81feeb04199f5bc1f1b8ab14c09678e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        261.5
                    ],
                    [
                        885.0,
                        687.8
                    ],
                    [
                        1572.2,
                        687.8
                    ],
                    [
                        1572.2,
                        261.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93417,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "2) AlphaSeq treats each intermediate state, with some sequence positions \ufb01lled and others vacant in the game, as an image. Each position is a pixel of the image. Given an input state (image), AlphaSeq makes use of a DNN to recognize it and approximate the optimal policy that maximizes the reward. The DRL framework in AlphaSeq is adapted from AlphaGo [12], in which DNN-guided Monte Carlo Tree Search MCTS) is used to select each move in the game. As in AlphaGo, there is an iterative self-learning process in AlphaSeq in that the experiences from the DNN-guided MCTS game playing are used to train the DNN; and the trained DNN, in turn, improves future game playing by the DNN-guided MCTS.",
        "type": "ListItem"
    },
    {
        "element_id": "bf144eaa600f46d606e9ef6380ee86a9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        690.4
                    ],
                    [
                        885.0,
                        1318.5
                    ],
                    [
                        1580.1,
                        1318.5
                    ],
                    [
                        1580.1,
                        690.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89513,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "3) We introduce two techniques in AlphaSeq that are absent in AlphaGo for our applications to search sequences. The \ufb01rst technique is to allow AlphaSeq to make moves at a time (i.e., \ufb01lling sequence positions at a time). Obviously, this technique is not applicable to the game of Go, hence AlphaGo. The choice of is a complexity tradeoff between the MCTS and the DNN. The second technique, dubbed \u201csegmented induction,\u201d is to change the reward function progressively to guide AlphaSeq toward good sequences in its learning process. In essence, we set a low target for AlphaSeq initially so that many sequence sets can have rewards close to 1, with few having rewards close to \u22121. As AlphaSeq plays more and more episodes of the game, we progressively raise the target so that fewer and fewer sequence sets have rewards close to 1, with more having rewards close to \u22121. In other words, the game becomes more and more demanding as AlphaSeq, starting as a novice, and learns to become an expert player.",
        "type": "ListItem"
    },
    {
        "element_id": "58847e2bab998ba0d8c962febf19de4d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1324.2
                    ],
                    [
                        866.7,
                        2008.9
                    ],
                    [
                        1568.1,
                        2008.9
                    ],
                    [
                        1568.1,
                        1324.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94338,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "We demonstrate the capability of AlphaSeq to discover two types of sequences. First, we use AlphaSeq to rediscover a set of complementary codes for multi-carrier (MC)-CDMA systems. In this application, AlphaSeq aims to discover a sequence set for which potential interferences in the MC-CDMA system can be canceled by simple signal process- ing. This particular problem already has analytical solutions. Our goal, here, is to test if AlphaSeq can rediscover these analytical solutions algorithmically rather than analytically. Second, we use AlphaSeq to discover new phase-coded sequences superior to the known sequences for pulse compres- sion radar systems. Speci\ufb01cally, our goal is to \ufb01nd phase-coded sequences commensurate with the mismatched \ufb01lter (MMF) estimator so that the estimator can yield an output with a high signal-to-interference ratio (SIR). The optimal sequences for MMF are not known and there is currently no known sequence that is provably optimal when the sequence is large. Bench- marked against the Legendre sequence [13], the sequence discovered by AlphaSeq triples the SIR, achieving 5.23-dB mean square error (MSE) gains for the estimation of radar",
        "type": "NarrativeText"
    },
    {
        "element_id": "074744395557fbefe2a6903514875a63",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1988.5
                    ],
                    [
                        866.7,
                        2016.2
                    ],
                    [
                        1442.1,
                        2016.2
                    ],
                    [
                        1442.1,
                        1988.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2
        },
        "text": "cross sections in pulse compression radar systems.",
        "type": "Title"
    },
    {
        "element_id": "b2130a35a48f0ea4976b467e27047339",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        2021.5
                    ],
                    [
                        866.7,
                        2082.5
                    ],
                    [
                        1565.1,
                        2082.5
                    ],
                    [
                        1565.1,
                        2021.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91896,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2,
            "parent_id": "074744395557fbefe2a6903514875a63"
        },
        "text": "The remainder of this article is organized as follows. Section II formulates the sequence discovery problem and",
        "type": "NarrativeText"
    },
    {
        "element_id": "641e8291ce5adec5ac64a0b61fabe3f1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.4
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76387,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 2,
            "parent_id": "074744395557fbefe2a6903514875a63"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "41a17449c5460925ed7c5f2827828e01",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        682.6,
                        104.7
                    ],
                    [
                        682.6,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80239,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "5eeb19f3fcf5fba3f9595440f2d48a1a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        161.8
                    ],
                    [
                        136.0,
                        355.5
                    ],
                    [
                        838.5,
                        355.5
                    ],
                    [
                        838.5,
                        161.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95205,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "41a17449c5460925ed7c5f2827828e01"
        },
        "text": "outlines the DRL framework of AlphaSeq. Sections III and IV present the applications of AlphaSeq in MC-CDMA systems and pulse compression radar systems, respectively. Section V concludes this article. Throughout this article, low- ercase bold letters denote vectors and uppercase bold letters denote matrices.",
        "type": "NarrativeText"
    },
    {
        "element_id": "237a156ef2b53ddfca9d55aa3d32cf32",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        367.7,
                        386.5
                    ],
                    [
                        367.7,
                        414.2
                    ],
                    [
                        602.0,
                        414.2
                    ],
                    [
                        602.0,
                        386.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "41a17449c5460925ed7c5f2827828e01"
        },
        "text": "II. METHODOLOGY",
        "type": "Title"
    },
    {
        "element_id": "db11c9fe2ff50fe4895cfbb71a24e38a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        135.9,
                        429.5
                    ],
                    [
                        135.9,
                        457.2
                    ],
                    [
                        411.1,
                        457.2
                    ],
                    [
                        411.1,
                        429.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.46842,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "A. Problem Formulation",
        "type": "NarrativeText"
    },
    {
        "element_id": "e8b97e956fb4621ec75c29187de52d6e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.5,
                        466.5
                    ],
                    [
                        133.5,
                        964.8
                    ],
                    [
                        836.8,
                        964.8
                    ],
                    [
                        836.8,
                        466.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.952,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "We consider the problem of discovering a sequence set C, the desirability of which is quanti\ufb01ed by a metric M(C). Set C consists of K different sequences of the same length N, i.e., {ck : k = 0, 1, . . . , K \u2212 1}, where the kth sequence is given by ck = (ck[0], ck[1], . . . , ck[N \u2212 1]). Each symbol of the sequences in C (i.e., ck[n]) is drawn from a discrete set A. Without loss of generality, this article focuses on binary sequences. That is, A is two-valued, and we can simply denote these two values by 1 and \u22121. The metric function M(C) varies with application scenarios. It is generally a function of all K sequences in C. The optimal metric value M\u2217 (i.e., the desired metric value) is achieved when C = C\u2217. Our objective is to \ufb01nd an optimal sequence set C\u2217 that yields M\u2217. For binary sequences, the complexity of the exhaustive search for C\u2217 is O(2N K ), which is prohibitive for large N and K .",
        "type": "NarrativeText"
    },
    {
        "element_id": "c7731e9cf7bfc9c6807640df4a92a526",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.5,
                        969.1
                    ],
                    [
                        134.5,
                        1906.3
                    ],
                    [
                        835.2,
                        1906.3
                    ],
                    [
                        835.2,
                        969.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9275,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "This sequence discovery problem can be transformed into an MDP. Specifically, we treat sequence-set discovery as a symbol-filling game. One play of the game is one episode, and each episode contains a series of time steps. In each episode, the player (agent) starts from an all-zero state (i.e., all the symbols in the set are 0) and takes one action per time step based on its current action policy. In each time step, \u20ac symbols in the sequence set are assigned with the value of | or \u20141, replacing the original 0 value. We emphasize that the player can only determine the values of the \u20ac symbols but not their positions. The / positions are predetermined: a simple tule is to place symbols sequence by sequence (specifically, we first place symbols in one sequence). When this sequence is completed-filled, we turn to fill the next sequence, and so on and so forth. This rule will be used throughout this article unless specified otherwise). An episode ends at a terminal state after [NK/\u20ac] time steps, whereupon a complete set C is obtained. In the terminal state, we measure the goodness of C by M(C) and return a reward R(C) for this episode to the player, where R(C) is, in general, a nonlinear function of M(C). For this MDP, episodic RL can be used to learn a policy that makes sequential decisions to maximize the reward R(C). In a general RL setup, the agent\u2019s objective at each state s; is to maximize the accumulated future reward >; yi Re\u015f, where y is a discount factor. The MDP above, by contrast, restricts the reward to only terminal state since the guality of the seguence set cannot be evaluated until all the elements are fixed. This is a delayed reward setup, the goal at",
        "type": "NarrativeText"
    },
    {
        "element_id": "352839e6a2236f9eb41064413245be48",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        161.8
                    ],
                    [
                        866.7,
                        521.5
                    ],
                    [
                        1569.5,
                        521.5
                    ],
                    [
                        1569.5,
                        161.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95359,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "state, and each vertex of the tree corresponds to a possible state, i.e., a partially \ufb01lled sequence-set pattern (completely \ufb01lled at a terminal state). The depth of the tree equals the number of time steps in an episode (i.e., N K /), and each vertex has exactly 2 branches. In each episode, the player will start from the root vertex and make sequential decisions along the tree based on its current policy until reaching a leaf vertex, whereupon a reward will be obtained. Given any vertex vi and an action, the next vertex vi+1 is conditionally independent of all previous vertices and actions, i.e., the transitions on the tree satisfy the Markov property.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7be6d02b8481bf1824d4004c844e8883",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        527.2
                    ],
                    [
                        866.7,
                        621.2
                    ],
                    [
                        1564.4,
                        621.2
                    ],
                    [
                        1564.4,
                        527.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93332,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "The objective of the player is then to reach a leaf vertex with the maximum reward. Toward this objective, the player performs the following.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5c8e337c540adf7200854b380c2ccc53",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        627.2
                    ],
                    [
                        885.0,
                        1108.3
                    ],
                    [
                        1572.5,
                        1108.3
                    ],
                    [
                        1572.5,
                        627.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92419,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "1) Distinguishing good states from bad states\u2014A reward is given to the player only upon its reaching a termi- nal stage. Although traversing the intermediate stage, the player must distinguish good intermediate states from bad intermediate states so that it can navigate toward a good terminal stage. In particular, the player must learn to approximate the expected end rewards of intermedi- ate states: this is, in fact, a process of value function approximation (in RL, the value of a state refers to the expected reward of being in that state, and a value function is a mapping from states to values. For terminal states, the value function is exactly the reward function). Moreover, we can imagine each state to be an image with each symbol being a pixel and make use of a DNN to",
        "type": "ListItem"
    },
    {
        "element_id": "2ce3b0d1f4b35490b5f645bb9181e896",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        922.0,
                        1092.2
                    ],
                    [
                        922.0,
                        1119.8
                    ],
                    [
                        1503.9,
                        1119.8
                    ],
                    [
                        1503.9,
                        1092.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "approximate the expected rewards of the \u201cimages.\u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "e13d5819bd2550c6d0062f9ddd35f469",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        1125.2
                    ],
                    [
                        885.0,
                        1418.5
                    ],
                    [
                        1565.0,
                        1418.5
                    ],
                    [
                        1565.0,
                        1125.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93063,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "2) Improving action policy based on cognition of subsequent states. Starting as a tabula rasa, the player\u2019s initial policy in earlier episodes is rather random. To gradually improve the action policy, the player can leverage the instrument of MCTS. MCTS is a simulated look-ahead tree search. At a vertex, MCTS can estimate the prospects of subsequent vertices by simulating multiple actions along the tree. The information collected during the simulations can then be used to decide the real action to be taken at this vertex.2",
        "type": "ListItem"
    },
    {
        "element_id": "140b7bd36cb7b3de6e79301a96b0f69f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1424.5
                    ],
                    [
                        866.7,
                        1684.5
                    ],
                    [
                        1567.3,
                        1684.5
                    ],
                    [
                        1567.3,
                        1424.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95364,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "A successful combination of DNN and MCTS has been demonstrated in AlphaGo [10], [12], [16], where the authors use DNN to assess the vertices during the MCTS simulation, as opposed to using random rollouts in standard MCTS [14]. In this article, we adapt the DRL framework in AlphaGo3 to solve the sequence set discovery problem associated with the underlying MDP. In deference to AlphaGo, we refer to this sequence discovering framework as \u201cAlphaSeq.\u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "607403447af12be1e5212049648dc571",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1690.2
                    ],
                    [
                        866.7,
                        1784.2
                    ],
                    [
                        1564.8,
                        1784.2
                    ],
                    [
                        1564.8,
                        1690.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93402,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "The overall algorithmic framework of AlphaGo/AlphaSeq can be outlined as an iterative \u201cgame-play with MCTS\u201d and \u201cDNN-update\u201d process, as shown in Fig. 2. On the one hand,",
        "type": "NarrativeText"
    },
    {
        "element_id": "5ee5098e5f5bb436c0e46588c43d430f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1900.2
                    ],
                    [
                        136.0,
                        1927.8
                    ],
                    [
                        793.8,
                        1927.8
                    ],
                    [
                        793.8,
                        1900.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "237a156ef2b53ddfca9d55aa3d32cf32"
        },
        "text": "each state is now to maximize the end-of-episode reward.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a3cc75a2c96db8cbc2ed307d62910975",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1978.8
                    ],
                    [
                        136.0,
                        2006.5
                    ],
                    [
                        318.0,
                        2006.5
                    ],
                    [
                        318.0,
                        1978.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70365,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "41a17449c5460925ed7c5f2827828e01"
        },
        "text": "B. Methodology",
        "type": "Title"
    },
    {
        "element_id": "3481ede1e47c3e705f2bba6db011182c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.5,
                        1826.4
                    ],
                    [
                        866.5,
                        2028.3
                    ],
                    [
                        1564.6,
                        2028.3
                    ],
                    [
                        1564.6,
                        1826.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95141,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "a3cc75a2c96db8cbc2ed307d62910975"
        },
        "text": "2The main concept in MCTS is tree policy. It determines how we sample the tree and select nodes. For a general overview on the core algorithms and variations, we refer the reader to the excellent survey [14] (in particular, the most popular algorithm in the MCTS family, the Upper Con\ufb01dence Bound for Trees (UCT), is introduced in Section III.3 of [14]). Reference [15] provides a more rigorous proof of the optimality of UCT. The authors showed that the probability that the UCT selects the optimal action converges to 1 at a polynomial rate.",
        "type": "NarrativeText"
    },
    {
        "element_id": "438841248d77625251d35f85f68bc0e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        2021.5
                    ],
                    [
                        136.0,
                        2082.5
                    ],
                    [
                        833.9,
                        2082.5
                    ],
                    [
                        833.9,
                        2021.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92702,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "a3cc75a2c96db8cbc2ed307d62910975"
        },
        "text": "Given the MDP, a tree can be constructed by all possible states in the game. In particular, the root vertex is the all-zero",
        "type": "NarrativeText"
    },
    {
        "element_id": "6aad2285a8ce0b6f2fcb7235991389bb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        2028.8
                    ],
                    [
                        866.7,
                        2081.3
                    ],
                    [
                        1567.7,
                        2081.3
                    ],
                    [
                        1567.7,
                        2028.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91533,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "a3cc75a2c96db8cbc2ed307d62910975"
        },
        "text": "3AlphaGo itself is evolving, the DRL framework in this article is based on AlphaGo Zero [12] and AlphaZero [16].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b7ed04d473c12606bbac598754bae0db",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.6
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74658,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3,
            "parent_id": "a3cc75a2c96db8cbc2ed307d62910975"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b6da196df2716a6b488d78e03837c947",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.7,
                        82.5
                    ],
                    [
                        1521.7,
                        106.2
                    ],
                    [
                        1565.8,
                        106.2
                    ],
                    [
                        1565.8,
                        82.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81377,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 3
        },
        "text": "3321",
        "type": "Header"
    },
    {
        "element_id": "1dbefad1f6560319ad9de56494094024",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.6,
                        81.8
                    ],
                    [
                        133.6,
                        105.9
                    ],
                    [
                        177.8,
                        105.9
                    ],
                    [
                        177.8,
                        81.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79506,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "3322",
        "type": "Header"
    },
    {
        "element_id": "219e071ee11bc39db340b07bc6753b7d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        574.0,
                        85.3
                    ],
                    [
                        574.0,
                        104.8
                    ],
                    [
                        1564.1,
                        104.8
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86732,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "5d0b37cb5be51c12749bc30f95ecaaee",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        205.3,
                        161.6
                    ],
                    [
                        205.3,
                        294.9
                    ],
                    [
                        753.5,
                        294.9
                    ],
                    [
                        753.5,
                        161.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79347,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Higher-quality Experiences Game-play with MCTS DNN Update > Improved DNN >",
        "type": "Image"
    },
    {
        "element_id": "b0e9017c18c1608dc677cd987370d4cc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        221.3,
                        161.1
                    ],
                    [
                        221.3,
                        295.9
                    ],
                    [
                        414.0,
                        295.9
                    ],
                    [
                        414.0,
                        161.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Game-play with MCTS >",
        "type": "Image"
    },
    {
        "element_id": "667b21f4f22c5f5eb1fc239f1755f513",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        552.6,
                        161.1
                    ],
                    [
                        552.6,
                        295.9
                    ],
                    [
                        746.7,
                        295.9
                    ],
                    [
                        746.7,
                        161.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "DNN Update >",
        "type": "Image"
    },
    {
        "element_id": "c6f81754935c9109ac259922319f5264",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        152.5
                    ],
                    [
                        866.7,
                        191.8
                    ],
                    [
                        1564.3,
                        191.8
                    ],
                    [
                        1564.3,
                        152.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "probabilistic policy TI(s;) (a distribution over all 2\u00b0 possible",
        "type": "Title"
    },
    {
        "element_id": "f463692d3c35cef00915d924f4aae8ed",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        164.0
                    ],
                    [
                        866.7,
                        291.7
                    ],
                    [
                        1566.8,
                        291.7
                    ],
                    [
                        1566.8,
                        164.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93863,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4,
            "parent_id": "c6f81754935c9109ac259922319f5264"
        },
        "text": "moves given by MCTS, not the raw policy estimation P of DNN) to choose symbols to \ufb01ll in the next positions in the sequence set. This action yields a new state si+1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "768dd441d2789874e90b4fd55cab7f6d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        327.2
                    ],
                    [
                        136.0,
                        399.3
                    ],
                    [
                        833.2,
                        399.3
                    ],
                    [
                        833.2,
                        327.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93414,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4,
            "parent_id": "c6f81754935c9109ac259922319f5264"
        },
        "text": "Fig. 2. Iterative algorithmic framework of AlphaGo/AlphaSeq. Improved DNN promotes the MCTS so that \u201cgame-play\u201d generates experiences with higher quality; higher quality experiences can further enhance the DNN.",
        "type": "FigureCaption"
    },
    {
        "element_id": "2fbfe7bd04269adff04650d477779b32",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        159.4,
                        451.4
                    ],
                    [
                        159.4,
                        814.5
                    ],
                    [
                        802.9,
                        814.5
                    ],
                    [
                        802.9,
                        451.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9367,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-5.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "0f9dab09beacd858996587da7675d430",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        842.0
                    ],
                    [
                        136.0,
                        943.6
                    ],
                    [
                        833.6,
                        943.6
                    ],
                    [
                        833.6,
                        842.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95065,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Fig. 3. Episode of game, where K = 2, N = 3, and / = 2. The NK positions are represented by colored squares: gray means that the positions are filled, while white means that the positions are vacant. At each time step, following the MCTS output TI, the player fills \u00a3 positions with value 1 or \u20141.",
        "type": "FigureCaption"
    },
    {
        "element_id": "c01e61954b866d99af4f39c754163513",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        294.8
                    ],
                    [
                        866.7,
                        1055.1
                    ],
                    [
                        1567.3,
                        1055.1
                    ],
                    [
                        1567.3,
                        294.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94603,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "The bottom half of Fig. 3 shows the MCTS process at each state s;, where each circle (vertex) represents a possible state in the look-ahead search. In the MCTS for state s;, we first set the root vertex vp to be s; and initialize a \u201cvisited tree\u201d (this visited tree is used to record all the vertices visited in the MCTS. It is initialized to have only one root vertex). Look-ahead simulations are then performed along the visited tree starting at the root vertex. Each simulation traces out a path of the visited tree and terminates when an unseen vertex vy is encountered. This unseen vertex will then be evaluated by DNN and added to the visited tree (i.e., a newly added vertex vz will be given the metric as wo(v_) = (PL, 1) to aid future simulations in evaluating which next move to select if the same vertex v; is visited again). As more and more simulations are performed, the tree grows in size. The metric used in selecting next move for the vertices will also change [i.e., (20) and (21) in Appendix A] as the vertices are visited more and more in successive simulations. In a nutshell, estimated good vertices are visited frequently, while estimated bad vertices are visited rarely. The resulting move-selection distribution at state sj, i.e., (si) = (70.71,....7x.)), is generated from the visiting counts of the root vertex\u2019s children in MCTS at states 5;.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b38c126faaa22620ad3364bb92dfa62d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1021.5
                    ],
                    [
                        136.0,
                        1281.8
                    ],
                    [
                        836.7,
                        1281.8
                    ],
                    [
                        836.7,
                        1021.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95728,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "\u201cgame-play with MCTS\u201d provides experiences to train the DNN so that the DNN can improve its assessments of the goodness of the states in the game. On the other hand, better evaluation on the states by the DNN allows the MCTS to make better decisions, which, in turn, provide higher quality experiences to train the DNN. Through an iterative process, the MCTS and the DNN mutually enhance each other in a progressive manner over an underlying RL process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bc6145c434ad07f1a32d9117e111b022",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        1052.5
                    ],
                    [
                        894.3,
                        1086.2
                    ],
                    [
                        1563.9,
                        1086.2
                    ],
                    [
                        1563.9,
                        1052.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Back to the upper part of Fig. 3, after N K / time steps,",
        "type": "NarrativeText"
    },
    {
        "element_id": "4c7b369ce214856e7bb17cc2089af43e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1060.9
                    ],
                    [
                        866.7,
                        1252.2
                    ],
                    [
                        1567.4,
                        1252.2
                    ],
                    [
                        1567.4,
                        1060.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95193,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Back to the upper part of Fig. 3, after [NK /\u20ac] time steps, the player obtains a complete sequence set C with metric value M(C) that gives a reward R(C). Then, we feed the R(C) to each state s; in this episode and store (s;, I1(s;), R) as an experience. One episode of game-play gives us [NK /\u20ac] experiences.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5c6643988314d0ceda127b3b2d86337b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1287.2
                    ],
                    [
                        136.0,
                        1447.8
                    ],
                    [
                        834.4,
                        1447.8
                    ],
                    [
                        834.4,
                        1287.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95125,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "In what follows, we dissect these two components and describe the relationship between them with more details. Differences between AlphaSeq and AlphaGo are presented at the end of this section. Further implementation details can be found in Appendix A.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b19294c8039bcc3769ac8bfc7f2fd6be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1454.8
                    ],
                    [
                        136.0,
                        1784.1
                    ],
                    [
                        835.9,
                        1784.1
                    ],
                    [
                        835.9,
                        1454.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95537,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "1) Input and Output of DNN: The DNN is designed to esti- mate the value function and policy function of an intermediate state. The value function is the estimated expected terminal reward given the intermediate state. Speci\ufb01cally, the output of DNN can be expressed as ( P, R) = \u03c8\u03b8 (si ): each time we feed an intermediate state si into the DNN \u03c8 with coef\ufb01cients it will output a reward estimation R (value function \u03b8 , estimation) and a probabilistic move-selection policy P (policy function estimation, policy P is a distribution over all possible next moves given the current state si ).",
        "type": "NarrativeText"
    },
    {
        "element_id": "93b135efa9355713ba9d78c1ec4e69e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1788.8
                    ],
                    [
                        136.0,
                        2085.1
                    ],
                    [
                        837.1,
                        2085.1
                    ],
                    [
                        837.1,
                        1788.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95217,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "2) Game-Play With MCTS: The \ufb01rst part of the algorithm iteration in Fig. 2 is game-play with MCTS. As illustrated in Fig. 3, we play the game under the guidance of MCTS. The upper half of Fig. 3 presents all the states in an episode, where squares represent the positions in the sequence set: gray squares mean that the position has already been \ufb01lled (with value 1 or \u22121); white squares mean that the position is still vacant (with value 0). The initial state of each episode is an all-zero state s0. In state si , the player will follow a",
        "type": "NarrativeText"
    },
    {
        "element_id": "90f0009d761ff514455a9ff5c298a778",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1257.5
                    ],
                    [
                        866.7,
                        1717.2
                    ],
                    [
                        1566.8,
                        1717.2
                    ],
                    [
                        1566.8,
                        1257.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94964,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "3) DNN Update: The second part of the algorithm iteration in Fig. 2 is the training of the DNN based on the accumulated experiences over successive episodes. First, from the descrip- tion above, we know that MCTS is guided by DNN. The capability of DNN determines the performance of MCTS since a better DNN yields more accurate evaluation of the vertices in MCTS. In the extreme, if the DNN perfectly knows which sequence-set patterns are good and which are bad, then the MCTS will always head toward an optimal direction, hence the chosen moves are also optimal. However, the fact is, DNN is randomly initialized, and its evaluation on vertices are quiet random and inaccurate initially. Thus, our goal is to improve this DNN using the experiences generated from game-play with MCTS.",
        "type": "NarrativeText"
    },
    {
        "element_id": "618f84c86744463f2b6481021c6cba3d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1722.8
                    ],
                    [
                        866.7,
                        2016.2
                    ],
                    [
                        1566.3,
                        2016.2
                    ],
                    [
                        1566.3,
                        1722.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95483,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "In the process of DNN update, the DNN is updated by learning the latest experiences accumulated in the game-play. Given experience (s;, T(s;), R) and wo(s;) = (P, R\u2019), 1) the real reward R can be used to improve the value-function approximation R\u2019 of DNN and 2) the policy TI(s;) given by MCTS at state s; can be used to improve the policy estimation P(si) of DNN (policy TI(s;) is generally more powerful than the raw output P(s;) of DNN). Thus, the training process is to make P and R\u2019 more closely match TI and R.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2e5c6c76ee242a3702a7269c80bea299",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        2021.2
                    ],
                    [
                        866.7,
                        2082.5
                    ],
                    [
                        1564.6,
                        2082.5
                    ],
                    [
                        1564.6,
                        2021.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90664,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Remark: When we play games with MCTS to gener- ate experiences, the Dirichlet noise is added to the prior",
        "type": "NarrativeText"
    },
    {
        "element_id": "166e632041608187a47ad3b139fa3df5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.7
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7108,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 4
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "480ed6b5afefc3464643caee8c6a4083",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        682.2,
                        104.7
                    ],
                    [
                        682.2,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85287,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "735c6459663c475f5c15fbca3d62c163",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        155.8
                    ],
                    [
                        136.0,
                        192.1
                    ],
                    [
                        833.6,
                        192.1
                    ],
                    [
                        833.6,
                        155.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "probability of root node v0 to induce exploration, as that",
        "type": "NarrativeText"
    },
    {
        "element_id": "7c1539fe17d2092b21eb260596efc104",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        165.2
                    ],
                    [
                        136.0,
                        422.2
                    ],
                    [
                        836.4,
                        422.2
                    ],
                    [
                        836.4,
                        165.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95778,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "in AlphaGo [12]. These games are also called noisy games. Instead of noisy games, we can also play noiseless games in which the Dirichlet noise is removed. Following the practice of AlphaGo, we play noisy games to generate the training experiences, but play noiseless games to evaluate the perfor- mance of AlphaSeq whose MCTS is guided by a particular trained DNN.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f36aa40aebb1df655fb519a4d9a0d9bd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        427.2
                    ],
                    [
                        136.0,
                        621.2
                    ],
                    [
                        835.4,
                        621.2
                    ],
                    [
                        835.4,
                        427.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95388,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "Overall, in one iteration, we: 1) play G episodes of noisy games with \u03c8\u03b8 -guided MCTS to generate experiences, where \u03c8\u03b8 is the current DNN; 2) use experiences gathered in the latest z \u00d7 G episodes of games to train for a new DNN \u03c8\u03b8 ; and 3) assess the new DNN \u03c8\u03b8 by running 50 noiseless games with \u03c8\u03b8 -guided MCTS.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bb05d550527f5c230c73c578eae175e7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        626.8
                    ],
                    [
                        136.0,
                        787.2
                    ],
                    [
                        839.4,
                        787.2
                    ],
                    [
                        839.4,
                        626.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94831,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "In the next iteration, we generate further experiences by playing G episodes of noisy games with \u03c8\u03b8 -guided MCTS. Then, these experiences are further used to train for yet another new DNN and so on and so forth. The pseudocode for AlphaSeq is given in Algorithm 1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "50ad8d98fce900aab41e91e0a0ae179d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        917.6,
                        161.5
                    ],
                    [
                        917.6,
                        289.2
                    ],
                    [
                        1567.1,
                        289.2
                    ],
                    [
                        1567.1,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93154,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "that is, the state at the beginning of the time step t has 2tl possible values. We found that imposing this restriction, while reducing complexity substantially, does not compromise the optimality of the sequence found.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2f9f06e9205486c4a0ecc2501d9da843",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        288.8
                    ],
                    [
                        885.0,
                        886.8
                    ],
                    [
                        1578.8,
                        886.8
                    ],
                    [
                        1578.8,
                        288.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87912,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "480ed6b5afefc3464643caee8c6a4083"
        },
        "text": "2) In AlphaSeq, the choice of is a complexity tradeoff between MCTS and DNN; in AlphaGo, is always 1. As mentioned above, the universe of all states in the game forms a tree. The depth of the tree is N K /, which is the number of steps in Fig. 3 from left to right. This is exactly the number of MCTS we need to run in an episode. Thus, the larger the , the fewer the MCTS we need to run. On the other hand, large yields more legal moves (i.e., 2) in each state, hence burdening the DNN with a larger action space. Overall, given N and K , for small , for example, = 1, the mission of DNN is light since it only needs to determine to place 1 or \u22121 in the next position. However, the number of MCTS we need to run in an episode is up to N K . In contrast, for large , for example, = K , the number of MCTS we need to run in an episode is reduced to N, but the DNN is burdened with a heavier task because it needs to evaluate 2K possible moves for each state.",
        "type": "ListItem"
    },
    {
        "element_id": "019002a89fe797e12c2113dce579ff19",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        830.5
                    ],
                    [
                        136.0,
                        832.9
                    ],
                    [
                        833.6,
                        832.9
                    ],
                    [
                        833.6,
                        830.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-6.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "cc2338b060decc6d707d3a0ec4a2c941",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        150.0,
                        835.2
                    ],
                    [
                        150.0,
                        863.2
                    ],
                    [
                        425.5,
                        863.2
                    ],
                    [
                        425.5,
                        835.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.4195,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "Algorithm 1: AlphaSeq",
        "type": "Title"
    },
    {
        "element_id": "010d60331cad1782d9dbe1ef742343a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        868.5
                    ],
                    [
                        136.0,
                        870.9
                    ],
                    [
                        833.6,
                        870.9
                    ],
                    [
                        833.6,
                        868.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-7.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "636dfd1a1fd9e1471a0b352570e1775e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        159.2,
                        873.2
                    ],
                    [
                        159.2,
                        900.8
                    ],
                    [
                        325.5,
                        900.8
                    ],
                    [
                        325.5,
                        873.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76278,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "Initialization:",
        "type": "Title"
    },
    {
        "element_id": "45b114b723b7dbdb2833c82842f55e81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        194.8,
                        906.5
                    ],
                    [
                        194.8,
                        1000.8
                    ],
                    [
                        759.6,
                        1000.8
                    ],
                    [
                        759.6,
                        906.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43072,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "636dfd1a1fd9e1471a0b352570e1775e"
        },
        "text": "Initialize parameters z and DNN update cycle G. Initialize a DNN \u03c8\u03b8 with parameter \u03b8 . Set episode g = 0.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5340af4a2e724eaffa7c729fbeac0c5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        161.4,
                        1006.2
                    ],
                    [
                        161.4,
                        1034.2
                    ],
                    [
                        290.0,
                        1034.2
                    ],
                    [
                        290.0,
                        1006.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.53483,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "while 1 do",
        "type": "Title"
    },
    {
        "element_id": "eb6b847b7b7b5976509af4c81d847d57",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        192.9,
                        1034.8
                    ],
                    [
                        192.9,
                        1062.5
                    ],
                    [
                        545.2,
                        1062.5
                    ],
                    [
                        545.2,
                        1034.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43273,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "Self-play to gain experience:",
        "type": "Title"
    },
    {
        "element_id": "ac461fdfb0c815d020743cc7a086d153",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        174.3,
                        1037.9
                    ],
                    [
                        174.3,
                        1521.5
                    ],
                    [
                        175.6,
                        1521.5
                    ],
                    [
                        175.6,
                        1037.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-8.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "d17fb48417e545ae1e2d5f0dfb9b1045",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        197.0,
                        1066.2
                    ],
                    [
                        197.0,
                        1067.5
                    ],
                    [
                        542.0,
                        1067.5
                    ],
                    [
                        542.0,
                        1066.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-9.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "fb9aca60ca0c630954938f6ef9234003",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        184.5,
                        1068.2
                    ],
                    [
                        184.5,
                        1261.8
                    ],
                    [
                        826.7,
                        1261.8
                    ],
                    [
                        826.7,
                        1068.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79131,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "Play one episode of game, output each immediate state si , the corresponding (si ) given by MCTS, and the discovered sequence set C when this episode ends. Compute metric M(C) and reward R(C). \u2200si , store (si , (si ), R(C)) as experience. g = g + 1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c548a6bad11a6235a7b0a00ebaf33362",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        194.9,
                        1267.2
                    ],
                    [
                        194.9,
                        1295.0
                    ],
                    [
                        358.3,
                        1295.0
                    ],
                    [
                        358.3,
                        1267.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.47207,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "DNN update:",
        "type": "Title"
    },
    {
        "element_id": "c7b1f2c00c294ef78ebe3edb2699b5d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        185.2,
                        1294.8
                    ],
                    [
                        185.2,
                        1551.0
                    ],
                    [
                        823.5,
                        1551.0
                    ],
                    [
                        823.5,
                        1294.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.38738,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "c548a6bad11a6235a7b0a00ebaf33362"
        },
        "text": "If mod(i, G) == 0 then Train DNN using the experiences accumulated in the latest z \u00d7 G episodes, get new parameters \u03b8 . Assess current AlphaSeq with new parameters \u03c8\u03b8 by playing 50 noiseless games. \u03c8\u03b8 = \u03c8\u03b8 . end",
        "type": "NarrativeText"
    },
    {
        "element_id": "8093972dc46bd2e04330fd3eb5a86dc0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        163.7,
                        1522.8
                    ],
                    [
                        163.7,
                        1550.5
                    ],
                    [
                        206.7,
                        1550.5
                    ],
                    [
                        206.7,
                        1522.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "end",
        "type": "Title"
    },
    {
        "element_id": "ed25d4821df67cb815b5377f0d78509e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1560.2
                    ],
                    [
                        136.0,
                        1562.5
                    ],
                    [
                        833.6,
                        1562.5
                    ],
                    [
                        833.6,
                        1560.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-10.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "f2f56098acc335c4562373daf85ffe81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1611.0
                    ],
                    [
                        136.0,
                        1672.5
                    ],
                    [
                        837.8,
                        1672.5
                    ],
                    [
                        837.8,
                        1611.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90827,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "In the following, we highlight some differences between AlphaSeq and AlphaGo.",
        "type": "NarrativeText"
    },
    {
        "element_id": "53865fcde5851f12d9b1cb041b3f0d4c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        1678.5
                    ],
                    [
                        154.7,
                        1977.8
                    ],
                    [
                        836.1,
                        1977.8
                    ],
                    [
                        836.1,
                        1678.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9372,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "1) In AlphaGo, the total number of legal states is O(3N K ) (in Go, N = K = 19; each position can be occupied by no stones, a white stone, or a black stone). If we allow AlphaSeq to \ufb01ll symbol positions in arbitrary order, then the complexity would be the same as AlphaGo in terms of the parameters N and K . However, for AlphaSeq, we impose the order in which symbol positions are \ufb01lled to reduce complexity. Now, the number of legal states reduces to",
        "type": "ListItem"
    },
    {
        "element_id": "bff8173d378396bf2ad10ce59d467efb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        362.7,
                        1989.8
                    ],
                    [
                        362.7,
                        2021.2
                    ],
                    [
                        436.7,
                        2021.2
                    ],
                    [
                        436.7,
                        1989.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "INK/\u20ac)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "61e0daadddd76c52bb5ca2b2b8885a23",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        381.0,
                        2021.0
                    ],
                    [
                        381.0,
                        2060.0
                    ],
                    [
                        418.0,
                        2060.0
                    ],
                    [
                        418.0,
                        2021.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": ">,",
        "type": "UncategorizedText"
    },
    {
        "element_id": "01c526532ab2dbfcb5b97ccc6376c14d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        382.3,
                        2060.5
                    ],
                    [
                        382.3,
                        2086.1
                    ],
                    [
                        417.2,
                        2086.1
                    ],
                    [
                        417.2,
                        2060.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "t =0",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ef88f202c2b0c4b0531d3367ac5859a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        441.3,
                        1996.8
                    ],
                    [
                        441.3,
                        2053.2
                    ],
                    [
                        639.8,
                        2053.2
                    ],
                    [
                        639.8,
                        1996.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "2t = 2N K + \u2212 1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f263166d2937e9b1df5ae68858717d69",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        540.7,
                        2037.8
                    ],
                    [
                        540.7,
                        2072.8
                    ],
                    [
                        613.8,
                        2072.8
                    ],
                    [
                        613.8,
                        2037.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "2 \u2212 1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8be334d07922acaadb928856e65c9f5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        514.6,
                        2039.9
                    ],
                    [
                        514.6,
                        2041.2
                    ],
                    [
                        640.3,
                        2041.2
                    ],
                    [
                        640.3,
                        2039.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-11.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "5c66a0a1e8ac358445f3e33fc57caff7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        801.0,
                        2025.5
                    ],
                    [
                        801.0,
                        2053.2
                    ],
                    [
                        833.5,
                        2053.2
                    ],
                    [
                        833.5,
                        2025.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "(1)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "1e9a326da90ede397d7f6103c4b74efd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        892.5
                    ],
                    [
                        885.0,
                        1219.2
                    ],
                    [
                        1568.7,
                        1219.2
                    ],
                    [
                        1568.7,
                        892.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93324,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "3) In the game of Go, the board is invariant to rotation and re\ufb02ection. Thus, we should augment the training data to let DNN learn these features. Speci\ufb01cally, in AlphaGo Zero, each experience (board state and move distribution) can be transformed by rotation and re\ufb02ection to obtain extra training data, and the state in an experience is randomly transformed before the experience is fed to the DNN [12]. On the other hand, in our game, no rotation or re\ufb02ection is required because all positions are prede- termined. Any rotated or re\ufb02ected state is an illegal state.",
        "type": "ListItem"
    },
    {
        "element_id": "00cdd5573e11cb35de725bca90820d30",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1221.8
                    ],
                    [
                        866.7,
                        1415.8
                    ],
                    [
                        1564.6,
                        1415.8
                    ],
                    [
                        1564.6,
                        1221.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93774,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "In the following sections, we demonstrate the searching capabilities of AlphaSeq in two applications: in Section III, we use AlphaSeq to rediscover an ideal complementary code set for MC-CDMA systems; and in Section IV, we use AlphaSeq to discover a new phase-coded sequence for pulse compression radar systems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7d660c5e7ba04ffbde7cd47e9914ca94",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        920.0,
                        1460.8
                    ],
                    [
                        920.0,
                        1488.5
                    ],
                    [
                        1510.2,
                        1488.5
                    ],
                    [
                        1510.2,
                        1460.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "III. REDISCOVER IDEAL COMPLEMENTARY CODE",
        "type": "Title"
    },
    {
        "element_id": "fb5d775d2e4b1d1d0af88b1dc7f0ab1b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        993.6,
                        1470.1
                    ],
                    [
                        993.6,
                        1521.8
                    ],
                    [
                        1440.3,
                        1521.8
                    ],
                    [
                        1440.3,
                        1470.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76271,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "FOR MULTI-CARRIER CDMA",
        "type": "Title"
    },
    {
        "element_id": "d42f459ef24d9da97f6c91ac9fadc36b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1537.5
                    ],
                    [
                        866.7,
                        1731.2
                    ],
                    [
                        1564.7,
                        1731.2
                    ],
                    [
                        1564.7,
                        1537.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94954,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "fb5d775d2e4b1d1d0af88b1dc7f0ab1b"
        },
        "text": "CDMA is a multiple-access technique that enables numerous users to communicate in the same frequency band simultaneously [3]. The fundamental principle of CDMA communications is to distinguish different users (or channels) by unique codes preassigned to them. Thus, CDMA code design lies at the heart of CDMA technology.",
        "type": "NarrativeText"
    },
    {
        "element_id": "54638952dc00eceef585a2b1166a7601",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        864.6,
                        1782.2
                    ],
                    [
                        864.6,
                        1809.8
                    ],
                    [
                        1280.7,
                        1809.8
                    ],
                    [
                        1280.7,
                        1782.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6757,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "fb5d775d2e4b1d1d0af88b1dc7f0ab1b"
        },
        "text": "A. Codes in Legacy CDMA Systems",
        "type": "NarrativeText"
    },
    {
        "element_id": "85930d452ed6ab0b79a1d5d15f76933d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        862.0,
                        1825.2
                    ],
                    [
                        862.0,
                        2085.2
                    ],
                    [
                        1567.4,
                        2085.2
                    ],
                    [
                        1567.4,
                        1825.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94998,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "fb5d775d2e4b1d1d0af88b1dc7f0ab1b"
        },
        "text": "Existing cellular CDMA systems work on a one-code- per-user basis [3], [17]. For example, the code set designed such that exactly one code is assigned to each user, e.g., the orthogonal variable spreading factor (OVSF) is code set used in W-CDMA downlink, the m-sequence set used in CDMA2000 uplink, and the Gold sequence set used in W-CDMA uplink [18]. However, legacy CDMA systems are self-jamming systems since the code sets being used",
        "type": "NarrativeText"
    },
    {
        "element_id": "8b4ba5f8d194be47bac86136996a0c27",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.4
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8224,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5,
            "parent_id": "fb5d775d2e4b1d1d0af88b1dc7f0ab1b"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1c39af3d31a662ab0915e055ba08cb14",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.7,
                        82.6
                    ],
                    [
                        1521.7,
                        106.0
                    ],
                    [
                        1566.1,
                        106.0
                    ],
                    [
                        1566.1,
                        82.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79393,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 5
        },
        "text": "3323",
        "type": "Header"
    },
    {
        "element_id": "120cbb52ba3d4843649a3c8388550038",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.1,
                        82.5
                    ],
                    [
                        134.1,
                        105.8
                    ],
                    [
                        177.6,
                        105.8
                    ],
                    [
                        177.6,
                        82.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79126,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "3324",
        "type": "Header"
    },
    {
        "element_id": "2ac9d7be66280bca408620dfe8020543",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        574.0,
                        85.3
                    ],
                    [
                        574.0,
                        104.7
                    ],
                    [
                        1564.1,
                        104.7
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80863,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "b62a15d1a572063daa30823bd54568d7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        169.6,
                        158.7
                    ],
                    [
                        169.6,
                        418.5
                    ],
                    [
                        780.9,
                        418.5
                    ],
                    [
                        780.9,
                        158.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75712,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-6-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543",
            "text_as_html": "<table><thead><tr><th></th><th rowspan=\"3\">@</th><th rowspan=\"3\">1X cq ina</th><th></th><th></th><th></th><th colspan=\"3\">he target signal bit</th></tr><tr><th></th><th></th><th>1x cq</th><th></th><th>-1X cq</th><th colspan=\"2\"></th></tr></thead><tbody><tr><td></td><td>{|</td><td>xe</td><td>|</td><td>Dee</td><td colspan=\"2\"></td></tr><tr><td>Potential</td><td>\u00a9</td><td></td><td colspan=\"2\"></td><td></td><td></td><td colspan=\"2\"></td></tr><tr><td colspan=\"9\">H</td></tr><tr><td>Interferences</td><td>\u00a9</td><td>-1X cp</td><td></td><td>1X eg</td><td>|</td><td>1xcg</td><td colspan=\"2\"></td></tr><tr><td></td><td>@</td><td></td><td colspan=\"3\">melee!</td><td>EEE</td><td colspan=\"2\"></td></tr></tbody></table>"
        },
        "text": "<a The target signal bit 1X eq Tx ca -1xe4 @ axe, 1 | 1xe, || \u2014ixe \u00a9 1xcg 11x \u00a2, 11x cq Potential - - Interferences \u00a9 \u2014ixeg |! 1xce |} ixep @ =1x cy} 1x cp! 1x cp",
        "type": "Table"
    },
    {
        "element_id": "a046f06722c1537bb62cbc686600b04c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        449.5
                    ],
                    [
                        136.0,
                        599.7
                    ],
                    [
                        833.5,
                        599.7
                    ],
                    [
                        833.5,
                        449.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93424,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "Fig. 4. Interferences caused by user asynchronies (misalignments of bit boundaries), multi-paths, and random signs of consecutive bits, in CDMA uplink. To decode user A\u2019s data, the receiver correlates the received sig- nal with code cA. Interferences are induced by (a) cyclic autocorrelation of cA, (b) \ufb02ipped autocorrelation of cA, (c) cyclic cross correlation between cA and cB, and (d) \ufb02ipped cross correlation between cA and cB.",
        "type": "FigureCaption"
    },
    {
        "element_id": "28acdd6227552564283f35af705164e5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        680.8
                    ],
                    [
                        136.0,
                        808.2
                    ],
                    [
                        833.9,
                        808.2
                    ],
                    [
                        833.9,
                        680.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93836,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "cannot guarantee user orthogonality under practical con- straints and considerations, such as user asynchronies, mul- tipath effects, and random signs of consecutive bits4 of user data streams [19].",
        "type": "NarrativeText"
    },
    {
        "element_id": "dde9159279e9a4a49a5276afb9eb5eeb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        813.8
                    ],
                    [
                        136.0,
                        1439.2
                    ],
                    [
                        834.6,
                        1439.2
                    ],
                    [
                        834.6,
                        813.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95059,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "In CDMA uplink, each user spreads its signal bits by modulating the assigned code, and the signals from multiple users overlap at the receiver. To decode a user A\u2019s signal bit, as shown in Fig. 4, the receiver cross correlates the received signal with the locally generated code of user A. However, due to user asynchronies, multi-paths, and random signs in consec- utive bits, the correlation results can suffer from interferences introduced by multiple paths of user A\u2019s signal or signal from another user B. The potential interferences can be computed by the correlations between the signal bit and two overlapping interfering bits: when the signs of the two interfering bits are the same, the interferences are cyclic correlation functions [i.e., Fig. 4(a) and (c)]; when the signs of the two interfering bits are different, the interferences are \ufb02ipped correlation functions [i.e., Fig. 4(b) and (d)]. On the other hand, CDMA downlink is a synchronous CDMA system and there are no asynchronies among signals of different users. However, multi-path and random signs in consecutive bits can still cause interferences through the above correlations among codes.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1956228e87dd19874fd5e85fa2fc02e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1444.5
                    ],
                    [
                        136.0,
                        1870.8
                    ],
                    [
                        835.1,
                        1870.8
                    ],
                    [
                        835.1,
                        1444.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95589,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "Mathematically, it has been proven that the ideal one-code- per-user code set that simultaneously zero forces the above correlation functions does not exist [20]. Code sets used in legacy CDMA systems tradeoff among these correlation functions. For example, the m-sequence set has nearly ideal cyclic autocorrelation property (to be exact, the autocorrelation function of the m-sequence is \u22121 for any nonzero shift, hence is \u201cnearly\u201d optimal), while its cyclic cross correlation func- tion (CCF) and \ufb02ipped correlation function are unbounded. The Gold sequence set and the Kasami sequence set (candidate in W-CDMA) have better cyclic cross correlation properties and acceptable cyclic autocorrelation properties, but their \ufb02ipped correlations are unbounded [18].",
        "type": "NarrativeText"
    },
    {
        "element_id": "286d4f2f64abfc7455043ed8d4008be5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        161.5
                    ],
                    [
                        866.7,
                        189.2
                    ],
                    [
                        1525.9,
                        189.2
                    ],
                    [
                        1525.9,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81204,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "B. Multi-Carrier CDMA and Ideal Complementary Codes",
        "type": "NarrativeText"
    },
    {
        "element_id": "1724ba26d91b825b7b14b6b4e36afcea",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        204.8
                    ],
                    [
                        866.7,
                        365.5
                    ],
                    [
                        1565.6,
                        365.5
                    ],
                    [
                        1565.6,
                        204.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94647,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "The limitations of legacy CDMA systems motivate researchers to develop MC-CDMA (MC-CDMA) systems where complementary codes can be used to simultane- ously null all correlation functions among codes that may cause interferences [19].",
        "type": "NarrativeText"
    },
    {
        "element_id": "c48509b2f1239801ca8b1f79c674bc30",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        370.8
                    ],
                    [
                        866.7,
                        863.5
                    ],
                    [
                        1566.6,
                        863.5
                    ],
                    [
                        1566.6,
                        370.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9504,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "The basic idea of complementary codes is to assign a \ufb02ock of M element codes to each user, as opposed to just one code in legacy CDMA systems. In MC-CDMA uplink [17], the signal bits of a user are spread by each of its M element codes and sent over M different subcarriers. When passing through the channel, the M subcarriers can be viewed as M separate virtual channels that have the same delay. The receiver \ufb01rst despreads the received signal in each individual subcarrier (i.e., correlate the received signal in each subcarrier with the corresponding element code) and sums up the despreading outcomes of all M subcarriers. In other words, the operations in each individual channel are the same as legacy CDMA systems: the new step is the summing of the outputs of the M virtual channels, which cancels out the interferences induced by individual correlations in the underlying subcarriers.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0522357e4127edff55418effaf3c083c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        869.2
                    ],
                    [
                        866.7,
                        1096.2
                    ],
                    [
                        1565.2,
                        1096.2
                    ],
                    [
                        1565.2,
                        869.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94735,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "To be speci\ufb01c, let us consider an MC-CDMA system with J users, where a \ufb02ock of M element codes of length N is assigned to each user. An ideal complementary code set C = {cm [n] : j = 0, 1, . . . , J \u2212 1; m = 0, 1, . . . , M \u2212 1; j n = 0, 1, . . . , N \u2212 1} that can enable interference-free MC-CDMA systems is a code set that meets the following criteria simultaneously.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8f3c4ea5e92e89bba4ffb3f5e1e34134",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.8,
                        1107.2
                    ],
                    [
                        882.8,
                        1234.8
                    ],
                    [
                        1564.3,
                        1234.8
                    ],
                    [
                        1564.3,
                        1107.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93079,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6,
            "parent_id": "2ac9d7be66280bca408620dfe8020543"
        },
        "text": "1) Ideal Cyclic Autocorrelation Function (CAF): For the M element codes assigned to a user j , i.e., {cm : m = 0, j 1, . . . , M \u2212 1}, the sum of the CAF of each code is zero for any nonzero shift",
        "type": "ListItem"
    },
    {
        "element_id": "6f5e4b5dda2dac3a2f97fe53dfe69474",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1000.7,
                        1251.8
                    ],
                    [
                        1000.7,
                        1346.4
                    ],
                    [
                        1564.2,
                        1346.4
                    ],
                    [
                        1564.2,
                        1251.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83079,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "M-1N-1 CAF;[v] = > \u015eiilerin +0] =0 (2) m=0 n=0",
        "type": "Formula"
    },
    {
        "element_id": "f7be3dbfa505943e5d571a321054c09d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        920.6,
                        1356.8
                    ],
                    [
                        920.6,
                        1456.8
                    ],
                    [
                        1564.6,
                        1456.8
                    ],
                    [
                        1564.6,
                        1356.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92411,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "where delay (chip-level) v = 1, 2, .., N \u2212 1. Hereinafter, the index additions in the square brackets refer modulo-N additions. to",
        "type": "NarrativeText"
    },
    {
        "element_id": "7d9892abb300e326ead19502a116a7e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        1461.8
                    ],
                    [
                        885.0,
                        1556.5
                    ],
                    [
                        1567.4,
                        1556.5
                    ],
                    [
                        1567.4,
                        1461.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92381,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "2) Ideal CCF: For two \ufb02ocks of codes assigned to users j1 and j2, i.e., {cm , cm : m = 0, 1, . . . , M\u22121}, the sum of j1 j2 their CCFs is always zero irrespective of the relative shift",
        "type": "ListItem"
    },
    {
        "element_id": "7a687bda030dfd3a2d0dba46fdac5859",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        980.9,
                        1573.1
                    ],
                    [
                        980.9,
                        1667.7
                    ],
                    [
                        1564.2,
                        1667.7
                    ],
                    [
                        1564.2,
                        1573.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81553,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "M-1N-1 CCF j,, j.[v] = > \u015eeiinlekln + v0 (3) m=0 n=0",
        "type": "Formula"
    },
    {
        "element_id": "1e1a25a2d2756cf21e04599131390518",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        922.0,
                        1678.2
                    ],
                    [
                        922.0,
                        1714.4
                    ],
                    [
                        1455.6,
                        1714.4
                    ],
                    [
                        1455.6,
                        1678.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "where delay v = 0, 1, 2, .., N \u2212 1 and j1 = j2.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8d97ab3aaa9ffbee100b70e74fe191a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        884.2,
                        1717.2
                    ],
                    [
                        884.2,
                        1877.8
                    ],
                    [
                        1567.1,
                        1877.8
                    ],
                    [
                        1567.1,
                        1717.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91694,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "3) Ideal Flipped Correlation Function (Fcf): For two \ufb02ocks of codes assigned to users j1 and j2, i.e., {cm , cm j1 j2 m = 0, 1, . . . , M \u22121}, the sum of their \ufb02ipped correlation : functions is always zero for any nonzero shift (\ufb02ipped correlation is only de\ufb01ned for nonzero delay)",
        "type": "ListItem"
    },
    {
        "element_id": "96324cdd0454267f5a3ae51dec0b8af3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        130.2,
                        1931.8
                    ],
                    [
                        130.2,
                        2084.0
                    ],
                    [
                        834.3,
                        2084.0
                    ],
                    [
                        834.3,
                        1931.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9318,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "4In CDMA, \u201cbit\u201d refers to the baseband modulated information symbols (only BPSK/QPSK modulated symbols are considered in this article, in gen- eral, it can be shown that the codes discussed in this section are applicable for higher order modulations), while \u201cchip\u201d refers to the entries in the spread spectrum code. Thus, with respect to the nomenclature in Section II, \u201cchips\u201d in CDMA corresponds to \u201csymbol\u201d of a code sequence in Section II.",
        "type": "NarrativeText"
    },
    {
        "element_id": "fc313f6e2ce435cbc0cac48e367e4424",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        929.3,
                        1894.8
                    ],
                    [
                        929.3,
                        2087.1
                    ],
                    [
                        1552.6,
                        2087.1
                    ],
                    [
                        1552.6,
                        1894.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8831,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "M-1[{N-v-1 FCF }j,, lo) = > > ci Inc} in +o] mol n=0 N-1 -> cimen 4vlf =0 (4) n=N-",
        "type": "Formula"
    },
    {
        "element_id": "8c62ff3a5cf3b75dd1c03353a1e04cd5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.2
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81288,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1095af17aec697a51ffd72d5cf8a24a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1531.7,
                        2026.8
                    ],
                    [
                        1531.7,
                        2054.5
                    ],
                    [
                        1564.2,
                        2054.5
                    ],
                    [
                        1564.2,
                        2026.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 6
        },
        "text": "(4)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "9bf861a48320bdce1a2b1a265255886c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        682.8,
                        104.7
                    ],
                    [
                        682.8,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81564,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "10bfd333abf6a7f3f6a0615557be11fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        191.3,
                        155.8
                    ],
                    [
                        191.3,
                        255.8
                    ],
                    [
                        834.2,
                        255.8
                    ],
                    [
                        834.2,
                        155.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90954,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "where delay v = 1, 2, .., N \u2212 1; j1 and j2 can be the same (\ufb02ipped autocorrelation function) or different (\ufb02ipped cross correlation function).",
        "type": "NarrativeText"
    },
    {
        "element_id": "f2d4a156c5c966e7efce51d9feeef325",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        258.2
                    ],
                    [
                        136.0,
                        651.8
                    ],
                    [
                        836.5,
                        651.8
                    ],
                    [
                        836.5,
                        258.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95595,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "Some known mathematical constructions of ideal comple- mentary codes are available in [17]. In this section, we make use of AlphaSeq to rediscover a set of ideal complementary codes. Our aim is to investigate and evaluate the searching capability of AlphaSeq, i.e., whether it can rediscover an ideal complementary code set and how it goes about doing so. Furthermore, we would like to investigate the impact of the hyperparameters used in the search algorithm on the overall performance of AlphaSeq, so as to obtain useful insights for discovering other unknown sequences (e.g., in Section IV, we will make use of AlphaSeq to discover phase-coded sequences for pulse compression radar systems)",
        "type": "NarrativeText"
    },
    {
        "element_id": "dad158d007982e91d28c3c9600f01cc6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        155.8
                    ],
                    [
                        866.7,
                        457.5
                    ],
                    [
                        1565.7,
                        457.5
                    ],
                    [
                        1565.7,
                        155.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95357,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "[see Appendix B for the derivation of maxC M(C)] and initialize the DNN to \u03c8\u03b80 (i.e., the parameters in the DNN are randomly set to \u03b80) to play 50 noiseless games. Then, Mu is set as the mean metric of the 50 sequences found by these 50 noiseless games, i.e., Mu = E[M]. After this, Mu will not be changed anymore in future games. We specify that the initial games do not \ufb01nd good sequences, but, nevertheless, the 50 sequences yield an E[M] much lower than maxC M(C). Using E[M] as Mu increases the slope of the \ufb01rst line in (6).",
        "type": "NarrativeText"
    },
    {
        "element_id": "075c467043544902801bddc7ac44c278",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        460.8
                    ],
                    [
                        866.7,
                        588.2
                    ],
                    [
                        1564.5,
                        588.2
                    ],
                    [
                        1564.5,
                        460.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93881,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "Based on the metric function and reward function de\ufb01ned above, we implemented AlphaSeq and trained DNN to redis- cover an ideal complementary code for MC-CDMA. A known ideal complementary code [17] is chosen as benchmark.",
        "type": "NarrativeText"
    },
    {
        "element_id": "76b0eed3de847e24f3c29526eb893383",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        588.5
                    ],
                    [
                        894.3,
                        622.2
                    ],
                    [
                        1564.2,
                        622.2
                    ],
                    [
                        1564.2,
                        588.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "3) Benchmark: When J = 2, M = 2, and N = 8,",
        "type": "ListItem"
    },
    {
        "element_id": "883f2ed34c8b7a057c0c1452652998fc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        597.7
                    ],
                    [
                        866.7,
                        688.5
                    ],
                    [
                        1568.2,
                        688.5
                    ],
                    [
                        1568.2,
                        597.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93548,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "the ideal complementary code set exists. The mathematical constructions in [17] gives us",
        "type": "NarrativeText"
    },
    {
        "element_id": "fb4814950a7a77b2f4ed5d988b2d4028",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.1,
                        696.8
                    ],
                    [
                        134.1,
                        724.5
                    ],
                    [
                        460.9,
                        724.5
                    ],
                    [
                        460.9,
                        696.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.55724,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "9bf861a48320bdce1a2b1a265255886c"
        },
        "text": "C. AlphaSeq for MC-CDMA",
        "type": "Title"
    },
    {
        "element_id": "36f006c8274a22d9d4a9cc91f46f36c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        738.8
                    ],
                    [
                        136.0,
                        932.8
                    ],
                    [
                        836.5,
                        932.8
                    ],
                    [
                        836.5,
                        738.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95439,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "fb4814950a7a77b2f4ed5d988b2d4028"
        },
        "text": "In this section, we use AlphaSeq to rediscover an ideal complementary code set for MC-CDMA systems. As stated above, the ideal complementary code set is the code set that ful\ufb01lls the three criteria in (2)\u2013(4). In this context, given a sequence set C, we de\ufb01ne the following metric function to measure how good set C is for MC-CDMA systems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "39abb3423b32bc831365d8c8ad95ee60",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        983.7,
                        691.2
                    ],
                    [
                        983.7,
                        718.8
                    ],
                    [
                        1005.8,
                        718.8
                    ],
                    [
                        1005.8,
                        691.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "fb4814950a7a77b2f4ed5d988b2d4028"
        },
        "text": "\u239b",
        "type": "UncategorizedText"
    },
    {
        "element_id": "c5bc8013d6353226baf033022338367f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1453.7,
                        691.2
                    ],
                    [
                        1453.7,
                        718.8
                    ],
                    [
                        1475.8,
                        718.8
                    ],
                    [
                        1475.8,
                        691.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7,
            "parent_id": "fb4814950a7a77b2f4ed5d988b2d4028"
        },
        "text": "\u239e",
        "type": "UncategorizedText"
    },
    {
        "element_id": "1a2c80f568b95495e2b0dbab0f3cd787",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        875.9,
                        709.8
                    ],
                    [
                        875.9,
                        822.1
                    ],
                    [
                        1564.2,
                        822.1
                    ],
                    [
                        1564.2,
                        709.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76685,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "[x +1 41 -1 41 41-1) 41] AH 1 41 41 41-1 -1 =I] Cbeneh \u2014 |r ar gn gt ag a a \u015fi ij ) +1 -1 41 41 -1 41 41\u00b0 41]",
        "type": "Formula"
    },
    {
        "element_id": "4cc081bc3ca472da270493d9548d23b3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.5,
                        837.5
                    ],
                    [
                        866.5,
                        940.4
                    ],
                    [
                        1570.3,
                        940.4
                    ],
                    [
                        1570.3,
                        837.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93264,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "As can be seen, there are J = 2 \ufb02ocks of codes in Cbench, each \ufb02ock contains M = 2 codes, and the length of each code is N = 8. It can be veri\ufb01ed that M(Cbench) = 0.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8df496b44bf51d0a231cec289e6db006",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        932.2
                    ],
                    [
                        136.0,
                        1098.5
                    ],
                    [
                        833.7,
                        1098.5
                    ],
                    [
                        833.7,
                        932.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94514,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "1) Metric Function: For a sequence set C = {cm j j = 0, 1, . . . , J \u22121; m = 0, 1, . . . , M\u22121; n = 0, 1, . . . , N \u22121} [n] : consisting of M J sequences of the same length N, the metric function M(C) in the following re\ufb02ects how good C is for MC-CDMA systems:",
        "type": "NarrativeText"
    },
    {
        "element_id": "960343ca88c132afce3254e8886fe58a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        139.0,
                        1112.5
                    ],
                    [
                        139.0,
                        1311.4
                    ],
                    [
                        833.5,
                        1311.4
                    ],
                    [
                        833.5,
                        1112.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89183,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "J-1N-1 J-1 M(C) = >) > ICAFjLoll + >, j=0 v= 1=0 > -1 N- N-1 SX Slcck pple J-1 =jit1 v=0 1 \u2014 J-1 >> A= ja=j, v=1 \u201c |FCF;,,p[ol]. | (5)",
        "type": "Formula"
    },
    {
        "element_id": "817d50b44cdead5da81f3188dd0188ea",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        943.2
                    ],
                    [
                        866.7,
                        1336.2
                    ],
                    [
                        1565.6,
                        1336.2
                    ],
                    [
                        1565.6,
                        943.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95336,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "To rediscover the code set, there are 32 symbols to be \ufb01lled in the game, and the number of all possible sequence-set patterns is 232 \u2248 O(109). Discovering the global optimum out of O(109) possible patterns is, in fact, not a dif\ufb01cult problem based on brute-force exhaustive search (even though it takes several days on our computer). The results of exhaustive search indicate that Cbench in (7) is not the only optimal pattern when J = 2, M = 2, and N = 8. There are in total 384 optimal patterns (that achieves M\u2217 = 0) that can be divided into 12 nonisomorphic types (i.e., each pattern has 31 other isomorphic patterns, see our technical report [21] for the speci\ufb01c shapes of isomorphic patterns).",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f2cc0e9dde4e9c10598ddc91ab0a184",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1318.1
                    ],
                    [
                        136.0,
                        1355.5
                    ],
                    [
                        833.6,
                        1355.5
                    ],
                    [
                        833.6,
                        1318.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "Note that our desired metric value M\u2217 = inf M(C) = 0. For",
        "type": "NarrativeText"
    },
    {
        "element_id": "e4d838f9aa7aa7f9f7da796d32365d30",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1329.3
                    ],
                    [
                        136.0,
                        1421.8
                    ],
                    [
                        833.7,
                        1421.8
                    ],
                    [
                        833.7,
                        1329.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93466,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "AlphaSeq, the objective is then to discover the sequence set that minimizes this metric function.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8e6d407f8e2dc0ab8144fa1b938aea57",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1427.2
                    ],
                    [
                        136.0,
                        1787.2
                    ],
                    [
                        834.7,
                        1787.2
                    ],
                    [
                        834.7,
                        1427.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95434,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "As an essential part of the training paradigm in AlphaSeq, a reward function is needed to map a found sequence set C to a reward R(C). In general, we could design this reward function to be a linear (or nonlinear) mapping from the value range of the metric function to the interval [\u22121, 1]. This is, in fact, a normalization process to \ufb01t general objectives to the architecture of AlphaSeq (speci\ufb01cally, normalizing the rewards of different problems allows these problems to share the same underlying hyperparameters in DNN and MCTS of the AlphaSeq architecture). To rediscover the ideal complementary code, we de\ufb01ne the reward function as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a2839c40986d675d78779ac59e0045f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        163.7,
                        1786.5
                    ],
                    [
                        163.7,
                        1820.2
                    ],
                    [
                        833.6,
                        1820.2
                    ],
                    [
                        833.6,
                        1786.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "2) Reward Function: For any sequence set C with metric",
        "type": "ListItem"
    },
    {
        "element_id": "18fe5f2bf8f9145d04995c110eabb632",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1795.9
                    ],
                    [
                        136.0,
                        1853.2
                    ],
                    [
                        835.0,
                        1853.2
                    ],
                    [
                        835.0,
                        1795.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92888,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "M(C), the reward R(C) for MC-CDMA systems is de\ufb01ned as",
        "type": "NarrativeText"
    },
    {
        "element_id": "26ab2c528a9e482afaa36597dd7e5824",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        311.3,
                        1844.5
                    ],
                    [
                        311.3,
                        1872.2
                    ],
                    [
                        333.2,
                        1872.2
                    ],
                    [
                        333.2,
                        1844.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "\u23a7",
        "type": "UncategorizedText"
    },
    {
        "element_id": "37f02773a660b538b96ec9b3f78448e9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        214.0,
                        1862.2
                    ],
                    [
                        214.0,
                        1972.5
                    ],
                    [
                        809.9,
                        1972.5
                    ],
                    [
                        809.9,
                        1862.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8588,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "1 \u2212 2M(C) \u23a8 , If 0 \u2264 M(C) \u2264 Mu R(C) = Mu \u23a9 \u22121, If M(C) > Mu",
        "type": "Formula"
    },
    {
        "element_id": "0e1ffc6f57def72ebb6f562c1a0718af",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        801.0,
                        1907.5
                    ],
                    [
                        801.0,
                        1935.2
                    ],
                    [
                        833.6,
                        1935.2
                    ],
                    [
                        833.6,
                        1907.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "(6)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "6f57ecbbf698a59a62c6d7790ae7945c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1342.2
                    ],
                    [
                        866.7,
                        1469.8
                    ],
                    [
                        1569.5,
                        1469.8
                    ],
                    [
                        1569.5,
                        1342.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93881,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "4) Implementation: We implemented and ran AlphaSeq on a computer with a single CPU (Intel Core i7-6700) and a single GPU (NVIDIA GeForce GTX 1080 Ti).5 The parameter settings are listed in Table I.",
        "type": "NarrativeText"
    },
    {
        "element_id": "27c0fdc0620e333e2a1d56c91d22b213",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        1469.5
                    ],
                    [
                        894.3,
                        1503.2
                    ],
                    [
                        1564.3,
                        1503.2
                    ],
                    [
                        1564.3,
                        1469.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "For the symbol \ufb01lling game, we set K = M J = 4, N = 8,",
        "type": "NarrativeText"
    },
    {
        "element_id": "9609e3b2bdfa1bcc01430e2291893f7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1478.5
                    ],
                    [
                        866.7,
                        1702.2
                    ],
                    [
                        1566.6,
                        1702.2
                    ],
                    [
                        1566.6,
                        1478.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95427,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "and = 4. In other words, in each time step, four symbols were placed in the 4 \u00d7 8 sequence set, and an episode ended after N K / = 8 time steps when we obtained a complete sequence set. The metric function and reward function were then calculated following (5) and (6). An episode gave us eioght experiences.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8ea085fc5a6bbe74b8037da1aab484f6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1707.5
                    ],
                    [
                        866.7,
                        1970.1
                    ],
                    [
                        1567.4,
                        1970.1
                    ],
                    [
                        1567.4,
                        1707.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95332,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "For DNN-guided MCTS, at each state s;, we first set sj as the root node vo, and then ran g = 400 look-ahead simu- lations starting from vg. For each simulation, Dirichlet noise Dir(lao,a\u0131,..., @\u00a2_1]) was added to the prior probability of vg to introduce exploration, where the parameters for Dirichlet distribution are set as a0 = a Ont} a = 0.05. After 400 simulations, the probabilistic move-selection policy TI(s;) was then calculated by (22), where we set 7 = | for the",
        "type": "NarrativeText"
    },
    {
        "element_id": "f3b7c1ac1f7b686f96e384c6a31e1efe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1982.5
                    ],
                    [
                        136.0,
                        2084.8
                    ],
                    [
                        833.7,
                        2084.8
                    ],
                    [
                        833.7,
                        1982.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91776,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "where [0, Mu] is the search range of M(C): when M(C) = Mu, then R(C) = \u22121; and when M(C) = 0, then R(C) = 1. We initially set Mu = maxC M(C)",
        "type": "NarrativeText"
    },
    {
        "element_id": "99666ec0d80bd39e5b60c58ce0e7654e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        2003.8
                    ],
                    [
                        866.7,
                        2081.3
                    ],
                    [
                        1564.5,
                        2081.3
                    ],
                    [
                        1564.5,
                        2003.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92994,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "5Given the listed computation resource, another experiment is presented in our technical report [21] to study the best found sequence versus time consumption in the RL process of AlphaSeq.",
        "type": "NarrativeText"
    },
    {
        "element_id": "72b1d49aa0cdc46efeebc9dd376ff5ff",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.6
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81385,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a8a7d415936412be874e30ab97d23b3c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.8,
                        82.5
                    ],
                    [
                        1521.8,
                        106.0
                    ],
                    [
                        1566.1,
                        106.0
                    ],
                    [
                        1566.1,
                        82.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80126,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 7
        },
        "text": "3325",
        "type": "Header"
    },
    {
        "element_id": "377683d7af54525831bd094c37c200a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.9,
                        82.6
                    ],
                    [
                        133.9,
                        105.8
                    ],
                    [
                        178.1,
                        105.8
                    ],
                    [
                        178.1,
                        82.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79762,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "3326",
        "type": "Header"
    },
    {
        "element_id": "85e3c9e2363289c02937e0f4781daa19",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        574.0,
                        85.3
                    ],
                    [
                        574.0,
                        104.7
                    ],
                    [
                        1564.1,
                        104.7
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81741,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "dfd5a50a4c43b0ca97b25b6202a423a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        438.0,
                        161.5
                    ],
                    [
                        438.0,
                        183.6
                    ],
                    [
                        533.6,
                        183.6
                    ],
                    [
                        533.6,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74395,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "85e3c9e2363289c02937e0f4781daa19"
        },
        "text": "TABLE I",
        "type": "Title"
    },
    {
        "element_id": "314440c98498bf1246688cc9f7a03986",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        202.6,
                        194.8
                    ],
                    [
                        202.6,
                        241.6
                    ],
                    [
                        765.1,
                        241.6
                    ],
                    [
                        765.1,
                        194.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56912,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "85e3c9e2363289c02937e0f4781daa19"
        },
        "text": "HYPERPARAMETERS OF ALPHASEQ FOR COMPLEMENTARY CODE DISCOVERY",
        "type": "Title"
    },
    {
        "element_id": "4a67173b1338659044ce11761ce80fea",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.7,
                        268.5
                    ],
                    [
                        149.7,
                        763.5
                    ],
                    [
                        798.0,
                        763.5
                    ],
                    [
                        798.0,
                        268.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93061,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-8-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "314440c98498bf1246688cc9f7a03986",
            "text_as_html": "<table><thead><tr><th>Items</th><th>Parameters</th><th>Definitions</th></tr></thead><tbody><tr><td rowspan=\"3\">The Designed Game</td><td>K=4</td><td>Number of sequences in the target set</td></tr><tr><td>| yg</td><td>Length of each sequence</td></tr><tr><td>2-4</td><td>Number of symbols filled in each time step</td></tr><tr><td rowspan=\"3\">MCTS</td><td>q = 400</td><td>Number of simulations in one MCTS</td></tr><tr><td>a=0.05</td><td>Dirichlet noise</td></tr><tr><td>110-4 or 1)</td><td>Determines the way we calculate the move selection policy based on their visiting counts</td></tr><tr><td rowspan=\"5\">DNN</td><td>100</td><td>Every G episodes, the DNN is updated using</td></tr><tr><td>z=</td><td>the experiences accumulated in the latest z x G episodes</td></tr><tr><td>K'=4</td><td>Width of input image</td></tr><tr><td>N'-8</td><td>Length of input image</td></tr><tr><td>batch = 64</td><td>Mini-batch size</td></tr></tbody></table>"
        },
        "text": "Items Parameters Definitions K=4 Number of sequences in the target set The Designed gn: N=8 Length of each sequence Game \u00a3=4 Number of symbols filled in each time step q = 400 Number of simulations in one MCTS MCTS 40.05 Dirichlet noise 7 10-4 gr 1) Determines the way we calculate the move \u00a9 selection policy based on their visiting counts Every G episodes, the DNN is updated using the experiences accumulated in the latest z x G episodes DNN Width of input image Length of input image batch = 64 Mini-batch size",
        "type": "Table"
    },
    {
        "element_id": "7a7a2c24c3dee2a648715304ddf4f750",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        802.2
                    ],
                    [
                        136.0,
                        929.5
                    ],
                    [
                        834.0,
                        929.5
                    ],
                    [
                        834.0,
                        802.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93558,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "314440c98498bf1246688cc9f7a03986"
        },
        "text": "\ufb01rst one third time steps (the probability of choosing a move is proportional to its visiting counts), and \u03c4 = 10\u22124 for the rest of the time steps (deterministically choose the move with the most visiting counts).",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d44b8120e494ab624e344c1a3500d01",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        935.2
                    ],
                    [
                        136.0,
                        1328.2
                    ],
                    [
                        835.6,
                        1328.2
                    ],
                    [
                        835.6,
                        935.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95517,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "314440c98498bf1246688cc9f7a03986"
        },
        "text": "The DNN implemented in AlphaSeq is a deep convolutional network (ConvNets). This DNN consists of six convolutional layers together with batch normalization and recti\ufb01er nonlin- earities (detailed architecture of this ConvNets can be found in Appendix A). The DNN update cycle G = 100 and z = 3, that is, every G = 100 episodes, we trained the ConvNets using the experiences accumulated in the latest z \u00d7 G = 300 episodes (i.e., 2400 experiences) by stochastic gradient descent. In particular, the minibatch size was set to 64, and we randomly sampled 2400/64 minibatches without replacement from the 2400 experiences to train the ConvNets. For each minibatch, the loss function is de\ufb01ned by (23) in Appendix A.",
        "type": "NarrativeText"
    },
    {
        "element_id": "caf29e8cc1b6297ad7902acb5ea5729f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.0,
                        166.4
                    ],
                    [
                        910.0,
                        540.1
                    ],
                    [
                        1522.5,
                        540.1
                    ],
                    [
                        1522.5,
                        166.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94413,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-8-12.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "saint 45 T T T T T 70 eee ew 4, see No. of visited States 60 a so ki 43, Ap 3 \u015fek ML \u0130zs? H\u0130 a i = yoy AlphaSeq (mean) 23 sop cal 4 < pry 2 c e... 15 20 / P\u0130NPON DF DF DAP 0 Tam 2000-5000 4000 5000-060 od 3008 plsodes",
        "type": "Image"
    },
    {
        "element_id": "5da83a07cd63d9da49f55039df2d90b7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1489.3,
                        163.1
                    ],
                    [
                        1489.3,
                        174.2
                    ],
                    [
                        1512.7,
                        174.2
                    ],
                    [
                        1512.7,
                        163.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-8-13.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "saint",
        "type": "Image"
    },
    {
        "element_id": "12a60d676a1f769a7cb93d58ea677efa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        557.8
                    ],
                    [
                        866.7,
                        654.6
                    ],
                    [
                        1564.1,
                        654.6
                    ],
                    [
                        1564.1,
                        557.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94537,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "Fig. 5. RL process of AlphaSeq to rediscover a set of ideal complemen- tary codes for MC-CDMA systems. Mean metric E[M], minimum metric min[M], and the number of visited states versus episodes, where the DNN update cycle G = 100 and z = 3.",
        "type": "FigureCaption"
    },
    {
        "element_id": "b33739a4b68416131d409a8b69efda8d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        697.5
                    ],
                    [
                        866.7,
                        990.8
                    ],
                    [
                        1568.9,
                        990.8
                    ],
                    [
                        1568.9,
                        697.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95705,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "mance of current AlphaSeq. As a balance, we let AlphaSeq play 50 games so that a smoothed mean performance curve can be obtained without excessive evaluation time. The mean metric E[M] and the minimum metric min[M] of the 50 found sequence sets were recorded and plotted in Fig. 5. Note that the plots of metric and reward functions differ only in scale. We plot the metric evolution in Fig. 5 as it is a direct quality measurement of a sequence set in this speci\ufb01c application.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a4befc123d348f55ea5df7c7e4035af1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        996.5
                    ],
                    [
                        866.7,
                        1156.8
                    ],
                    [
                        1566.8,
                        1156.8
                    ],
                    [
                        1566.8,
                        996.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94669,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "As can be seen from Fig. 5, with the continuous train- ing of DNN, AlphaSeq gradually discovered sequence sets with smaller and smaller metric values. After 4100 episodes, AlphaSeq rediscovered an ideal complementary code set Calpha given by",
        "type": "NarrativeText"
    },
    {
        "element_id": "3ef86219b7f58e47e270888b4eebb980",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1078.3,
                        1157.2
                    ],
                    [
                        1078.3,
                        1184.8
                    ],
                    [
                        1100.4,
                        1184.8
                    ],
                    [
                        1100.4,
                        1157.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "\u239b",
        "type": "UncategorizedText"
    },
    {
        "element_id": "3066c97374cd90bf0c795ecaa663d6d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1397.7,
                        1157.2
                    ],
                    [
                        1397.7,
                        1184.8
                    ],
                    [
                        1419.8,
                        1184.8
                    ],
                    [
                        1419.8,
                        1157.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "\u239e",
        "type": "UncategorizedText"
    },
    {
        "element_id": "b0213964e649ca5bd0847dbad8d543f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        967.9,
                        1175.8
                    ],
                    [
                        967.9,
                        1287.9
                    ],
                    [
                        1575.5,
                        1287.9
                    ],
                    [
                        1575.5,
                        1175.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84389,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "-141-1-141-1-1 al] 1-1-1 41 4141-141 Calpha = +1-1414+141-1-1-1] J \u00b0 (8) +1 4141-1 4141-141",
        "type": "Formula"
    },
    {
        "element_id": "644253047499146a0c0f9536dfef7375",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1333.2
                    ],
                    [
                        136.0,
                        1792.8
                    ],
                    [
                        834.4,
                        1792.8
                    ],
                    [
                        834.4,
                        1333.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95304,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "Remark: In Table I, the width and length of the input image fed into DNN are chosen to match with N and K , i.e., K = K = 4 and N = N = 8. However, it should be emphasized that this is not an absolute necessity. In general, we \ufb01nd that setting the input of the DNN to be an \u00d7N K / image can speed up the learning process of DNN. For example, if we had set = 5 instead of = 4 in this experiment, then it would better to set K = 5 and N = 7 (i.e., DNN takes an 5 \u00d7 7 image as input, and in each time step, one row of the image is \ufb01lled). Accordingly, any intermediate state (i.e., a partially \ufb01lled 4 \u00d7 8 sequence set pattern) must \ufb01rst be transformed to a 5\u00d77 image before it is fed into the ConvNets (the last three symbols in the 5 \u00d7 7 set will be padded with 0 because the original 4 \u00d7 8 set has three fewer symbols).",
        "type": "NarrativeText"
    },
    {
        "element_id": "36ae07c82b324abb26d177b8e68c2328",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1840.8
                    ],
                    [
                        136.0,
                        1868.5
                    ],
                    [
                        450.6,
                        1868.5
                    ],
                    [
                        450.6,
                        1840.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71231,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "D. Performance Evaluation",
        "type": "Title"
    },
    {
        "element_id": "ddf4b397ebd8130884bb10b0d1ac7073",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1877.2
                    ],
                    [
                        136.0,
                        2134.2
                    ],
                    [
                        835.9,
                        2134.2
                    ],
                    [
                        835.9,
                        1877.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94725,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "36ae07c82b324abb26d177b8e68c2328"
        },
        "text": "Over the course of training, AlphaSeq ran 8 \u00d7 103 episodes, in which 6.4 \u00d7 104 experiences were generated. To monitor the evolution of AlphaSeq, every G = 100 episodes when the DNN was updated, we evaluated the searching capability of AlphaSeq by using it (with the updated DNN) to play 50 noise- less games. In general, the more evaluation games AlphaSeq plays, the better their mean performance captures the perfor-",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f0b797e3365a99742fc95ba0cdd5736",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1301.2
                    ],
                    [
                        866.7,
                        1536.7
                    ],
                    [
                        1568.6,
                        1536.7
                    ],
                    [
                        1568.6,
                        1301.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95202,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "36ae07c82b324abb26d177b8e68c2328"
        },
        "text": "It is straightforward to see that Calpha is an isomorphic version to Cbench: if we denote Cbench by ([a1, a2]T , i.e., [b1, b2]T )T , then Calpha = ([\u2212b2, \u2212b1]T , [a2, a1]T )T . We specify that AlphaSeq could \ufb01nd different ideal sequence sets in different runs. For example, in another run, AlphaSeq eventually discovered a nonisomorphic ideal sequence set to Cbench, giving",
        "type": "NarrativeText"
    },
    {
        "element_id": "8f28478428a8e8a94bd5b3aeb44db8aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        981.3,
                        1534.5
                    ],
                    [
                        981.3,
                        1562.2
                    ],
                    [
                        1003.4,
                        1562.2
                    ],
                    [
                        1003.4,
                        1534.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "36ae07c82b324abb26d177b8e68c2328"
        },
        "text": "\u239b",
        "type": "UncategorizedText"
    },
    {
        "element_id": "de4f15575065c47052ec2a0303d29df6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1451.3,
                        1534.5
                    ],
                    [
                        1451.3,
                        1562.2
                    ],
                    [
                        1473.4,
                        1562.2
                    ],
                    [
                        1473.4,
                        1534.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8,
            "parent_id": "36ae07c82b324abb26d177b8e68c2328"
        },
        "text": "\u239e",
        "type": "UncategorizedText"
    },
    {
        "element_id": "e4d1e75cfbbcd47ae552a52d91405ad9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        868.8,
                        1553.1
                    ],
                    [
                        868.8,
                        1665.8
                    ],
                    [
                        1569.7,
                        1665.8
                    ],
                    [
                        1569.7,
                        1553.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7956,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "| \u2014 -1 -1 1 -1 4l zl / -1 +1 41 41-1 -1 \u015e\u0130 -1 alpha ~ | +1 -1 -1 41 41\u00b0 41 \u201c| - O +1 -1 +1 41 41 41 410-1",
        "type": "Formula"
    },
    {
        "element_id": "e00ede7a4b113bfa68bad1c165d5e3b5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1683.8
                    ],
                    [
                        866.7,
                        1910.8
                    ],
                    [
                        1564.9,
                        1910.8
                    ],
                    [
                        1564.9,
                        1683.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95325,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "The complexity of AlphaSeq is measured by means of distinct states that have been visited. Speci\ufb01cally, we stored all the states (including intermediate states and terminal states) encountered over the course of training in a Hash table. Every G episodes, we recorded the length of the Hash table (i.e., the total number of visited states by then) and plotted them in Fig. 5 as the training goes on.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0cad6b7844aa9edf103bdc3d0d964aa8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1916.5
                    ],
                    [
                        866.7,
                        2110.2
                    ],
                    [
                        1564.7,
                        2110.2
                    ],
                    [
                        1564.7,
                        1916.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94727,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "An interesting observation is that there is a turning point on the curve of the number of distinct visited states. The slope of this curve corresponds to the extent to which AlphaSeq is exploring new states in its choice of actions. Under the framework of AlphaSeq, there are two kinds of explo- ration as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bb0989c7fcdd45faacd3c446b15b69ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        135.2,
                        2122.8
                    ],
                    [
                        135.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75811,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 8
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c7c5ad5610a79db4c4c79b33440c6749",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        83.0
                    ],
                    [
                        136.0,
                        105.0
                    ],
                    [
                        684.4,
                        105.0
                    ],
                    [
                        684.4,
                        83.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81382,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "70902dd5e7737add6cca1ed1d7ba3f49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.5,
                        161.5
                    ],
                    [
                        149.5,
                        289.3
                    ],
                    [
                        837.3,
                        289.3
                    ],
                    [
                        837.3,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92461,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "c7c5ad5610a79db4c4c79b33440c6749"
        },
        "text": "1) Inherent Exploration: This is introduced by the variance of the action-selection policy. That is, the more random the action-selection policy is, the more new states are likely to be explored by AlphaSeq.",
        "type": "ListItem"
    },
    {
        "element_id": "331ec5a8afe2702b5b6f42826195a344",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        294.5
                    ],
                    [
                        154.7,
                        355.6
                    ],
                    [
                        833.5,
                        355.6
                    ],
                    [
                        833.5,
                        294.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90088,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "c7c5ad5610a79db4c4c79b33440c6749"
        },
        "text": "2) Arti\ufb01cial Exploration: We deliberately add extra arti\ufb01cial randomness to AlphaSeq to let it explore more states.",
        "type": "ListItem"
    },
    {
        "element_id": "740af0cda1bd3a2412827a7cbfb5d903",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        394.5
                    ],
                    [
                        136.0,
                        720.8
                    ],
                    [
                        836.0,
                        720.8
                    ],
                    [
                        836.0,
                        394.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95455,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "c7c5ad5610a79db4c4c79b33440c6749"
        },
        "text": "For example, the Dirichlet noise added to the root vertex in DNN-guided MCTS, the temperature parameter \u03c4 that deter- mines how to calculate the policy all add to the randomness. At the beginning of the game (i.e., episode 0), the policy of AlphaSeq is quite random inherently because the DNN is randomly initialized. Thus, both inherent exploration and arti\ufb01cial exploration contribute to the slope of this curve. At the end of the game (i.e., episode 8 \u00d7 103), the policy converges; hence, the inherent exploration drops off, and only arti\ufb01cial exploration remains.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3cb98f73e1b412b4f17c5d97ea981866",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        955.2,
                        163.4
                    ],
                    [
                        955.2,
                        555.3
                    ],
                    [
                        1480.0,
                        555.3
                    ],
                    [
                        1480.0,
                        163.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93121,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-9-14.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "20 F 40 Metric (AlphaSeq) 60 80 1 1 v 120 100 80 60 40 20 0 Metric (DNN player)",
        "type": "Image"
    },
    {
        "element_id": "0f565dd1f508f2f26216b3dab906f8ed",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        585.5
                    ],
                    [
                        866.7,
                        707.3
                    ],
                    [
                        1563.7,
                        707.3
                    ],
                    [
                        1563.7,
                        585.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95388,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "Fig. 6. Polynomial \ufb01t convergence curve for AlphaSeq and DNN player, where the DNN update cycle G = 100. The positive direction of the x-axis is a direction of the performance improvement for DNN, while the positive direction of the y-axis is a direction of the performance improvement for AlphaSeq.",
        "type": "FigureCaption"
    },
    {
        "element_id": "ce3bc38888a98a78857c09d059a4f3fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        726.5
                    ],
                    [
                        136.0,
                        1219.3
                    ],
                    [
                        835.0,
                        1219.3
                    ],
                    [
                        835.0,
                        726.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95089,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "This turning point was, in fact, observed in all simu- lations of AlphaSeq in various applications we tried (not just the application for rediscovering complementary code here; see Section IV on application of AlphaSeq to dis- cover phase-coded sequences for pulse compression radar). In general, we can then divide the overall RL process of AlphaSeq into two phases based on this turning point. Phase I is an exploration-dominant phase (before the turning point), in which the behaviors of AlphaSeq are quite random. As a result, AlphaSeq actively explores increasingly more states per G episodes in the overall solution space. After gaining familiarity with the whole solution space, AlphaSeq enters an exploitation-dominant phase (after the turning point), in which instead of exploring for more states, AlphaSeq tends to focus more on exploitation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "52601551a090abb9acac81b9daad30cf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        768.5
                    ],
                    [
                        866.7,
                        804.5
                    ],
                    [
                        1564.1,
                        804.5
                    ],
                    [
                        1564.1,
                        768.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "i.e., P(si ), to sample the next move without relying on the",
        "type": "NarrativeText"
    },
    {
        "element_id": "268093f95d7a313bc55930bd3e930cc1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        778.2
                    ],
                    [
                        866.7,
                        837.8
                    ],
                    [
                        1567.0,
                        837.8
                    ],
                    [
                        1567.0,
                        778.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91779,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "ie., P(s;), to sample the next move without relying on the MCTS outputs II(s;).",
        "type": "NarrativeText"
    },
    {
        "element_id": "1e95ead83353ef5c7b00afc34a3dc495",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        831.5
                    ],
                    [
                        894.3,
                        868.5
                    ],
                    [
                        1564.3,
                        868.5
                    ],
                    [
                        1564.3,
                        831.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "Fig. 6 shows all the (E[M], E[M]) pairs over the course",
        "type": "NarrativeText"
    },
    {
        "element_id": "e0faabac4a1e4fcd04a715cd131c2d81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        842.6
                    ],
                    [
                        866.7,
                        1134.2
                    ],
                    [
                        1568.1,
                        1134.2
                    ],
                    [
                        1568.1,
                        842.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95674,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "of training and the corresponding polynomial \ufb01tted con- vergence curve. In particular, the positive direction of the x-axis in Fig. 6 is a direction of performance improvement for DNN, and the positive direction of y-axis is a direc- tion of performance improvement for AlphaSeq. The con- vergence curve in Fig. 6 re\ufb02ects how the two ingredients, \u201cMCTS-guided game-play\u201d and \u201cDNN update,\u201d interplay and mutually improve in the RL process of AlphaSeq.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9ff4d2a6c015d5e40206916fe56d1580",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        916.8,
                        1175.8
                    ],
                    [
                        916.8,
                        1203.5
                    ],
                    [
                        1513.1,
                        1203.5
                    ],
                    [
                        1513.1,
                        1175.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83282,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "IV. ALPHASEQ FOR PULSE COMPRESSION RADAR",
        "type": "Title"
    },
    {
        "element_id": "86c9abde1f7a8410cacbba594c8d45ea",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1224.2
                    ],
                    [
                        136.0,
                        1517.8
                    ],
                    [
                        836.0,
                        1517.8
                    ],
                    [
                        836.0,
                        1224.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95817,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "Remark: The DNN update cycle G is important to guar- antee that the algorithmic iteration proceeds in a direction of performance improvement. In AlphaSeg, given a DNN wo, the move-selection policy TI given by the ye-guided MCTS is usually much stronger than the raw policy output P of yo. Thus, we first run yg-guided MCTS to play G games and generate | NK //) x G experiences. Then, we use these experiences to train a new DNN yy, so that yg: can learn the stronger move given by yp-guided MCTS.",
        "type": "NarrativeText"
    },
    {
        "element_id": "06ff3ee28acf3a7399977326fa2bca94",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1523.2
                    ],
                    [
                        136.0,
                        1750.5
                    ],
                    [
                        836.5,
                        1750.5
                    ],
                    [
                        836.5,
                        1523.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95731,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "In this context, the DNN update cycle G must be chosen so that the [VK/\u20ac] x G experiences are sufficient to capture the fine details of TI given by yp-guided MCTS. In particular, parameter G is closely related to /: a larger / means more elements in TI (i.e., TI must capture 2\u2018 possible moves in each step), and hence, a larger G is needed to guarantee that II is well represented by the (NK //| x G experiences.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2f6ab55df5ed46a65608bc6ae29cb640",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1755.8
                    ],
                    [
                        136.0,
                        2082.5
                    ],
                    [
                        834.8,
                        2082.5
                    ],
                    [
                        834.8,
                        1755.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95468,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "As stated in Section II, the essence of AlphaSeq is a process of iterative \u201cgame-play with DNN-guided MCTS\u201d and \u201cDNN update\u201d: the improvement of DNN brings about improvement of the DNN-guided MCTS, and the experiences generated by the improved MCTS, in turn, bring about further improvement of the DNN through training. To verify this, each time when the DNN is updated, we assess the new DNN by using it (without MCTS, and no noise) to discover 50 sequences and record their mean metric E[M]. Speci\ufb01cally, at each state si , the player directly adopts the raw policy output of the DNN,",
        "type": "NarrativeText"
    },
    {
        "element_id": "78dd106eb99f5fb8fdbac5a0897bd3c9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1217.8
                    ],
                    [
                        866.7,
                        1577.5
                    ],
                    [
                        1567.6,
                        1577.5
                    ],
                    [
                        1567.6,
                        1217.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95508,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "Radar radiates radio pulses for the detection and loca- tion of re\ufb02ecting objects [4]. A classical dilemma in radar systems arises from the choice of pulse duration: given a constant power, longer pulses have higher energy, providing greater detection range; shorter pulses, on the other hand, have larger bandwidth, yielding a higher resolution. Thus, there is a tradeoff between distance and resolution. Pulse compression radar can enable high-resolution detection over a large distance [4], [22], [23]. The key is to use modulated pulses (e.g., phase-coded pulse) rather than conventional non- modulated pulses.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a9638efcc59194e574caf1e6cc5d8cf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        862.9,
                        1625.2
                    ],
                    [
                        862.9,
                        1652.8
                    ],
                    [
                        1401.1,
                        1652.8
                    ],
                    [
                        1401.1,
                        1625.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79905,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "A. Pulse Compression Radar and Phase codes",
        "type": "NarrativeText"
    },
    {
        "element_id": "165cc9a03b53d27bb5d47aab4298c7b5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.3,
                        1667.5
                    ],
                    [
                        866.3,
                        1894.5
                    ],
                    [
                        1570.2,
                        1894.5
                    ],
                    [
                        1570.2,
                        1667.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95298,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9,
            "parent_id": "9ff4d2a6c015d5e40206916fe56d1580"
        },
        "text": "The transmitter of a binary phased-coded pulse compression radar system transmits a pulse modulated by N rectangular subpulses. The subpulses are a binary phase code s of length N. Each entry of the code is +1 or \u22121, corresponding to phase 0 and \u03c0. Following the de\ufb01nition in [23] and [24], after subpulse-matched \ufb01ltering (MF) and analog-to-digital conversion, the received sequence y is",
        "type": "NarrativeText"
    },
    {
        "element_id": "20371504411f75ef98f3e6ad6d2cb535",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1019.7,
                        1908.8
                    ],
                    [
                        1019.7,
                        2003.7
                    ],
                    [
                        1564.2,
                        2003.7
                    ],
                    [
                        1564.2,
                        1908.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78861,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "N-1 y =hos + > hnJns kw (10) n=1\u2014N,n40",
        "type": "Formula"
    },
    {
        "element_id": "9fd734ccd212a52167d723b5506ebaa3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        2015.5
                    ],
                    [
                        866.7,
                        2082.5
                    ],
                    [
                        1564.9,
                        2082.5
                    ],
                    [
                        1564.9,
                        2015.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90146,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "where: 1) {hn : n = 1 \u2212 N, 2 \u2212 N, . . . , N \u2212 2, N \u2212 1} are coef\ufb01cients proportional to the radar cross sections of different",
        "type": "NarrativeText"
    },
    {
        "element_id": "9398c46a3da643364ed5a3f47b372da3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.9
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7301,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "94f1742771c55a45fe1a8583d19065d2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.8,
                        82.1
                    ],
                    [
                        1521.8,
                        105.8
                    ],
                    [
                        1566.4,
                        105.8
                    ],
                    [
                        1566.4,
                        82.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79798,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 9
        },
        "text": "3327",
        "type": "Header"
    },
    {
        "element_id": "c22791270ca7ee0f71637c77eeff82cf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.2,
                        82.5
                    ],
                    [
                        134.2,
                        106.1
                    ],
                    [
                        176.5,
                        106.1
                    ],
                    [
                        176.5,
                        82.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79408,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "3328",
        "type": "Header"
    },
    {
        "element_id": "850634fd24c806f1c55c56a15637f47e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        573.3,
                        85.3
                    ],
                    [
                        573.3,
                        104.8
                    ],
                    [
                        1564.1,
                        104.8
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63202,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "bf0c17d3e7f0f1584c8767d1c96ab56e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.0,
                        161.5
                    ],
                    [
                        134.0,
                        355.5
                    ],
                    [
                        834.9,
                        355.5
                    ],
                    [
                        834.9,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95364,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10,
            "parent_id": "850634fd24c806f1c55c56a15637f47e"
        },
        "text": "range bins [23]. In particular, h0 corresponds to the range bin of interest, and the radar\u2019s objective is to estimate h0 given the received sequence y; 2) w is the white Gaussian noise; and 3) matrix Jn, as given in (11), is a shift matrix capturing the different propagation time needed for the clutter to return from different range bins [24].",
        "type": "NarrativeText"
    },
    {
        "element_id": "570884f2c3f5d0e514433c28a01481e9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        276.6,
                        380.1
                    ],
                    [
                        276.6,
                        671.5
                    ],
                    [
                        736.1,
                        671.5
                    ],
                    [
                        736.1,
                        380.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76428,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Column; 0 1 2",
        "type": "Formula"
    },
    {
        "element_id": "cdd4a65d7e37e1976abb42a09e498e6e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        706.8
                    ],
                    [
                        136.0,
                        875.8
                    ],
                    [
                        838.7,
                        875.8
                    ],
                    [
                        838.7,
                        706.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94905,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "where i = 1, 2, . . . , N\u22121 and J\u2212i = J T . That is, in matrix Jn, i all entries except for that on the nth off-diagonal are 0. The effect of matrix Jn is to right-shift or left-shift the phase code s with zero padding: when n < 0, Jns is a right-shifted version of s; when n > 0, Jns is a left-shifted version of s.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2197735c9b17adb5276239933cb43705",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        878.5
                    ],
                    [
                        136.0,
                        939.8
                    ],
                    [
                        837.8,
                        939.8
                    ],
                    [
                        837.8,
                        878.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90968,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "To estimate the coef\ufb01cient h0, a widely studied estimator is the MF estimator [25]\u2013[27]",
        "type": "NarrativeText"
    },
    {
        "element_id": "00df6bcba8a03e8bcd59e1f57aa91ca5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        787.3,
                        648.5
                    ],
                    [
                        787.3,
                        676.2
                    ],
                    [
                        833.9,
                        676.2
                    ],
                    [
                        833.9,
                        648.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "(11)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ba56d017079821271394b9765d777411",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        864.2,
                        161.8
                    ],
                    [
                        864.2,
                        189.5
                    ],
                    [
                        1157.6,
                        189.5
                    ],
                    [
                        1157.6,
                        161.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83457,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "received sequence, giving",
        "type": "NarrativeText"
    },
    {
        "element_id": "74349b70e97b1b92397a0229354a616c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        976.4,
                        203.8
                    ],
                    [
                        976.4,
                        301.2
                    ],
                    [
                        1570.8,
                        301.2
                    ],
                    [
                        1570.8,
                        203.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83433,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "T Nol T ~ xiy Xx Ins ho = = = ho+ > ln x's xls n=1-N,n#0 (14)",
        "type": "Formula"
    },
    {
        "element_id": "771691c2ee780f337b712e5554d23ad4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        316.2
                    ],
                    [
                        866.7,
                        413.1
                    ],
                    [
                        1566.1,
                        413.1
                    ],
                    [
                        1566.1,
                        316.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93524,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "where the real-valued sequence x is to be optimized at the receiver. The problem is then to \ufb01nd a pair of sequences (s, x) so that the SIR \u03b3MMF in (15) can be maximized",
        "type": "NarrativeText"
    },
    {
        "element_id": "eaf436324001f14444955c58a06f10a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1019.3,
                        424.8
                    ],
                    [
                        1019.3,
                        510.0
                    ],
                    [
                        1564.6,
                        510.0
                    ],
                    [
                        1564.6,
                        424.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84088,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "(Ts? Di (xT Ins)? n=1\u2014N,n40 n YMMF = (15)",
        "type": "Formula"
    },
    {
        "element_id": "87c660985fed232529e6164b95e1a726",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        523.2
                    ],
                    [
                        866.7,
                        617.8
                    ],
                    [
                        1565.9,
                        617.8
                    ],
                    [
                        1565.9,
                        523.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92832,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "It had been shown in [24] that, given a phase code s, the optimal sequence x that maximizes \u03b3MMF is x\u2217 = R\u22121s, where matrix R is given by",
        "type": "NarrativeText"
    },
    {
        "element_id": "4a0ceaeaca454dcc84a4c4361435299b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1153.7,
                        632.1
                    ],
                    [
                        1153.7,
                        661.8
                    ],
                    [
                        1196.5,
                        661.8
                    ],
                    [
                        1196.5,
                        632.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "N-1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8ac3e9cff897930ca4f4b9cde034b31d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1059.3,
                        640.0
                    ],
                    [
                        1059.3,
                        727.1
                    ],
                    [
                        1564.6,
                        727.1
                    ],
                    [
                        1564.6,
                        640.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80861,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "R = JnssT J T n . (16) n=1\u2212N,n=0",
        "type": "Formula"
    },
    {
        "element_id": "667367e402be3aea09863f0480aba82f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        740.8
                    ],
                    [
                        866.7,
                        777.8
                    ],
                    [
                        1297.6,
                        777.8
                    ],
                    [
                        1297.6,
                        740.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73353,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Substituting x\u2217 = R\u22121s in (15) gives",
        "type": "NarrativeText"
    },
    {
        "element_id": "a432a87a92b50720eb9f1724476267c9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1105.3,
                        790.8
                    ],
                    [
                        1105.3,
                        831.7
                    ],
                    [
                        1305.9,
                        831.7
                    ],
                    [
                        1305.9,
                        790.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "\u03b3MMF = sT R\u22121s.",
        "type": "Title"
    },
    {
        "element_id": "f4ec869f410933b96317282f12e4f3e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1107.1,
                        799.2
                    ],
                    [
                        1107.1,
                        830.2
                    ],
                    [
                        1564.6,
                        830.2
                    ],
                    [
                        1564.6,
                        799.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70103,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "(17)",
        "type": "Formula"
    },
    {
        "element_id": "f46dc6ab707d0997da9054f8d5731a4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        847.2
                    ],
                    [
                        866.7,
                        949.7
                    ],
                    [
                        1566.6,
                        949.7
                    ],
                    [
                        1566.6,
                        847.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93584,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Note that \u03b3MMF only depends on the phase code s; hence, the objective for the design of the MMF estimator is then to discover a phase-code s that can maximize \u03b3MMF in (17).",
        "type": "NarrativeText"
    },
    {
        "element_id": "d77dd910782350455bdf65d5074fff6a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        247.1,
                        965.5
                    ],
                    [
                        247.1,
                        1060.5
                    ],
                    [
                        850.1,
                        1060.5
                    ],
                    [
                        850.1,
                        965.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86697,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "N-1 ~ sly sl Ins ho = > =hot+ > hn \u2014 sis sis n=1-N.n#0 (12)",
        "type": "Formula"
    },
    {
        "element_id": "19d7698a32147df5470aac0be47a7d5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1090.8
                    ],
                    [
                        136.0,
                        1287.1
                    ],
                    [
                        838.5,
                        1287.1
                    ],
                    [
                        838.5,
                        1090.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95107,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "where the additive white Gaussian noise (AWGN) is ignored since the received signal is interference-limited (i.e., the inter- ference power dominates over the noise power). Given the fact that we have no information on {hn : n = 0}, the problem is then to discover a phase code s that can maximize the SIR \u03b3MF (larger SIR yields better estimation performance)",
        "type": "NarrativeText"
    },
    {
        "element_id": "cc5c900ee32286f22b9bbb370048bb85",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        952.2
                    ],
                    [
                        866.7,
                        1212.8
                    ],
                    [
                        1566.1,
                        1212.8
                    ],
                    [
                        1566.1,
                        952.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95281,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Remark: The MMF estimator is superior to the MF esti- mator since \u03b3MMF is not less than \u03b3MF given the same phase code s. However, the problem of discovering a phase code s that maximizes (17) did not receive much attention from the research community compared with the merit factor problem [i.e., discovering a code s that maximizes (15)]. This is perhaps due to the more complex criterion and the lack of suitable mathematical tools [27].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b59c58511c1a4b7aae8d16ade48fb816",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1218.2
                    ],
                    [
                        866.7,
                        1279.2
                    ],
                    [
                        1564.3,
                        1279.2
                    ],
                    [
                        1564.3,
                        1218.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9171,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "In this section, we make use of AlphaSeq to discover phase codes for pulse compression radar with MMF estimator.",
        "type": "NarrativeText"
    },
    {
        "element_id": "86958b3303bfafd749369dcc7c73070f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        294.5,
                        1312.2
                    ],
                    [
                        294.5,
                        1396.4
                    ],
                    [
                        844.1,
                        1396.4
                    ],
                    [
                        844.1,
                        1312.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83737,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "(575) SAN Nazo (77,5) YMF = (13)",
        "type": "Formula"
    },
    {
        "element_id": "d24a7f45e7bd16c2c322e480fcbf3df3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.2,
                        1326.5
                    ],
                    [
                        866.2,
                        1354.2
                    ],
                    [
                        1358.0,
                        1354.2
                    ],
                    [
                        1358.0,
                        1326.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70211,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "B. AlphaSeq for Pulse Compression Radar",
        "type": "NarrativeText"
    },
    {
        "element_id": "850ab78bfca8b6f103ad8bfb03befc98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        1368.8
                    ],
                    [
                        894.3,
                        1396.5
                    ],
                    [
                        1490.2,
                        1396.5
                    ],
                    [
                        1490.2,
                        1368.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73594,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "We choose (17) as the metric function of AlphaSeq",
        "type": "NarrativeText"
    },
    {
        "element_id": "b98b81ec3f7232583ca056cb1c2356a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1423.8
                    ],
                    [
                        136.0,
                        1816.8
                    ],
                    [
                        835.5,
                        1816.8
                    ],
                    [
                        835.5,
                        1423.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95507,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "In fact, this is the well-known \u201cmerit factor problem\u201d occur- ring in various guises in many disciplines [26]. In the past few decades, a variety of phase codes have been devised to achieve large SIR (merit factor), e.g., the Rudin\u2013Shapiro sequences (asymptotically, \u03b3MF = 3), m-sequences (asymp- totically, \u03b3MF = 3), and Legendre sequences (asymptotically, \u03b3MF = 6) (see the excellent surveys [26], [27] and the references therein). Overall, the merit factor problem remains open. Experiment results show that \u03b3MF does not increase as the sequence length N increases. So far, the best-known merit factor of 14.08 is achieved by the Barker sequence of length 13.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7579730b625ee3fc7a310e12f883ea77",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1108.3,
                        1409.5
                    ],
                    [
                        1108.3,
                        1447.5
                    ],
                    [
                        1302.8,
                        1447.5
                    ],
                    [
                        1302.8,
                        1409.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "M(s) = sT R\u22121s",
        "type": "Title"
    },
    {
        "element_id": "046b20f8e9e7cd3b5d1bbb140e7fa3a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1111.7,
                        1418.9
                    ],
                    [
                        1111.7,
                        1447.8
                    ],
                    [
                        1564.6,
                        1447.8
                    ],
                    [
                        1564.6,
                        1418.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6772,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "(18)",
        "type": "Formula"
    },
    {
        "element_id": "eaa9e9dd2ef7760d329762bcf1218187",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1471.2
                    ],
                    [
                        866.7,
                        1565.8
                    ],
                    [
                        1566.2,
                        1565.8
                    ],
                    [
                        1566.2,
                        1471.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93086,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "where matrix R is given in (16). The objective of AlphaSeq is then to discover the sequence that can maximize this metric function.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a88fc085857ae96d7a328484087783f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1565.2
                    ],
                    [
                        866.7,
                        1632.2
                    ],
                    [
                        1564.2,
                        1632.2
                    ],
                    [
                        1564.2,
                        1565.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.915,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Given a phase code s with metric M(s), the linear reward function is de\ufb01ned as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "19559245c73c69fafafe591af29e07e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1037.7,
                        1636.2
                    ],
                    [
                        1037.7,
                        1711.1
                    ],
                    [
                        1566.9,
                        1711.1
                    ],
                    [
                        1566.9,
                        1636.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84113,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "R(s) = 2M(s) \u2212 Mu \u2212 Ml Mu \u2212 Ml (19)",
        "type": "Formula"
    },
    {
        "element_id": "85146c48e2a05e8d27a42e1edb575a7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1716.8
                    ],
                    [
                        866.7,
                        1819.1
                    ],
                    [
                        1567.9,
                        1819.1
                    ],
                    [
                        1567.9,
                        1716.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93242,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "where Mu and Ml are the upper and lower bounds for the search range of M(s). In general, we could set Mu = maxs M(s) and Ml = mins M(s).",
        "type": "NarrativeText"
    },
    {
        "element_id": "9b58922d127476825f856dd105146fba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1822.5
                    ],
                    [
                        136.0,
                        2016.2
                    ],
                    [
                        833.8,
                        2016.2
                    ],
                    [
                        833.8,
                        1822.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9533,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "The motivation of the MF estimator comes from the fact that MF provides the highest signal-to-noise ratio (SNR) in the presence of white Gaussian noise [28]. However, in the case of Radar, the received signal is interference-limited; hence, inter- ference suppression is much more important. This motivates researchers to devise a MMF estimator [23], [24], [29].",
        "type": "NarrativeText"
    },
    {
        "element_id": "fbb6b00a15be10df79b8809963e1d2d8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        2021.2
                    ],
                    [
                        136.0,
                        2082.5
                    ],
                    [
                        833.6,
                        2082.5
                    ],
                    [
                        833.6,
                        2021.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91635,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Instead of using the transmitted phase-code s, the MMF estimator uses a general real-valued code x to correlate the",
        "type": "NarrativeText"
    },
    {
        "element_id": "ec7e9e81cfddaeb7d12849720c03ebc5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1822.2
                    ],
                    [
                        866.7,
                        2082.5
                    ],
                    [
                        1564.6,
                        2082.5
                    ],
                    [
                        1564.6,
                        1822.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94911,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Remark: In Appendix C of our technical report [21], the value ranges of M(s) are derived as maxs M(s) = 37 and mins M(s) = (16/9N 3). However, we empirically the search range Mu = 37 \ufb01nd that, if we directly set and Ml = (16/9N 3), AlphaSeq will be trapped in the exploration-dominant phase for a long time. This is because Mu \u2212Ml is too large. In other words, we are asking AlphaSeq to search over a large solution space for s all at once. We will",
        "type": "NarrativeText"
    },
    {
        "element_id": "a4e8d0e9f2dc312ec31743264a783527",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.8
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70259,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 10
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2c3b5dd849aa7f4e2ea2a016e76d76fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        685.6,
                        104.7
                    ],
                    [
                        685.6,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88458,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "9e618f053e0ebea2bb98ce157d95b1d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        438.7,
                        161.5
                    ],
                    [
                        438.7,
                        183.6
                    ],
                    [
                        532.0,
                        183.6
                    ],
                    [
                        532.0,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62155,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "2c3b5dd849aa7f4e2ea2a016e76d76fb"
        },
        "text": "TABLE II",
        "type": "Title"
    },
    {
        "element_id": "14ba2114a585a669b0944a4f5b806ad4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        171.3,
                        194.8
                    ],
                    [
                        171.3,
                        217.6
                    ],
                    [
                        799.8,
                        217.6
                    ],
                    [
                        799.8,
                        194.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64206,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "HYPERPARAMETERS OF ALPHASEQ FOR PHASE CODE DISCOVERY",
        "type": "FigureCaption"
    },
    {
        "element_id": "67b51a0a04aa93c9693c9b429819717d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        150.5,
                        231.1
                    ],
                    [
                        150.5,
                        724.3
                    ],
                    [
                        807.7,
                        724.3
                    ],
                    [
                        807.7,
                        231.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93388,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-11-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5",
            "text_as_html": "<table><thead><tr><th>Items</th><th>Parameters</th><th>Definitions</th></tr></thead><tbody><tr><td rowspan=\"3\">The Designed Game</td><td>K=1</td><td>Number of sequences in the target set</td></tr><tr><td>| y 5g</td><td>Length of each sequence</td></tr><tr><td>2-5</td><td>Number of symbols filled in each time step</td></tr><tr><td rowspan=\"3\">MCTS</td><td>q = 900</td><td>Number of simulations in one MCTS</td></tr><tr><td>a=0.1</td><td>Dirichlet noise</td></tr><tr><td>1 10-4 or 1) ii</td><td>Determines the way we calculate the move selection policy based on their visiting counts</td></tr><tr><td rowspan=\"5\">DNN</td><td>G = 300</td><td>Every G episodes, the DNN is updated using</td></tr><tr><td>z=2</td><td>the experiences accumulated in the latest z x G episodes</td></tr><tr><td>K'=5</td><td>Width of input image</td></tr><tr><td>N'=12</td><td>Length of input image</td></tr><tr><td>batch = 64</td><td>Mini-batch size</td></tr></tbody></table>"
        },
        "text": "Items Parameters Definitions K=1 Number of sequences in the target set The Designed |\u00bb. 59 Length of each sequence Game 2-5 Number of symbols filled in each time step q = 900 Number of simulations in one MCTS MCTS a=0.1 Dirichlet noise 1 10-4 or 1) Determines the way we calculate the move ii selection policy based on their visiting counts G = 300 Every G episodes, the DNN is updated using the experiences accumulated in the latest z x z=2 G episodes DNN K'=5 Width of input image N'=12 Length of input image batch = 64 Mini-batch size",
        "type": "Table"
    },
    {
        "element_id": "3e1809367fdbaf92f64d80038c8e62d6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        155.8
                    ],
                    [
                        866.7,
                        355.5
                    ],
                    [
                        1567.8,
                        355.5
                    ],
                    [
                        1567.8,
                        155.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "N K / = 12 time steps, where 60 symbols are obtained. Then, we ignore the last symbol and calculate the metric function and reward function following (18) and (19). The DNN update cycle G is set to 300 and z = 2. That is, every G = 300 episodes, DNN will be updated using the experiences accumulated in the latest 600 episodes.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2b42789f291932dd1dfcfad772bca7e7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        361.2
                    ],
                    [
                        866.7,
                        788.1
                    ],
                    [
                        1570.4,
                        788.1
                    ],
                    [
                        1570.4,
                        361.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94923,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "Given the huge solution space, it is challenging for our computer to train AlphaSeq to \ufb01nd the optimal solution. For one thing, each episode in this problem consumes much more time than the complementary code rediscovery problem in Section III, because of the larger number of MCTSs run in each episode and the larger number of simulations run in each MCTS. For another, the large solution space in this problem requires a massive number of exploration-dominant episodes so that AlphaSeq can visit enough number of states to gain familiarity with the whole solution space. As a result, the exploration phase will last a long time before AlphaSeq enters the exploitation phase. To tackle the above challenges, we use the following two techniques to accelerate the training process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7ec6fd4d021e070401ec78eabd727aa6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        772.8
                    ],
                    [
                        136.0,
                        999.8
                    ],
                    [
                        837.1,
                        999.8
                    ],
                    [
                        837.1,
                        772.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95265,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "later introduce a technique dubbed \u201csegmented induction\u201d to induce AlphaSeq to zoomed-in view to a good solution. In essence, segmented induction uses a smaller range of [(16/9N 3), 37], but progressively changes Mu and Ml as better M(s) is obtained (i.e., focus our search within a subspace of s each time, but progressively changing the focus of the subspace within which we search).",
        "type": "NarrativeText"
    },
    {
        "element_id": "e73efe3c813cc11fa51237261d8758cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1005.5
                    ],
                    [
                        136.0,
                        1132.8
                    ],
                    [
                        842.0,
                        1132.8
                    ],
                    [
                        842.0,
                        1005.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93688,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "Based on the metric function and reward function de\ufb01ned above, we implemented AlphaSeq and trained DNN to dis- cover a phase code for the MMF estimator. A Legendre sequence [13] is chosen as the benchmark.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3241f2366dcdc8703fbf1189ad44ca4e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.0,
                        796.9
                    ],
                    [
                        885.0,
                        824.9
                    ],
                    [
                        1562.3,
                        824.9
                    ],
                    [
                        1562.3,
                        796.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "1) Make more ef\ufb01cient use of experiences. Every G",
        "type": "ListItem"
    },
    {
        "element_id": "117d4af8990dd0dbd18c2fb22395e5a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        885.7,
                        804.2
                    ],
                    [
                        885.7,
                        1157.5
                    ],
                    [
                        1565.0,
                        1157.5
                    ],
                    [
                        1565.0,
                        804.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.892,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "episodes, we trained the DNN using the experiences accumulated in the latest zG episodes (zGN K / experiences in total) by stochastic gradient descent. In Section III, the minibatches were randomly sampled without replacement. That gave us zGN K //64 minibatches (64 was the minibatch size). Here, we want to make more ef\ufb01cient use of experiences. To this end, every G episodes, we randomly sample zGN K //64\u00d76 minibatches with replacement from the latest zGN K / experiences to train the ConvNets.",
        "type": "ListItem"
    },
    {
        "element_id": "2b8865892be685bd7efc2c4e9d15cf2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1137.8
                    ],
                    [
                        136.0,
                        1198.8
                    ],
                    [
                        839.6,
                        1198.8
                    ],
                    [
                        839.6,
                        1137.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92204,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "1) Benchmarks: We choose the Legendre sequence of length N = 59 as our benchmark",
        "type": "NarrativeText"
    },
    {
        "element_id": "ff5ef50a4ec8bcdbc51f83052744095b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        230.0,
                        1195.5
                    ],
                    [
                        230.0,
                        1223.2
                    ],
                    [
                        250.2,
                        1223.2
                    ],
                    [
                        250.2,
                        1195.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11,
            "parent_id": "9e618f053e0ebea2bb98ce157d95b1d5"
        },
        "text": "\u23a1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "822076425ad2076f0f2df068248ab6ea",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.0,
                        1209.5
                    ],
                    [
                        158.0,
                        1346.1
                    ],
                    [
                        804.5,
                        1346.1
                    ],
                    [
                        804.5,
                        1209.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81708,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "+1 +1 \u22121 +1 +1 \u22121 +1 \u22121 +1 \u22121 \u22121 \u22121 sL = \u23a2 \u23a3 +1 \u22121 +1 +1 \u22121 +1 +1 +1 \u22121 +1 \u22121 +1 \u22121 \u22121 +1 \u22121 \u22121 +1 +1 +1 \u22121 +1 +1 +1 \u23a5 \u23a6 . +1 \u22121 \u22121 +1 +1 +1 +1 +1 \u22121 \u22121 \u22121 \u22121 \u22121 +1 +1 \u22121 \u22121 \u22121 \u22121 +1 \u22121 \u22121 \u22121",
        "type": "Formula"
    },
    {
        "element_id": "f56ac3810394aecd3bf1075e9c0f58c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        750.0,
                        1195.5
                    ],
                    [
                        750.0,
                        1223.2
                    ],
                    [
                        770.2,
                        1223.2
                    ],
                    [
                        770.2,
                        1195.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "\u23a4",
        "type": "UncategorizedText"
    },
    {
        "element_id": "7c3734f7d7ec6099f12b8555924ae83d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1357.5
                    ],
                    [
                        136.0,
                        1454.4
                    ],
                    [
                        836.1,
                        1454.4
                    ],
                    [
                        836.1,
                        1357.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92883,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "For the MMF estimator, this Legendre sequence yields SIR \u03b3MMF \u2248 10.98. For reference, sL yields a merit factor of \u03b3MF \u2248 6.19 when the MF estimator is used.",
        "type": "NarrativeText"
    },
    {
        "element_id": "065af4af4e327f4b3eba79105121203c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1457.2
                    ],
                    [
                        136.0,
                        1783.8
                    ],
                    [
                        836.9,
                        1783.8
                    ],
                    [
                        836.9,
                        1457.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95594,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "For the corresponding AlphaSeq game, there are 59 symbols to \ufb01ll. The number of all possible sequence-set patterns is 259. The complexity of the exhaustive search for the global optimum is O(1018), and it would take more than one million years for our computer to \ufb01nd the optimal solution. In other words, the optimal solution of s when N = 59 is unavailable. In this context, the second benchmark we choose is a random search. For a random search, we randomly create 59-symbol sequences and record the maximum SIR obtained given a \ufb01xed budget of random trials.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b430b0d8ff697fa0cb651a4378ba5f98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1788.8
                    ],
                    [
                        136.0,
                        2082.5
                    ],
                    [
                        835.2,
                        2082.5
                    ],
                    [
                        835.2,
                        1788.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95618,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "2) Implementation: In the AlphaSeq implementation, the parameter settings are listed in Table II. As seen in the table, we aim to discover one sequence of length 59 wherein K = 1 and N = 59 in the AlphaSeq game. The number of symbols \ufb01lled in each time step is set to = 5, and the ConvNets takes 5\u00d712 images as input. To feed an intermediate state (i.e., a partially \ufb01lled 1 \u00d7 59 pattern) into the ConvNets, we \ufb01rst transform it to a 5 \u00d7 12 image (the missing 1 symbol will be padded with 0). A complete sequence is obtained after",
        "type": "NarrativeText"
    },
    {
        "element_id": "dfa88864c08ba1906ba55747aa66e9bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        879.4,
                        1155.5
                    ],
                    [
                        879.4,
                        1943.0
                    ],
                    [
                        1575.5,
                        1943.0
                    ],
                    [
                        1575.5,
                        1155.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74555,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "2) Segmented Induction: This technique is particularly use- ful when the upper and lower bounds of the metric function span a large range, or when there is no way to bound the metric function. The essence of segmented induction is to segment the large range of the metric function to several small ranges and de\ufb01ne the linear reward in small ranges rather than in a single large range. To be more speci\ufb01c, assuming a metric function with values within the range [0, D]. Then, rather than initializing the search range Ml = 0 and Mu = D in (19), we segment [0, D] to three small overlapping ranges6 [0, D/2], [D/3, 2D/3], and [D/2, D] and de\ufb01ne the linear reward in these small ranges: in episode 0, we de\ufb01ne the reward function in the \ufb01rst small range and initialize Ml = 0 and Mu = D/2. With the training of DNN, AlphaSeq is able to discover better and better sequences in the range [0, D/2]. When AlphaSeq discov- ers sequences with reward approaching 1 (i.e., the mean metric function of the found sequences approaches D/2), we then rede\ufb01ne the reward with the second range [D/3, 2D/3]. That is, we set Ml = D/3, Mu = 2D/3, and let AlphaSeq discovers sequences in the second small range. When AlphaSeq is able to discover sequences",
        "type": "ListItem"
    },
    {
        "element_id": "35f22348dfa74877b6ca1ae27fe86a7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        922.0,
                        1926.5
                    ],
                    [
                        922.0,
                        1954.2
                    ],
                    [
                        1564.2,
                        1954.2
                    ],
                    [
                        1564.2,
                        1926.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "with reward approaching 1 again, we rede\ufb01ne the reward",
        "type": "NarrativeText"
    },
    {
        "element_id": "a106ee229ad62cfaf80c0873326e4636",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1979.1
                    ],
                    [
                        866.7,
                        2081.3
                    ],
                    [
                        1566.8,
                        2081.3
                    ],
                    [
                        1566.8,
                        1979.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93413,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "6a) Nonoverlapping intervals are inadvisable. Experimental results show that AlphaSeq cannot learn well when using nonoverlapping intervals. b) The small ranges segmented here are for illustration purpose only. In general, we need to design the ranges according to the speci\ufb01cs in different problems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0e2ae97568438a430bd162d7504b4ea7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.4
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78551,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a0093ed70ff8e019a6ab70000eaa55b9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.8,
                        82.6
                    ],
                    [
                        1521.8,
                        106.0
                    ],
                    [
                        1565.9,
                        106.0
                    ],
                    [
                        1565.9,
                        82.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80842,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 11
        },
        "text": "3329",
        "type": "Header"
    },
    {
        "element_id": "9459d8898cd096e49971c86bb2d02e7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.8,
                        82.8
                    ],
                    [
                        133.8,
                        105.2
                    ],
                    [
                        178.5,
                        105.2
                    ],
                    [
                        178.5,
                        82.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77009,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "3330",
        "type": "Header"
    },
    {
        "element_id": "f9068bd9ab1660eb3078e5d0253cc497",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        574.0,
                        85.3
                    ],
                    [
                        574.0,
                        104.8
                    ],
                    [
                        1564.1,
                        104.8
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8191,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "255c8ebafe46c185826e5f486b613fe6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        180.4,
                        165.3
                    ],
                    [
                        180.4,
                        552.7
                    ],
                    [
                        790.3,
                        552.7
                    ],
                    [
                        790.3,
                        165.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93963,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-12-15.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "35 30 F 2k p Metric AphaSeqimesn) 42 No. of visited States Legendre (MMF) L L L L L 1 1 0 2000 4000 6000 8000 70000 \u2014 72000 14000\u00b0 Episodes",
        "type": "Image"
    },
    {
        "element_id": "28f239e0109deda6235d223d5c223b63",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        754.6,
                        162.8
                    ],
                    [
                        754.6,
                        173.9
                    ],
                    [
                        778.0,
                        173.9
                    ],
                    [
                        778.0,
                        162.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-12-16.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "e46a96a7a12febbcca3981adf200f862",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        942.7,
                        159.2
                    ],
                    [
                        942.7,
                        608.1
                    ],
                    [
                        1488.3,
                        608.1
                    ],
                    [
                        1488.3,
                        159.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92461,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-12-17.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "35 Random Search - = Legendre sequence 30 | |-B\u2014AlphaSeg 10\u00b0 104 108 10\u00b0 107 10\u00b0 10\u00b0 No. of visited States",
        "type": "Image"
    },
    {
        "element_id": "5e70b300c7d1c76be80ac368327fa84d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        582.8
                    ],
                    [
                        136.0,
                        679.6
                    ],
                    [
                        833.4,
                        679.6
                    ],
                    [
                        833.4,
                        582.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94856,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "Fig. 7. RL process of AlphaSeq to discover a phase-coded sequence for pulse compression radar. Mean metric E[M], maximum metric max[M], and a total number of visited states versus episodes, where the DNN update cycle G = 300 and z = 2.",
        "type": "FigureCaption"
    },
    {
        "element_id": "3d8429d5ac973f0569fce77209a0d004",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        632.8
                    ],
                    [
                        866.7,
                        704.6
                    ],
                    [
                        1566.3,
                        704.6
                    ],
                    [
                        1566.3,
                        632.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92213,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "Fig. 8. Searching capability comparison of AlphaSeq and random search. The AlphaSeq curve is the maximal metric max[M] versus the number of visited states.",
        "type": "FigureCaption"
    },
    {
        "element_id": "f47f671b381f7b24e049cedc8667eb9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        186.8,
                        714.5
                    ],
                    [
                        186.8,
                        875.2
                    ],
                    [
                        835.9,
                        875.2
                    ],
                    [
                        835.9,
                        714.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94651,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "in the third small range, and so on and so forth. Overall, with a smaller range at a given time, the slope of the reward function in (19) increases, allowing AlphaSeq to distinguish the relative quality of different sequences with higher contrast.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9bee064d6f49aa7bce79a5ac756ed294",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.8,
                        924.5
                    ],
                    [
                        133.8,
                        952.2
                    ],
                    [
                        451.0,
                        952.2
                    ],
                    [
                        451.0,
                        924.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48645,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "C. Performance Evaluation",
        "type": "NarrativeText"
    },
    {
        "element_id": "a697a7d433434ddd3679ef4a8d93c09e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        960.8
                    ],
                    [
                        136.0,
                        1226.8
                    ],
                    [
                        833.9,
                        1226.8
                    ],
                    [
                        833.9,
                        960.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95402,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "For training, we ran AlphaSeq over 1.44 \u00d7 104 episodes, generating 1.73 \u00d7 105 experiences in total. As in Section III, to monitor the evolution of AlphaSeq, every G = 300 episodes when the DNN was updated, we evaluated the searching capability of AlphaSeq by using AlphaSeq (with the updated DNN) to play 50 noiseless games and recorded their mean metric E[M] and maximum metric max[M]. Fig. 7 shows the E[M] and max[M] versus episodes during the process of RL.",
        "type": "NarrativeText"
    },
    {
        "element_id": "43871a792b94c50ddbf8074059555d2b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        932.2,
                        744.5
                    ],
                    [
                        932.2,
                        1187.9
                    ],
                    [
                        1496.9,
                        1187.9
                    ],
                    [
                        1496.9,
                        744.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94274,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-12-18.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "= = Legendre (MF) \u2014G ~ Legendre (MMF) EH AlphaSeq (MMF) # g 10% 10\u00b0 10? 107 2 o",
        "type": "Image"
    },
    {
        "element_id": "4939a574d601ad04c7d78ebe1445f6bd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1232.5
                    ],
                    [
                        136.0,
                        1362.4
                    ],
                    [
                        838.0,
                        1362.4
                    ],
                    [
                        838.0,
                        1232.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9369,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "As can be seen, the \ufb01rst 3300 episodes are the exploration-dominant phase and the episodes after that are the exploitation-dominant phase. After 1.26 \u00d7 104 episodes, AlphaSeq discovers a sequence with metric M(salpha) \u2248 33.45",
        "type": "NarrativeText"
    },
    {
        "element_id": "ea9a54beb60bd22b17c0dd8c3ca11714",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        246.3,
                        1360.2
                    ],
                    [
                        246.3,
                        1387.8
                    ],
                    [
                        266.5,
                        1387.8
                    ],
                    [
                        266.5,
                        1360.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "\u23a1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "537c7943d10a96e95db0b8f62c019d9c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        152.7,
                        1374.1
                    ],
                    [
                        152.7,
                        1512.0
                    ],
                    [
                        813.5,
                        1512.0
                    ],
                    [
                        813.5,
                        1374.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90102,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "+1 +1 +1 +1 +1 +1 +1 +1 +1 +1 +1 +1 salpha = \u23a2 \u23a3 +1 +1 +1 +1 \u22121 \u22121 \u22121 \u22121 \u22121 \u22121 \u22121 \u22121 +1 +1 +1 \u22121 \u22121 +1 +1 \u22121 +1 +1 \u22121 +1 \u23a5 \u23a6 . \u22121 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 +1 \u22121",
        "type": "Formula"
    },
    {
        "element_id": "82dd8f56c16a78a8a9e1b6e9edcdbf4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        766.7,
                        1360.2
                    ],
                    [
                        766.7,
                        1387.8
                    ],
                    [
                        786.8,
                        1387.8
                    ],
                    [
                        786.8,
                        1360.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "\u23a4",
        "type": "UncategorizedText"
    },
    {
        "element_id": "fc08789b6fe58150b120dac55b41392a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.5,
                        1523.2
                    ],
                    [
                        133.5,
                        1584.2
                    ],
                    [
                        833.4,
                        1584.2
                    ],
                    [
                        833.4,
                        1523.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92034,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "Compared with the Legendre sequence, salpha triples the SIR at the output of an MMF estimator.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b34e3d0c9aa8a941f470ebaaf8d95ce1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        163.7,
                        1583.8
                    ],
                    [
                        163.7,
                        1617.5
                    ],
                    [
                        833.3,
                        1617.5
                    ],
                    [
                        833.3,
                        1583.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "Remark: In this implementation, the value range of M(s),",
        "type": "UncategorizedText"
    },
    {
        "element_id": "51f0ca2aaae59320f67b08b9261e99c4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1592.4
                    ],
                    [
                        136.0,
                        1850.2
                    ],
                    [
                        835.2,
                        1850.2
                    ],
                    [
                        835.2,
                        1592.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9554,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "i.e., [16/9N 3, 37] \u2248 [0, 37], is segmented to three small ranges [0, 15], [5, 25], and [10, 37]. In the \ufb01rst 8100 episodes, the linear reward is de\ufb01ned in the \ufb01rst small range [0, 15]: metric 0 corresponds to reward \u22121, and 15 corresponds to reward 1; from episode 8101 to 11400, the linear reward is de\ufb01ned in the second small range [5, 25]; after episode 11401, the linear reward is de\ufb01ned in the last small range [10, 37].",
        "type": "NarrativeText"
    },
    {
        "element_id": "26e3ae4a38aa96f80d078f93ef1ab71c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1218.2
                    ],
                    [
                        866.7,
                        1265.6
                    ],
                    [
                        1567.7,
                        1265.6
                    ],
                    [
                        1567.7,
                        1218.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91223,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "Fig. 9. MSE of salpha and sL for h0 estimation in pulse compression radar systems.",
        "type": "FigureCaption"
    },
    {
        "element_id": "e3cc2643666172c7e5350560a234be32",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1300.5
                    ],
                    [
                        866.7,
                        1660.5
                    ],
                    [
                        1567.5,
                        1660.5
                    ],
                    [
                        1567.5,
                        1300.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95635,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "a transcription of two curves in Fig. 7: we combine the two curves, max[/M] versus episodes and number of visited states versus episodes, into one curve here. Fig. 8 also shows the maximal metric versus the number of visited states for random search.\u2019 To get this curve, given a state-visit budget, we performed 20 runs of the experiments. For each run 7, we traced the maximum metric value obtained after a given number of random trials, denoted by Miax (nv), where n, is the number of trials, which correspond to the number of visited (terminal) states. The black curve in Fig. 8 is (1/20) \u00a9; M\u0130, (ny) (i.e., a mean-max curve).",
        "type": "NarrativeText"
    },
    {
        "element_id": "c05f1198565377793698ba31b72dca14",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        975.3,
                        1645.8
                    ],
                    [
                        975.3,
                        1666.8
                    ],
                    [
                        981.2,
                        1666.8
                    ],
                    [
                        981.2,
                        1645.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "i",
        "type": "Title"
    },
    {
        "element_id": "c07ce3db1df37208a81e1ddb6aaa9ca3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1022.7,
                        1645.4
                    ],
                    [
                        1022.7,
                        1666.4
                    ],
                    [
                        1058.8,
                        1666.4
                    ],
                    [
                        1058.8,
                        1645.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12
        },
        "text": "max",
        "type": "Title"
    },
    {
        "element_id": "f62a62eacf878f70bded03d31b82ec74",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1665.8
                    ],
                    [
                        866.7,
                        1859.5
                    ],
                    [
                        1566.2,
                        1859.5
                    ],
                    [
                        1566.2,
                        1665.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9559,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "As can be seen from Fig. 8, the largest metric that random search can \ufb01nd is on average a log-linear function of the num- ber visited states. After randomly visiting 108 states, the best sequence random search can \ufb01nd is on average with metric 11.71. On the other hand, AlphaSeq discovers sequences with max[M] = 33.45 after visiting only 4 \u00d7 107 states.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d17c6eb79e58a3417cc037adee2d014",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1855.5
                    ],
                    [
                        136.0,
                        2049.2
                    ],
                    [
                        835.6,
                        2049.2
                    ],
                    [
                        835.6,
                        1855.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95099,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "We next compare the searching capability of AlphaSeq with random search given the same complexity budget, where com- plexity is measured by the number of distinct visited states. For AlphaSeq, the visited states include both intermediate states and terminal states, while for a random search, only terminal states (i.e., completely \ufb01lled sequences) will be searched.",
        "type": "NarrativeText"
    },
    {
        "element_id": "99442e8a165e8398b6ac969649426e25",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1864.8
                    ],
                    [
                        866.7,
                        1959.2
                    ],
                    [
                        1564.5,
                        1959.2
                    ],
                    [
                        1564.5,
                        1864.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93335,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "Finally, we assess the estimation performance of salpha benchmarked against the Legendre sequence sL when used in a pulse compression radar system. In the simulation, we assume",
        "type": "NarrativeText"
    },
    {
        "element_id": "dd6edaf62eb4695638bc2f26b9e41afc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1987.4
                    ],
                    [
                        866.7,
                        2114.6
                    ],
                    [
                        1563.7,
                        2114.6
                    ],
                    [
                        1563.7,
                        1987.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93935,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "7In our technique report [21], a deep Q-learning (DQL)-based approach, named DQLSeq, is developed to solve the MDP associated with the sequence discovery problem. The performance comparison between AlphaSeq and DQLSeq can be found in Appendix E of [21]. Overall, DQLSeq converges faster than AlphaSeq but is prone to getting stuck in local optimum.",
        "type": "NarrativeText"
    },
    {
        "element_id": "083cb2bbc9c9c945f162e59f78ef6fb2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        2054.8
                    ],
                    [
                        136.0,
                        2142.8
                    ],
                    [
                        843.1,
                        2142.8
                    ],
                    [
                        843.1,
                        2054.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92426,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "In Fig. 8, the AlphaSeq curve is the maximal metric max[M] versus the number of visited states. This curve is",
        "type": "NarrativeText"
    },
    {
        "element_id": "d11ed4b627e6a7072e97b667bce6dce6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2123.7
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 12,
            "parent_id": "c07ce3db1df37208a81e1ddb6aaa9ca3"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e55e3a8323ba6a589c035f05d0b3a2ca",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        682.2,
                        104.7
                    ],
                    [
                        682.2,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84093,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "37d2ce983af55f1972691e96fa741872",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        161.5
                    ],
                    [
                        136.0,
                        424.7
                    ],
                    [
                        835.8,
                        424.7
                    ],
                    [
                        835.8,
                        161.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9592,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "e55e3a8323ba6a589c035f05d0b3a2ca"
        },
        "text": "that the radar radiates pulse internally modulated by Saipha or s\u0131. The received signal is given in (10), where (h,) are Gaussian random variables with zero mean and same vari- ance o?, and AWGN noise is ignored. The receiver estimates ho using an MMF estimator, and we measure the estimation performance by MSE e = (ho \u2014 ho)?. Fig. 9 shows MSE versus o? for Salpha and s1. As can be seen, Saipha Outperforms s\u0131, and the MSE gains are up to about 5.23 dB.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3d7eeac9ba360b3c15b3ea9ee7d17777",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        383.0,
                        450.5
                    ],
                    [
                        383.0,
                        478.2
                    ],
                    [
                        587.1,
                        478.2
                    ],
                    [
                        587.1,
                        450.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83406,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "e55e3a8323ba6a589c035f05d0b3a2ca"
        },
        "text": "V. CONCLUSION",
        "type": "Title"
    },
    {
        "element_id": "dd1556feba2a220917588e0dc853280e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        492.5
                    ],
                    [
                        136.0,
                        653.2
                    ],
                    [
                        834.3,
                        653.2
                    ],
                    [
                        834.3,
                        492.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94579,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "3d7eeac9ba360b3c15b3ea9ee7d17777"
        },
        "text": "This article has demonstrated the power of DRL for sequence discovery. We believe that sequence discovery by DRL is a good supplement to sequence construction by mathe- matical tools, especially for problems with complex objectives intractable to mathematical analysis.",
        "type": "NarrativeText"
    },
    {
        "element_id": "92c840b3dae5f5f5c021dbdae91f23be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.6,
                        658.5
                    ],
                    [
                        154.6,
                        686.2
                    ],
                    [
                        764.3,
                        686.2
                    ],
                    [
                        764.3,
                        658.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82614,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "3d7eeac9ba360b3c15b3ea9ee7d17777"
        },
        "text": "Our speci\ufb01c contributions and results are as follows.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f5283a081e57f90fc97e03db8710ac81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        688.2
                    ],
                    [
                        154.7,
                        1014.5
                    ],
                    [
                        839.2,
                        1014.5
                    ],
                    [
                        839.2,
                        688.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94761,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "3d7eeac9ba360b3c15b3ea9ee7d17777"
        },
        "text": "1) We proposed a new DRL-based paradigm, AlphaSeq, to algorithmically discover a set of sequences with the desired property. AlphaSeq leverages the DRL framework of AlphaGo to solve an MDP associated with the sequence discovery problem. The MDP is a symbol-\ufb01lling game, where a player follows a policy to consecutively \ufb01ll symbols in the vacant positions of a sequence set. In particular, AlphaSeq treats the intermediate states in the MDP as images and makes use of DNN to recognize them.",
        "type": "ListItem"
    },
    {
        "element_id": "74d8ac2aa3720df8b7de90c74289715e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        1020.2
                    ],
                    [
                        154.7,
                        1280.2
                    ],
                    [
                        838.0,
                        1280.2
                    ],
                    [
                        838.0,
                        1020.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95249,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "3d7eeac9ba360b3c15b3ea9ee7d17777"
        },
        "text": "2) We introduced two new techniques in AlphaSeq to accel- erate the training process. The \ufb01rst technique is to allow AlphaSeq to make moves at a time (i.e., \ufb01lling sequence positions at a time). The choice of is a complexity tradeoff between the MCTS and the DNN. The second technique, dubbed segmented induction, is to change the reward function progressively to guide AlphaSeq to good sequences in its learning process.",
        "type": "ListItem"
    },
    {
        "element_id": "eba5e98b29a757450975135eab36ae2e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        154.7,
                        1285.8
                    ],
                    [
                        154.7,
                        1612.5
                    ],
                    [
                        836.0,
                        1612.5
                    ],
                    [
                        836.0,
                        1285.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95308,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "3d7eeac9ba360b3c15b3ea9ee7d17777"
        },
        "text": "3) We demonstrated the searching capabilities of AlphaSeq in two applications: 1) in MC CDMA systems, we used AlphaSeq to rediscover a set of ideal complementary codes that can zero-force all potential interferences and 2) in pulse compression radar systems, we used AlphaSeq to discover a new phase-coded sequence that triples the SIR at the output of a MMF estimator, benchmarked against the well-known Legendre sequence. The MSE gains are up to 5.23 dB for the estimation of radar cross sections.",
        "type": "ListItem"
    },
    {
        "element_id": "aed4ec7d9c0af0018ed07e0b3b5d8454",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        409.0,
                        1637.2
                    ],
                    [
                        409.0,
                        1664.8
                    ],
                    [
                        562.9,
                        1664.8
                    ],
                    [
                        562.9,
                        1637.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81474,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "e55e3a8323ba6a589c035f05d0b3a2ca"
        },
        "text": "APPENDIX A",
        "type": "Title"
    },
    {
        "element_id": "edef7b78b7c5e37d18964b6aea0d6fe3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1679.2
                    ],
                    [
                        136.0,
                        1806.5
                    ],
                    [
                        834.2,
                        1806.5
                    ],
                    [
                        834.2,
                        1679.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93559,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "This appendix describes the implementation details of AlphaSeq. Other than some custom features for our purpose, the general implementation follows AlphaGo Zero [12] and AlphaZero [16]. The source code can be found at GitHub [30].",
        "type": "NarrativeText"
    },
    {
        "element_id": "c3efb5ee67ad9e64e4be7f503e10392a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        161.8
                    ],
                    [
                        866.7,
                        388.9
                    ],
                    [
                        1565.4,
                        388.9
                    ],
                    [
                        1565.4,
                        161.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94805,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "promising in the simulations, i.e., how to evaluate a vertex in MCTS. In standard MCTS algorithms, this vertex-evaluation is achieved by means of random rollouts. For example, for a new vertex encountered in each simulation, we run random rollout from this vertex to a leaf vertex such that a reward can be obtained (see [14] for more details). The randomly sampled rewards overall simulations are then used to evaluate a vertex.",
        "type": "NarrativeText"
    },
    {
        "element_id": "047a510eb38cf3dd8365893b9809ef46",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.3,
                        394.5
                    ],
                    [
                        866.3,
                        922.5
                    ],
                    [
                        1567.0,
                        922.5
                    ],
                    [
                        1567.0,
                        394.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9479,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "In AlphaGo/AlphaSeq, instead of random rollouts, DNN is introduced to evaluate a vertex. The only two ingredients needed for MCTS are a root vertex v0 and a DNN \u03c8\u03b8 . First, given the root vertex v0, a search tree can be constructed where each vertex contains 2 edges (since there are 2 possible moves for each state). Each edge, denoted by (vi , a j ), i = 0, 1, 2, . . . , j = 0, 1, 2, . . . , 2 \u2212 1, stores three statistics: a visit count N(vi , a j ), a mean reward Q(vi , a j ), and an edge-selection prior probability P(vi , a j ). Second, MCTS uses DNN \u03c8\u03b8 to evaluate each vertex (state). The input of \u03c8\u03b8 is vi and the output is ( P, R) = \u03c8\u03b8 (vi ). Speci\ufb01cally, each time we feed a vertex vi into the DNN, it outputs a policy estimation P and a reward estimation R. Each entry in distribution P is exactly the prior probability P(vi , a j ) for each edge of vertex vi , and R will be used for updating the mean reward Q(vi , a j ), given by (21) later.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d6d96ecbbd2fb864c15d3b1c5d9b0a37",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        925.8
                    ],
                    [
                        866.7,
                        1119.5
                    ],
                    [
                        1566.8,
                        1119.5
                    ],
                    [
                        1566.8,
                        925.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95183,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "MCTS is operated by means of look-ahead simulations. Speci\ufb01cally, at a root vertex v0, MCTS \ufb01rst initializes a \u201cvisited tree\u201d (this visited tree is used to record all the vertices visited in the MCTS. It is initialized to have only one root vertex) and runs q simulations on the visited tree. Each simulation proceeds as follows [12]:",
        "type": "NarrativeText"
    },
    {
        "element_id": "700a0b42e4f505fafc46ffdd8437cd07",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1120.5
                    ],
                    [
                        866.7,
                        1320.2
                    ],
                    [
                        1565.0,
                        1320.2
                    ],
                    [
                        1565.0,
                        1120.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95238,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "1) Select: All the simulations start from the root vertex v0 and \ufb01nish when a vertex that has not been seen is encountered for the \ufb01rst time. During a simulation, we always choose the edge that yields a maximum upper con\ufb01dence bound. Speci\ufb01cally, at each vertex vi , the simulation selects edge j \u2217 to visit, and",
        "type": "NarrativeText"
    },
    {
        "element_id": "4509377e7795581e1b39cb1b1b291743",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1033.7,
                        1326.8
                    ],
                    [
                        1033.7,
                        1354.5
                    ],
                    [
                        1055.6,
                        1354.5
                    ],
                    [
                        1055.6,
                        1326.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "aed4ec7d9c0af0018ed07e0b3b5d8454"
        },
        "text": "\u23a7",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d2907b66f3272d8b8eb5e7a5e00441f3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        886.0,
                        1348.5
                    ],
                    [
                        886.0,
                        1458.4
                    ],
                    [
                        1522.9,
                        1458.4
                    ],
                    [
                        1522.9,
                        1348.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8549,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "j \u2217 = arg max j \u23a8 \u23a9Q(vi , a j )+c p P(vi , a j ) j N(vi , a j ) 1+ N(vi , a j ) \u23ac \u23ad",
        "type": "Formula"
    },
    {
        "element_id": "a903c28a691c0f8a41a62b0e07160aba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        865.7,
                        1481.8
                    ],
                    [
                        865.7,
                        1543.2
                    ],
                    [
                        1564.6,
                        1543.2
                    ],
                    [
                        1564.6,
                        1481.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91916,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "where c p is a constant controls the tradeoff between explo- ration and exploitation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4bba1b88d3ee3e5e7e148103b529995b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1549.8
                    ],
                    [
                        866.7,
                        1812.5
                    ],
                    [
                        1565.1,
                        1812.5
                    ],
                    [
                        1565.1,
                        1549.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95439,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "2) Expand and Evaluate: When encountering a previ- ously unseen vertex v L (for the \ufb01rst simulation, this v L in fact, v0), is, giving, ( PL , R L PL = {PL ( j ) : the simulation evaluates it using DNN, ) = \u03c8\u03b8 (v L ), where the policy distribution j = 0, 1, 2, . . . , 2 \u2212 1}. Then, we add this new vertex v L to the visited tree, and the statistics of v L \u2019s edges are initialized by N(v L , a j ) = 0, Q(v L , a j ) = 0, and P(v L , a j ) = PL ( j ) for j = 0, 1, 2, . . . , 2 \u2212 1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "03e684eb6bc42a5795b9d1b244bd17e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1498.3,
                        1326.8
                    ],
                    [
                        1498.3,
                        1354.5
                    ],
                    [
                        1520.3,
                        1354.5
                    ],
                    [
                        1520.3,
                        1326.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "\u23ab",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8062094b115dc4e7dd2e8749c15ae149",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.3,
                        1849.2
                    ],
                    [
                        134.3,
                        1876.8
                    ],
                    [
                        242.1,
                        1876.8
                    ],
                    [
                        242.1,
                        1849.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76431,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "A. MCTS",
        "type": "Title"
    },
    {
        "element_id": "e0e8ce6b5a684f410a17f9b6380cb51c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1891.2
                    ],
                    [
                        136.0,
                        2085.2
                    ],
                    [
                        834.1,
                        2085.2
                    ],
                    [
                        834.1,
                        1891.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95221,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "8062094b115dc4e7dd2e8749c15ae149"
        },
        "text": "MCTS is performed at each intermediate state s; to deter- mine policy II(s;), and this is achieved by multiple look-ahead simulations along the tree. In the simulations, more promising vertices are visited frequently, while less promising vertices are visited less frequently. The problem is how to deter- mine which vertices are more promising and which are less",
        "type": "NarrativeText"
    },
    {
        "element_id": "33f81e8688ddf3c256f53832f66c277d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1811.2
                    ],
                    [
                        866.7,
                        1946.8
                    ],
                    [
                        1567.8,
                        1946.8
                    ],
                    [
                        1567.8,
                        1811.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93664,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "8062094b115dc4e7dd2e8749c15ae149"
        },
        "text": "3) Backup: After adding vertex v L to the visited tree, the simulation updates all the vertices along the trajectory of encountering v L . Speci\ufb01cally, for each edge (vi , a j ) on the trajectory (including v L ), we update",
        "type": "NarrativeText"
    },
    {
        "element_id": "d1bec94e5252384ab4b4a48d3e9de84c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        975.7,
                        1975.2
                    ],
                    [
                        975.7,
                        2011.1
                    ],
                    [
                        1276.2,
                        2011.1
                    ],
                    [
                        1276.2,
                        1975.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13,
            "parent_id": "8062094b115dc4e7dd2e8749c15ae149"
        },
        "text": "N(vi , a j ) = N(vi , a j ) + 1",
        "type": "NarrativeText"
    },
    {
        "element_id": "4c7fc73607e4a1651554a6eec1234b2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        986.4,
                        1980.2
                    ],
                    [
                        986.4,
                        2008.8
                    ],
                    [
                        1564.2,
                        2008.8
                    ],
                    [
                        1564.2,
                        1980.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65053,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "(20)",
        "type": "Formula"
    },
    {
        "element_id": "7a22aca0dfb6395a75a151f1e00b098b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1267.7,
                        2009.8
                    ],
                    [
                        1267.7,
                        2049.5
                    ],
                    [
                        1435.7,
                        2049.5
                    ],
                    [
                        1435.7,
                        2009.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "Q(vi , a j ) \u2212 R",
        "type": "NarrativeText"
    },
    {
        "element_id": "1eb820ef23e6c8d6bb3691a081bd4269",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        975.7,
                        2020.6
                    ],
                    [
                        975.7,
                        2089.5
                    ],
                    [
                        1564.6,
                        2089.5
                    ],
                    [
                        1564.6,
                        2020.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8203,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "Q(vi , a j ) = Q(vi , a j ) \u2212 N(vi , a j ) L . (21)",
        "type": "Formula"
    },
    {
        "element_id": "c38c5cb049a217856d6bad3367c06038",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.4
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78261,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3d672c875038129af741fc4235ac7ac2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.7,
                        82.4
                    ],
                    [
                        1521.7,
                        105.9
                    ],
                    [
                        1566.3,
                        105.9
                    ],
                    [
                        1566.3,
                        82.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80107,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 13
        },
        "text": "3331",
        "type": "Header"
    },
    {
        "element_id": "80093b2d3f0d3f2b3175e2e189b8ac2e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        134.3,
                        82.0
                    ],
                    [
                        134.3,
                        105.5
                    ],
                    [
                        177.5,
                        105.5
                    ],
                    [
                        177.5,
                        82.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77623,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "3332",
        "type": "Header"
    },
    {
        "element_id": "2275cac9429068bca7ced64df634878a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        573.9,
                        85.3
                    ],
                    [
                        573.9,
                        104.7
                    ],
                    [
                        1564.1,
                        104.7
                    ],
                    [
                        1564.1,
                        85.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83914,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 31, NO. 9, SEPTEMBER 2020",
        "type": "Header"
    },
    {
        "element_id": "47b4c649341aa704f1d41028d7d43f87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        145.5,
                        160.8
                    ],
                    [
                        145.5,
                        591.5
                    ],
                    [
                        829.9,
                        591.5
                    ],
                    [
                        829.9,
                        160.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94218,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-14-19.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "EE ee anl 1 \u2018Cony Layer: 2 lere of size 1X1, side 1, same padding Input: 2 batch of K\u0130N x3 images Cony Layer: | fier of \u2018size 1X I, stride I, same padding Convolutional a 5 Unit 1 ; Batch Normalization Batch Normalization 13 T Rectifier Nonlinearity Rectifier Nonlinearity : ge cones ~ Convolutional! T T T Unit2 H FClayer:to a hidden FC layer: toa hidden \u0130 i Rectifier Nonlinearity Convolutional | | Ban Normal, Batch Normalization T Unit 3 ; i 1 i Rector Nonlinear ect Nonlinearity : 1 : : | Felayertoa hidden Fe layer to aiden layer of 2 neurons, layer of 1 neuron, selamlar eee T P R Cony Layer: 256 iler ofsize 3% 3, stride 1 | Convolutional Unit 4",
        "type": "Image"
    },
    {
        "element_id": "806982d5a0000fbe007549c7573d9c0f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        613.5
                    ],
                    [
                        136.0,
                        685.8
                    ],
                    [
                        832.8,
                        685.8
                    ],
                    [
                        832.8,
                        613.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93816,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "Fig. 10. Deep ConvNets implemented in AlphaSeq. This ConvNets consists of six convolutional layers together with batch normalization and recti\ufb01er nonlinearities.",
        "type": "FigureCaption"
    },
    {
        "element_id": "8d7c3825f462455ed4054be63193dc85",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        152.5
                    ],
                    [
                        866.7,
                        192.1
                    ],
                    [
                        1564.2,
                        192.1
                    ],
                    [
                        1564.2,
                        152.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "the K \u00d7 N \u00d7 1 image, and X2(i, j ) = 0 elsewhere. The third",
        "type": "NarrativeText"
    },
    {
        "element_id": "df6870f858e8ab085c7cc5e19c543375",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        164.1
                    ],
                    [
                        866.7,
                        291.7
                    ],
                    [
                        1566.3,
                        291.7
                    ],
                    [
                        1566.3,
                        164.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93707,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "plane, X3, indicates the presentence of \u201c0\u201d in the K \u00d7 N \u00d7 1 image: X3(i, j ) = 1 if the intersection (i, j ) has value \u201c0\u201d in the K \u00d7 N \u00d7 1 image, and X3(i, j ) = 0 elsewhere.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cf8796a70b0c460d8441c960c89f3e8b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        293.5
                    ],
                    [
                        866.7,
                        423.5
                    ],
                    [
                        1564.5,
                        423.5
                    ],
                    [
                        1564.5,
                        293.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93833,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "3) Output: For each state si , DNN will output a policy estimation (i.e., a probability distribution) P(si ) = ( p0, p1,\u2026, p2\u22121) as the prior probability for the 2 edges of si , and a scalar estimation R \u2208 [\u22121, 1] on the expected reward of si .",
        "type": "NarrativeText"
    },
    {
        "element_id": "37795df04801a5c4cd0aff39bb336258",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        425.2
                    ],
                    [
                        866.7,
                        685.8
                    ],
                    [
                        1566.1,
                        685.8
                    ],
                    [
                        1566.1,
                        425.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95522,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "4) Training: Every G games, we use the experiences accu- mulated in the most recent z \u00d7 G games (i.e., zGN K / experiences) to update the DNN by stochastic gradient descent. The minibatch size is set to 64, and we randomly sample zGN K //64 minibatches without replacement from the zGN K / experiences to train the ConvNets. For each mini- batch, the loss function is de\ufb01ned to minimize the summation of mean-squared error and cross-entropy loss [12]",
        "type": "NarrativeText"
    },
    {
        "element_id": "27d07ed08f753ecd2a4f756568fb21d9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        707.2
                    ],
                    [
                        136.0,
                        771.1
                    ],
                    [
                        833.6,
                        771.1
                    ],
                    [
                        833.6,
                        707.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92567,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "After q simulations, MCTS then outputs a move selection probability for root vertex v0 by",
        "type": "NarrativeText"
    },
    {
        "element_id": "5b798e0440943ce92f364901bee2eaac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        263.3,
                        778.8
                    ],
                    [
                        263.3,
                        847.3
                    ],
                    [
                        833.6,
                        847.3
                    ],
                    [
                        833.6,
                        778.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8393,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "TM (vo) = softmax | - log N(vo, ap} . (22)",
        "type": "Formula"
    },
    {
        "element_id": "6339fad6ef7b0813ea1c3dce889b79b8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        996.0,
                        694.8
                    ],
                    [
                        996.0,
                        733.2
                    ],
                    [
                        1414.2,
                        733.2
                    ],
                    [
                        1414.2,
                        694.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "L=(R-RY \u2014 H\u0130 log P + cll ||",
        "type": "Title"
    },
    {
        "element_id": "af792b0b6bdb59dd179a9b12beb3aeb6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        998.9,
                        701.9
                    ],
                    [
                        998.9,
                        733.2
                    ],
                    [
                        1564.6,
                        733.2
                    ],
                    [
                        1564.6,
                        701.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66736,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "(23)",
        "type": "Formula"
    },
    {
        "element_id": "f06b64a2c897ff754420f3fc5a3e1552",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        752.5
                    ],
                    [
                        866.7,
                        813.8
                    ],
                    [
                        1565.0,
                        813.8
                    ],
                    [
                        1565.0,
                        752.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9215,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "where the last term is L2 regularization to prevent over\ufb01tting. Over the course of training, the learning rate is \ufb01xed to 10\u22124.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4683047f8e2543bd85bef011acb78a98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        862.5
                    ],
                    [
                        136.0,
                        1122.8
                    ],
                    [
                        834.8,
                        1122.8
                    ],
                    [
                        834.8,
                        862.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95686,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "For example, the move selection probability is determined by the visiting counts of the root vertex\u2019s edges. Parameter \u03c4 is a temperature parameter as in AlphaGo Zero [12]. In an episode, we set \u03c4 = 1 (i.e., the move-selection probability is proportional to the visiting counts of each edge, yielding more exploration) for the \ufb01rst one third time steps and \u03c4 = 10\u22124 (deterministically choose the move that has the most visiting counts) for the rest of the time steps.",
        "type": "NarrativeText"
    },
    {
        "element_id": "79b1b21598eeb493a3efd0bb8ae6c15a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1128.2
                    ],
                    [
                        136.0,
                        1388.5
                    ],
                    [
                        836.2,
                        1388.5
                    ],
                    [
                        836.2,
                        1128.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95666,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "In the training iteration, when we play games to provide experiences for DNN, the Dirichlet noise, i.e., Dir([\u03b10, \u03b11, . . . , \u03b1 ]) with positive real parameters \u03b10, \u03b11, . . . , \u03b1 2\u22121 is added to the prior probability of root node v0 to guarantee additional exploration. Thus, these games are called noisy games. Accordingly, there are noiseless games, in which the Dirichlet noise is removed. Generally, we play noiseless games to evaluate the performance of AlphaSeq with a trained DNN. 2\u22121,",
        "type": "NarrativeText"
    },
    {
        "element_id": "ad1bc596ba9a390aca861994cb071158",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1139.4,
                        841.5
                    ],
                    [
                        1139.4,
                        869.2
                    ],
                    [
                        1290.9,
                        869.2
                    ],
                    [
                        1290.9,
                        841.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83596,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "APPENDIX B",
        "type": "Title"
    },
    {
        "element_id": "0f4212b0c33d65e446f8e71722e5302b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        877.5
                    ],
                    [
                        894.3,
                        911.2
                    ],
                    [
                        1563.8,
                        911.2
                    ],
                    [
                        1563.8,
                        877.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "ad1bc596ba9a390aca861994cb071158"
        },
        "text": "This section derives the supremum of M(C) in (5) in",
        "type": "NarrativeText"
    },
    {
        "element_id": "07efa2b14f9e91a0edc8ed0cd829e1c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        887.9
                    ],
                    [
                        866.7,
                        977.5
                    ],
                    [
                        1565.1,
                        977.5
                    ],
                    [
                        1565.1,
                        887.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93595,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "ad1bc596ba9a390aca861994cb071158"
        },
        "text": "Section III. Given the de\ufb01nitions of the correlation functions in (2)\u2013(4), we \ufb01rst rewrite (5) as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "d89ec6f4b556641897b760a688475398",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        998.7,
                        986.8
                    ],
                    [
                        998.7,
                        1016.5
                    ],
                    [
                        1039.2,
                        1016.5
                    ],
                    [
                        1039.2,
                        986.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "ad1bc596ba9a390aca861994cb071158"
        },
        "text": "J\u20141",
        "type": "UncategorizedText"
    },
    {
        "element_id": "25697f59663116d1305864ead8057f3a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1053.3,
                        986.8
                    ],
                    [
                        1053.3,
                        1016.5
                    ],
                    [
                        1093.5,
                        1016.5
                    ],
                    [
                        1093.5,
                        986.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "ad1bc596ba9a390aca861994cb071158"
        },
        "text": "J-1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "c5f3c075684ed64e98d735c22fb6fbd6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1105.7,
                        986.8
                    ],
                    [
                        1105.7,
                        1016.5
                    ],
                    [
                        1148.5,
                        1016.5
                    ],
                    [
                        1148.5,
                        986.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "ad1bc596ba9a390aca861994cb071158"
        },
        "text": "N-1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "813913c4496ccdeef5cc18b1d8f125e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        887.6,
                        994.8
                    ],
                    [
                        887.6,
                        1186.1
                    ],
                    [
                        1551.0,
                        1186.1
                    ],
                    [
                        1551.0,
                        994.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88851,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "J\u20141 J-1 N-1 MO DY) DY CCE), loll + PCE, teli) j0 fr=j) v=1 J\u20141 J-1 +> SY cer; lOll. (24) A=0 jra=fitl",
        "type": "Formula"
    },
    {
        "element_id": "31474fba1f1e249fbe94290a3465895e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        1197.2
                    ],
                    [
                        894.3,
                        1224.8
                    ],
                    [
                        1323.1,
                        1224.8
                    ],
                    [
                        1323.1,
                        1197.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88504,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "For the second term in (24), we have",
        "type": "NarrativeText"
    },
    {
        "element_id": "a319b8c4403c5dc4c936e35c2fd82545",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        965.0,
                        1234.1
                    ],
                    [
                        965.0,
                        1263.8
                    ],
                    [
                        1005.5,
                        1263.8
                    ],
                    [
                        1005.5,
                        1234.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "J-1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f10e9a182cb23fcfd8c6072d7616d611",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1033.0,
                        1234.1
                    ],
                    [
                        1033.0,
                        1263.8
                    ],
                    [
                        1073.5,
                        1263.8
                    ],
                    [
                        1073.5,
                        1234.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "J-1",
        "type": "UncategorizedText"
    },
    {
        "element_id": "f6b89e298a3c94a23def3ab95ab37137",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        965.0,
                        1243.2
                    ],
                    [
                        965.0,
                        1331.4
                    ],
                    [
                        1564.6,
                        1331.4
                    ],
                    [
                        1564.6,
                        1243.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8467,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "j1=0 j2= j1+1 |CCF j1, j2 [0]| \u2264 J (J \u2212 1) 2 N M. (25)",
        "type": "Formula"
    },
    {
        "element_id": "910c614a9364d019a427c13b0b1b4d7f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        894.3,
                        1336.5
                    ],
                    [
                        894.3,
                        1370.2
                    ],
                    [
                        1564.3,
                        1370.2
                    ],
                    [
                        1564.3,
                        1336.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "Moreover, the \ufb01rst term in (24) can be simpli\ufb01ed as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "8302f6c1e265f19770f6fab1295bd9bd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1430.5
                    ],
                    [
                        136.0,
                        1458.2
                    ],
                    [
                        228.3,
                        1458.2
                    ],
                    [
                        228.3,
                        1430.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79329,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "B. DNN",
        "type": "Title"
    },
    {
        "element_id": "eb4f03f2e03320c78a7d86171d4ca81a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1473.5
                    ],
                    [
                        136.0,
                        1600.8
                    ],
                    [
                        834.7,
                        1600.8
                    ],
                    [
                        834.7,
                        1473.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93728,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "8302f6c1e265f19770f6fab1295bd9bd"
        },
        "text": "The DNN implemented in AlphaSeq is a deep convolutional network (ConvNets). This ConvNets consists of six convolu- tional layers together with batch normalization and recti\ufb01er nonlinearities, the details of which are shown in Fig. 10.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6af7d848412282ac0c1a347f5833154d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        163.7,
                        1596.8
                    ],
                    [
                        163.7,
                        1634.2
                    ],
                    [
                        833.4,
                        1634.2
                    ],
                    [
                        833.4,
                        1596.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "8302f6c1e265f19770f6fab1295bd9bd"
        },
        "text": "1) Input: The ConvNets takes K \u00d7 N \u00d7 3 image stack as",
        "type": "ListItem"
    },
    {
        "element_id": "fdba07a4370e9d86e2734d7367c1f665",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1607.4
                    ],
                    [
                        136.0,
                        1800.2
                    ],
                    [
                        835.1,
                        1800.2
                    ],
                    [
                        835.1,
                        1607.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95437,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "8302f6c1e265f19770f6fab1295bd9bd"
        },
        "text": "input. For a state si (i.e., an K \u00d7 N partially \ufb01lled sequence-set pattern), we \ufb01rst transform it to a K \u00d7 N \u00d7 1 image (in general, we set K = and N = N K /; zero-padding if K N > K N), and then perform feature extraction to trans- form it to a K \u00d7 N \u00d7 3 image stack.",
        "type": "NarrativeText"
    },
    {
        "element_id": "607f7cf40c5ac5b88d83c37748bea118",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1805.5
                    ],
                    [
                        136.0,
                        2101.7
                    ],
                    [
                        833.9,
                        2101.7
                    ],
                    [
                        833.9,
                        1805.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95103,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14,
            "parent_id": "8302f6c1e265f19770f6fab1295bd9bd"
        },
        "text": "2) Feature Extraction: Feature extraction is a process to transform a K \u00d7 N \u00d7 1 image to a K \u00d7 N \u00d7 3 image stack comprising three binary feature planes. The three binary feature planes are constructed as follows. The \ufb01rst plane, X1, indicates the presentence of \u201c1\u201d in the K \u00d7 N \u00d7 1 image: X1(i, j ) = 1 if the intersection (i, j ) has value \u201c1\u201d in the K \u00d7 N \u00d7 1 image, and X1(i, j ) = 0 elsewhere. The second plane, X2, indicates the presentence of \u201c\u22121\u201d in the K \u00d7 N \u00d71 image: X2(i, j ) = 1 if the intersection (i, j ) has value \u201c\u22121\u201d in",
        "type": "NarrativeText"
    },
    {
        "element_id": "f058944ffda21158f96cc58d9f20d21c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        918.0,
                        1380.5
                    ],
                    [
                        918.0,
                        1681.4
                    ],
                    [
                        1538.6,
                        1681.4
                    ],
                    [
                        1538.6,
                        1380.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82952,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "J\u20141 J-1 N-1 DX DY CCE) lll + PCE; loll) A=0 joa=j) v=1 J\u20141 J-1 N-1 = DY) 5S davi + Atoll + lalol \u2014 Atoll) A=0 ja=j, v=1 J\u20141 J-1 N-1 = DD > 2max(latol, |All) ji=0 jo=ji v=",
        "type": "Formula"
    },
    {
        "element_id": "52c8927104449cc71f05d65d37a6bf9f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1689.1
                    ],
                    [
                        866.7,
                        1778.8
                    ],
                    [
                        1563.9,
                        1778.8
                    ],
                    [
                        1563.9,
                        1689.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90856,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "where a[v] = Mt Nol cl [nlc [n + ol, and Blo] = M-1~N-1 m m i m=0 Qan=N\u2014\u00bb Gill + 0].",
        "type": "NarrativeText"
    },
    {
        "element_id": "53cc1753b89b2a3dbd3d2f533ac9ae66",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1768.5
                    ],
                    [
                        866.7,
                        1868.5
                    ],
                    [
                        1566.2,
                        1868.5
                    ],
                    [
                        1566.2,
                        1768.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92458,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "Note that set C is binary, hence, \u03b1[v] and \u03b2[v] are summa- tions of (N \u2212 v)M and v M terms of 1 or \u22121, respectively. As a result, we have",
        "type": "NarrativeText"
    },
    {
        "element_id": "80803121546407f57d9c3faad047d73b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        862.1,
                        1876.8
                    ],
                    [
                        862.1,
                        2099.2
                    ],
                    [
                        1552.9,
                        2099.2
                    ],
                    [
                        1552.9,
                        1876.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88797,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "N-1 N-1 2 max(laloll, 181) < > 2max{(N \u2014 v)M,vM} v=1 3N7\u20144N41 v=1 M, for N odd 2 3N?\u20144N 2 M, for N even.",
        "type": "Formula"
    },
    {
        "element_id": "08fd6a42bd8891a01035119ed0f36e9f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.8,
                        2122.5
                    ],
                    [
                        144.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2122.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79707,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6bea04aca620cfafe3e060f3a62f9d58",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1517.7,
                        1123.2
                    ],
                    [
                        1517.7,
                        1150.8
                    ],
                    [
                        1564.2,
                        1150.8
                    ],
                    [
                        1564.2,
                        1123.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "(24)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "a8fc35b4d8fdfcd4d02df1fb8edf7d2f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1518.0,
                        1618.8
                    ],
                    [
                        1518.0,
                        1646.5
                    ],
                    [
                        1564.6,
                        1646.5
                    ],
                    [
                        1564.6,
                        1618.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 14
        },
        "text": "(26)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "9d974c30da8fcfddb5ea84d496859c8f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        85.1
                    ],
                    [
                        136.0,
                        104.7
                    ],
                    [
                        682.8,
                        104.7
                    ],
                    [
                        682.8,
                        85.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84851,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "SHAO et al.: ALPHASEQ: SEQUENCE DISCOVERY WITH DRL",
        "type": "Header"
    },
    {
        "element_id": "c633f1d7a9a11425c60c1ee553900368",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1521.7,
                        82.4
                    ],
                    [
                        1521.7,
                        105.5
                    ],
                    [
                        1566.5,
                        105.5
                    ],
                    [
                        1566.5,
                        82.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80279,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "3333",
        "type": "Header"
    },
    {
        "element_id": "865d4d0a3c00c60d850e0036454b436f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        163.7,
                        155.8
                    ],
                    [
                        163.7,
                        189.5
                    ],
                    [
                        833.6,
                        189.5
                    ],
                    [
                        833.6,
                        155.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "c633f1d7a9a11425c60c1ee553900368"
        },
        "text": "Finally, we can bound M(C) from (25) and (26) as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "aab3743eda53a9df3b179021dce9b5ec",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        233.0,
                        182.5
                    ],
                    [
                        233.0,
                        210.2
                    ],
                    [
                        254.9,
                        210.2
                    ],
                    [
                        254.9,
                        182.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "c633f1d7a9a11425c60c1ee553900368"
        },
        "text": "\u23a7",
        "type": "UncategorizedText"
    },
    {
        "element_id": "a584eb59758f115b67988aca28dc316a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        135.4,
                        201.5
                    ],
                    [
                        135.4,
                        337.5
                    ],
                    [
                        813.3,
                        337.5
                    ],
                    [
                        813.3,
                        201.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84018,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "(3N 2 +1)(J 2+ J )\u22122N J (J +3) \u23aa\u23a8 M, for N odd 4 M(C)\u2264 3N 2(J 2 + J )\u22122N J (J +3) \u23aa\u23a9 M, for N even. 4",
        "type": "Formula"
    },
    {
        "element_id": "33e79f4ff8df228265198edf6ee49b29",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        337.8
                    ],
                    [
                        136.0,
                        371.5
                    ],
                    [
                        833.6,
                        371.5
                    ],
                    [
                        833.6,
                        337.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "The bounds can be achieved by an all \u22121 (or an all \u22121)",
        "type": "NarrativeText"
    },
    {
        "element_id": "3d634c3829f4a0f1b4fa6719c5ec9001",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        133.6,
                        346.4
                    ],
                    [
                        133.6,
                        404.5
                    ],
                    [
                        828.7,
                        404.5
                    ],
                    [
                        828.7,
                        346.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91947,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "sequence set, hence is tight.",
        "type": "NarrativeText"
    },
    {
        "element_id": "758dba7dc4be9c0b2d53f69aa0d8b2a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        165.9
                    ],
                    [
                        866.7,
                        213.3
                    ],
                    [
                        1563.3,
                        213.3
                    ],
                    [
                        1563.3,
                        165.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91562,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[27] J. Jedwab, \u201cWhat can be used instead of a Barker sequence?\u201d Contemp. Math., vol. 461, pp. 153\u2013178, Feb. 2008.",
        "type": "ListItem"
    },
    {
        "element_id": "a40c7a2a86fa19923608f8829066074e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        214.5
                    ],
                    [
                        866.7,
                        286.3
                    ],
                    [
                        1563.9,
                        286.3
                    ],
                    [
                        1563.9,
                        214.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92777,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[28] V.-P. Kaasila and A. Mammela, \u201cBit error probability of a matched \ufb01lter in a Rayleigh fading multipath channel,\u201d IEEE Trans. Commun., vol. 42, no. 234, pp. 826\u2013828, Apr. 1994.",
        "type": "ListItem"
    },
    {
        "element_id": "b2d81d2be9a3af2003eec3635a137e4b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        287.5
                    ],
                    [
                        866.7,
                        359.3
                    ],
                    [
                        1563.9,
                        359.3
                    ],
                    [
                        1563.9,
                        287.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93079,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[29] M. H. Ackroyd and F. Ghani, \u201cOptimum mismatched \ufb01lters for sidelobe suppression,\u201d IEEE Trans. Aerosp. Electron. Syst., vol. AES-9, no. 2, pp. 214\u2013218, Mar. 1973.",
        "type": "ListItem"
    },
    {
        "element_id": "ab9477f112ae1210d554cf11528d1d88",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        360.2
                    ],
                    [
                        866.7,
                        432.5
                    ],
                    [
                        1563.2,
                        432.5
                    ],
                    [
                        1563.2,
                        360.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92862,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[30] Y. Shao, S. C. Liew, and T. Wang. (2018). AlphaSeq: Sequence Discovery with Deep Reinforcement Learning. [Online]. Available: https://github.com/lintonshaw/AlphaSeq",
        "type": "ListItem"
    },
    {
        "element_id": "478701df2012c8fd6c94c88e6bf4ff06",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        407.3,
                        437.5
                    ],
                    [
                        407.3,
                        465.2
                    ],
                    [
                        562.3,
                        465.2
                    ],
                    [
                        562.3,
                        437.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "REFERENCES",
        "type": "Title"
    },
    {
        "element_id": "d223b0da51413833994705b2bf6b667c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        481.6
                    ],
                    [
                        147.0,
                        528.6
                    ],
                    [
                        832.9,
                        528.6
                    ],
                    [
                        832.9,
                        481.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91074,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[1] OEIS. (1964). The On-Line Encyclopedia of Integer Sequences. [Online]. Available: https://oeis.org/A000040",
        "type": "ListItem"
    },
    {
        "element_id": "1123684476057653f948f66b92cdb737",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        529.8
                    ],
                    [
                        147.0,
                        577.0
                    ],
                    [
                        833.3,
                        577.0
                    ],
                    [
                        833.3,
                        529.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91339,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[2] J. M. Heather and B. Chain, \u201cThe sequence of sequencers: The history of sequencing DNA,\u201d Genomics, vol. 107, no. 1, pp. 1\u20138, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "97898dde9003acfd7b29888a751661fa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        577.9
                    ],
                    [
                        147.0,
                        625.0
                    ],
                    [
                        834.3,
                        625.0
                    ],
                    [
                        834.3,
                        577.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.912,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[3] A. J. Viterbi, CDMA: Principles of Spread Spectrum Communication, vol. 122. Reading, MA, USA: Addison-Wesley, 1995.",
        "type": "ListItem"
    },
    {
        "element_id": "2ec1007a7518df6e044d98b1461d6f5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        625.9
                    ],
                    [
                        147.0,
                        673.3
                    ],
                    [
                        832.6,
                        673.3
                    ],
                    [
                        832.6,
                        625.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91145,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[4] M. I. Skolnik, Radar Handbook. New York, NY, USA: McGraw-Hill, 2008.",
        "type": "ListItem"
    },
    {
        "element_id": "618b0d2f3425bc8613fe95ea12cc3745",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.0,
                        674.2
                    ],
                    [
                        146.0,
                        771.3
                    ],
                    [
                        833.7,
                        771.3
                    ],
                    [
                        833.7,
                        674.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93954,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[5] Y. Chen, Y.-H. Lo, K. W. Shum, W. S. Wong, and Y. Zhang, \u201cCRT sequences with applications to collision channels allowing successive interference cancellation,\u201d IEEE Trans. Inf. Theory, vol. 64, no. 4, pp. 2910\u20132923, Apr. 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "8f02b69421f2bd2fb88e1d8fbfb53e7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        771.9
                    ],
                    [
                        147.0,
                        819.3
                    ],
                    [
                        833.3,
                        819.3
                    ],
                    [
                        833.3,
                        771.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91754,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15,
            "parent_id": "478701df2012c8fd6c94c88e6bf4ff06"
        },
        "text": "[6] R. S. Sutton and A. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "7ffb2a42ac422ed6b62625f183f2b465",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        864.0,
                        554.5
                    ],
                    [
                        864.0,
                        809.3
                    ],
                    [
                        1069.3,
                        809.3
                    ],
                    [
                        1069.3,
                        554.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-15-20.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "20807c7ed57ef69dae5859423026fd45",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1091.6,
                        548.2
                    ],
                    [
                        1091.6,
                        720.0
                    ],
                    [
                        1567.5,
                        720.0
                    ],
                    [
                        1567.5,
                        548.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91527,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Yulin Shao (S\u201917) received the B.E. and M.S. degrees from Xidian University (XDU), Xi\u2019an, China, in 2013 and 2016, respectively. He is currently pursuing the Ph.D. degree with the Department of Information Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d12abd1178a040935e319cc74d57c06e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1089.2,
                        722.8
                    ],
                    [
                        1089.2,
                        819.6
                    ],
                    [
                        1569.8,
                        819.6
                    ],
                    [
                        1569.8,
                        722.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83673,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "He was a Research Assistant with the Institute of Network Coding, CUHK, from March 2015 to August 2016. He was a Visiting Scholar with the Research Laboratory of Electronics, Massachusetts",
        "type": "NarrativeText"
    },
    {
        "element_id": "93064e0dd2f3b6a11f6f16ffd6d1d4ad",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        820.2
                    ],
                    [
                        147.0,
                        867.6
                    ],
                    [
                        833.1,
                        867.6
                    ],
                    [
                        833.1,
                        820.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91421,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[7] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming. Hoboken, NJ, USA: Wiley, 2014.",
        "type": "ListItem"
    },
    {
        "element_id": "5944e268e2ec69455b8abe133ca8cb55",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.0,
                        868.2
                    ],
                    [
                        147.0,
                        915.6
                    ],
                    [
                        833.5,
                        915.6
                    ],
                    [
                        833.5,
                        868.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91522,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[8] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, p. 436, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "c5ffc08750d340cf31c9952e29d468a6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        821.5
                    ],
                    [
                        866.7,
                        919.3
                    ],
                    [
                        1563.8,
                        919.3
                    ],
                    [
                        1563.8,
                        821.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68611,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Institute of Technology, Cambridge, MA, USA, from September 2018 to March 2019. His current research interests include signal processing, fundamentals of wireless communications and networking, and machine learning (deep reinforcement learning, in particular).",
        "type": "NarrativeText"
    },
    {
        "element_id": "4059f02a73c44b27a4f492ac81989a4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.8,
                        916.6
                    ],
                    [
                        146.8,
                        963.6
                    ],
                    [
                        833.4,
                        963.6
                    ],
                    [
                        833.4,
                        916.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91301,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[9] V. Mnih et al., \u201cHuman-level control through deep reinforcement learn- ing,\u201d Nature, vol. 518, no. 7540, p. 529, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "2ecd9b8f14ca20e31793b994554ab3cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        964.6
                    ],
                    [
                        136.0,
                        1012.0
                    ],
                    [
                        836.0,
                        1012.0
                    ],
                    [
                        836.0,
                        964.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91452,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[10] D. Silver et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "44c0dcec4fc74a74d736d074e4dc5d1e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1013.2
                    ],
                    [
                        136.0,
                        1060.0
                    ],
                    [
                        833.2,
                        1060.0
                    ],
                    [
                        833.2,
                        1013.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91317,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[11] Y. Li, \u201cDeep reinforcement learning: An overview,\u201d arXiv:1701.07274. [Online]. Available: https://arxiv.org/abs/1701.07274 2017,",
        "type": "ListItem"
    },
    {
        "element_id": "733568c7cc0ad758b53812032ba26e60",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1060.9
                    ],
                    [
                        136.0,
                        1108.3
                    ],
                    [
                        832.6,
                        1108.3
                    ],
                    [
                        832.6,
                        1060.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90942,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[12] D. Silver et al., \u201cMastering the game of go without human knowledge,\u201d Nature, vol. 550, no. 7676, p. 354, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "4fc6549b0d15b6f9f8a5ede674987a44",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1109.2
                    ],
                    [
                        136.0,
                        1156.3
                    ],
                    [
                        839.1,
                        1156.3
                    ],
                    [
                        839.1,
                        1109.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91517,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[13] M. Golay, \u201cThe merit factor of Legendre sequences (corresp.),\u201d IEEE Trans. Inf. Theory, vol. IT-29, no. 6, pp. 934\u2013936, Nov. 1983.",
        "type": "ListItem"
    },
    {
        "element_id": "b07aae28da74be9ff27808ff5a62ecc7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1157.2
                    ],
                    [
                        136.0,
                        1229.3
                    ],
                    [
                        836.7,
                        1229.3
                    ],
                    [
                        836.7,
                        1157.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92973,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[14] C. B. Browne et al., \u201cA survey of Monte Carlo tree search methods,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 4, no. 1, pp. 1\u201343, Mar. 2012.",
        "type": "ListItem"
    },
    {
        "element_id": "f0066c96b451eb65f4a4df3848a5fce5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1230.5
                    ],
                    [
                        136.0,
                        1302.6
                    ],
                    [
                        833.3,
                        1302.6
                    ],
                    [
                        833.3,
                        1230.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93491,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[15] L. Kocsis and C. Szepesv\u00e1ri, \u201cBandit based Monte-Carlo planning,\u201d in Proc. Eur. Conf. Mach. Learn. Berlin, Germany: Springer, 2006, pp. 282\u2013293.",
        "type": "ListItem"
    },
    {
        "element_id": "30416d7ddfd0ddfab3f0e5ad3e01a616",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        864.9,
                        1036.4
                    ],
                    [
                        864.9,
                        1289.9
                    ],
                    [
                        1068.6,
                        1289.9
                    ],
                    [
                        1068.6,
                        1036.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-15-21.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "6b27edc14ee3bb2c6c36c3079a2b2556",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1091.3,
                        1029.6
                    ],
                    [
                        1091.3,
                        1126.6
                    ],
                    [
                        1569.2,
                        1126.6
                    ],
                    [
                        1569.2,
                        1029.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91563,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Soung Chang Liew (S\u201984\u2013M\u201987\u2013SM\u201992\u2013F\u201912) received the S.B., S.M., E.E., and Ph.D. degrees from the Massachusetts Institute of Technology (MIT), Cambridge, MA, USA.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e379fb327401e663337d94a429ef2b8e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1081.3,
                        1128.3
                    ],
                    [
                        1081.3,
                        1301.0
                    ],
                    [
                        1563.9,
                        1301.0
                    ],
                    [
                        1563.9,
                        1128.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87842,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "From 1984 to 1988, he was with the MIT Laboratory for Information and Decision Systems, Cambridge, where he investigated \ufb01ber-optic communications networks. From March 1988 to July 1993, he was with Bellcore (now Telcordia), Piscataway, NJ, USA, where he engaged in broad- band network research. Since 1993, he has been",
        "type": "NarrativeText"
    },
    {
        "element_id": "dd6e007d3172ecfb617d67ce202406d8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1303.2
                    ],
                    [
                        136.0,
                        1375.6
                    ],
                    [
                        832.9,
                        1375.6
                    ],
                    [
                        832.9,
                        1303.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93191,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[16] D. Silver et al., \u201cMastering chess and shogi by self-play with a general reinforcement learning algorithm,\u201d 2017, arXiv:1712.01815. [Online]. Available: https://arxiv.org/abs/1712.01815",
        "type": "ListItem"
    },
    {
        "element_id": "8339eb3c431682ee644564650ef48c0d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1376.6
                    ],
                    [
                        136.0,
                        1423.6
                    ],
                    [
                        834.2,
                        1423.6
                    ],
                    [
                        834.2,
                        1376.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90992,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[17] H. H. Chen, The Next Generation CDMA Technologies. Hoboken, NJ, USA: Wiley, 2007.",
        "type": "ListItem"
    },
    {
        "element_id": "7cd0e6554b98bb4d41046fe4a639133c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1424.8
                    ],
                    [
                        136.0,
                        1496.6
                    ],
                    [
                        833.3,
                        1496.6
                    ],
                    [
                        833.3,
                        1424.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93551,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[18] J. M. Velazquez-Gutierrez and C. Vargas-Rosales, \u201cSequence sets in wireless communication systems: A survey,\u201d IEEE Commun. Surveys Tuts., vol. 19, no. 2, pp. 1225\u20131248, 2nd Quart., 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "164a33e9420a476dfaf58a9cf8b60dfb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        865.9,
                        1303.7
                    ],
                    [
                        865.9,
                        1500.3
                    ],
                    [
                        1564.8,
                        1500.3
                    ],
                    [
                        1564.8,
                        1303.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92922,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "a Professor with the Department of Information Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong. He is currently the Division Head of the Department of Information Engineering and a Co-Director of the Institute of Network Coding with CUHK. He is also serving as a Board Member for the Hong Kong Applied Science and Technology Institute (ASTRI), Hong Kong. He holds 15 U.S. patents. His current research interests include wireless networks, Internet of Things, intelligent transport systems, Internet protocols, multimedia communications, and packet switch design.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bfe035fae52820c25d991d1cf4fb504c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1497.8
                    ],
                    [
                        136.0,
                        1594.6
                    ],
                    [
                        835.5,
                        1594.6
                    ],
                    [
                        835.5,
                        1497.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9387,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[19] H.-H. Chen, J.-F. Yeh, and N. Suehiro, \u201cA multicarrier CDMA archi- tecture based on orthogonal complementary codes for new generations of wideband wireless communications,\u201d IEEE Commun. Mag., vol. 39, no. 10, pp. 126\u2013135, Oct. 2001.",
        "type": "ListItem"
    },
    {
        "element_id": "b783589f7c4204480d49c2c6347e43a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1503.2
                    ],
                    [
                        866.7,
                        1575.0
                    ],
                    [
                        1566.7,
                        1575.0
                    ],
                    [
                        1566.7,
                        1503.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92478,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Dr. Liew is a fellow of IET and HKIE. He was a recipient of the \ufb01rst Vice-Chancellor Exemplary Teaching Award in 2000 and the Research Excellence Award in 2013 at the Chinese University of Hong Kong.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7772269a64358ae02a0a0fa8f70309cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1595.8
                    ],
                    [
                        136.0,
                        1668.0
                    ],
                    [
                        833.0,
                        1668.0
                    ],
                    [
                        833.0,
                        1595.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93074,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[20] L. Welch, \u201cLower bounds on the maximum cross correlation of signals (Corresp.),\u201d IEEE Trans. Inf. Theory, vol. IT-20, no. 3, pp. 397\u2013399, May 1974.",
        "type": "ListItem"
    },
    {
        "element_id": "582589d27a4815ab6f1a110aa696e9ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1668.8
                    ],
                    [
                        136.0,
                        1741.0
                    ],
                    [
                        833.7,
                        1741.0
                    ],
                    [
                        833.7,
                        1668.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93686,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[21] Y. Shao, S. C. Liew, and T. Wang, \u201cAlphaSeq: Sequence discov- ery with deep reinforcement learning,\u201d 2018, [Online]. Available: https://arxiv.org/abs/1810.01218",
        "type": "ListItem"
    },
    {
        "element_id": "58810552a5cbed041ea189cbb2c19f9a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1741.6
                    ],
                    [
                        136.0,
                        1789.0
                    ],
                    [
                        833.7,
                        1789.0
                    ],
                    [
                        833.7,
                        1741.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90803,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[22] A. Boehmer, \u201cBinary pulse compression codes,\u201d IEEE Trans. Inf. Theory, vol. IT-13, no. 2, pp. 156\u2013167, Apr. 1967.",
        "type": "ListItem"
    },
    {
        "element_id": "a67a85125ee78454e45929c9634138f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1790.2
                    ],
                    [
                        136.0,
                        1862.0
                    ],
                    [
                        835.4,
                        1862.0
                    ],
                    [
                        835.4,
                        1790.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93124,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[23] R. M. Davis, R. L. Facnte, and R. P. Perry, \u201cPhase-coded waveforms for radar,\u201d IEEE Trans. Aerosp. Electron. Syst., vol. 43, no. 1, pp. 401\u2013408, Jan. 2007.",
        "type": "ListItem"
    },
    {
        "element_id": "f8543706cc1269ef36c8df6fcdd4382d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1863.2
                    ],
                    [
                        136.0,
                        1935.3
                    ],
                    [
                        835.5,
                        1935.3
                    ],
                    [
                        835.5,
                        1863.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93564,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[24] P. Stoica, J. Li, and M. Xue, \u201cOn sequences with good correlation properties: A new perspective,\u201d in Proc. IEEE Inf. Theory Workshop (ITW), Jul. 2007, pp. 1\u20135.",
        "type": "ListItem"
    },
    {
        "element_id": "06957a0bee732c0a819fa80fe771354d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        1936.2
                    ],
                    [
                        136.0,
                        2008.3
                    ],
                    [
                        834.4,
                        2008.3
                    ],
                    [
                        834.4,
                        1936.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93393,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[25] M. Golay, \u201cThe merit factor of long low autocorrelation binary sequences,\u201d IEEE Trans. Inf. Theory, vol. IT-28, no. 3, pp. 543\u2013549, May 1982.",
        "type": "ListItem"
    },
    {
        "element_id": "ca9f23030ddeba0b7ae5afdb0c24d5b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        136.0,
                        2008.9
                    ],
                    [
                        136.0,
                        2081.3
                    ],
                    [
                        834.9,
                        2081.3
                    ],
                    [
                        834.9,
                        2008.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92765,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "[26] T. H\u00f8holdt, \u201cThe merit factor problem for binary sequences,\u201d in Proc. Int. Symp. Appl. Algebra, Algebr. Algorithms, Error-Correcting Codes (AAECC). Berlin, Germany: Springer, 2006, pp. 51\u201359.",
        "type": "ListItem"
    },
    {
        "element_id": "d45fadd60517e7ba011d3e0ec8e12994",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        865.0,
                        1685.1
                    ],
                    [
                        865.0,
                        1947.9
                    ],
                    [
                        1078.4,
                        1947.9
                    ],
                    [
                        1078.4,
                        1685.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-15-22.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "a",
        "type": "Image"
    },
    {
        "element_id": "c543d9880d453f8e1cd6b1cd94c542b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1094.4,
                        1685.2
                    ],
                    [
                        1094.4,
                        1907.0
                    ],
                    [
                        1565.4,
                        1907.0
                    ],
                    [
                        1565.4,
                        1685.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92932,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Taotao Wang (M\u201916) received the B.S. degree in electrical engineering from the University of Elec- tronic Science and Technology of China, Chengdu, China, in 2008, the M.S. degree in information and signal processing from the Beijing University of Posts and Telecommunications, Beijing, China, in 2011, and the Ph.D. degree in information engi- neering from The Chinese University of Hong Kong (CUHK), Hong Kong, in 2015.",
        "type": "NarrativeText"
    },
    {
        "element_id": "537c763103b19fb198830f7b4ead66df",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        866.7,
                        1908.6
                    ],
                    [
                        866.7,
                        2056.3
                    ],
                    [
                        1572.1,
                        2056.3
                    ],
                    [
                        1572.1,
                        1908.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72864,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "From 2015 to 2016, he was a Post-Doctoral Research Fellow with the Institute of Network Cod- ing, CUHK. He joined the College of Information Engineering, Shenzhen University, Shenzhen, China, as an Assistant Professor. His current research interests include wireless communications and networking, statistical signal and data processing, and blockchain networks.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a3f75c0d342a673478de4ad5483879a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.7,
                        2059.2
                    ],
                    [
                        883.7,
                        2081.3
                    ],
                    [
                        1461.7,
                        2081.3
                    ],
                    [
                        1461.7,
                        2059.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81264,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Dr. Wang was a recipient of the Hong Kong Ph.D. Fellowship.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ddee96e1448fee501b3e67b6982bf4eb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        140.8,
                        2121.8
                    ],
                    [
                        140.8,
                        2143.2
                    ],
                    [
                        1555.2,
                        2143.2
                    ],
                    [
                        1555.2,
                        2121.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82794,
            "file_directory": "./uol-docs",
            "filename": "AlphaSeq_Sequence_Discovery_With_Deep_Reinforcement_Learning.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:12:43",
            "page_number": 15
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:12:31 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    }
]