[
    {
        "element_id": "c6075821fc2c0ce30afe8b52cf6f0790",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        0.0,
                        200.0
                    ],
                    [
                        0.0,
                        272.2
                    ],
                    [
                        72.2,
                        272.2
                    ],
                    [
                        72.2,
                        200.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "Check for updates.",
        "type": "Image"
    },
    {
        "element_id": "77c4937c1b365a32f056b58fd2521361",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.2,
                        95.2
                    ],
                    [
                        148.2,
                        118.4
                    ],
                    [
                        308.0,
                        118.4
                    ],
                    [
                        308.0,
                        95.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82437,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "4e6c48aa0eab124e08b9670cb718b8e9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1041.4,
                        96.8
                    ],
                    [
                        1041.4,
                        118.1
                    ],
                    [
                        1553.9,
                        118.1
                    ],
                    [
                        1553.9,
                        96.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86035,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "bede9bfa7602b3a1b6db93ea10704637",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        192.7,
                        229.0
                    ],
                    [
                        192.7,
                        332.2
                    ],
                    [
                        1508.6,
                        332.2
                    ],
                    [
                        1508.6,
                        229.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58178,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "4e6c48aa0eab124e08b9670cb718b8e9"
        },
        "text": "Playing Games in the Dark: An Approach for Cross-Modality Transfer in Reinforcement Learning",
        "type": "Title"
    },
    {
        "element_id": "100b1b19880609b8a924027bb946fd35",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        213.7,
                        361.6
                    ],
                    [
                        213.7,
                        566.3
                    ],
                    [
                        576.3,
                        566.3
                    ],
                    [
                        576.3,
                        361.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81838,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "bede9bfa7602b3a1b6db93ea10704637"
        },
        "text": "Rui Silva INESC-ID & Instituto Superior T\u00e9cnico, Universidade de Lisboa Lisbon, Portugal Carnegie Mellon University Pittsburgh, PA, USA",
        "type": "NarrativeText"
    },
    {
        "element_id": "7cb66d51d0a604eb5090886c0082fbcd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        665.1,
                        361.6
                    ],
                    [
                        665.1,
                        501.5
                    ],
                    [
                        1032.2,
                        501.5
                    ],
                    [
                        1032.2,
                        361.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81751,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "bede9bfa7602b3a1b6db93ea10704637"
        },
        "text": "Miguel Vasco INESC-ID & Instituto Superior T\u00e9cnico, Universidade de Lisboa Lisbon, Portugal",
        "type": "NarrativeText"
    },
    {
        "element_id": "8a115fdc1c2fee1995c2ee843528db85",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1125.7,
                        361.6
                    ],
                    [
                        1125.7,
                        501.5
                    ],
                    [
                        1488.2,
                        501.5
                    ],
                    [
                        1488.2,
                        361.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87012,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "bede9bfa7602b3a1b6db93ea10704637"
        },
        "text": "Francisco S. Melo INESC-ID & Instituto Superior T\u00e9cnico, Universidade de Lisboa Lisbon, Portugal",
        "type": "NarrativeText"
    },
    {
        "element_id": "d6027bf82e22512b6a8dcbbb86718d0f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        552.2,
                        590.2
                    ],
                    [
                        552.2,
                        623.4
                    ],
                    [
                        690.6,
                        623.4
                    ],
                    [
                        690.6,
                        590.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "4e6c48aa0eab124e08b9670cb718b8e9"
        },
        "text": "Ana Paiva",
        "type": "Title"
    },
    {
        "element_id": "d810b7883fc75b76db8ff1279b5baaf6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        440.3,
                        599.4
                    ],
                    [
                        440.3,
                        732.3
                    ],
                    [
                        802.9,
                        732.3
                    ],
                    [
                        802.9,
                        599.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65703,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "d6027bf82e22512b6a8dcbbb86718d0f"
        },
        "text": "INESC-ID & Instituto Superior T\u00e9cnico, Universidade de Lisboa Lisbon, Portugal",
        "type": "NarrativeText"
    },
    {
        "element_id": "a321f1eb5045e71b71c7da0cf0360a0d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        916.9,
                        590.2
                    ],
                    [
                        916.9,
                        696.2
                    ],
                    [
                        1242.9,
                        696.2
                    ],
                    [
                        1242.9,
                        590.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.60818,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "d6027bf82e22512b6a8dcbbb86718d0f"
        },
        "text": "Manuela Veloso Carnegie Mellon University Pittsburgh, PA, USA",
        "type": "NarrativeText"
    },
    {
        "element_id": "70a087ee33ceac6493093a8fa8a5f9fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        748.3
                    ],
                    [
                        149.4,
                        778.6
                    ],
                    [
                        311.0,
                        778.6
                    ],
                    [
                        311.0,
                        748.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "4e6c48aa0eab124e08b9670cb718b8e9"
        },
        "text": "ABSTRACT",
        "type": "Title"
    },
    {
        "element_id": "1c94dd7c1d527cd18417e8a15a3f3a81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        791.7
                    ],
                    [
                        148.5,
                        1185.4
                    ],
                    [
                        825.1,
                        1185.4
                    ],
                    [
                        825.1,
                        791.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95529,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "70a087ee33ceac6493093a8fa8a5f9fe"
        },
        "text": "In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different sub- sets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality\u2014for ex- ample, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6e6398625ef699be7739c6bb5bca9f89",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        922.6,
                        743.8
                    ],
                    [
                        922.6,
                        1014.2
                    ],
                    [
                        1510.5,
                        1014.2
                    ],
                    [
                        1510.5,
                        743.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89502,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "RL (\u00a9 \u011fi B ~ Image 7 A Sound 7",
        "type": "Image"
    },
    {
        "element_id": "2d3af7237c956124dceb641e41418676",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1037.7
                    ],
                    [
                        883.2,
                        1062.6
                    ],
                    [
                        1550.5,
                        1062.6
                    ],
                    [
                        1550.5,
                        1037.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "Figure 1: Concrete scenario where a policy trained over one",
        "type": "NarrativeText"
    },
    {
        "element_id": "b6f3405367640547180725aa1acdab78",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1043.9
                    ],
                    [
                        883.2,
                        1127.7
                    ],
                    [
                        1555.0,
                        1127.7
                    ],
                    [
                        1555.0,
                        1043.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92215,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "input modality (game videoframes) is transferred to a differ- ent modality (game sound).",
        "type": "FigureCaption"
    },
    {
        "element_id": "56c506c49ece41cc0a2b16e3eb2e2d79",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1214.0
                    ],
                    [
                        149.4,
                        1250.7
                    ],
                    [
                        321.3,
                        1250.7
                    ],
                    [
                        321.3,
                        1214.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84662,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "KEYWORDS",
        "type": "Title"
    },
    {
        "element_id": "c2d012d5adffae3d45dc48dfef8e6270",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.3,
                        1257.4
                    ],
                    [
                        148.3,
                        1288.0
                    ],
                    [
                        667.2,
                        1288.0
                    ],
                    [
                        667.2,
                        1257.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86212,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "56c506c49ece41cc0a2b16e3eb2e2d79"
        },
        "text": "Deep Reinforcement Learning; Multi-task learning",
        "type": "NarrativeText"
    },
    {
        "element_id": "29434c21a3dce4b34838bd79dc070872",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.6,
                        1305.3
                    ],
                    [
                        148.6,
                        1332.7
                    ],
                    [
                        392.7,
                        1332.7
                    ],
                    [
                        392.7,
                        1305.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8205,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "ACM Reference Format:",
        "type": "Title"
    },
    {
        "element_id": "3ab7933b2b56a5b9760315ae2d7b57e2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.1,
                        1332.9
                    ],
                    [
                        147.1,
                        1470.8
                    ],
                    [
                        833.1,
                        1470.8
                    ],
                    [
                        833.1,
                        1332.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93607,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "29434c21a3dce4b34838bd79dc070872"
        },
        "text": "Rui Silva, Miguel Vasco, Francisco S. Melo, Ana Paiva, and Manuela Veloso. 2020. Playing Games in the Dark: An Approach for Cross-Modality Transfer in Reinforcement Learning. In Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), Auckland, New Zealand, May 9\u201313, 2020, IFAAMAS, 9 pages.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8843102e7a559b27e5c3d1e02dc67bf9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1523.7
                    ],
                    [
                        149.4,
                        1554.0
                    ],
                    [
                        434.8,
                        1554.0
                    ],
                    [
                        434.8,
                        1523.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "1 INTRODUCTION",
        "type": "Title"
    },
    {
        "element_id": "3022c509277d408452465f9fba1482be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1567.1
                    ],
                    [
                        148.5,
                        1839.4
                    ],
                    [
                        824.4,
                        1839.4
                    ],
                    [
                        824.4,
                        1567.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9566,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "8843102e7a559b27e5c3d1e02dc67bf9"
        },
        "text": "Recent works have shown how low-dimensional representations captured by generative models can be successfully exploited in re- inforcement learning (RL) settings. Among others, these generative models have been used to learn low-dimensional latent represen- tations of the state space to improve the learning efficiency of RL algorithms [6, 19], or to allow the generalization of policies learned on a source domain to other target domains [4, 5, 7]. The Disen- tAngled Representation Learning Agent (DARLA) approach [7], in particular, builds such latent representations using variational",
        "type": "NarrativeText"
    },
    {
        "element_id": "5210305890a88032d8cdbcedb10e3305",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1178.7
                    ],
                    [
                        883.2,
                        1268.8
                    ],
                    [
                        1557.3,
                        1268.8
                    ],
                    [
                        1557.3,
                        1178.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93619,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "8843102e7a559b27e5c3d1e02dc67bf9"
        },
        "text": "autoencoder (VAE) models [8, 14], and shows how learning dis- entangled features of the observed environment can allow an RL agent to learn a policy robust to shifts in the original domain.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a4458b66a0320873d0779075c9a921c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1270.0
                    ],
                    [
                        910.9,
                        1294.9
                    ],
                    [
                        1554.7,
                        1294.9
                    ],
                    [
                        1554.7,
                        1270.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "8843102e7a559b27e5c3d1e02dc67bf9"
        },
        "text": "In this work, we explore the application of these latent repre-",
        "type": "NarrativeText"
    },
    {
        "element_id": "64768cae17485b8eca6f929670862c69",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1277.8
                    ],
                    [
                        883.2,
                        1909.6
                    ],
                    [
                        1556.0,
                        1909.6
                    ],
                    [
                        1556.0,
                        1277.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94493,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1,
            "parent_id": "8843102e7a559b27e5c3d1e02dc67bf9"
        },
        "text": "sentations in capturing different input sensory modalities to be considered in the context of RL tasks. We build upon recent work that extends VAE methods to learn joint distributions of multiple modalities, by forcing the individual latent representations of each modality to be similar [16, 18]. These multimodal VAEs allow for cross-modality inference, replicating more closely what seems to be the nature of the multimodal representation learning performed by humans [3, 11]. Inspired by these advances, we explore the impact of such multimodal latent representations in allowing a reinforce- ment learning agent to learn and exploit policies over different input modalities. Among others, we envision scenarios where RL agents are provided the ability of learning a visual policy (policy learned over image inputs), and then (re-)using such policy at test time when only sound inputs are available. For example, in au- tonomous cars, which are equipped with cameras and microphones, the sound input could be used to detect the sound of sleeper lines if the cameras stop working momentarily. Figure 1 instantiates another application to the case of video games\u2014a policy is learned over images and then re-used when only the game sounds are available, i.e., when playing \u201cin the dark\u201d.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d8f8ae471ea7c38e549a8b9ae7875f25",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        143.9,
                        1881.8
                    ],
                    [
                        143.9,
                        1963.4
                    ],
                    [
                        826.9,
                        1963.4
                    ],
                    [
                        826.9,
                        1881.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82694,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May 9\u201313, 2020, Auckland, New Zealand. \u00a9 2020 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
        "type": "Footer"
    },
    {
        "element_id": "28a6c37cfac72a012ff32281e53c8bb7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1909.3
                    ],
                    [
                        910.9,
                        1934.2
                    ],
                    [
                        1554.8,
                        1934.2
                    ],
                    [
                        1554.8,
                        1909.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "To achieve this, we contribute an approach for multimodal trans-",
        "type": "NarrativeText"
    },
    {
        "element_id": "ccfaf0fd3552d0aa836dd60f9ed67fdd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1917.4
                    ],
                    [
                        883.2,
                        1967.7
                    ],
                    [
                        1555.2,
                        1967.7
                    ],
                    [
                        1555.2,
                        1917.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91973,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "fer reinforcement learning, which effectively allows an RL agent",
        "type": "NarrativeText"
    },
    {
        "element_id": "1562f254b98afea4d9a6810ffa3586bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.7,
                        2085.9
                    ],
                    [
                        828.7,
                        2109.0
                    ],
                    [
                        872.7,
                        2109.0
                    ],
                    [
                        872.7,
                        2085.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81836,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 1
        },
        "text": "1260",
        "type": "Footer"
    },
    {
        "element_id": "57633f229928c6806f999c9f224706b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.8,
                        97.5
                    ],
                    [
                        148.8,
                        118.3
                    ],
                    [
                        307.2,
                        118.3
                    ],
                    [
                        307.2,
                        97.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73769,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "aeba0f3397cd695ea872c57fac2cc0d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1042.8,
                        97.7
                    ],
                    [
                        1042.8,
                        117.8
                    ],
                    [
                        1549.9,
                        117.8
                    ],
                    [
                        1549.9,
                        97.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84389,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "046c584313fe708e874389879ab47d46",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        183.0,
                        229.2
                    ],
                    [
                        183.0,
                        612.3
                    ],
                    [
                        1527.2,
                        612.3
                    ],
                    [
                        1527.2,
                        229.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91548,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-2-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "(a)  (b)  (c) ",
        "type": "Image"
    },
    {
        "element_id": "1fa48d81249f827219b6cb26ed050e8d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        641.1
                    ],
                    [
                        149.4,
                        666.0
                    ],
                    [
                        1552.1,
                        666.0
                    ],
                    [
                        1552.1,
                        641.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "Figure 2: Networks of different generative models, highlighting the models\u2019 data encoding (orange) and decoding (green)",
        "type": "NarrativeText"
    },
    {
        "element_id": "81afe8b4d7ddb1b469486f064bd154f5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.5,
                        647.6
                    ],
                    [
                        144.5,
                        792.9
                    ],
                    [
                        1555.8,
                        792.9
                    ],
                    [
                        1555.8,
                        647.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83946,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "pipelines. The similarity constraints imposed by the training procedures are presented in dashed lines. 2a) The VAE model learns a latent representation of the data distribution of a single modality. 2b) The AVAE model extends the VAEs to account for multiple modalities, allowing for cross-modality inference. 2c) The JMVAE model learns a representation of both modalities, allowing for both single and joint modality reconstruction, and cross-modality inference.",
        "type": "NarrativeText"
    },
    {
        "element_id": "10727e5b8ccfa908dd139c8d1c06559d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        841.1
                    ],
                    [
                        149.4,
                        866.0
                    ],
                    [
                        821.0,
                        866.0
                    ],
                    [
                        821.0,
                        841.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "to learn robust policies over input modalities, achieving better out-",
        "type": "NarrativeText"
    },
    {
        "element_id": "6c23423e1ebfb292486a5aa6169328eb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.1,
                        851.8
                    ],
                    [
                        146.1,
                        1387.1
                    ],
                    [
                        823.3,
                        1387.1
                    ],
                    [
                        823.3,
                        851.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95128,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "of-the-box performance when compared to different baselines. We start by first learning a generalized latent space over the different input modalities that the agent has access to. This latent space is con- structed using a multimodal generative model, allowing the agent to establish mappings between the different modalities\u2014for exam- ple, \u201cwhich sounds do I typically associate with this visual sensory information\u201d. Then, in the second step, the RL agent learns a policy directly on top of this latent space, while (possibly) only having ac- cess to a subset of the input modalities (say, images but not sound). In practice, this translates in the RL agent learning a policy over a latent space constructed relying only on some modalities. Finally, the transfer occurs in the third step, where, at test time, the agent may have access to a different subset of modalities, but still per- form the task using the same policy. These results hold consistently across different OpenAI Gym [2] and Atari-like [1] environments. This is the case even when using different multimodal generative models [18] and reinforcement learning algorithms [10, 12].",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d1a587c400bfa06b8346284cf8b36cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        835.8
                    ],
                    [
                        883.2,
                        866.1
                    ],
                    [
                        1285.7,
                        866.1
                    ],
                    [
                        1285.7,
                        835.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "2.1 Deep Generative Models",
        "type": "Title"
    },
    {
        "element_id": "d1e6f3161bf6ebd5c67a082fe9662f2b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        879.2
                    ],
                    [
                        883.2,
                        1091.3
                    ],
                    [
                        1554.8,
                        1091.3
                    ],
                    [
                        1554.8,
                        879.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95335,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "2d1a587c400bfa06b8346284cf8b36cd"
        },
        "text": "2.1.1 Variational Autoencoders. Deep generative models have shown great promise in learning generalized representations of data. For single-modality data, the VAE is widely used [8]. The VAE model learns a joint distribution p\u03b8 (x, z) of data x generated by a latent variable z. Figure 2a depicts this model. The latent variable is often of lower dimensionality in comparison with the modality itself, and acts as the representation vector in which data is encoded.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f31502f591b7533664c00cdb92403918",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        880.5,
                        1092.2
                    ],
                    [
                        880.5,
                        1240.9
                    ],
                    [
                        1557.9,
                        1240.9
                    ],
                    [
                        1557.9,
                        1092.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93687,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "2d1a587c400bfa06b8346284cf8b36cd"
        },
        "text": "The joint distribution takes the form p\u03b8 (x, z) = p\u03b8 (x | z) p(z), where p(z) (the prior distribution) is often a unitary Gaussian (z \u223c N (0, I)). The generative distribution p\u03b8 (x | z), parameterized by \u03b8 , is usually composed with a simple likelihood term (e.g. Gauss- ian or Bernoulli).",
        "type": "NarrativeText"
    },
    {
        "element_id": "4a272c808d81bf2fa41b7e4b1b2ad6f6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.9,
                        1244.5
                    ],
                    [
                        882.9,
                        1397.0
                    ],
                    [
                        1554.7,
                        1397.0
                    ],
                    [
                        1554.7,
                        1244.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94711,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "2d1a587c400bfa06b8346284cf8b36cd"
        },
        "text": "The training procedure of the VAE model involves the maxi- mization of the evidence likelihood p(x), by marginalizing over the latent variable and resorting to an inference network q\u03d5 (z|x) to approximate the posterior distribution. We obtain a lower-bound on the log-likelihood of the evidence (ELBO) log p(x) \u2265 LVAE(x):",
        "type": "NarrativeText"
    },
    {
        "element_id": "11faadc6627ab8431ab17b615622207f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1389.1
                    ],
                    [
                        177.1,
                        1414.0
                    ],
                    [
                        816.8,
                        1414.0
                    ],
                    [
                        816.8,
                        1389.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "2d1a587c400bfa06b8346284cf8b36cd"
        },
        "text": "The third and last step unveils what sets our work apart from",
        "type": "NarrativeText"
    },
    {
        "element_id": "7bad940daa087e89d18e6fbf58253375",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.2,
                        1395.8
                    ],
                    [
                        148.2,
                        1661.2
                    ],
                    [
                        822.8,
                        1661.2
                    ],
                    [
                        822.8,
                        1395.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95734,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "2d1a587c400bfa06b8346284cf8b36cd"
        },
        "text": "the existing literature. By using (single-modality) VAE methods, the current state-of-art approaches implicitly assume that the source and target domains are characterized by similar inputs, such as raw observations of a camera. In these approaches, the latent space is used to capture isolated properties (such as colors or shapes) that may vary throughout the tasks. This is in contrast with our approach, where the latent space is seen as a mechanism to create a mapping between different input modalities.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f5e8bc64e8cb2972450e8eeab2a75a7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.0,
                        1410.6
                    ],
                    [
                        910.0,
                        1459.6
                    ],
                    [
                        1524.3,
                        1459.6
                    ],
                    [
                        1524.3,
                        1410.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63201,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "Lyar(x) = AEg,(z|x) [log pe(xlz)] \u2014 6 EL [gg(zlx) || pa).",
        "type": "Formula"
    },
    {
        "element_id": "0ac47b1c4d529997253b5ef51e8d9e31",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        1466.3
                    ],
                    [
                        882.3,
                        1506.2
                    ],
                    [
                        1548.3,
                        1506.2
                    ],
                    [
                        1548.3,
                        1466.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "where the Kullback-Leibler divergence term KL laglx) Il p(z)|",
        "type": "Title"
    },
    {
        "element_id": "044aa4a5167206fcfe386de3badc6703",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.4,
                        1476.6
                    ],
                    [
                        882.4,
                        1681.9
                    ],
                    [
                        1556.3,
                        1681.9
                    ],
                    [
                        1556.3,
                        1476.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95517,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "0ac47b1c4d529997253b5ef51e8d9e31"
        },
        "text": "promotes a balance between the latent channel\u2019s capacity and the encoding process of data. Moreover, in the model\u2019s training pro- cedure, the hyperparameters \u03bb and \u03b2 weight the importance of reconstruction quality and latent space disentanglement, respec- tively. The optimization of the ELBO is performed resorting to gradient-based methods.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1ce603b6841532687a36336a28389c5b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1663.0
                    ],
                    [
                        148.5,
                        1814.1
                    ],
                    [
                        823.0,
                        1814.1
                    ],
                    [
                        823.0,
                        1663.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9533,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "0ac47b1c4d529997253b5ef51e8d9e31"
        },
        "text": "The remainder of the paper is structured as follows. In Section 2 we introduce relevant background and related work on generative models and RL. Then, in Section 3 we introduce our approach to multimodal transfer reinforcement learning, and evaluate it in Section 4. We finish with some final considerations in Section 5.",
        "type": "NarrativeText"
    },
    {
        "element_id": "da129ca2c86cf43cda23b65991c33be0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1856.9
                    ],
                    [
                        149.4,
                        1887.2
                    ],
                    [
                        432.4,
                        1887.2
                    ],
                    [
                        432.4,
                        1856.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "2 PRELIMINARIES",
        "type": "Title"
    },
    {
        "element_id": "e6f6510eebfedd5bf182b060e7c0b220",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.5,
                        1703.7
                    ],
                    [
                        882.5,
                        1919.6
                    ],
                    [
                        1554.9,
                        1919.6
                    ],
                    [
                        1554.9,
                        1703.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94695,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "da129ca2c86cf43cda23b65991c33be0"
        },
        "text": "2.1.2 Multimodal Variational Autoencoders. VAE models have been extended in order to perform inference across different modal- ities. The Associative Variational Autoencoder (AVAE) model [18], depicted in Figure 2b, is able to learn a common latent represen- tation of two modalities (x, y). It does so by imposing a similarity restriction on the separate single-modality latent representations (zx , zy ), employing a KL divergence term on the ELBO of the model:",
        "type": "NarrativeText"
    },
    {
        "element_id": "e47dd1a642e53dfecfaaf6e4a7fc0153",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.7,
                        1900.3
                    ],
                    [
                        148.7,
                        1925.2
                    ],
                    [
                        816.8,
                        1925.2
                    ],
                    [
                        816.8,
                        1900.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "da129ca2c86cf43cda23b65991c33be0"
        },
        "text": "This section introduces required background on deep generative",
        "type": "NarrativeText"
    },
    {
        "element_id": "3e48edb6731b56f79b48290b8f100036",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1907.6
                    ],
                    [
                        149.4,
                        1959.2
                    ],
                    [
                        822.9,
                        1959.2
                    ],
                    [
                        822.9,
                        1907.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93678,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "da129ca2c86cf43cda23b65991c33be0"
        },
        "text": "models and deep reinforcement learning.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cc35bd0ac3d97703d3d6da3ee09cdf20",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.7,
                        1933.0
                    ],
                    [
                        883.7,
                        1972.9
                    ],
                    [
                        1548.3,
                        1972.9
                    ],
                    [
                        1548.3,
                        1933.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2,
            "parent_id": "da129ca2c86cf43cda23b65991c33be0"
        },
        "text": "Lavaz(x, y) = Lva(0)* Lva(y)\u2014 a KL* [gg @xlx) ll ag(zyly)]",
        "type": "NarrativeText"
    },
    {
        "element_id": "2f717ac6b5611b4fac23d50a1f004063",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.9,
                        2086.6
                    ],
                    [
                        828.9,
                        2108.3
                    ],
                    [
                        873.5,
                        2108.3
                    ],
                    [
                        873.5,
                        2086.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.32565,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 2
        },
        "text": "1261",
        "type": "Footer"
    },
    {
        "element_id": "a74edede7c2cdcfc604b947bdc635a6b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.7,
                        97.2
                    ],
                    [
                        148.7,
                        118.2
                    ],
                    [
                        308.3,
                        118.2
                    ],
                    [
                        308.3,
                        97.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80173,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "5791872e81d8c6e1b58c5e17d089ae94",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        235.1
                    ],
                    [
                        148.5,
                        266.0
                    ],
                    [
                        816.8,
                        266.0
                    ],
                    [
                        816.8,
                        235.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "where KL\u22c6 [p \u2225 q] is the symmetrical Kullback-Leibler between",
        "type": "NarrativeText"
    },
    {
        "element_id": "6053abc1a3614710e8d49885f5f29c3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        143.9,
                        243.9
                    ],
                    [
                        143.9,
                        418.1
                    ],
                    [
                        822.7,
                        418.1
                    ],
                    [
                        822.7,
                        243.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94924,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "two distributions p and q, and \u03b1 is a constant that weights the importance of keeping similar latent spaces in the training proce- dure [18]. We note that each modality is associated with a different encoder-decoder pair. Moreover, the encoder and the decoder can be implemented as neural networks following different architectures.",
        "type": "NarrativeText"
    },
    {
        "element_id": "453be7ce962bbb60eb37bc282dfb005e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        417.9
                    ],
                    [
                        177.1,
                        442.8
                    ],
                    [
                        821.0,
                        442.8
                    ],
                    [
                        821.0,
                        417.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "Other models aim at learning a joint distribution of both modal-",
        "type": "NarrativeText"
    },
    {
        "element_id": "69f7639802518b363a3b92f6fa35c6ba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        145.6,
                        426.6
                    ],
                    [
                        145.6,
                        630.3
                    ],
                    [
                        824.4,
                        630.3
                    ],
                    [
                        824.4,
                        426.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94978,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "ities p\u03b8 (x, y). Examples include the Joint Multimodal Variational Autoencoder (JMVAE) [16] or the Multi-Modal Variational Autoen- coder (M2VAE) [9]. These generative models are able to build a representation space of both modalities simultaneously while main- taining similarity restrictions with the single-modality representa- tions, as shown in the JMVAE model presented in Figure 2c.",
        "type": "NarrativeText"
    },
    {
        "element_id": "dd15aaad63ebc40f807a9714876902ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        631.0
                    ],
                    [
                        177.1,
                        655.9
                    ],
                    [
                        816.8,
                        655.9
                    ],
                    [
                        816.8,
                        631.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "However, a fundamental feature of all multimodal generative",
        "type": "NarrativeText"
    },
    {
        "element_id": "ccee6c4b76457d7f4f9df8976e07bdef",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.6,
                        638.5
                    ],
                    [
                        147.6,
                        842.5
                    ],
                    [
                        825.5,
                        842.5
                    ],
                    [
                        825.5,
                        638.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95448,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "models is the ability to perform cross-modality inference, that is the ability to input modality-specific data, encode the corresponding latent representation, and, from that representation, generate data of a different modality. This is possible due to the forced approxima- tion of the latent representations of each modality, and the process follows the orange and green arrows in Figure 2.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5eeb1c1e11cc7280c3082cb9740e4b13",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        884.7
                    ],
                    [
                        149.4,
                        915.0
                    ],
                    [
                        556.0,
                        915.0
                    ],
                    [
                        556.0,
                        884.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "a74edede7c2cdcfc604b947bdc635a6b"
        },
        "text": "2.2 Reinforcement Learning",
        "type": "Title"
    },
    {
        "element_id": "d31a7c6b31e19456dd145aff3d000cb8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        928.1
                    ],
                    [
                        149.4,
                        953.0
                    ],
                    [
                        821.0,
                        953.0
                    ],
                    [
                        821.0,
                        928.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "5eeb1c1e11cc7280c3082cb9740e4b13"
        },
        "text": "Reinforcement learning (RL) is a framework for optimizing the be-",
        "type": "NarrativeText"
    },
    {
        "element_id": "7f8065629c0af9dc7b1737f753d1ac66",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        935.1
                    ],
                    [
                        148.5,
                        1262.3
                    ],
                    [
                        824.9,
                        1262.3
                    ],
                    [
                        824.9,
                        935.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95604,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "5eeb1c1e11cc7280c3082cb9740e4b13"
        },
        "text": "haviour of an agent operating in a given environment. This frame- work is formalized as a Markov decision process (MDP)\u2014a tuple M = (X, A, P, r , \u03b3 ) that describes a sequential decision problem under uncertainty. X and A are the state and action spaces, respec- tively, and both are known by the agent. When the agent takes an action a \u2208 A while in state x \u2208 X, the world transitions to state y \u2208 X with probability P(y | x, a) and the agent receives an immediate reward r (x, a). Typically, functions P and r are unknown to the agent. Finally, the discount factor \u03b3 \u2208 [0, 1) sets the relative importance of present and future rewards.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ba918e0fe0439909ba5fd0ad87d5cd38",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1261.6
                    ],
                    [
                        177.1,
                        1293.7
                    ],
                    [
                        816.8,
                        1293.7
                    ],
                    [
                        816.8,
                        1261.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "5eeb1c1e11cc7280c3082cb9740e4b13"
        },
        "text": "Solving the MDP consists in finding an optimal policy \u03c0 \u2217\u2014a",
        "type": "NarrativeText"
    },
    {
        "element_id": "4aa31c3070ffc782813b134ab778d729",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1270.5
                    ],
                    [
                        149.4,
                        1415.4
                    ],
                    [
                        824.2,
                        1415.4
                    ],
                    [
                        824.2,
                        1270.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94106,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "5eeb1c1e11cc7280c3082cb9740e4b13"
        },
        "text": "mapping from states to actions\u2014which ensures that the agent col- lects as much reward as possible. Such policy can be found from the optimal Q-function, which is defined recursively for every state action pair (x, a) \u2208 X \u00d7 A as",
        "type": "NarrativeText"
    },
    {
        "element_id": "eae41e56b2df00837ac7d426f76c64c2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        223.1,
                        1460.0
                    ],
                    [
                        223.1,
                        1523.4
                    ],
                    [
                        750.9,
                        1523.4
                    ],
                    [
                        750.9,
                        1460.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84386,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "Q\u2217(x, a) = r (x, a) + \u03b3 y \u2208X P(y | x, a) max a\u2032 \u2208A Q\u2217(y, a\u2032).",
        "type": "Formula"
    },
    {
        "element_id": "1c44a4035a8ecdcdb0fa91ab4016438f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1565.9
                    ],
                    [
                        149.4,
                        1590.8
                    ],
                    [
                        817.3,
                        1590.8
                    ],
                    [
                        817.3,
                        1565.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "Multiple methods can be used in computing this function [15], for",
        "type": "NarrativeText"
    },
    {
        "element_id": "9535e5ca82b61b008563cebd72bb5de8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.2,
                        1573.8
                    ],
                    [
                        147.2,
                        1627.1
                    ],
                    [
                        814.8,
                        1627.1
                    ],
                    [
                        814.8,
                        1573.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91877,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "example Q-learning [17].",
        "type": "NarrativeText"
    },
    {
        "element_id": "a3334b44cf992b477285d77d016882d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1626.8
                    ],
                    [
                        149.4,
                        1960.0
                    ],
                    [
                        823.8,
                        1960.0
                    ],
                    [
                        823.8,
                        1626.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9573,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "More recently, research has geared towards applying deep learn- ing methods in RL problems, leading to new methods. For example, Deep Q Network (DQN) is a variant of the Q-learning algorithm that uses a deep neural network to parameterize an approxima- tion of the Q-functions Q(x, a; \u03b8 ), with parameters \u03b8 . DQN assumes discrete action spaces A, and has been proved suitable for learn- ing policies that beat Atari games [12]. Continuous action spaces require specialized algorithms. For example, Deep Deterministic Policy Gradient (DDPG) is an actor-critic, policy gradient algorithm that can deal with continuous action spaces, and has been shown to perform well in complex control tasks [10].",
        "type": "NarrativeText"
    },
    {
        "element_id": "1e140e62fbfd17522aad6ced77a3f6e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        829.3,
                        2086.8
                    ],
                    [
                        829.3,
                        2108.5
                    ],
                    [
                        872.9,
                        2108.5
                    ],
                    [
                        872.9,
                        2086.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.47206,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "1262",
        "type": "Footer"
    },
    {
        "element_id": "89d728859320a4036de06e2a3f7d435b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1043.2,
                        97.6
                    ],
                    [
                        1043.2,
                        117.9
                    ],
                    [
                        1549.9,
                        117.9
                    ],
                    [
                        1549.9,
                        97.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85558,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "65057ba8210acf71978fa870a747df5c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        230.0
                    ],
                    [
                        883.2,
                        260.3
                    ],
                    [
                        1303.3,
                        260.3
                    ],
                    [
                        1303.3,
                        230.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "89d728859320a4036de06e2a3f7d435b"
        },
        "text": "3 MULTIMODAL TRANSFER",
        "type": "Title"
    },
    {
        "element_id": "1c435bba9038dc78cdc0203be1b20421",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        887.4,
                        241.7
                    ],
                    [
                        887.4,
                        303.8
                    ],
                    [
                        1362.3,
                        303.8
                    ],
                    [
                        1362.3,
                        241.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84159,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "89d728859320a4036de06e2a3f7d435b"
        },
        "text": "REINFORCEMENT LEARNING",
        "type": "Title"
    },
    {
        "element_id": "73d82ade718f3723aa1ae85d890f4caa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        309.3
                    ],
                    [
                        883.2,
                        334.2
                    ],
                    [
                        1550.5,
                        334.2
                    ],
                    [
                        1550.5,
                        309.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "Consider an agent facing a sequential decision problem described",
        "type": "NarrativeText"
    },
    {
        "element_id": "bb13217ac11e2c8d5fdc394e7875f186",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.9,
                        316.7
                    ],
                    [
                        881.9,
                        553.1
                    ],
                    [
                        1555.9,
                        553.1
                    ],
                    [
                        1555.9,
                        316.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95391,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "as an MDP M = (X, A, P, r, \u03b3 ). This agent is endowed with a set {I1, I2, . . . , IN } of N different input modalities, which can be used in perceiving the world and building a possibly partial observation of the current state x \u2208 X. Different modalities may provide more, or less, perceptual information than others. Some modalities may be redundant (i.e., provide the same perceptual information) or complement each other (i.e., jointly provide more information).",
        "type": "NarrativeText"
    },
    {
        "element_id": "100a3ccbb286ec1c1e5b65754388d22c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        552.8
                    ],
                    [
                        910.9,
                        577.7
                    ],
                    [
                        1551.2,
                        577.7
                    ],
                    [
                        1551.2,
                        552.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "Our goal is for the agent to learn a policy while observing only",
        "type": "NarrativeText"
    },
    {
        "element_id": "e5bb2979053858559154a97778026cbc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        560.8
                    ],
                    [
                        882.3,
                        673.4
                    ],
                    [
                        1557.8,
                        673.4
                    ],
                    [
                        1557.8,
                        560.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93915,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "a subset of input modalities I train, and then use that same policy when observing a possibly different subset of modalities, I test, with as minimal performance degradation as possible.",
        "type": "NarrativeText"
    },
    {
        "element_id": "754de589980b33011ee72e9a6cb26b98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        674.6
                    ],
                    [
                        910.9,
                        699.5
                    ],
                    [
                        1405.8,
                        699.5
                    ],
                    [
                        1405.8,
                        674.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "Our approach consists of a three-stages pipeline:",
        "type": "NarrativeText"
    },
    {
        "element_id": "fa4339c6ca39dafee759c39e05ca26a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        913.5,
                        726.2
                    ],
                    [
                        913.5,
                        756.8
                    ],
                    [
                        1329.8,
                        756.8
                    ],
                    [
                        1329.8,
                        726.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.53575,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "(1) Learn a perceptual model of the world.",
        "type": "ListItem"
    },
    {
        "element_id": "bf51f7c3e4642670b5459510f3eb4e07",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        913.5,
                        756.6
                    ],
                    [
                        913.5,
                        787.3
                    ],
                    [
                        1198.7,
                        787.3
                    ],
                    [
                        1198.7,
                        756.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "(2) Learn to act in the world.",
        "type": "NarrativeText"
    },
    {
        "element_id": "771ae8dc18362712db352b0db4556e83",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        913.5,
                        787.0
                    ],
                    [
                        913.5,
                        817.7
                    ],
                    [
                        1109.2,
                        817.7
                    ],
                    [
                        1109.2,
                        787.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65763,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "(3) Transfer policy.",
        "type": "ListItem"
    },
    {
        "element_id": "542e25004811078c56ad225b99613e6e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        838.6
                    ],
                    [
                        910.9,
                        863.5
                    ],
                    [
                        1346.3,
                        863.5
                    ],
                    [
                        1346.3,
                        838.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "1c435bba9038dc78cdc0203be1b20421"
        },
        "text": "We now discuss each step in further detail.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2c26439c47235cf1e47d96b0a1ee9d15",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        908.5
                    ],
                    [
                        883.2,
                        938.8
                    ],
                    [
                        1476.7,
                        938.8
                    ],
                    [
                        1476.7,
                        908.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "89d728859320a4036de06e2a3f7d435b"
        },
        "text": "3.1 Learn a perceptual model of the world",
        "type": "Title"
    },
    {
        "element_id": "7385121e774d35c630dbb269bb1717f8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        879.4,
                        951.8
                    ],
                    [
                        879.4,
                        1102.4
                    ],
                    [
                        1560.3,
                        1102.4
                    ],
                    [
                        1560.3,
                        951.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93218,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "2c26439c47235cf1e47d96b0a1ee9d15"
        },
        "text": "Let I denote the Cartesian product of input modalities, I = I1 \u00d7 I2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 IN . Intuitively, we can think of I as the complete percep- tual space of the agent. Figure 3a depicts an example on a game, where the agent can have access to two modalities, Iimage and Isound, corresponding to visual and sound information.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3dd524b1275d3a19dad3d37fe04d5ef4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        1104.1
                    ],
                    [
                        882.3,
                        1408.8
                    ],
                    [
                        1557.0,
                        1408.8
                    ],
                    [
                        1557.0,
                        1104.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9551,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "2c26439c47235cf1e47d96b0a1ee9d15"
        },
        "text": "We write i to denote an element of I. At each moment t, the agent may not have access to the complete perception i(t) \u2208 I, but only to a partial view thereof. Following our discussion in Section 1, we are interested in learning a multimodal latent representation of the perceptions in I. Such representation amounts to a set of latent mappings F = {F1, . . . , FL }. Each map F\u2113 takes the form F\u2113 : proj \u2113 \u2192 Z, where Z is a common latent space and proj projects I to some subspace of K modalities, I\u2113 = I\u21131 \u00d7I\u21132 \u00d7. . .\u00d7I\u2113K In Figure 3a the set of mappings F is used to compute a latent representation z from sound and image data. \u2113 .",
        "type": "NarrativeText"
    },
    {
        "element_id": "45bf64ee43004d53205718075435a296",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1408.5
                    ],
                    [
                        910.9,
                        1439.2
                    ],
                    [
                        1548.6,
                        1439.2
                    ],
                    [
                        1548.6,
                        1408.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "2c26439c47235cf1e47d96b0a1ee9d15"
        },
        "text": "To learn such mappings, we start by collecting a dataset of M",
        "type": "NarrativeText"
    },
    {
        "element_id": "9031059921fc4e346602076d6db3497d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        878.2,
                        1416.4
                    ],
                    [
                        878.2,
                        1466.8
                    ],
                    [
                        1554.9,
                        1466.8
                    ],
                    [
                        1554.9,
                        1416.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91608,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3,
            "parent_id": "2c26439c47235cf1e47d96b0a1ee9d15"
        },
        "text": "examples of coupled sensorial information:",
        "type": "NarrativeText"
    },
    {
        "element_id": "a523e33a6a740e812abcb77589fee29f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1091.0,
                        1506.4
                    ],
                    [
                        1091.0,
                        1562.7
                    ],
                    [
                        1342.8,
                        1562.7
                    ],
                    [
                        1342.8,
                        1506.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78396,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "D(L) = {i, Ni Mo} ,",
        "type": "Formula"
    },
    {
        "element_id": "8bb1569634395aac4cdc2eea1e0ed62d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.0,
                        1596.9
                    ],
                    [
                        882.0,
                        1621.8
                    ],
                    [
                        1554.7,
                        1621.8
                    ],
                    [
                        1554.7,
                        1596.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "We then follow an unsupervised learning approach, and train a mul-",
        "type": "NarrativeText"
    },
    {
        "element_id": "b503bc3ccf73c317cfa07c33cccf6bab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1603.8
                    ],
                    [
                        883.2,
                        1870.3
                    ],
                    [
                        1554.8,
                        1870.3
                    ],
                    [
                        1554.8,
                        1603.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95376,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "timodal VAE on dataset D(I) to learn a generalized latent space over the agent\u2019s input modalities. The latent mappings in F corre- spond to the encoders of the VAE model, while the decoders can be seen as a set of inverse latent mappings, F \u22121 = {F \u22121 1 , . . . , F \u22121 L that allow for modality reconstruction and cross-modality infer- ence. Figure 3b depicts a concrete example of how the multimodal latent space can be used to perform cross-modality inference of sound data given an image input using the modality-specific maps. }",
        "type": "NarrativeText"
    },
    {
        "element_id": "bcf3dc97168d1b79e51f10ffc1be2ce4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1870.9
                    ],
                    [
                        910.9,
                        1899.6
                    ],
                    [
                        1550.6,
                        1899.6
                    ],
                    [
                        1550.6,
                        1870.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "The collection of the initial data needed to generate D(I) can",
        "type": "NarrativeText"
    },
    {
        "element_id": "233904da67b5eae789a5cbf8f446e4cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        880.2,
                        1878.7
                    ],
                    [
                        880.2,
                        1960.0
                    ],
                    [
                        1558.1,
                        1960.0
                    ],
                    [
                        1558.1,
                        1878.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93131,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 3
        },
        "text": "be easier or harder depending on the complexity of the task. In Section 4 we discuss mechanisms to perform this.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6b9243803caad0ec3c4dd09af79a88f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.2,
                        94.9
                    ],
                    [
                        148.2,
                        119.1
                    ],
                    [
                        308.4,
                        119.1
                    ],
                    [
                        308.4,
                        94.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71737,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "376cfbeb6e9398de41d59a3af44885f2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1042.8,
                        97.5
                    ],
                    [
                        1042.8,
                        117.9
                    ],
                    [
                        1550.0,
                        117.9
                    ],
                    [
                        1550.0,
                        97.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82468,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "fbb7ce68e0ee8079f32a1cfcadafd744",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        172.8,
                        236.8
                    ],
                    [
                        172.8,
                        593.4
                    ],
                    [
                        1536.7,
                        593.4
                    ],
                    [
                        1536.7,
                        236.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91861,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "(a)  (b) ",
        "type": "Image"
    },
    {
        "element_id": "98d3b790a651a4deefe2794f78cd33c3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        169.9,
                        280.4
                    ],
                    [
                        169.9,
                        515.8
                    ],
                    [
                        385.9,
                        515.8
                    ],
                    [
                        385.9,
                        280.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-5.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "29d83ba6366a56dea668c1b615798399",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        620.4
                    ],
                    [
                        149.4,
                        645.3
                    ],
                    [
                        1550.5,
                        645.3
                    ],
                    [
                        1550.5,
                        620.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "Figure 3: 3a) Each time step of a game includes visual and sound components that are intrinsically coupled. This coupling can",
        "type": "NarrativeText"
    },
    {
        "element_id": "3c69b286c1f4ca1698de15c04d7fb450",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        628.1
                    ],
                    [
                        149.4,
                        750.3
                    ],
                    [
                        1551.7,
                        750.3
                    ],
                    [
                        1551.7,
                        628.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88816,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "be encoded in a latent representation using the family of latent maps F . 3b) Depicts how the multimodal latent representation can be used in inferring the sound associated with a given image, using the image latent map Fimage and sound inverse latent map F \u22121 sound .",
        "type": "FigureCaption"
    },
    {
        "element_id": "2c30882e101474a2d8ee232e6d77465a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        789.6
                    ],
                    [
                        149.4,
                        819.9
                    ],
                    [
                        555.6,
                        819.9
                    ],
                    [
                        555.6,
                        789.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "3.2 Learn to act in the world",
        "type": "NarrativeText"
    },
    {
        "element_id": "6ea73b3e704994ffbe7be4757cd6885b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.0,
                        832.9
                    ],
                    [
                        148.0,
                        1043.1
                    ],
                    [
                        822.3,
                        1043.1
                    ],
                    [
                        822.3,
                        832.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95356,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "After learning a perceptual model of the world, the agent then learns how to perform the task. We follow a reinforcement learning approach to learn an optimal policy for the task described by MDP M. During this learning phase, we assume the agent may only have access to a subset of input modalities I train. As a result, during its interaction with the environment, the agent collects a sequence of triplets",
        "type": "NarrativeText"
    },
    {
        "element_id": "b857ae5e3a001b87e34b090990934ae3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        270.7,
                        1064.2
                    ],
                    [
                        270.7,
                        1123.0
                    ],
                    [
                        705.7,
                        1123.0
                    ],
                    [
                        705.7,
                        1064.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81022,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "(a!) (a a), ro) in | ,",
        "type": "Formula"
    },
    {
        "element_id": "5889979ce688a0559599bbec6dab6a11",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        879.9,
                        794.9
                    ],
                    [
                        879.9,
                        992.5
                    ],
                    [
                        1556.6,
                        992.5
                    ],
                    [
                        1556.6,
                        794.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94805,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "In order to reuse the policy z, the agent starts by pre-processing this perceptual observation, again using the set of maps \u00a5 previ- ously trained, but now generating a latent state zl) = Frest (ia) where Ftest \u20ac F now maps Itest into Z. Since policy 2 maps the latent space Z to the action space A, it can now be used directly to select the optimal action at the new state z(?).",
        "type": "NarrativeText"
    },
    {
        "element_id": "80deaf3f5ded0973e2bde3d4ae9211a5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        992.2
                    ],
                    [
                        910.9,
                        1022.9
                    ],
                    [
                        1550.5,
                        1022.9
                    ],
                    [
                        1550.5,
                        992.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "Effectively, the agent is reusing a policy \u03c0 that was learned over a",
        "type": "NarrativeText"
    },
    {
        "element_id": "6969746b24d85c14be6a37d788486e91",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.9,
                        1000.7
                    ],
                    [
                        881.9,
                        1112.0
                    ],
                    [
                        1554.7,
                        1112.0
                    ],
                    [
                        1554.7,
                        1000.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9408,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "(possibly) different set of input modalities, with no additional train- ing. This corresponds to a zero-shot modality transfer of policies. Figure 4 summarizes the three-steps pipeline hereby described.",
        "type": "NarrativeText"
    },
    {
        "element_id": "96f58a1753c93702cb293ef9cae584e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        229.1,
                        1140.9
                    ],
                    [
                        229.1,
                        1162.7
                    ],
                    [
                        251.9,
                        1162.7
                    ],
                    [
                        251.9,
                        1140.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "(t )",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8b067b979bcee3f358d86e4f1d7b4bf8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        145.7,
                        1144.6
                    ],
                    [
                        145.7,
                        1207.1
                    ],
                    [
                        819.5,
                        1207.1
                    ],
                    [
                        819.5,
                        1144.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91082,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "train, a(t ), r (t ) correspond to the perceptual observations, where i action executed, and rewards obtained at timestep t, respectively.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7dc0129cdd5d6c519c152fa50074f85c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1206.8
                    ],
                    [
                        177.1,
                        1231.7
                    ],
                    [
                        816.8,
                        1231.7
                    ],
                    [
                        816.8,
                        1206.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "However, our reinforcement learning agent does not use this",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a357ee7114183a1093a1a08fc0c1e40",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.8,
                        1215.1
                    ],
                    [
                        147.8,
                        1402.5
                    ],
                    [
                        822.0,
                        1402.5
                    ],
                    [
                        822.0,
                        1215.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94581,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "However, our reinforcement learning agent does not use this sequence of triplets directly. Instead, it pre-processes the perceptual observations using the previously learned latent maps F to encode the multimodal latent state at each time step as Zt) = Frrain (e) where Frain \u20ac F maps \u0130\u015frain into Z. In practice, the RL agent uses a seguence of triplets",
        "type": "NarrativeText"
    },
    {
        "element_id": "15d7a1ac0659dbbb98fc9cdb0bb683ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        296.8,
                        1420.8
                    ],
                    [
                        296.8,
                        1479.2
                    ],
                    [
                        675.2,
                        1479.2
                    ],
                    [
                        675.2,
                        1420.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8181,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "9,0, 1) , (2, al?) Ni }",
        "type": "Formula"
    },
    {
        "element_id": "aee27b3c19d6a70f9d31d230ded5be1f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.4,
                        1493.1
                    ],
                    [
                        147.4,
                        1675.9
                    ],
                    [
                        825.1,
                        1675.9
                    ],
                    [
                        825.1,
                        1493.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95493,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "to learn a policy \u03c0 : Z \u2192 A, that maps the latent states to actions. Any continuous-state space reinforcement learning algorithm can be used to learn this policy \u03c0 over the latent states. These latent states are encoded using the generative model trained in the pre- vious section, and as such, the weights of this model are frozen during the RL training.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3c8a8140840d61fb3f575bff73ec246a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1705.4
                    ],
                    [
                        149.4,
                        1735.7
                    ],
                    [
                        429.4,
                        1735.7
                    ],
                    [
                        429.4,
                        1705.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "3.3 Transfer policy",
        "type": "Title"
    },
    {
        "element_id": "6a8c2a9f4ffeac21de11f65141db90c0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.7,
                        1748.8
                    ],
                    [
                        148.7,
                        1970.0
                    ],
                    [
                        825.6,
                        1970.0
                    ],
                    [
                        825.6,
                        1748.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95567,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4,
            "parent_id": "3c8a8140840d61fb3f575bff73ec246a"
        },
        "text": "The transfer of policies happens once the agent has learned how to perceive and act in the world. At this time, we assume the agent may now have access to a subset of input modalities I test, potentially different from I train, i.e., the set of modalities it used in learning the task policy \u03c0 . As a result, during its interaction with the environ- ment, at each time step t, the agent will now observe perceptual information i (t ) test.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d92aac9cfb79cadddef26e26aa8e1359",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        899.4,
                        1177.5
                    ],
                    [
                        899.4,
                        1561.3
                    ],
                    [
                        1533.3,
                        1561.3
                    ],
                    [
                        1533.3,
                        1177.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93045,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-6.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "Action |: a Latent: Ni Representation Image A Sound Image A Sound",
        "type": "Image"
    },
    {
        "element_id": "89ceb85745b6a11b11efa6fde96bda29",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        878.0,
                        1592.4
                    ],
                    [
                        878.0,
                        1956.3
                    ],
                    [
                        1558.3,
                        1956.3
                    ],
                    [
                        1558.3,
                        1592.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90096,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "Figure 4: Summary of the three-steps approach for cross- modality transfer in reinforcement learning. The first step learns a perceptual model of the world, described by the la- tent mappings F (and corresponding inverses), which map perceptions to a common latent space Z. In the second step, the agent learns a policy \u03c0 that maps the latent space to ac- tions, with an RL approach using observations from a given subset of input modalities. The third step concerns the reuse of the same policy \u03c0 , assuming new observations from a po- tentially different subset of modalities. This is possible by first encoding the new observations in the multimodal la- tent space.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e774c780af138d8ccf770b23b7ebc65e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.4,
                        2084.6
                    ],
                    [
                        828.4,
                        2108.6
                    ],
                    [
                        874.0,
                        2108.6
                    ],
                    [
                        874.0,
                        2084.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43162,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 4
        },
        "text": "1263",
        "type": "Header"
    },
    {
        "element_id": "7d1fe6df2a2b9e21c6a33d92a9e6d2b4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.9,
                        93.6
                    ],
                    [
                        147.9,
                        119.3
                    ],
                    [
                        308.5,
                        119.3
                    ],
                    [
                        308.5,
                        93.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78827,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "f8cd0c141617d6c1de1fafbf018b4b95",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        366.5,
                        233.7
                    ],
                    [
                        366.5,
                        466.7
                    ],
                    [
                        599.5,
                        466.7
                    ],
                    [
                        599.5,
                        233.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-7.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "277d18352a26d6a03bf61a388bb3a73d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        498.4
                    ],
                    [
                        149.4,
                        523.3
                    ],
                    [
                        816.8,
                        523.3
                    ],
                    [
                        816.8,
                        498.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "Figure 5: Visual and sound perceptual information in the",
        "type": "Title"
    },
    {
        "element_id": "8f7e2eca72e1a343d8a7913f3483a519",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.5,
                        504.9
                    ],
                    [
                        147.5,
                        620.4
                    ],
                    [
                        821.2,
                        620.4
                    ],
                    [
                        821.2,
                        504.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80811,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "277d18352a26d6a03bf61a388bb3a73d"
        },
        "text": "pendulum scenario. The tip of the pendulum emits a fre- quency that is received by three microphones placed at the bottom left and right (bl, br ) and middle top (mt).",
        "type": "FigureCaption"
    },
    {
        "element_id": "56993e1fc1f968b64c0324c0f4520ba4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.8,
                        709.0
                    ],
                    [
                        146.8,
                        746.9
                    ],
                    [
                        634.9,
                        746.9
                    ],
                    [
                        634.9,
                        709.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84674,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "4 EXPERIMENTAL EVALUATION",
        "type": "Title"
    },
    {
        "element_id": "c61859cf702ae37c134a9128150aedac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        144.9,
                        752.4
                    ],
                    [
                        144.9,
                        964.5
                    ],
                    [
                        824.6,
                        964.5
                    ],
                    [
                        824.6,
                        752.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9549,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "56993e1fc1f968b64c0324c0f4520ba4"
        },
        "text": "We evaluate and analyze the performance of our approach on differ- ent scenarios of increasing complexity, not only on the task but also on the input modalities. We start by considering a modified version of the pendulum environment from OpenAI gym, with a simple sound source. Then, we consider hyperhot, a space invaders-like game that assesses the performance of our approach in scenarios with more complex and realistic generation of sounds.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6b12e3b6dc920dee2172e90ca003eb3e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1000.1
                    ],
                    [
                        149.4,
                        1030.4
                    ],
                    [
                        366.9,
                        1030.4
                    ],
                    [
                        366.9,
                        1000.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "4.1 pendulum",
        "type": "Title"
    },
    {
        "element_id": "6e3e92731879b094ed5953b65367d648",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.2,
                        1043.5
                    ],
                    [
                        148.2,
                        1316.1
                    ],
                    [
                        822.8,
                        1316.1
                    ],
                    [
                        822.8,
                        1043.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95597,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "6b12e3b6dc920dee2172e90ca003eb3e"
        },
        "text": "We consider a modified version of the pendulum environment from OpenAI gym\u2014a classic control problem, where the goal is to swing the pendulum up so it stays upright. We modify this environment so that the observations include both an image and a sound component. For the sound component, we assume that the tip of the pendulum emits a constant frequency f0, which is received by a set of S sound receivers {\u03c11, . . . , \u03c1S }. Figure 5 depicts this scenario, where the pendulum and its sound are in red, and the sound receivers correspond to the circles.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cf6ee89ab3c9c6ba2f4ea442b794646b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.4,
                        1317.4
                    ],
                    [
                        147.4,
                        1528.0
                    ],
                    [
                        821.0,
                        1528.0
                    ],
                    [
                        821.0,
                        1317.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95338,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "6b12e3b6dc920dee2172e90ca003eb3e"
        },
        "text": "Formally, we let I = Iimage \u00d7 Isound denote the complete per- ceptual space of the agent. The visual input modality of the agent, Iimage, consists of the raw image observation of the environment. On the other hand, the sound input modality, Isound, consists of the frequency and amplitude received by each of the S microphones of the agent. Moreover, both image and sound observations may be stacked to account for the dynamics of the scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "aa3496e9498c117a889c07b2af50fe77",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1530.5
                    ],
                    [
                        177.1,
                        1555.5
                    ],
                    [
                        821.0,
                        1555.5
                    ],
                    [
                        821.0,
                        1530.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "6b12e3b6dc920dee2172e90ca003eb3e"
        },
        "text": "In this scenario, we assume a simple model for the sound gener-",
        "type": "NarrativeText"
    },
    {
        "element_id": "4ed5612859724f24d139c6e499ce1a26",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.8,
                        1537.9
                    ],
                    [
                        146.8,
                        1743.9
                    ],
                    [
                        821.4,
                        1743.9
                    ],
                    [
                        821.4,
                        1537.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95118,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5,
            "parent_id": "6b12e3b6dc920dee2172e90ca003eb3e"
        },
        "text": "ation. Specifically, we assume that, at each timestep, the frequency f \u2032 heard by each sound receiver \u03c1i follows the Doppler effect. The i Doppler effect measures the change in frequency heard by an ob- server as it moves towards or away from the source. Slightly abusing our notation, we let \u03c1i denote the position of sound receiver \u03c1i and e the position of the sound emitter. Formally,",
        "type": "NarrativeText"
    },
    {
        "element_id": "b43cf6d5b8202279c7c2f3b0955f2923",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        339.9,
                        1771.4
                    ],
                    [
                        339.9,
                        1849.6
                    ],
                    [
                        631.4,
                        1849.6
                    ],
                    [
                        631.4,
                        1771.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8273,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "c+ pi (e~ pi) ei lp ze) (| Ji",
        "type": "Formula"
    },
    {
        "element_id": "2f4a839c5b86efe49ea4a664549d62db",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1873.3
                    ],
                    [
                        148.5,
                        1960.4
                    ],
                    [
                        821.0,
                        1960.4
                    ],
                    [
                        821.0,
                        1873.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93364,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "where c is the speed of sound and we use the dot notation to repre- sent velocities. Figure 6a depicts the Doppler effect in the pendulum scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f251e878ca96acef615555d3f42c7c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        829.0,
                        2087.0
                    ],
                    [
                        829.0,
                        2108.2
                    ],
                    [
                        873.6,
                        2108.2
                    ],
                    [
                        873.6,
                        2087.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43816,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "1264",
        "type": "Footer"
    },
    {
        "element_id": "6241ab123d64f40e3249445d708aa6c2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1041.8,
                        97.1
                    ],
                    [
                        1041.8,
                        118.1
                    ],
                    [
                        1552.0,
                        118.1
                    ],
                    [
                        1552.0,
                        97.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84188,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "d4f27f9323f10d8f97fdfaec041701d2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        899.1,
                        227.9
                    ],
                    [
                        899.1,
                        508.7
                    ],
                    [
                        1535.0,
                        508.7
                    ],
                    [
                        1535.0,
                        227.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83482,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-8.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "(a)  (b) ",
        "type": "Image"
    },
    {
        "element_id": "2dbc515dc2f88c3891ec07b81e9e77ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.6,
                        539.6
                    ],
                    [
                        881.6,
                        751.3
                    ],
                    [
                        1560.0,
                        751.3
                    ],
                    [
                        1560.0,
                        539.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8259,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "Figure 6: Different sound properties in the pendulum sce- nario. 6a) Depicts the Doppler effect: as the sound source moves near (away from) the observer, the arrival time of the emitted waves decreases (increases), thus increasing (de- creasing) the frequency. 6b) Depicts how the amplitude of the sound decreases with the distance from the source. Fad- ing semi-circles denote smaller intensities.",
        "type": "NarrativeText"
    },
    {
        "element_id": "dd926892909624d69ce0b7ebda0447bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        800.6
                    ],
                    [
                        883.2,
                        856.9
                    ],
                    [
                        1554.9,
                        856.9
                    ],
                    [
                        1554.9,
                        800.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90729,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "We then let the amplitude ai heard by receiver \u03c1i follow the inverse square law",
        "type": "NarrativeText"
    },
    {
        "element_id": "23e232fa4812821a915456fbaf5022a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1134.0,
                        861.8
                    ],
                    [
                        1134.0,
                        927.7
                    ],
                    [
                        1308.8,
                        927.7
                    ],
                    [
                        1308.8,
                        861.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77814,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "ai = K \u2225e \u2212 \u03c1i \u22252 ,",
        "type": "Formula"
    },
    {
        "element_id": "e182184ea94d8edca14cb40f4df1788a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.4,
                        923.7
                    ],
                    [
                        881.4,
                        1013.1
                    ],
                    [
                        1554.4,
                        1013.1
                    ],
                    [
                        1554.4,
                        923.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9207,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "where K is a scaling constant. Figure 6b depicts the inverse square law applied to the pendulum scenario, showing how the amplitude of the sound generated decreases with the distance to the source.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3dd7f88d6efeaff19c27041ca99926d6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1015.0
                    ],
                    [
                        910.9,
                        1039.9
                    ],
                    [
                        1550.5,
                        1039.9
                    ],
                    [
                        1550.5,
                        1015.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "We now provide details on how our approach was set up. All",
        "type": "NarrativeText"
    },
    {
        "element_id": "d52f3b53ba208ab143e74ed07bd9cac0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        1022.3
                    ],
                    [
                        882.3,
                        1104.7
                    ],
                    [
                        1552.8,
                        1104.7
                    ],
                    [
                        1552.8,
                        1022.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92945,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "constants and training hyper-parameters used are summarized in Appendix A.1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6e6f390bf68c6017a69e2598e5a60284",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.5,
                        1121.6
                    ],
                    [
                        882.5,
                        1407.5
                    ],
                    [
                        1554.7,
                        1407.5
                    ],
                    [
                        1554.7,
                        1121.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95749,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "4.1.1 Learn a perceptual model of the world. For this task, we adopted the AVAE model to learn the family of latent mappings \u00a5. The AVAE was trained over a dataset D(Z) with M observations of m jm \u2018image\u2019 \u2018sound ler proved to be enough to cover the state space. Before training, the images were preprocessed to black and white and resized to 60 x 60 pixels. The sounds were normalized to the range [0, 1], assuming the minimum and maximum values found in the M samples. images and sounds i\u201d = (i ). collected using a random controller. The random control!",
        "type": "NarrativeText"
    },
    {
        "element_id": "5ffc9a1bfd7f7cd549a719a29a725384",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1407.2
                    ],
                    [
                        910.9,
                        1432.1
                    ],
                    [
                        1550.5,
                        1432.1
                    ],
                    [
                        1550.5,
                        1407.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "For the image-specific encoder we adopted an architecture with",
        "type": "NarrativeText"
    },
    {
        "element_id": "c2a8d934c5ca38be60b7fc9d9d22231f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        1415.1
                    ],
                    [
                        882.3,
                        1711.9
                    ],
                    [
                        1557.0,
                        1711.9
                    ],
                    [
                        1557.0,
                        1415.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95525,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "two convolutional layers and two fully connected layers. The two convolutional layers learned 32 and 64 filters, respectively, each with kernel size 4, stride 2 and padding 1. The two fully connected layers had 256 neurons each. Swish activations were used [13]. For the sound-specific encoder, we adopted an architecture with two fully connected layers, each with 50 neurons. One dimension batch normalization was used between the two layers. The decoders fol- lowed similar architectures. The optimization used Adam gradient with pytorch\u2019s default parameters, and learning rate \u03b7avae.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2023cf7593ce9370269e71f99910247e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        1711.7
                    ],
                    [
                        910.9,
                        1736.6
                    ],
                    [
                        1554.8,
                        1736.6
                    ],
                    [
                        1554.8,
                        1711.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "The AVAE loss function penalized poor reconstruction of the im-",
        "type": "NarrativeText"
    },
    {
        "element_id": "24d93980dbc18715b6a81130c674a4ee",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.6,
                        1718.6
                    ],
                    [
                        881.6,
                        1894.6
                    ],
                    [
                        1557.1,
                        1894.6
                    ],
                    [
                        1557.1,
                        1718.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94882,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "age and sound. Image reconstruction loss was measured by binary cross entropy scaled by constant \u03bbimage, and sound reconstruc- tion loss was measured by mean squared error scaled by constant \u03bbsound. The prior divergence loss terms were scaled by \u03b2, and the symmetrical KL divergence term by \u03b1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "720f1266299a59c2ed75e6740e085dbb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.1,
                        1909.5
                    ],
                    [
                        883.1,
                        1967.9
                    ],
                    [
                        1554.8,
                        1967.9
                    ],
                    [
                        1554.8,
                        1909.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92056,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 5
        },
        "text": "4.1.2 Learn to act in the world. The agent learned how to per- form the task using the DDPG algorithm, while only having access",
        "type": "NarrativeText"
    },
    {
        "element_id": "25cc6b684366d65dfb9df988dedee9f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.1,
                        97.8
                    ],
                    [
                        149.1,
                        118.2
                    ],
                    [
                        307.1,
                        118.2
                    ],
                    [
                        307.1,
                        97.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79093,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "1e439905cd7b092ccddb060209acc6f8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        235.2
                    ],
                    [
                        149.4,
                        266.0
                    ],
                    [
                        821.0,
                        266.0
                    ],
                    [
                        821.0,
                        235.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "to the image input modality\u2014that is I train = Iimage. These image ob-",
        "type": "NarrativeText"
    },
    {
        "element_id": "8edeaf039ddc2dabd6e5431dd71bba1f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.2,
                        243.4
                    ],
                    [
                        149.2,
                        357.3
                    ],
                    [
                        827.5,
                        357.3
                    ],
                    [
                        827.5,
                        243.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94152,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "servations are encoded into the latent space using Ftrain = Fimage\u2014 the image-specific encoder of the AVAE trained in 4.1.1. Thus, the agent learns a policy \u03c0 that maps latent states to actions.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f6aebdc6a3d94ef85422722a9c04afc1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        357.0
                    ],
                    [
                        177.1,
                        381.9
                    ],
                    [
                        816.8,
                        381.9
                    ],
                    [
                        816.8,
                        357.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "The actor and critic networks consisted of two fully connected",
        "type": "NarrativeText"
    },
    {
        "element_id": "a351a20c9eb0411d960cb28f465dccf4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.9,
                        365.4
                    ],
                    [
                        147.9,
                        541.0
                    ],
                    [
                        826.5,
                        541.0
                    ],
                    [
                        826.5,
                        365.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9526,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "layers of 256 neurons each. The replay buffer was initially filled with samples obtained using a controller based on the Ornstein- Uhlenbeck process, with the parameters suggested by Lillicrap et al. [10]. The Adam gradient was used for optimizing both networks, with learning rates \u03b7critic and \u03b7actor.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8454d146e1caf1113e4f5976b01f7411",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        556.7
                    ],
                    [
                        149.4,
                        650.1
                    ],
                    [
                        823.7,
                        650.1
                    ],
                    [
                        823.7,
                        556.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93353,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "4.1.3 Transfer policy. We evaluated the performance of the pol- icy trained in 4.1.2, when the agent only has access to the sound input modality, i.e., I test = Isound.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9c28a3466af7613233ade4d4e4ee783d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        648.0
                    ],
                    [
                        177.1,
                        672.9
                    ],
                    [
                        816.8,
                        672.9
                    ],
                    [
                        816.8,
                        648.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "Given a sound observation, the agent first preprocesses it using",
        "type": "NarrativeText"
    },
    {
        "element_id": "0fe3a99926958a2ec0d4117a6ae5620a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.6,
                        656.3
                    ],
                    [
                        148.6,
                        769.1
                    ],
                    [
                        823.9,
                        769.1
                    ],
                    [
                        823.9,
                        656.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93385,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "the latent map Ftest = Fsound, generating a multimodal latent state z\u2014we denote this process as avaes. The agent then uses the policy to select the optimal action in this latent state.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b9c6ee69d0eed1b2840a86e835b3abdd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        769.8
                    ],
                    [
                        177.1,
                        794.7
                    ],
                    [
                        817.2,
                        794.7
                    ],
                    [
                        817.2,
                        769.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "As a result, we are measuring the zero-shot modality transfer",
        "type": "NarrativeText"
    },
    {
        "element_id": "0cf80cd735304f8a4ac9af4063a01800",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        777.1
                    ],
                    [
                        149.4,
                        1012.3
                    ],
                    [
                        825.7,
                        1012.3
                    ],
                    [
                        825.7,
                        777.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95618,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "performance of policy \u03c0 \u2014that is, the ability of the agent to per- form its task while being provided perceptual information that is completely different from what it saw during the reinforcement learning step, without any further training. Table 1 summarizes the transfer performance in terms of average reward observations throughout an episode of 300 frames. Our approach avaes + ddpg is compared with two baselines:",
        "type": "NarrativeText"
    },
    {
        "element_id": "c9ecfbc60ac95571efa17a9f567298fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        188.8,
                        1020.3
                    ],
                    [
                        188.8,
                        1141.7
                    ],
                    [
                        828.7,
                        1141.7
                    ],
                    [
                        828.7,
                        1020.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92644,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "\u2022 random baseline, which depicts the performance of an un- trained agent. This effectively simulates the performance one would expect from a non-transferable policy trained over image inputs, and later tested over sound inputs",
        "type": "ListItem"
    },
    {
        "element_id": "b646e62c19f85c08028d5d06425811ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        187.9,
                        1142.1
                    ],
                    [
                        187.9,
                        1263.3
                    ],
                    [
                        828.8,
                        1263.3
                    ],
                    [
                        828.8,
                        1142.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92744,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "\u2022 sound ddpg baseline, a DDPG agent trained directly over sound inputs (i.e. the sounds correspond to the states). Pro- vides an estimate on the performance an agent trained di- rectly over the test input modality may achieve.",
        "type": "ListItem"
    },
    {
        "element_id": "6eb3a676e8b31ddba6194ef49435d7a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1270.9
                    ],
                    [
                        148.5,
                        1481.0
                    ],
                    [
                        823.4,
                        1481.0
                    ],
                    [
                        823.4,
                        1270.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95393,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "From Table 1, we conclude our approach provides the agent with an out-of-box performance improvement of over 300%, when compared to the untrained agent (non-transferable policy). It is also interesting to observe that the difference in performance between our agent and sound ddpg seems small, supporting our empirical observation that the transfer policy succeeds very often in the task: swinging the pole up1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "619ac8d4d9e469046c3036d5cd4f84d8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1508.8
                    ],
                    [
                        149.4,
                        1539.1
                    ],
                    [
                        367.0,
                        1539.1
                    ],
                    [
                        367.0,
                        1508.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "4.2 hyperhot",
        "type": "Title"
    },
    {
        "element_id": "6af1a7d848412dd089ac0804b25e1937",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.2,
                        1552.2
                    ],
                    [
                        148.2,
                        1577.1
                    ],
                    [
                        817.2,
                        1577.1
                    ],
                    [
                        817.2,
                        1552.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "619ac8d4d9e469046c3036d5cd4f84d8"
        },
        "text": "We consider the hyperhot scenario, a novel top-down shooter",
        "type": "NarrativeText"
    },
    {
        "element_id": "237aa268e8fda0f916d5600d4f6f25a9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1559.9
                    ],
                    [
                        149.4,
                        1672.2
                    ],
                    [
                        819.7,
                        1672.2
                    ],
                    [
                        819.7,
                        1559.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93834,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "619ac8d4d9e469046c3036d5cd4f84d8"
        },
        "text": "game scenario inspired by the space invaders Atari game2, where the goal of the agent is to shoot the enemies above, while avoiding their bullets by moving left and right.",
        "type": "NarrativeText"
    },
    {
        "element_id": "951fc2c1b55cf4decb5a8faaa8af4577",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1673.9
                    ],
                    [
                        177.1,
                        1698.8
                    ],
                    [
                        816.8,
                        1698.8
                    ],
                    [
                        816.8,
                        1673.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "25cc6b684366d65dfb9df988dedee9f7"
        },
        "text": "Similarly to the pendulum, in this scenario, the observations",
        "type": "Title"
    },
    {
        "element_id": "0eae9abce864a1201a066f785bbf8898",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1681.0
                    ],
                    [
                        149.4,
                        1804.6
                    ],
                    [
                        821.6,
                        1804.6
                    ],
                    [
                        821.6,
                        1681.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93366,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "951fc2c1b55cf4decb5a8faaa8af4577"
        },
        "text": "of the environment include both image and sound components. In hyperhot, however, the environmental sound is generated by multiple entities ei emitting a predefined frequency f (i) 0 :",
        "type": "NarrativeText"
    },
    {
        "element_id": "1daac434f34f047e43dc5252432cf6e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.1,
                        1828.1
                    ],
                    [
                        149.1,
                        1850.7
                    ],
                    [
                        816.8,
                        1850.7
                    ],
                    [
                        816.8,
                        1828.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "951fc2c1b55cf4decb5a8faaa8af4577"
        },
        "text": "1We also note that the performance achieved by the sound ddpg agent is similar to",
        "type": "NarrativeText"
    },
    {
        "element_id": "321cb231b1bfc2f302ab161e2bcfcb69",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.2,
                        1834.8
                    ],
                    [
                        147.2,
                        1898.9
                    ],
                    [
                        825.8,
                        1898.9
                    ],
                    [
                        825.8,
                        1834.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67982,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "that reported in the OpenAI gym leaderboard for the pendulum scenario with state observations as the position and velocity of the pendulum.",
        "type": "Footer"
    },
    {
        "element_id": "f8bacd8121e7e2bf230a520bbb6f7375",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        135.0,
                        1896.5
                    ],
                    [
                        135.0,
                        1964.1
                    ],
                    [
                        836.9,
                        1964.1
                    ],
                    [
                        836.9,
                        1896.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74404,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "2We opted to use a custom environment implemented in pygame, since the space invaders environment in OpenAI gym does not provide access to game state, making it hard to generate simulated sounds.",
        "type": "Footer"
    },
    {
        "element_id": "e9cba2a168b35f3871bd93a6b8d7cb63",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.9,
                        2086.8
                    ],
                    [
                        828.9,
                        2107.8
                    ],
                    [
                        873.6,
                        2107.8
                    ],
                    [
                        873.6,
                        2086.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.39712,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "1265",
        "type": "Footer"
    },
    {
        "element_id": "ce24e08d261df68a6935ea8e9437b95d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1043.1,
                        96.2
                    ],
                    [
                        1043.1,
                        118.1
                    ],
                    [
                        1550.3,
                        118.1
                    ],
                    [
                        1550.3,
                        96.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82315,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "25a832ee224fb334eee5ed4c6ac2f6b3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1108.6,
                        232.5
                    ],
                    [
                        1108.6,
                        472.3
                    ],
                    [
                        1325.2,
                        472.3
                    ],
                    [
                        1325.2,
                        232.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-6-9.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "7678f5cd6fb2af6a1b5a351e4be567a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        879.8,
                        502.0
                    ],
                    [
                        879.8,
                        624.0
                    ],
                    [
                        1557.0,
                        624.0
                    ],
                    [
                        1557.0,
                        502.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76197,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "Figure 7: Visual and sound perceptual information in the hy- perhot scenario. All enemies and bullets emit sounds that are received by four microphones at bottom left and right (bl, br ) and paddle left and right (pl, pr ).",
        "type": "FigureCaption"
    },
    {
        "element_id": "a9c91a9e645a9c768476f92e0f099d9a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        922.2,
                        694.9
                    ],
                    [
                        922.2,
                        764.7
                    ],
                    [
                        1559.8,
                        764.7
                    ],
                    [
                        1559.8,
                        694.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88739,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "\u2022 Left-side enemy units, e0, and right-side enemy units, e1, (0) (1) emit sounds with frequencies f and f , respectively. 0 0",
        "type": "ListItem"
    },
    {
        "element_id": "86a1e59a0e9ceec92a3071cf91c31350",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1445.6,
                        761.8
                    ],
                    [
                        1445.6,
                        785.0
                    ],
                    [
                        1468.4,
                        785.0
                    ],
                    [
                        1468.4,
                        761.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "(2)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "44ee877e1e2b0ac8086a171202b1d7fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1430.0,
                        767.0
                    ],
                    [
                        1430.0,
                        843.0
                    ],
                    [
                        1521.0,
                        843.0
                    ],
                    [
                        1521.0,
                        767.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "(2 : f2..",
        "type": "UncategorizedText"
    },
    {
        "element_id": "511a6acff5bd6a9086e80cf50e7534e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        927.6,
                        769.9
                    ],
                    [
                        927.6,
                        800.6
                    ],
                    [
                        1439.8,
                        800.6
                    ],
                    [
                        1439.8,
                        769.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "Enemy bullets. e2, emit sounds with frequency f",
        "type": "ListItem"
    },
    {
        "element_id": "86420de32f80100860ece65c7a83086e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        927.6,
                        809.1
                    ],
                    [
                        927.6,
                        839.9
                    ],
                    [
                        1487.2,
                        839.9
                    ],
                    [
                        1487.2,
                        809.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "The agent\u2019s bullets, e3, emit sounds with frequency f",
        "type": "ListItem"
    },
    {
        "element_id": "c1362a0106f1fca1eeb684e16261d962",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        850.5
                    ],
                    [
                        882.3,
                        1061.8
                    ],
                    [
                        1554.7,
                        1061.8
                    ],
                    [
                        1554.7,
                        850.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94818,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "The sounds produced by these entities are received by a set of S sound receivers {\u03c11, . . . , \u03c1S }. Figure 7 depicts the scenario, where the yellow circles are the enemies; the green and blue bullets are friendly and enemy fire, respectively; the agent is in red; and the sound receivers correspond to the white circles. The agent is re- warded for shooting the enemies, with the following reward func- tion:",
        "type": "NarrativeText"
    },
    {
        "element_id": "3a46b91b6b22453fbd8f381f563e9d18",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        937.7,
                        1083.7
                    ],
                    [
                        937.7,
                        1190.0
                    ],
                    [
                        1479.9,
                        1190.0
                    ],
                    [
                        1479.9,
                        1083.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.61626,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "r = 10 \u22121 if all enemies are killed, i.e., win if player is killed or time is up, i.e., lose 0 otherwise",
        "type": "Formula"
    },
    {
        "element_id": "78cc47b024a92d57cc2c9582411f5b39",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.5,
                        1209.4
                    ],
                    [
                        882.5,
                        1234.3
                    ],
                    [
                        1550.5,
                        1234.3
                    ],
                    [
                        1550.5,
                        1209.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "The environment resets whenever the agent collects a non-zero",
        "type": "NarrativeText"
    },
    {
        "element_id": "8c928c1eea0810e7e013d3ca4cefb706",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        878.8,
                        1216.4
                    ],
                    [
                        878.8,
                        1268.8
                    ],
                    [
                        1556.0,
                        1268.8
                    ],
                    [
                        1556.0,
                        1216.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93181,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "reward, be it due to winning or losing the game.",
        "type": "NarrativeText"
    },
    {
        "element_id": "79a7ba1d0894b8763f20c8692be33ad4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.3,
                        1270.3
                    ],
                    [
                        882.3,
                        1519.4
                    ],
                    [
                        1556.5,
                        1519.4
                    ],
                    [
                        1556.5,
                        1270.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95404,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "We let the perceptual space of the agent be as I = Iimage \u00d7Isound, with the visual input modality of the agent, Ivision, consisting in the raw image observation of the environment. The sound, however, is generated in a more complex and realistic way. We model the sinusoidal wave of each sound-emitter ei considering its specific fre- (i) (i) quency f and amplitude a 0 . At every frame, we take the sound 0 waves of every emitter present in the screen, considering their dis- tance to each sound receiver in S. The sound wave generated by",
        "type": "NarrativeText"
    },
    {
        "element_id": "40fed5e83a735c8d0906ceb260ac3874",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.4,
                        1580.7
                    ],
                    [
                        882.4,
                        1605.6
                    ],
                    [
                        1551.1,
                        1605.6
                    ],
                    [
                        1551.1,
                        1580.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "Table 1: Zero-shot performance of the policy trained over",
        "type": "NarrativeText"
    },
    {
        "element_id": "691f0115c3250642f1f8c58b9d3b9ab3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1587.7
                    ],
                    [
                        883.2,
                        1700.0
                    ],
                    [
                        1557.6,
                        1700.0
                    ],
                    [
                        1557.6,
                        1587.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91932,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "the image input modality, when using sound inputs only. Presents the average reward per episode, over 75 episodes. Results averaged over 10 randomly seeded runs.",
        "type": "NarrativeText"
    },
    {
        "element_id": "eba3d0cd78b765b2185ca6ddacc27f3d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1162.7,
                        1735.6
                    ],
                    [
                        1162.7,
                        1760.5
                    ],
                    [
                        1271.1,
                        1760.5
                    ],
                    [
                        1271.1,
                        1735.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6
        },
        "text": "pendulum",
        "type": "Title"
    },
    {
        "element_id": "abaedc04e03e04664f65736ce082db68",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1031.2,
                        1760.2
                    ],
                    [
                        1031.2,
                        1960.9
                    ],
                    [
                        1398.4,
                        1960.9
                    ],
                    [
                        1398.4,
                        1760.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88866,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-6-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 6,
            "parent_id": "eba3d0cd78b765b2185ca6ddacc27f3d",
            "text_as_html": "<table><thead><tr><th></th><th>Rewards</th></tr></thead><tbody><tr><td colspan=\"2\">Approach avg + std</td></tr><tr><td colspan=\"2\">AVAEs + DDPG \u20142.00 + 0.97</td></tr><tr><td colspan=\"2\">RANDOM \u20146.30 + 0.29</td></tr><tr><td>SOUND DDPG</td><td>\u20141.41+0.91</td></tr></tbody></table>"
        },
        "text": "Rewards Approach avg \u00b1 std avaes + ddpg \u22122.00 \u00b1 0.97 random \u22126.30 \u00b1 0.29 sound ddpg \u22121.41 \u00b1 0.91",
        "type": "Table"
    },
    {
        "element_id": "412bc0909fe4c23abf237a068cc3cb47",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.0,
                        97.7
                    ],
                    [
                        149.0,
                        118.3
                    ],
                    [
                        307.1,
                        118.3
                    ],
                    [
                        307.1,
                        97.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73297,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "675b4199bb8d4ed58b904b98e1b2cde5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        235.3
                    ],
                    [
                        149.4,
                        268.6
                    ],
                    [
                        544.5,
                        268.6
                    ],
                    [
                        544.5,
                        235.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "412bc0909fe4c23abf237a068cc3cb47"
        },
        "text": "emitter ei is observed by receiver \u03c1j as",
        "type": "NarrativeText"
    },
    {
        "element_id": "d78fd5432ef6b8d41703885749f831b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        326.8,
                        273.9
                    ],
                    [
                        326.8,
                        327.8
                    ],
                    [
                        640.5,
                        327.8
                    ],
                    [
                        640.5,
                        273.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77908,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "a) = al! exp (dllei \u2014 pl).",
        "type": "Formula"
    },
    {
        "element_id": "5278c399f4123b0c34f33d6179ff3770",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        146.2,
                        333.9
                    ],
                    [
                        146.2,
                        642.4
                    ],
                    [
                        823.6,
                        642.4
                    ],
                    [
                        823.6,
                        333.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95488,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "where \u03b4 is a scaling constant, ei and \u03c1 j denote the positions of sound emmitter ei and sound receiver \u03c1j , respectively. We generate each sinusoidal sound wave for a total of 1047 discrete time steps, considering an audio sample rate of 31 400 Hz and a video frame- rate of 30 fps. As such, each sinusoidal sound wave represents the sound heard for the duration of a single video-frame of the game (similarly to what is performed in Atari videogames). Finally, for each sound receiver, we sum all emitted waves and encode the amplitude values in 16-bit audio depth, considering a maximum amplitude value of aM and a minimum value of \u2212aM .",
        "type": "NarrativeText"
    },
    {
        "element_id": "10f6453b47bf6161978c96ba3f9f5416",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        638.9
                    ],
                    [
                        177.1,
                        663.8
                    ],
                    [
                        816.8,
                        663.8
                    ],
                    [
                        816.8,
                        638.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "We now provide details on how our approach was set up. All",
        "type": "NarrativeText"
    },
    {
        "element_id": "5de8add9c841314c8f2143128938a090",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.3,
                        647.5
                    ],
                    [
                        149.3,
                        698.2
                    ],
                    [
                        820.6,
                        698.2
                    ],
                    [
                        820.6,
                        647.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92122,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "constants and training hyper-parameters used are in Appendix A.1.",
        "type": "NarrativeText"
    },
    {
        "element_id": "41c90dc2c032311a52079aea4707e779",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        715.5
                    ],
                    [
                        148.5,
                        897.7
                    ],
                    [
                        821.9,
                        897.7
                    ],
                    [
                        821.9,
                        715.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95261,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "4.2.1 Learn a perceptual model of the world. We trained an AVAE model to learn the family of latent mapping F , with a dataset D(I) with M observations of images and sounds collected using a random controller. Before training, the images were preprocessed to black and white and resized to 80 \u00d7 80 pixels, and the sounds normalized to the range [0, 1].",
        "type": "NarrativeText"
    },
    {
        "element_id": "ea076f19b00a8d50d3313e786b6fb2a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        898.1
                    ],
                    [
                        177.1,
                        923.0
                    ],
                    [
                        816.8,
                        923.0
                    ],
                    [
                        816.8,
                        898.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "For the image-specific encoder we adopted an architecture with",
        "type": "NarrativeText"
    },
    {
        "element_id": "69ab5dc89b3133fae1535a5e43c5b4dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.6,
                        905.0
                    ],
                    [
                        148.6,
                        1262.3
                    ],
                    [
                        824.8,
                        1262.3
                    ],
                    [
                        824.8,
                        905.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95681,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "three convolutional layers and two fully connected layers. The three convolutional layers learned 32, 64 and 64 filters, respectively. The filters were parameterized by kernel sizes 8, 4 and 2; strides 4, 2 and 1; and paddings 2, 1 and 1. ReLU activations were used throughout. For the sound-specific component, we used two fully connected layers of 512 neurons each, with one dimension batch normalization between the layers. The decoders followed similar architectures. The increase in size of these layers when compared to the pendulum task is due to more complex nature of the sounds considered in this scenario. The optimizer and loss function were configured in the same way as in the previous scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9ee43945c4890fa5e6be126abf5b0cdf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.4,
                        1279.1
                    ],
                    [
                        148.4,
                        1490.7
                    ],
                    [
                        823.3,
                        1490.7
                    ],
                    [
                        823.3,
                        1279.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95418,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "4.2.2 Learn to act in the world. The agent learned how to play the game using the DQN algorithm, while having access only to image observations, I train = Iimage, corresponding to the video game frames. The image observations are encoded into the latent space using Ftrain = Fimage\u2014the image-specific encoder of the AVAE model trained in the previous step. As such, the learned policy maps these latent states to actions.",
        "type": "NarrativeText"
    },
    {
        "element_id": "26000f6f8690df2e5d6af736b2e62d33",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1492.2
                    ],
                    [
                        177.1,
                        1517.1
                    ],
                    [
                        816.8,
                        1517.1
                    ],
                    [
                        816.8,
                        1492.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "The policy and target networks consisted of two fully connected",
        "type": "NarrativeText"
    },
    {
        "element_id": "522c854cd0033c41ad51cb25980a25fa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1499.4
                    ],
                    [
                        149.4,
                        1553.3
                    ],
                    [
                        820.6,
                        1553.3
                    ],
                    [
                        820.6,
                        1499.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91956,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "layers of 512 neurons each. We adopted a decaying \u03f5-greedy policy.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0c491538e836a2bce508e61aad223eb3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.7,
                        1568.7
                    ],
                    [
                        148.7,
                        1780.5
                    ],
                    [
                        823.7,
                        1780.5
                    ],
                    [
                        823.7,
                        1568.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95346,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "4.2.3 Transfer policy. We then evaluated the performance of the policy learned with image inputs, when the agent only has access to the sound modality, i.e., I test = Isound. Given a sound observation, the agent preprocesses it using the latent map Ftest = Fsound, thus generating a multimodal latent state z\u2014this process is denoted as avaes. The agent then uses the policy to select the optimal action in this latent state.",
        "type": "NarrativeText"
    },
    {
        "element_id": "208026b80392c3c13936f58b256eb217",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1781.8
                    ],
                    [
                        177.1,
                        1806.7
                    ],
                    [
                        821.0,
                        1806.7
                    ],
                    [
                        821.0,
                        1781.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "Table 2 summarizes the transfer performance of the policy pro-",
        "type": "NarrativeText"
    },
    {
        "element_id": "1e65b106b5f1d43b350f759c87fa29b0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1788.8
                    ],
                    [
                        149.4,
                        1900.4
                    ],
                    [
                        827.8,
                        1900.4
                    ],
                    [
                        827.8,
                        1788.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93765,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "duced by our approach avaes + dqn, in terms of average discounted rewards and game win rates over 100 episodes. We compare the performance of our approach with additional baselines:",
        "type": "NarrativeText"
    },
    {
        "element_id": "3afe905adbb448f3a71a2b06342f40f1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        191.4,
                        1909.3
                    ],
                    [
                        191.4,
                        1967.6
                    ],
                    [
                        826.4,
                        1967.6
                    ],
                    [
                        826.4,
                        1909.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88867,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "\u2022 avaev + dqn, an agent similar to ours, but which encodes the latent space with visual observations (as opposed to sounds).",
        "type": "ListItem"
    },
    {
        "element_id": "f53afa5bb353f93be2ee41361fddf131",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.9,
                        2086.8
                    ],
                    [
                        828.9,
                        2108.3
                    ],
                    [
                        873.1,
                        2108.3
                    ],
                    [
                        873.1,
                        2086.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.60912,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "1266",
        "type": "Footer"
    },
    {
        "element_id": "2bdf74084274d33daa58c063cf2635c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1043.9,
                        97.1
                    ],
                    [
                        1043.9,
                        117.9
                    ],
                    [
                        1549.9,
                        117.9
                    ],
                    [
                        1549.9,
                        97.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83192,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "c28a88972abf1e6db078d56ebe433dd5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        927.6,
                        235.3
                    ],
                    [
                        927.6,
                        263.9
                    ],
                    [
                        1554.2,
                        263.9
                    ],
                    [
                        1554.2,
                        235.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "image dqn, a DQN agent trained directly over visual inputs.",
        "type": "ListItem"
    },
    {
        "element_id": "5a225255181f4795755d057e5fd44091",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        910.9,
                        275.7
                    ],
                    [
                        910.9,
                        300.6
                    ],
                    [
                        1384.8,
                        300.6
                    ],
                    [
                        1384.8,
                        275.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "Considering the results in Table 2, we observe:",
        "type": "NarrativeText"
    },
    {
        "element_id": "ea2c0514f5ab5e170b1ce4b8c5157d01",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        927.6,
                        316.1
                    ],
                    [
                        927.6,
                        344.7
                    ],
                    [
                        1550.5,
                        344.7
                    ],
                    [
                        1550.5,
                        316.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "A considerable performance improvement of our approach",
        "type": "ListItem"
    },
    {
        "element_id": "78ab1b27d8c2682c0f048d1ac0680cc5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        915.6,
                        323.3
                    ],
                    [
                        915.6,
                        527.1
                    ],
                    [
                        1566.2,
                        527.1
                    ],
                    [
                        1566.2,
                        323.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94275,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "over the untrained agent. The average discounted reward of the random baseline is negative, meaning this agent tends to get shot often, and rather quickly. This is in contrast with the positive rewards achieved by our approach. Moreover, the win rates achieved by our approach surpass those of the untrained agent by 5-fold.",
        "type": "ListItem"
    },
    {
        "element_id": "31710edeeb78fe4adf4696a1b240edfc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        916.9,
                        529.2
                    ],
                    [
                        916.9,
                        741.1
                    ],
                    [
                        1565.7,
                        741.1
                    ],
                    [
                        1565.7,
                        529.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94705,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "\u2022 A performance comparable to that of the agent trained di- rectly on the sound, sound dqn. In fact, the average dis- counted rewards achieved by our approach are slightly high- er. However, the sound dqn agent followed the same DQN architecture and number of training steps used in our ap- proach. It is plausible that with further parameter tuning, the sound dqn agent could achieve better performances.",
        "type": "ListItem"
    },
    {
        "element_id": "9de5ac5468f22c5b068119cb30908ef6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        917.1,
                        742.3
                    ],
                    [
                        917.1,
                        922.7
                    ],
                    [
                        1557.5,
                        922.7
                    ],
                    [
                        1557.5,
                        742.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93726,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "\u2022 The approach that could fine-tune to the most informative perceptual modality, image dqn, achieved the highest perfor- mances. While achieving lower performances, our approach is the only able to perform cross-modality policy transfer, that is, being able to reuse a policy trained on a different modality. One may argue that this trade-off is worthwhile.",
        "type": "ListItem"
    },
    {
        "element_id": "7188adf4382f4352a6652f240a75a1cc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.8,
                        934.9
                    ],
                    [
                        881.8,
                        995.4
                    ],
                    [
                        1560.0,
                        995.4
                    ],
                    [
                        1560.0,
                        934.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91196,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "The DQN networks of all approaches followed similar architectures and were trained for the same number of iterations.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ce63850067475231061f806347c04a2d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        1023.7
                    ],
                    [
                        883.2,
                        1059.7
                    ],
                    [
                        1104.1,
                        1059.7
                    ],
                    [
                        1104.1,
                        1023.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86565,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "4.3 Discussion",
        "type": "Title"
    },
    {
        "element_id": "67f43ed0aa4ef70e9c8f70237e15c3c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        882.5,
                        1067.0
                    ],
                    [
                        882.5,
                        1400.9
                    ],
                    [
                        1557.4,
                        1400.9
                    ],
                    [
                        1557.4,
                        1067.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95857,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "ce63850067475231061f806347c04a2d"
        },
        "text": "The experimental evaluation performed shows the efficacy and applicability of our approach. The results show that this approach effectively enables an agent to learn and exploit policies over dif- ferent subsets of input modalities. This sets our work apart from existing ideas in the literature. For example, DARLA follows a simi- lar three-stages architecture to allow RL agents to learn policies that are robust to some shifts in the original domains [7]. However, that approach implicitly assumes that the source and target domains are characterized by similar inputs, such as raw observations of a camera. This is in contrast with our work, which allows agents to transfer policies across different input modalities.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6c81f677959eb2d2235012c52509df04",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        881.1,
                        1459.5
                    ],
                    [
                        881.1,
                        1641.1
                    ],
                    [
                        1557.0,
                        1641.1
                    ],
                    [
                        1557.0,
                        1459.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95546,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "ce63850067475231061f806347c04a2d"
        },
        "text": "Table 2: Zero-shot performance of the policy trained over the image modality, when using sound inputs only. Provides a comparison with different baselines. Middle column is the average discounted reward per episode. Right column is the win rate of the agent. Both averaged over 100 episodes. Re- sults averaged over 10 randomly seeded runs.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ed34e9ee4622225c60fd26201b1a5605",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1164.1,
                        1675.2
                    ],
                    [
                        1164.1,
                        1700.1
                    ],
                    [
                        1269.7,
                        1700.1
                    ],
                    [
                        1269.7,
                        1675.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "2bdf74084274d33daa58c063cf2635c5"
        },
        "text": "hyperhot",
        "type": "Title"
    },
    {
        "element_id": "d5a02c12a860e409c9516f29008fc588",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        904.5,
                        1689.0
                    ],
                    [
                        904.5,
                        1959.9
                    ],
                    [
                        1525.2,
                        1959.9
                    ],
                    [
                        1525.2,
                        1689.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92174,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-7-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 7,
            "parent_id": "ed34e9ee4622225c60fd26201b1a5605",
            "text_as_html": "<table><thead><tr><th rowspan=\"2\">Approach</th><th>Rewards</th><th rowspan=\"2\">Win pet avg + std</th></tr><tr><th>avg + std</th></tr></thead><tbody><tr><td>AVAEs + DQN</td><td>0.15 + 0.16</td><td>36.10 + 10.38</td></tr><tr><td>AVAEy + DQN</td><td>0.21+0.11</td><td>43.20 + 7.03</td></tr><tr><td>RANDOM</td><td>\u20140.33 + 0.16</td><td>8.30 + 5.75</td></tr><tr><td>SOUND DQN</td><td>0.10 + 0.22</td><td>27.30 + 21.44</td></tr><tr><td>IMAGE DQN</td><td>1.54 + 0.20</td><td>75.00 + 5.33</td></tr></tbody></table>"
        },
        "text": "Rewards Win pct Approach avg \u00b1 std avg \u00b1 std avaes + dqn 0.15 \u00b1 0.16 36.10 \u00b1 10.38 avaev + dqn 0.21 \u00b1 0.11 43.20 \u00b1 7.03 random \u22120.33 \u00b1 0.16 8.30 \u00b1 5.75 sound dqn 0.10 \u00b1 0.22 27.30 \u00b1 21.44 image dqn 1.54 \u00b1 0.20 75.00 \u00b1 5.33",
        "type": "Table"
    },
    {
        "element_id": "58ba6ae19cba5773a550a6134dfd6671",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.0,
                        97.1
                    ],
                    [
                        149.0,
                        118.2
                    ],
                    [
                        307.4,
                        118.2
                    ],
                    [
                        307.4,
                        97.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78821,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "580a1746611779f20900b7b184d6480c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        235.3
                    ],
                    [
                        177.1,
                        260.2
                    ],
                    [
                        816.8,
                        260.2
                    ],
                    [
                        816.8,
                        235.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "Our approach achieves this by first learning a shared latent",
        "type": "NarrativeText"
    },
    {
        "element_id": "55a16d3a4508bc6288935c7bf340f31e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.4,
                        243.8
                    ],
                    [
                        148.4,
                        599.5
                    ],
                    [
                        824.0,
                        599.5
                    ],
                    [
                        824.0,
                        243.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9551,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "representation that captures the different input modalities. In our experimental evaluation, for this first step, we used the AVAE model, which approximates modality-specific latent representations, as discussed in Section 3.1. This model is well-suited to the scenarios considered, since these focused on the transfer of policies trained and reused over distinct input modalities. We envision other sce- narios where training could potentially take into account multiple input modalities at the same time. Our approach supports these scenarios as well, when considering a generative model such as JMVAE [16], which can learn joint modality distributions and en- code/decode both modalities simultaneously.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f6ab3ac8e8840395aae65f2306937cbf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        600.6
                    ],
                    [
                        177.1,
                        625.5
                    ],
                    [
                        816.8,
                        625.5
                    ],
                    [
                        816.8,
                        600.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "Furthermore, our approach also supports scenarios where the",
        "type": "NarrativeText"
    },
    {
        "element_id": "48c1e8909e228a6a6782c1b112482e8b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        609.1
                    ],
                    [
                        149.4,
                        813.0
                    ],
                    [
                        822.6,
                        813.0
                    ],
                    [
                        822.6,
                        609.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95423,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "agent has access to more than two input modalities. The AVAE model can be extended to approximate additional modalities, by introducing extra loss terms that compute the divergence of the new modality specific latent spaces. However, it may be benefi- cial to employ generative models specialized on larger number of modalities, such as the M2VAE [9].",
        "type": "NarrativeText"
    },
    {
        "element_id": "688e37bec034df52ae6b0a039c543f35",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        856.7
                    ],
                    [
                        149.4,
                        887.0
                    ],
                    [
                        408.5,
                        887.0
                    ],
                    [
                        408.5,
                        856.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "5 CONCLUSIONS",
        "type": "Title"
    },
    {
        "element_id": "8460a69e1afcd199775e8c8fd9e09c59",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        900.0
                    ],
                    [
                        149.4,
                        1081.5
                    ],
                    [
                        825.2,
                        1081.5
                    ],
                    [
                        825.2,
                        900.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94931,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "688e37bec034df52ae6b0a039c543f35"
        },
        "text": "In this paper we explored the use of multimodal latent representa- tions to capture multiple input modalities, in order to allow agents to learn and reuse policies over different modalities. We were partic- ularly motivated by scenarios of RL agents that learn visual policies to perform their tasks, and which afterwards, at test time, may only have access to sound inputs.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f58fd3ae6f5db0d8308a87b4bf0906ca",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1082.7
                    ],
                    [
                        177.1,
                        1107.6
                    ],
                    [
                        816.8,
                        1107.6
                    ],
                    [
                        816.8,
                        1082.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "688e37bec034df52ae6b0a039c543f35"
        },
        "text": "To this end, we formalized the multimodal transfer reinforcement",
        "type": "NarrativeText"
    },
    {
        "element_id": "b0f556ebcc4b78f980db2ac222f1ac51",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1090.4
                    ],
                    [
                        148.5,
                        1447.0
                    ],
                    [
                        821.1,
                        1447.0
                    ],
                    [
                        821.1,
                        1090.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95478,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "688e37bec034df52ae6b0a039c543f35"
        },
        "text": "learning problem, and contributed a three stages approach that effectively allows RL agents to learn robust policies over input modalities. The first step builds upon recent advances in multimodal variational autoencoders, to create a generalized latent space that captures the dependencies between the different input modalities of the agent, allowing for cross-modality inference. In the second step, the agent learns how to perform its task over this latent space. During this training step, the agent may only have access to a subset of input modalities, with the latent space being encoded accordingly. Finally, at test time, the agent may execute its task while having access to a possibly different subset of modalities.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1b6e25f6c44ee9cbe65206267f72b6a5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        177.1,
                        1448.0
                    ],
                    [
                        177.1,
                        1472.9
                    ],
                    [
                        821.0,
                        1472.9
                    ],
                    [
                        821.0,
                        1448.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "688e37bec034df52ae6b0a039c543f35"
        },
        "text": "We assessed the applicability and efficacy of our approach in dif-",
        "type": "NarrativeText"
    },
    {
        "element_id": "ef4a857127fd21b21db2db2e8eeaa0ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.5,
                        1454.5
                    ],
                    [
                        148.5,
                        1661.3
                    ],
                    [
                        821.0,
                        1661.3
                    ],
                    [
                        821.0,
                        1454.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95418,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "688e37bec034df52ae6b0a039c543f35"
        },
        "text": "ferent domains of increasing complexity. We extended well-known scenarios in the reinforcement learning literature to include, both the typical raw image observations, but also the novel sound com- ponents. The results show that the policies learned by our approach were robust to these different input modalities, effectively enabling reinforcement learning agents to play games in the dark.",
        "type": "NarrativeText"
    },
    {
        "element_id": "35805de33923a96a85c565f43a726696",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        1704.0
                    ],
                    [
                        149.4,
                        1734.3
                    ],
                    [
                        476.0,
                        1734.3
                    ],
                    [
                        476.0,
                        1704.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "58ba6ae19cba5773a550a6134dfd6671"
        },
        "text": "ACKNOWLEDGMENTS",
        "type": "Title"
    },
    {
        "element_id": "4878d096b5eafa4c61c1142118bbbc61",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        147.9,
                        1747.4
                    ],
                    [
                        147.9,
                        1957.8
                    ],
                    [
                        826.7,
                        1957.8
                    ],
                    [
                        826.7,
                        1747.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94895,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "35805de33923a96a85c565f43a726696"
        },
        "text": "This work was partially supported by national funds through the Portuguese Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia under project UIDB/50021/2020 (INESC-ID multi annual funding) and the Car- negie Mellon Portugal Program and its Information and Communi- cations Technologies Institute, under project CMUP-ERI/HCI/0051/ 2013. Rui Silva acknowledges the PhD grant SFRH/BD/113695/2015. Miguel Vasco acknowledges the PhD grant SFRH/BD/139362/2018.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7b8f34cae513bb5d389035d5e990c77f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.8,
                        2085.9
                    ],
                    [
                        828.8,
                        2108.2
                    ],
                    [
                        872.8,
                        2108.2
                    ],
                    [
                        872.8,
                        2085.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.33846,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8
        },
        "text": "1267",
        "type": "Header"
    },
    {
        "element_id": "cfed0e0ac4d5111dd69255d472e4b22d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1042.3,
                        97.5
                    ],
                    [
                        1042.3,
                        118.0
                    ],
                    [
                        1549.3,
                        118.0
                    ],
                    [
                        1549.3,
                        97.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86592,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "eb9308d4a1df5c1d80a0297b8a65e7df",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        230.0
                    ],
                    [
                        883.2,
                        260.3
                    ],
                    [
                        1091.6,
                        260.3
                    ],
                    [
                        1091.6,
                        230.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "cfed0e0ac4d5111dd69255d472e4b22d"
        },
        "text": "A APPENDIX",
        "type": "Title"
    },
    {
        "element_id": "2a53927036eab17c31d1399a61289d87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        273.6
                    ],
                    [
                        883.2,
                        303.9
                    ],
                    [
                        1410.6,
                        303.9
                    ],
                    [
                        1410.6,
                        273.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "cfed0e0ac4d5111dd69255d472e4b22d"
        },
        "text": "A.1 Constants and hyper-parameters",
        "type": "Title"
    },
    {
        "element_id": "6c7993eaa1bf2d5c04ac80b18c4a9a03",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        970.3,
                        354.6
                    ],
                    [
                        970.3,
                        938.6
                    ],
                    [
                        1443.9,
                        938.6
                    ],
                    [
                        1443.9,
                        354.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88008,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-8-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "2a53927036eab17c31d1399a61289d87",
            "text_as_html": "<table><thead><tr><th></th><th>fo</th><th>440.0Hz</th></tr></thead><tbody><tr><td rowspan=\"4\">SCENARIO</td><td>K</td><td>1.0</td></tr><tr><td>c</td><td>20.0</td></tr><tr><td>sound receivers</td><td>(lb,rb,mt)</td></tr><tr><td>frame stack</td><td>2</td></tr><tr><td rowspan=\"6\">AVAE</td><td>latent space</td><td>10</td></tr><tr><td>images Asound&gt; Bs &amp;</td><td>1.0</td></tr><tr><td>batch size</td><td>128</td></tr><tr><td>epochs</td><td>500</td></tr><tr><td>NAVAE</td><td>1e-3</td></tr><tr><td>M</td><td>20000</td></tr><tr><td rowspan=\"7\">DDPG</td><td>batch size</td><td>128</td></tr><tr><td>Nactor&gt; Ycriric</td><td>1e\u20144, le-3</td></tr><tr><td></td><td>0.99</td></tr><tr><td>max episode length</td><td>300 frames</td></tr><tr><td>replay buffer</td><td>25 000</td></tr><tr><td>max frames</td><td>150 000</td></tr><tr><td>T</td><td>1e-3</td></tr></tbody></table>"
        },
        "text": "f0 440.0Hz K 1.0 scenario c 20.0 sound receivers frame stack 2 latent space 10 \u03bbimage, \u03bbsound, \u03b2, \u03b1 1.0 avae batch size 128 epochs 500 \u03b7avae 1e\u22123 M 20 000 batch size 128 \u03b7actor, \u03b7critic ddpg \u03b3 0.99 max episode length replay buffer 25 000 max frames 150 000 \u03c4 1e\u22123",
        "type": "Table"
    },
    {
        "element_id": "7427c4bf42817a53852897b79e51c28d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1336.1,
                        454.8
                    ],
                    [
                        1336.1,
                        481.8
                    ],
                    [
                        1448.4,
                        481.8
                    ],
                    [
                        1448.4,
                        454.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "cfed0e0ac4d5111dd69255d472e4b22d"
        },
        "text": "{lb, rb, mt }",
        "type": "Title"
    },
    {
        "element_id": "3055814dcd189cd145b77293ba650631",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1336.3,
                        750.1
                    ],
                    [
                        1336.3,
                        778.7
                    ],
                    [
                        1448.2,
                        778.7
                    ],
                    [
                        1448.2,
                        750.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "7427c4bf42817a53852897b79e51c28d"
        },
        "text": "1e\u22124, 1e\u22123",
        "type": "UncategorizedText"
    },
    {
        "element_id": "90d4665601c12e5c42c6ccd8e639358c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1337.3,
                        811.0
                    ],
                    [
                        1337.3,
                        835.9
                    ],
                    [
                        1447.2,
                        835.9
                    ],
                    [
                        1447.2,
                        811.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "cfed0e0ac4d5111dd69255d472e4b22d"
        },
        "text": "300 frames",
        "type": "Title"
    },
    {
        "element_id": "6e88fdb0916e01461f8d1431fc3d06e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        930.4,
                        941.8
                    ],
                    [
                        930.4,
                        966.7
                    ],
                    [
                        1502.5,
                        966.7
                    ],
                    [
                        1502.5,
                        941.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "90d4665601c12e5c42c6ccd8e639358c"
        },
        "text": "Table 3: Constants used in the pendulum scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2fa3229c37d30ad2fbcd2fb4a8812faa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        911.7,
                        1078.6
                    ],
                    [
                        911.7,
                        1751.6
                    ],
                    [
                        1515.8,
                        1751.6
                    ],
                    [
                        1515.8,
                        1078.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91984,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-8-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "90d4665601c12e5c42c6ccd8e639358c",
            "text_as_html": "<table><thead><tr><th></th><th>ge foo fo, fo</th><th>(261,329, 392, 466) Hz</th></tr></thead><tbody><tr><td></td><td>a0, Ap. 49, 4, AM</td><td>1.0</td></tr><tr><td>SCENARIO</td><td>\u00f6</td><td>0.025</td></tr><tr><td></td><td>c</td><td>20.0</td></tr><tr><td></td><td>sound receivers</td><td>{Ib, rb, pl, pr}</td></tr><tr><td rowspan=\"10\">AVAE</td><td>frame stack</td><td>2</td></tr><tr><td>latent space</td><td>40</td></tr><tr><td>Jimage</td><td>0.02</td></tr><tr><td>Asound</td><td>0.015</td></tr><tr><td>B</td><td>le-5</td></tr><tr><td>a</td><td>0.05</td></tr><tr><td>batch size</td><td>128</td></tr><tr><td>epochs</td><td>250</td></tr><tr><td>AVAR</td><td>1e\u20143</td></tr><tr><td>M</td><td>32000</td></tr><tr><td rowspan=\"6\">DON</td><td>batch size</td><td>128</td></tr><tr><td>n</td><td>le-5</td></tr><tr><td>y</td><td>0.99</td></tr><tr><td>max episode length</td><td>450 frames</td></tr><tr><td>replay buffer</td><td>350 000</td></tr><tr><td>max frames</td><td>1750 000</td></tr></tbody></table>"
        },
        "text": "scenario a 1 2 0 3 0 , f 0 , f 0 , f f 0 1 2 0 3 0, a 0, a 0, aM 0, a \u03b4 (261, 329, 392, 466) Hz 1.0 0.025 c 20.0 sound receivers {lb, rb, pl, pr } frame stack 2 latent space 40 \u03bbimage 0.02 \u03bbsound 0.015 \u03b2 1e\u22125 avae \u03b1 0.05 batch size 128 epochs 250 \u03b7avae 1e\u22123 M 32 000 batch size 128 \u03b7 1e\u22125 dqn \u03b3 0.99 max episode length 450 frames replay buffer 350 000 max frames 1 750 000",
        "type": "Table"
    },
    {
        "element_id": "a3efafdbcb73d1650e700dbeb3fe74c1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        951.8,
                        1757.9
                    ],
                    [
                        951.8,
                        1782.8
                    ],
                    [
                        1481.2,
                        1782.8
                    ],
                    [
                        1481.2,
                        1757.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 8,
            "parent_id": "90d4665601c12e5c42c6ccd8e639358c"
        },
        "text": "Table 4: Constants used in hyperhot scenario.",
        "type": "NarrativeText"
    },
    {
        "element_id": "df2bdab8d5cafbdd863a172536c4bc5a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.1,
                        96.3
                    ],
                    [
                        149.1,
                        118.4
                    ],
                    [
                        307.6,
                        118.4
                    ],
                    [
                        307.6,
                        96.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77445,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9
        },
        "text": "Research Paper",
        "type": "Header"
    },
    {
        "element_id": "d6309fd5c85ebe7f4d0813ad298ef224",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        149.4,
                        230.0
                    ],
                    [
                        149.4,
                        260.3
                    ],
                    [
                        342.3,
                        260.3
                    ],
                    [
                        342.3,
                        230.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "df2bdab8d5cafbdd863a172536c4bc5a"
        },
        "text": "REFERENCES",
        "type": "Title"
    },
    {
        "element_id": "0d3edf1a0f2a3ecb43147451a1a2a071",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        148.9,
                        239.9
                    ],
                    [
                        148.9,
                        266.7
                    ],
                    [
                        345.4,
                        266.7
                    ],
                    [
                        345.4,
                        239.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85034,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "df2bdab8d5cafbdd863a172536c4bc5a"
        },
        "text": "REFERENCES",
        "type": "Title"
    },
    {
        "element_id": "fabfe6e43e0b126e220197d16c8fa0ed",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        156.7,
                        270.6
                    ],
                    [
                        156.7,
                        339.8
                    ],
                    [
                        820.2,
                        339.8
                    ],
                    [
                        820.2,
                        270.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92918,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "0d3edf1a0f2a3ecb43147451a1a2a071"
        },
        "text": "[1] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research 47 (Jun 2013), 253\u2013279.",
        "type": "ListItem"
    },
    {
        "element_id": "d888e05b57a6344dbc65e21d4eb08408",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        337.0
                    ],
                    [
                        158.4,
                        356.3
                    ],
                    [
                        816.8,
                        356.3
                    ],
                    [
                        816.8,
                        337.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "df2bdab8d5cafbdd863a172536c4bc5a"
        },
        "text": "[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John",
        "type": "Title"
    },
    {
        "element_id": "ab008b7f548ce887ba9a54ff2754dd22",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        162.3,
                        342.2
                    ],
                    [
                        162.3,
                        404.5
                    ],
                    [
                        819.8,
                        404.5
                    ],
                    [
                        819.8,
                        342.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92639,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "Schulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. arXiv:cs.LG/1606.01540 (2016).",
        "type": "ListItem"
    },
    {
        "element_id": "069e8d5f02edd26779e9e1be1635c3fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        403.4
                    ],
                    [
                        158.4,
                        471.5
                    ],
                    [
                        822.4,
                        471.5
                    ],
                    [
                        822.4,
                        403.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93144,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[3] Antonion R. Damasio. 1989. Time-locked multiregional retroactivation: A systems-level proposal for the neural substrates of recall and recognition. Cogni- tion 33, 1-2 (1989), 25\u201362.",
        "type": "ListItem"
    },
    {
        "element_id": "94f3e09e21a5ab71021d6fc44094cba4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        469.8
                    ],
                    [
                        158.4,
                        537.9
                    ],
                    [
                        821.3,
                        537.9
                    ],
                    [
                        821.3,
                        469.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92812,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[4] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. 2016. Deep spatial autoencoders for visuomotor learning. In 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 512\u2013519.",
        "type": "ListItem"
    },
    {
        "element_id": "dd56ae8e4277f663047d96ee36ef93af",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        536.2
                    ],
                    [
                        158.4,
                        604.4
                    ],
                    [
                        823.2,
                        604.4
                    ],
                    [
                        823.2,
                        536.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92453,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[5] Shani Gamrian and Yoav Goldberg. 2019. Transfer Learning for Related Reinforce- ment Learning Tasks via Image-to-Image Translation. In Proceedings of the 36th International Conference on Machine Learning (ICML), Vol. 97. PMLR, 2063\u20132072.",
        "type": "ListItem"
    },
    {
        "element_id": "37dc483b38b7144390315662365fdd26",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        602.6
                    ],
                    [
                        158.4,
                        671.0
                    ],
                    [
                        822.4,
                        671.0
                    ],
                    [
                        822.4,
                        602.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92411,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[6] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Belle- mare. 2019. DeepMDP: Learning Continuous Latent Space Models for Represen- tation Learning. (2019). arXiv:cs.LG/1906.02736",
        "type": "ListItem"
    },
    {
        "element_id": "7c7ddfa9936c76b2ad16d98cd35313a5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        669.1
                    ],
                    [
                        158.4,
                        779.4
                    ],
                    [
                        822.8,
                        779.4
                    ],
                    [
                        822.8,
                        669.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93988,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[7] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexan- der Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. 2017. DARLA: Improving Zero-Shot Transfer in Reinforcement Learning. In Proceed- ings of the 34th International Conference on Machine Learning, Vol. 70. PMLR, 1480\u20131490.",
        "type": "ListItem"
    },
    {
        "element_id": "8738d87917374c8b5ab620b9f4fd4186",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        779.8
                    ],
                    [
                        158.4,
                        799.1
                    ],
                    [
                        819.8,
                        799.1
                    ],
                    [
                        819.8,
                        779.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[8] Diederik P. Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes.",
        "type": "NarrativeText"
    },
    {
        "element_id": "30f6fa2664c6d0eb469d87397b57cb83",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        161.1,
                        784.7
                    ],
                    [
                        161.1,
                        826.5
                    ],
                    [
                        815.5,
                        826.5
                    ],
                    [
                        815.5,
                        784.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91902,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "(2013). arXiv:stat.ML/1312.6114",
        "type": "ListItem"
    },
    {
        "element_id": "c5895c6cb4e36288e1002f8c6140a81f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        158.4,
                        822.1
                    ],
                    [
                        158.4,
                        844.6
                    ],
                    [
                        816.8,
                        844.6
                    ],
                    [
                        816.8,
                        822.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "[9] Timo Korthals. 2019. M2VAE - Derivation of a Multi-Modal Variational",
        "type": "NarrativeText"
    },
    {
        "element_id": "2522cc7360491d9f21acbe6b70e115d7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        162.7,
                        829.5
                    ],
                    [
                        162.7,
                        870.7
                    ],
                    [
                        824.3,
                        870.7
                    ],
                    [
                        824.3,
                        829.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90855,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "d888e05b57a6344dbc65e21d4eb08408"
        },
        "text": "Autoencoder Objective from the Marginal Joint Log-Likelihood. (2019).",
        "type": "ListItem"
    },
    {
        "element_id": "c05be6d4a02bb057b7dfb99fe42858a6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        828.8,
                        2086.9
                    ],
                    [
                        828.8,
                        2107.7
                    ],
                    [
                        872.8,
                        2107.7
                    ],
                    [
                        872.8,
                        2086.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.30902,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9
        },
        "text": "1268",
        "type": "Header"
    },
    {
        "element_id": "350976d92489f3df46bcfd6b0d6d5d3e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        1043.0,
                        97.6
                    ],
                    [
                        1043.0,
                        117.8
                    ],
                    [
                        1551.3,
                        117.8
                    ],
                    [
                        1551.3,
                        97.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.86788,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9
        },
        "text": "AAMAS 2020, May 9-13, Auckland, New Zealand",
        "type": "Header"
    },
    {
        "element_id": "2fc48d43ae720347a623d008937427f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        921.4,
                        240.8
                    ],
                    [
                        921.4,
                        263.5
                    ],
                    [
                        1115.9,
                        263.5
                    ],
                    [
                        1115.9,
                        240.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83714,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "arXiv:cs.LG/1903.07303",
        "type": "NarrativeText"
    },
    {
        "element_id": "ea4e07a0ed86f744c279e7a60aa72230",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        262.9
                    ],
                    [
                        883.2,
                        282.3
                    ],
                    [
                        1550.6,
                        282.3
                    ],
                    [
                        1550.6,
                        262.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[10] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom",
        "type": "NarrativeText"
    },
    {
        "element_id": "a2a902d3863f8b0d35097ec6fffb6551",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        889.6,
                        267.8
                    ],
                    [
                        889.6,
                        331.3
                    ],
                    [
                        1551.5,
                        331.3
                    ],
                    [
                        1551.5,
                        267.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92632,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. (2015). arXiv:cs.LG/1509.02971",
        "type": "ListItem"
    },
    {
        "element_id": "8406a93d24b2dffb8fff6371ed433aef",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        329.4
                    ],
                    [
                        883.2,
                        397.1
                    ],
                    [
                        1552.6,
                        397.1
                    ],
                    [
                        1552.6,
                        329.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92628,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[11] Kaspar Meyer and Antonio Damasio. 2009. Convergence and divergence in a neural architecture for recognition and memory. Trends in Neurosciences 32, 7 (2009), 376\u2013382.",
        "type": "ListItem"
    },
    {
        "element_id": "d44ea9ea6757216f8a0e8d4c0f1c3d25",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        395.8
                    ],
                    [
                        883.2,
                        486.1
                    ],
                    [
                        1553.6,
                        486.1
                    ],
                    [
                        1553.6,
                        395.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9347,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529.",
        "type": "ListItem"
    },
    {
        "element_id": "ff0fb5b3e3060485d8493927eccc5f2c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        484.3
                    ],
                    [
                        883.2,
                        529.9
                    ],
                    [
                        1552.0,
                        529.9
                    ],
                    [
                        1552.0,
                        484.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90666,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[13] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2017. Searching for Activation Functions. (2017). arXiv:cs.NE/1710.05941",
        "type": "ListItem"
    },
    {
        "element_id": "06bff2c87e7f2787ac4b16b88feff612",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        528.6
                    ],
                    [
                        883.2,
                        595.6
                    ],
                    [
                        1553.6,
                        595.6
                    ],
                    [
                        1553.6,
                        528.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92325,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. (2014). arXiv:stat.ML/1401.4082",
        "type": "ListItem"
    },
    {
        "element_id": "ea2ed6a3247e80910d0047693d8dd6a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        595.0
                    ],
                    [
                        883.2,
                        640.2
                    ],
                    [
                        1553.6,
                        640.2
                    ],
                    [
                        1553.6,
                        595.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90915,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[15] Richard Sutton and Andrew Barto. 1998. Reinforcement Learning: An Introduction. MIT press Cambridge.",
        "type": "ListItem"
    },
    {
        "element_id": "81eaf6963083edef2c8b1d92b54affce",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        639.3
                    ],
                    [
                        883.2,
                        685.4
                    ],
                    [
                        1550.6,
                        685.4
                    ],
                    [
                        1550.6,
                        639.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91534,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[16] Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. 2016. Joint Multimodal Learning with Deep Generative Models. (2016). arXiv:stat.ML/1611.01891",
        "type": "ListItem"
    },
    {
        "element_id": "a7e11d3d1bbc9628cf147ee42455be42",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        683.6
                    ],
                    [
                        883.2,
                        729.4
                    ],
                    [
                        1553.5,
                        729.4
                    ],
                    [
                        1553.5,
                        683.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90727,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[17] Christopher Watkins. 1989. Learning from delayed rewards. Ph.D. Dissertation. Cambridge University.",
        "type": "ListItem"
    },
    {
        "element_id": "bf045e5dc8aacd0d00e8e44daa621143",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        727.9
                    ],
                    [
                        883.2,
                        796.0
                    ],
                    [
                        1553.7,
                        796.0
                    ],
                    [
                        1553.7,
                        727.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92534,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[18] Hang Yin, Francisco S. Melo, Aude Billard, and Ana Paiva. 2017. Associate Latent Encodings in Learning from Demonstrations. In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI). AAAI Press, 3848\u20133854.",
        "type": "ListItem"
    },
    {
        "element_id": "4a7caf9eb46450734203cebf4765d06b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1700,
                "points": [
                    [
                        883.2,
                        794.3
                    ],
                    [
                        883.2,
                        861.6
                    ],
                    [
                        1554.2,
                        861.6
                    ],
                    [
                        1554.2,
                        794.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92508,
            "file_directory": "./uol-docs",
            "filename": "3398761.3398907.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:21:29",
            "page_number": 9,
            "parent_id": "350976d92489f3df46bcfd6b0d6d5d3e"
        },
        "text": "[19] Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey Levine. 2018. SOLAR: Deep Structured Representations for Model- Based Reinforcement Learning. (2018). arXiv:cs.LG/1808.09105",
        "type": "ListItem"
    }
]