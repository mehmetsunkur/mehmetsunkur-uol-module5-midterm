Computers in Human Behavior 116 (2021) 106640
Contents lists available at ScienceDirect   Computers in Human Behavior   journal homepage: http://www.elsevier.com/locate/comphumbeh  
ELSEVIER

Full length article
# Decoding emotional changes of android-gamers using a fused Type-2 fuzzy
# deep neural network
heck for os
# Lidia Ghosh a, Sriparna Saha b, *, Amit Konar a
- a Department of Electronics & Tele-Communication Engineering, Jadavpur University, West Bengal, India
- b Department of Computer Science & Engineering, Maulana Abul Kalam Azad University of Technology, West Bengal, India
# A R T I C L E I N F O
# A B S T R A C T
# Keywords:
# BCI
Electroencephalography Deep learning Type-2 fuzzy set Emotion recognition
# Android games
With the fastest growing popularity of gaming applications on android phone, analyzing emotion changes of steadfast android-gamers have become a study of utmost interest among most of the psychologists. Recently, some android games are producing negative impacts to the gamers; even in the worst cases the effect is becoming life-threatening too. Most of the existing research works are based on psychological view-point of exploring the impact (positive/negative) of playing android games for the child and adult age-group. However, the online recognition of emotional state changes of the android-gamers while playing video games may be relatively unexplored. To fill this void, the present study proposes a novel method of identifying the emotional state changes of android-gamers by decoding their brain signals and facial images simultaneously during playing video games. Besides above, the second novelty of the paper lies in designing a multimodal fusion method between brain signals and facial images for the said application. To address this challenge, the paper proposes a fused type-2 fuzzy deep neural network (FT2FDNN) which integrates the brain signal processing approach by a general type-2 fuzzy reasoning algorithm with the flavor of the image/video processing approach using a deep con- volutional neural network. FT2FDNN uses multiple modalities to extract the similar information (here, emotional changes) simultaneously from the type-2 fuzzy and deep neural representations. The proposed fused type-2 fuzzy deep learning paradigm demonstrates promising results in classifying the emotional changes of gamers with high classification accuracy. Thus the proposed work explores a new era for future researchers.
- 1. Introduction
Playing android games and beating the scores of others has become a ‘happening’ thing for young individuals for quite some time now. People used to play games on their desktops for leisure purpose earlier. But with the rapid technological improvements, newer android game-playing through mobile phones is getting popularized by a larger mass of peo- ple. The matter of concern is that if only good games are played for recreational purposes, then that would have positive impacts in terms of releasing stress. However, people especially young minds tend to get addicted to games (Ryan et al., 2006) irrespective of focusing on their works. The obsession for android games and lack of focus in day-to-day activities bring negative impacts (Schneider et al., 2004). On the other hand, there are several violent games, available in the online market, which can be freely downloadable and playing the games for a longer
(Andersonet al., 2010)- (Carnagey et al., 2007) which justify our claim of emotional changes due to playing android games (Andersonet al., 2010). Also, it is to be added that not only mood swings, but the adverse impacts of playing such violent games also can be much deeper in young minds. Now a days, cyber criminals are targeting through gaming platforms also (Messias et al., 2011). Just a few years ago in 2016, a treacherous game named as ‘Blue Whale Challenge’ released. Many people had even committed suicide by just playing that game. This type of incidents inspire us to develop an algorithm which can automatically identify the emotional state change of android gamers. The multi-modal approach utilizes both the electroencephalographic (EEG) signal and facial images of the gamers while playing video games. The automatic recognition of emotional state (Cowie and Cornelius, 2003) of android game player can be used for self-monitoring of the gamers and/or for
# parental control as well.
# time create severe disorder on the mental condition of the players.
Already some works are present in the existing literature
Already some works are present in the existing
# literature
Kühn et al. in (Kühn et al., 2019) showed that how playing violent
android games can have detrimental effect rather than playing
- * Corresponding author.
- Corresponding author.
E-mail addresses: lidiaghosh.bits@gmail.com (L. Ghosh), sahasriparna@gmail.com (S. Saha), konaramit@yahoo.co.in (A. Konar).
E-mail addresses: lidiaghosh.bits@gmail.com (L. Ghosh), sahasriparna@gmail.com (S. Saha), konaramit@yahoo.co.in (A. Konar).
https://doi.org/10.1016/j.chb.2020.106640
Received 21 June 2020; Received in revised form 28 October 2020; Accepted 20 November 2020
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
global ~ /Q> 16 frames local2 li SS im el o © = o © (mf Fusion Layer Dense Layer o iye o / le x1 : Xx 4 EEG Time Series Phase-Sensitive CSP Feature Extractior Technique ; Convolution and Max-pooling layer Fuzzification Layer Fully Connected layer - Class 00000 } Centroid Computation Layer Fuzzy Rule Layer
Fig. 1. Architectural overview of the FT2FDNN.
non-violent games. Przybylski and Weinstein measured the aggression level in adolescents spending their time in android games and found more aggression in them (Przybylski & Weinstein, 2019). In a similar type of work, Prescott, Sargent and Hull claimed that the generated physical aggression increases over time with more involvement in vio- lent games (Prescott et al., 2018). Likewise, Hasan et al. checked the long-term effects of violent android games (Hasan et al., 2013). Arriaga et al. studied the effects of playing violent android games for a longer duration for college students (Arriaga et al., 2011). Carnagey et al. re- ported that psychological damage can occur in the chronic game players (Carnagey et al., 2007).
All the above cited papers show that how playing games for a longer duration could be detrimental to our health. The studies that have already been done mainly are from the psychological perspectives. There is hardly any work in the technical ground to justify the claims of the psychologists in terms of machine learning (ML) algorithms to decode the emotional changes of the gamers in real-time. The proposed work is a novel one proposed by the authors in terms of mainly appli- cability of ML approaches in recognizing changes in emotions of players by processing facial expressions and EEG signals simultaneously. The authors did not find any relevant work done so far where these two modalities have been combined to classify emotions of gamers. Although there exist a few works, where both of these modalities have been incorporated for the application in other domains, but not for detecting
# emotional state changes of android-game addicted people.
To grasp emotion quantitatively, different physiological (Balters & Steinert, 2017)- (Purves et al., 2012, p. 713), behavioral (Coulson, 2004)- (Gottman & Krokoff, 1989) and subjective (Fredrickson et al., 2003)- (Watson et al., 1988) measures were adopted in the existing literature. The physiological changes, that express human emotions, actually constitute energy in motion, which can be quantified using
behavioral and subjective aspects. Thus, a greater amount of work has been carried out to decode the physiological responses for distinct emotions. In this context, several types of measures, such as, Autonomic Nervous System (ANS)- mediated changes (Balters & Steinert, 2017) like heart-rate changes (using electrocardiogram sensor) (Anttonen & Sur- akka, 2005)- (Pollatos et al., 2007), blood pressure changes (using pulse oximeter, manual auscultatory and digital oscillometric techniques) (Sarlo et al., 2005), breathing rate changes (using respiratory trans- ducer) (Homma & Masaoka, 2008), changes in brain signal character- istics (using EEG) (Murugappan et al., 2008) and brain activations (using functional near infrared spectroscopy/fNIRs and/or functional magnetic resonance imaging/fMRI) (FakhrHosseini et al., 2015)- (Kesler et al., 2001) etc. have been used in the literature. However, the recent advances in brain wave and brain image analysis enable offering more sophisticated and refined testing of emotional contents (T Dasborough
et al., 2008).
Huang et al. in (Huang et al., 2017) propose a work to recognize four emotional states, where movie clips have been used as stimulus to the subjects. The study shows that multi-modal approach is far better than using a single mode of either facial images or EEG signals, when neural network is taken as the classifier. This motivated us to use multi-modal approach. Another work by Sokolov et al. shows that Hjorth parameters can be used as EEG features and for extracting features from facial im- ages, principal component analysis can be undertaken (Sokolov et al., 2017). However, the work lacks in achieving a high accuracy, whereas the present approach surely increases the accuracy. On the other hand, Petrantonakis and Hadjileon in (Petrantonakis & Hadjileontiadis, 2010) implemented hybrid adaptive filtering approach for extracting the relevant EEG features. In the current paper, the authors implemented a novel EEG feature extraction method using phase-sensitive CSP algo- rithm, which successfully outperforms the standard CSP algorithm (Delorme & Makeig, 2004)- (Abdi & Williams, 2010) and the other
various physiological sensors more accurately as compared to the
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.

# Fig. 2. Vertical Slice based GT2FS.
existing feature extraction techniques.
Apart from designing CSP feature extractor, another primary moti- vation here is to design a fused type-2 fuzzy deep neural network (FT2FDNN) using the concept of multimodal fusion for online emotion recognition of gamers when they engaged themselves playing android games. The proposed model simultaneously extracts information from the brain signal using general type-2 fuzzy set (GT2FS) induced reasoning and from the image/video data of facial expression (Friesen and Ekman, 1978) using a deep convolutional neural network (CNN) representation. The knowledge learnt by the GT2FS and CNN repre- sentations are then combined together in the fusion layer to produce the final fused data-representation for getting an ultimate solution for pattern-classification. From our previous experience (Ghosh et al., 2018)– (Saha et al., 2016) and based on the existing literature (Ghosh et al., 2019), it is evident that brain signals have wide fluctuations over time which results in uncertainties in the data representation. We select general type-2 fuzzy logic (GT2FL) for EEG data analysis for its inherent capability of handling such uncertainties (Rakshit et al., 2016). On the other hand, convolutional neural network (CNN) (Lawrence et al., 1997) has proved its efficacy dealing with image/video data while removing the noise from the original data. This inspires us to use a 3-dimensional (3D) CNN for image (video) data analysis. The proposed FT2FDNN utilizes the outputs obtained by the GT2FS and 3D-CNN to form the fused representation and finally fed to a classifier unit to get the desired six emotion classes: happiness, sadness, anger, surprise, disgust and neutral. Thereby, FT2FDNN is more potentially suitable for online classification of emotional contents of the gamers as the model can efficiently handle the high level of data ambiguity and noise.
The organization of the remainder in this paper is as follows. In section 2, the principles & methodologies adopted to design proposed algorithm of this paper are discussed from the viewpoint of mathe- matics. Section 3 gives an overview of the experimental framework, EEG data acquisition during playing the android games, and the results ob- tained from the experiments are presented in Section 4 and 5. The results of all the experiments are summarized in the discussion section 6 and
concluding remarks are listed in Section 7.
- 2. The proposed approach
Fuzzy Deep Neural Network (FT2FDNN) is designed, which integrates the classical image-dataset processing approach using a 3-dimensional convolutional neural network (3D-CNN) (Lawrence et al., 1997) with the flavor of EEG signal (Acharya et al., 2018) classification using type-2 fuzzy set (T2FS). The proposed FT2FDNN is shown in Fig. 1. As shown in the figure, the proposed system consists of 3 major modules: i) Image processing by 3D-CNN, ii) EEG signal processing by type-2 fuzzy reasoning and iii) Fusion module. To detect emotional contents of the human subject based on their brain signals as well as images of facial expressions, the proposed system takes both the information parallelly as input. Image data (i.e., video) are fed to the 3D-CNN module and EEG time-series data are fed to the type-2 fuzzy reasoning (GT2FR) module simultaneously. Subsequently, the data representations obtained from 3D-CNN and GT2FR modules are fused together in the fusion part with an ultimate aim of data classification. All the three modules are dis-
cussed briefly in the following sub-sections.
2.1. Image/video signal processing using 3D-CNN
Facial image processing using 3D-CNN to accurately decode the emotional expressions from videos, a robust video feature representa- tion scheme is required which can incorporate both the global facial gestures and local facial gestures which involves the active contour in- formation of different facial parts including lip-contour and eye-contour (with eye-brow). The reason behind choosing the lip and eye-contour as local facial gestures is straight-forward as there exist many literature which validates the fact that the detection of the changes in lip (Halder et al., 2011) and eye-contours (Zheng et al., 2014) play a significant role in emotion recognition (Liu et al., 2010), (Peng et al., 2005). As shown in Fig. 2, a three-stream 3D-CNN is designed based on the model proposed by Huang et al. in 2018 (Huang et al., 2018), which can extract both the spatial and temporal image features from the video clip of facial expression changes during playing android games. A video clip con- taining adjacent 16 frames, captured by front camera (8 MP front camera of mobile phone) is used as the input to the 3D-CNN. As shown in Fig. 2, the first (upper-most) input data-stream of the 3D-CNN is resized (227 × 227) complete video frames to extract the global facial gestures. The lower two input data-streams contain the local detailed information about lip-contour and eye-contour and both of the inputs are cropped into 227 × 227 tracked image blotches comprising tight-bounding boxes
In this section, the methodology adopted to design an efficient online
emotion detector of android gamers, has been discussed. A Fused Type-2
of the respective local information. Each input stream shares the same
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
# Table 1
2.2. EEG signal processing using general Type-2 fuzzy reasoning
# Details of the participants.
Gender Number Age Health Playing Game Daily Playing Issues Years Time Male 20 16 ± 8 No 8 ± 2 yrs. 6 ± 1 h. yrs. Female 15 18 ± 5 No 6 ± 2 yrs. 5 ± 2 h. yrs.
network architecture of C3D network, proposed by Tran et al. (Tran et al., 2015), which includes 8 convolutional layers and 5 pooling layers. The global and local information from the three data-streams are then combined together by 2 fully connected layers. The output of the fully
connected layer can be mathematically expressed as
The EEG signal processing includes two progressive steps: feature extraction using Common Spatial Pattern technique and class-centroid computation using General Type-2 Fuzzy reasoning.
B.1 Phase-sensitive CSP Feature Extraction Algorithm: CSP has emerged as the most promising algorithm that extracts spatial filters encoding the most discriminative information. CSP provides a robust approach for learning spatial filters, capable of maximizing the discriminiability be- tween two or more classes. CSP algorithm yields features by assigning spatial weights to each EEG electrode such that the variances of the EEG signals associated with two different classes are maximally discrimi- nated. Here, the main aim is to discriminate the different emotions of android gamers into six different emotion classes: happiness, sadness, surprise, anger, disgust and neutral from the acquired EEG signals..
(
)
ofl = f WT fl x + bfl (1)
where, x is the input vector of the fully-connected layer, Wfl is the randomly initialized weight vector, T denotes the transpose operation and bfl is the bias term. f (.) is the activation function. Instead of using sigmoid non-linearity in the fully connected layer, we utilize Exponen-
tial Linear Sigmoid Squashing (ELiSH), defined by (2).
Let the EEG signals acquired from m channels be represented by a data-matrix of the form Xl i →l = [ x i,1 →l , x i, 2 →l , ..., x i, m ]having dimension n × →l m, where x i,cis a vector representing the instantaneous amplitudes of n EEG samples of c-th channel, c ε [1, m] and l-th trial for class i. The phase information of the signal is extracted by evaluating the Discrete Hilbert →l
# transform (DHT) (Prucnal & Polak, 2018) of x
i,c as follows
(
)
⎧
# a
G 3) 420 "| YY (2)
(
)
Halal = DFT (3), (3)
where, for even N,
⎧
The ELiSH function is a combination of Exponential Linear Unit (ELU) (Laukka et al., 1995) and sigmoid (Delorme & Makeig, 2004) - (Maclinet al., 2011). The three-stream 3D-CNN is first pre-trained with the training dataset to fix all the weights of subsequent layers. Once the training of the 3D-CNN is complete, the last fully-connected layer is discarded and truncated to the first fully connected layer during the test
it X= ; (4a)
# phase.
and for odd N,
Fixation Playing game tits (2 sect EN e — Rest min), —y, poy Trial 1 — Trial s —>
Fig. 3. Timing Diagram of the stimulus presented during offline game playing.
; Happiness Surprise Sadness rh Neutral Anger Disgust
Fig. 4. Images depicting 6 different emotions.
Fig. 4. Images depicting 6 different emotions.
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
Max (d)
Fig. 5. sLORETA activations (top and bottom views) for the emotion (a) happiness, (b) sadness, (c) surprise, (d) anger, (e) disgust and (f) neutral.
ww happiness — sadness _.... surprise e. anger — neutral disgust » We “ww % 1 0 Time _____»
Fig. 6. Discriminating 6 distinct emotional states using (a) Standard CSP and (b) the Proposed CSP features.
Fig. 6. Discriminating 6 distinct emotional states using (a) Standard CSP and (b) the Proposed CSP features.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
# Table 2
# Comparison of the feature extraction Algorithm.
Feature Extraction methods CA True Positive Proposed CSP Standard CSP Time-domain EEG Features (RMS (%) 87.45 70.64 76.43 Rate (TPR) 0.89 0.80 0.78 0.07 0.19 0.19 value, mean values, variance, 1st and 2nd order derivatives, peak counts) Frequency-domain Features (spectral energy, power spectral density, discrete fast Fourier transform 80.53 0.86 0.10 coefficients) Time and Frequency domain Features (Discrete Wavelet Transform) Hybrid adaptive filtering ( 86.16 82.29 0.89 0.87 0.14 0.13 Petrantonakis & Hadjileontiadis, 2010)
# False Positive
# Rate (FPR)
T ∑
JZ = S S T ∑ * 1S * 2S . (7)
∑
In (7), the co-variance matrix * i of Zl i for class i can be obtained by
[
]
∑
* i = Cov Zi, Zi .
Now, it is to be worth mentioning that in (7), the spatial filter S is a data-matrix of dimension m × m containing complex variables and S is T∑ * 2S = 1must its complex conjugate. To maximize Jz, the constraint S be considered. Therefore, the Rayleigh Quotient problem, presented by Jz in (7) can be transformed into an optimization problem after ∑ * computing the co-variance matrices i for class i. The constrained optimization can then be solved by Lagrange Multiplier technique. The Lagrange objective function is thus defined as
⎧
N-I =i x(n). n=0 > fi e ET “ j emt k= 1 İY xin). — 10
denotes inverse discrete Fourier transform. In (4a) and
In (3), DFT! denotes inverse discrete Fourier transform. In (4a) and (4b), Yİ x{n].e"¥ = Xiklrepresents the discrete Fourier transform (DFT) of x [n]. When N is even, the samples X[k]for k = 1, 2, ..., x — l,are called whereas the k
called positive harmonics, whereas the samples X{k|for k = — (: — 1),
(4b)
(
)
L(A, s-sDs(sDs- ) (9)
To optimize the Lagrange function L, the first and foremost criteria is that the filter S should be such that,
(
)
(
)
Cn 0 [orm BİR (s 5) 23 (5 >) =0 (10)
Equation (10) cab solved by following the methods adopted in
(Mendel, 2013) and thus we obtain
∑
∑
# el
* 2 . * 1 S = λS (11)
— (: — 2) ,.. — 2, —1,are called negative harmonics (Todoran et al.,
{
[
(
)]
[
(
)]
2008). The negative harmonics {x|- (: — Dİ | - (: — 2). ves X(— 2], X[-1] Yare eguivalent to Gi + 1 ; İk + 2]. . X(N — 2],
Therefore, the possible solutions to the specified problem would be the eigenvalues of Y” = yt Xi. The principal components evaluated against the corresponding largest and lowest eigen values of X5” are the required CSP filters. Thus, for ann x m dimension EEG data-matrix, CSP produces p such spatial filters, given by n x p matrix S with p Eigen values in decreasing order.
X(N — 1) Vas the sample X [n-k] = X [-k] has a correspondent spectral
density sample with the negative frequencyX( — kwo).Similarly when N is odd, the samples X{k]for k = 1, 2, ..., x are the positive harmonics and that for k = %34, 2, ..., N—1, are called negative harmonics. The samples against k = O and Yare discarded as they are continuous components.
# components.
Now, H{x [n]} can also be re-written as
H{x[n]} = Re[n] + j Im[n].
where, the phase angle of H{x(n]}isO = tan Now, the amplitude x! . and phase content 6} ,of the EEG signal acquired from c-th channel in I-th trial is integrated such that a complex data matrix Z; is produced, defined as
defined as
(5)
B.2 Centroid Computation using General Type-2 Fuzzy Reasoning (GT2FR): A General Type-2 (GT2) Vertical Slice (Mendel, 2013) approach to fuzzy reasoning is adopted in the present application. Let f1, f2, …, fn be the extracted n CSP features of the EEG signal and yj is a measure of emotional state j by a given subject. Let fi is ˜ Aibe a fuzzy proposition used to build up the antecedent part of the fuzzy rule j, and yj is ˜Bj is a fuzzy proposition to develop the consequent of the same rule. Here, ˜ Aifor i = 1 to n are vertical slice based GT2FS given by ((fi, u), μ˜ A(fi)(u)), where μ˜ A(fi)(u) is the vertical slice at given at a given fi for m discretizations u1,u2, …,um along the u-axis. Similarly, ˜Bjis the conse- quent in the university of discourse yj. As shown in Fig. 1, the T2FR module computes the class-centroids into 3 major steps, demonstrated by 3 layers: i) fuzzification layer, ii) fuzzy rule layer and iii) centroid computation layer.
Zi. =x x ee, (6)
# Zl
The CSP problem can be formulated to derive the co-efficient of the spatial filters ‘S’ that maximizes the following Rayleigh Quotient
problem,
The fuzzification layer converts the crisp input into two fuzzified output values, namely Upper Membership Function (UMF) and Lower
# Membership Function (LMF) by the following strategy:
Trial 1 A ( < 10min 10 min 4———© Emotion recognition] Performance - Emotion recognition | Performance Android Game | CSP Feature] using Proposed | evaluation in | Adroid Game | CSP Feature | using Proposed — | evaluation in REST Trial 2 se] paying | extraction iy e accuracy | Paving | extraction EEDNN % accuracy — = _ 3s Tmin 35 ‘Smin
Fig. 7. Stimulus preparation for online emotion recognition.
# Fig. 7. Stimulus preparation for online emotion recognition.
(8)
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
( EEG data and facial image acguisition for offline analysis Labeled training dataset Training data (Image +EEG) Image 1 1 1 1 1 1 1 1 1 1 1 1 Global and local facial gesture extraction Image signal processing Global and Local Images Training feature set (Image + EEG) Training parameters (a) selection using Unlabeled test dataset Test data (Image +EEG) S EEG data Active brain region sLORETA software ra 3u1ss9901d [eusis II CSP feature extraction CSP features Test feature set (Image +EEG) FT2FDNN classifier testing Emotion classes EEG data and facial image acquisition for online analysis Online test data (Image + EEG) Image extraction i ep data 4 R= ! = ? J il Pre-processing Ri Q J0771771 it . £4) Globaland | | and Artifact pi > | local facial | | Removal DE |) gesture ' | 3 % | extraction ! | 3 Yl... ot § 5 ! = > I H CSP feature & 1 1 1 I features Online Test feature set (Image +EEG) Training Classification parameters —> using obtained during, FT2FDNN offline analysis Online Emotion recognition (b)
Fig. 8. Flow diagram for (a) offline data analysis and (b) online emotion recognition.
Fig. 8. Flow diagram for (a) offline data analysis and (b) online emotion recognition.
# Table 3
# Table 3
# Parameters of FT2FDNN.
Modules of FT2FDNN Parameters of the classifiers Parameter values 3D-CNN No. of convolution Layers No. of max-pooling layers 8 5 No. of filters for 8 convolution layers 64, 128, 256, 256, 512, 512, 512, 512 3D convolution kernel size 3 × 3 × 3 GT2FS Pooling kernel size Initial learning rate No. of neurons in fuzzification layer No. of neurons in 2 × 2 × 2 0.003 5 (no. of classes) 5 × n fuzzification layer
# and variance
(
)2
(13) is 2 (« ww fi, .) o, a /. 15
to construct one type-1 Gaussian MF: Ai,v, for v = 1 to 10.
- 2. Then the footprint of uncertainty (FOU) is constructed using type-1 Ai,
v, for v = 1 to 10 with LMF and UMF obtained by
(
)
μ˜Ai (fi) = UMF(fi) = Max10 v=1 μ˜Ai,v (fi) , (14)
˜
- 1. At first Ai is constructed by collecting 15 daily samples of feature fi over 10 days. Let the measurement l of fi on day v be denoted by fi,l,v. Thus for l = 1 to 15, we obtain the mean
(
)
μ˜Ai (fi) = LMF(fi) = Min10 v=1 μ˜Ai,v (fi) ,
(15)
∑15
fi,l,v f i,v = l=1 ,
15
(12)
In order to maintain the convexity criteria of the proposed GT2FS, the peaks of the constituent type-1 MFs are joined with a straight line of
zero slope, resulting in a flat-top approximation.
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
# Table 4
# Comparative analysis of the FT2FDNN algorithm with state-of-the-art algorithms.
Classifier Algorithms Training Validation Testing CA (%) TPR FPR CA (%) TPR FPR CA (%) TPR FPR FT2FDNN 88.26 0.87 0.16 86.05 0.87 0.0874 87.58 0.85 0.07 Decision level fusion (Huang et al., 2017) 80.96 0.81 0.25 79.75 0.81 0.3295 81.34 0.79 0.2857 Statistical features and LSVM in (Sokolov et al., 2017) 82.72 0.81 0.30 81.24 0.7511 0.3297 82.95 0.78 0.6241 Emotion detection in the loop (Savranet al., 2006) 78.42 0.79 0.58 79.47 0.78 0.53 78.41 0.76 0.61 From Brain signals only 75.57 0.75 0.65 73.13 0.73 0.25 74.32 0.75 0.63 From facial expressions only 72.83 0.73 0.75 71.98 0.72 0.76 72.53 0.71 0.78
108 —— Training data 108 — Training data — Testing data — Testing data 7 10’ 107. v 2 3 3 Rİ Ss > > as 6 5 6 — 10 a 10 10° i Rem 0° i Dani 10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 No. of iterations —+p No. of iterations ————p (a) (b)
Fig. 9. Convergence analysis of FT2FDNN on (a) JU database and (b) MAKAUT database.
′
- 3. Now, at the measurement points fi = f peak = 1 is drawn to represent μ˜ ′ , an isosceles triangle with i )(u), the secondary plane.
# A(f
# i
# Table 5
# Verification by alternative fusion approaches.
˜
is In the fuzzy rule layer, by the given GT2 fuzzy rule j, “if fi is Ai,then yj ˜ Bj” and the fuzzified value of (14) and (15), the AND fuzzy logic operation using the well-known α-cut representation, is performed. For every pair of points, let (f ′ 1 , f ′ 2 ), such that f ′ i ∈ dom μ˜ Ai (f ′ i ), we compute the meet operation as follows,
∫
[
/ {[
)
)
)
)}]
j 1 ∩ ˜A j 2 = sup α a1α ′ f 1 ∧ a2α ′ f 2 , b1α f ′ 1 ∧ b2α f ′ 2 /f ∀α∈[0, 1]
# ˜A
(16)
Fusion Approaches 3DCNN + GT2FS CNN + GT2FS CA (%) 87.58 85.84 TPR 0.85 0.83 FPR 0.07 0.23 CNN + SVM LSTM + GT2FS RNN + GT2FS 83.75 81.29 78.54 0.78 0.75 0.74 0.39 0.47 0.54 3DCNN + IT 75.67 0.71 0.58
# 3DCNN + IT2FS
74.31
0.69
0.65
where,
C˜Bj = sup ∀α∈[0,1] C˜Bj α (19)
[
[
(
)
(
)]]
∫
˜Ai = ∀f ′ i ∈F sup ∀α∈[0,1] α / aiα ′ f i , biα ′ f i / f ′ i (17)
and the α-cuts of the triangle secondary MFs are given by
where, C˜ Bj α is the centroid of ˜Bjα at α-cut and completely specified by its left and right end-point centroids l˜ Bjα and r˜ Bjα respectively as follows
[
]
C˜Bjα = α / l˜Bjα , r˜Bjα (20)
⎧
[
)
)]
⎪⎪⎪⎪⎪⎨
i ! ! Af) = [ain(fi), bi) , , = a(t) vaz) ula (1s)
⎪⎪⎪⎪⎪⎩
(18)
l˜ Bjα and r˜ Bjα are obtained using extended Karnik-Mendel algorithm (Abdi & Williams, 2010). The centroid C˜ Bj is then used as the output of the
type-2 fuzzy represented block and is denoted by of.
# ği
˜ j ∩ ˜ j .
Equ. (16) is the vertical slice of.
# A
1
# A
2
# 2.3. Multimodal fusion
The centroid computation layer then performs the centroid type reduction strategy to map the GT2FS into its corresponding type-1 fuzzy set. There are many type-reducer techniques available in the literature. We adopt the technique described by Mendel in (Abdi & Williams, 2010) and obtain the centroid type-reduction of consequent GT2FS ˜ Bj as
follows
In the decision-level fusion, the outputs generated by the previous two modules of facial image processing and EEG signal processing are combined together, inspired by the concept of multimodal learning (Saha & Karia, 2019). The approach of multimodal fusion usually pro- duces multiple features from different traits and represents them by high-level feature values for achieving better performance of the clas-
sifier. In the proposed FT2FDNN, we employ 3D-CNN and GT2FS
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
100 < 601 £ Ss 40 20 i 0 01 02 03 04 05 (a) 100 g < O 20 (b) 04 06 08 10 -2- JU database MAKAUT database 06 07 08 09 bs ———> JU database MAKAUT database
Fig. 10. Parametric sensitivity analysis of FT2FDNN with respect to the hyper-parameters (a) β and (b) γ
# Table 6
# Table 6
ofl and GT2FS module, defined by of, are fused together with the weights
# wfl and wf, respectively.
# Statistical validation.
Prediction Average Rank over 7 Friedman Statistics χ2 F Algorithms datasets 3DCNN þ GT2FS 1 28.19 (Null Hypothesis CNN + GT2FS 2.06 rejected) CNN + SVM LSTM + GT2FS RNN + GT2FS 3DCNN + IT 6.77 3.14 4.28 5.56 3DCNN + IT2FS 6.21
2.4. Classification
# 2.4. Classification
Finally, the fused information foi is connected with a dense layer for classification into its corresponding emotion classes. In this layer, a sparsemax function (Balters & Steinert, 2017) is used to get the desired class labels: happiness, sadness, surprise, anger, disgust and neutral based on probability measurements. We compute coupling coefficient using Sparsemax function, defined as follows
modules to provide a better representation of the respective input data by decreasing the uncertainty and noise level from the data. Here, we employ the commonly used multimodal NN structure (Hasan et al., 2013) to combine the 3D-CNN and GT2FS outputs with dense-connected
fusion layers, defined as follows
(
)
(0 Pi ( (Pop) om fol = (21) where, p = (wa).” (04) yy (w)” (0) + b». (22)
where, p = (wa).” (04) yy (w)” (0) + b». (22)
In (22), the outputs obtained from the 3D-CNN module, defined by
In (22), the outputs obtained from the 3D-CNN module, defined by
c; =argmin||r — fo;|| (23) reset
# reset
⃒
where, AS! : = (re R°|1’r = 1,r> O}is the G— 1dimensional simplex. The computation of Sparsemax function is able to provide sparse dis- tributions where very small probabilities are reduced to zero values. Finally, an Euclidean norm is taken on the output vector to predict the class label, Yi.
# class label, ̂yi
# (21) 2.5. Loss function and training of the proposed FT2FDNN
2.5. Loss function and training of the proposed FT2FDNN
(21)
For a desired class label, yi, the mean square loss of the proposed
FT2FDNN over l training samples can be defined as
⃒
⃒
⃒
1 L L-7X (24) Yi
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
250 200 150 mDay 10 mDay 20 100 = Day 30 50 Happiness Surprise Sadness Anger Disgust Neutral (a) 250 200 150 m Day 10 ™ Day 20 100 m Day 30 | | i 0 ' 1 1 1 1 Happiness Surprise Sadness Anger Disgust Neutral (b)
Fig. 11. Changes in emotional states over 30 days of experiment for (a) Candy Crush Saga and (b) Stickman Archers.
Fig. 11. Changes in emotional states over 30 days of experiment for (a) Candy Crush Saga and (b) Stickman Archers.
The training goal of a network is to minimize the Loss function (LF) with a simultaneous maximization of the mutual information between the input data and the representation learned by the network (Torkkola, 2003). The mean square LF only can preserve the similarity measure between the input data vectors during the training phase of the network. As the 1st order structural proximity describes the local network configuration and the 2nd order structural proximity describes the global network configuration, we here define the LF, which can preserve both the local and global structural features of the proposed network. To preserve the 1st order structural proximity between the outputs of directly connected n number of node-pairs (i, j) of the FT2FDNN, the
The final LF is then defined as the combination of the 1st and 2nd order structural proximities (Torkkola, 2003) as follows
LF = L1 + β⋅L2 + γ⋅Lr
where, β and γ are the small positive weights and Lr is the ||L2|| regu- larization term in the LF to prevent overfitting (Torkkola, 2003), as
defined below
Lr = 1 2 ∑K ⃒ ⃒ ⃒ ⃒ ⃒ ⃒W (k) ⃒ ⃒|2 ⃒ F (28)
k=1
(27)
following LF is defined
2 IL: L= isi o; ii Şi) i j (25)
where, W(k), k = 1, 2, …, K is the weight matrix of the kth layer. The hyper-parameters: β and γ of the LF are fine-tuned using grid search
algorithm and the respective sensitivity analysis is given in section 5.
(K)
where, sijis the edge weight between the ith and jth node-pair, o i and o (K) j are the output representation of a pair of node (i, j) and K is the number layers of the neural network. Similarly, to preserve the 2nd
order proximity (Torkkola, 2003), we define the LF as
- 3. Experimental framework
The section describes the experimental protocol undertaken to
perform the experiments.
⃒
Elles) 2 (26)
where, ∘ is the Schur product (Todoran et al., 2008) and rs i for i = 1 to n are the penalty parameters for non-zero adjacency elements. rs i,j is
assigned to 1, if sij = 0 and is assigned to a value greater than 1, if. sij ∕= 0.
# 3.1. Experimental framework
The data collection process has been carried in two distinct univer- sities to get the essence of diversities of changes in emotional states in subjects while playing android games. Thus, two databases have been prepared named as JU database and MAKAUT database. For the former
database, a 21-channel Nihon Kohden EEG system is used in the
10
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
Artificial Intelligence Laboratory of Jadavpur University. For the latter one, a 24- channel EEG sensor by Brain Tech is used in Human Computer Interaction Laboratory of Maulana Abul Kalam Azad University of Technology. For the current study EEG data is collected from pre-frontal, frontal, motor cortex, parietal, occipital and temporal regions by following the norms of international 10/20 electrode placement system. To capture the facial expressions, for both the databases front camera of a mobile phone with 16 frames and 227 × 227 configuration is imple- mented. The chosen two games are of different kind: Candy Crush saga is a leisure game, while Stickman Archers is a violent game.
# 3.2. Participants
The participants include 35 healthy volunteers (20 male and 15 fe- male in 13–25 years age-group), with no physical disabilities. Before selecting the participants, we have given them a questionnaire, where among other trivial questions, two vital questions were asked whether they are inclined to one or more video games and if yes then how much time they invest each day in those games. We have taken this survey from more than 100 subjects and among them only 35 is chosen who have answered 4 or more hours as an answer to the second question. The information regarding the age group, health issues, playing game years and daily playing times of male and female groups are enlisted in Table 1.
# 3.3. Artifact removal
A Chebyshev band pass filter has been used to get rid of the inter- ference of physiological and environmental artifacts from the EEG
# signals.
- 4. Offline and online analysis of image and EEG data
The experiments are conducted into two ways: offline data analysis and online data analysis.
different than that of ‘anger’. Therefore, we use the cropped images of eye contour and lip contour as the local detailed information for the 3D- CNN and the full face image as global information of the 3D-CNN.
A.2 Offline EEG Data Analysis: The filtered EEG signal is analyzed offline to i) identify the brain regions activated during different emotion arousals using sLORETA software and 2) validate the supremacy of the proposed CSP algorithm in extracting relevant EEG features.
The first experiment aims at assessing the electrical activations of the intra-cortical distribution using sLORETA software from the EEG signal acquired during different emotional state changes while playing android games (Levenson, 2014). The top and bottom views of the sLORETA solutions for the 6 emotions: happiness, sadness, surprise, anger, disgust and neutral are portrayed in Fig. 5(a–f). In the color label, white color designates deactivation, whereas yellow indicates highest activation. Analyzing the sLORETA solutions, it is observed that pre-frontal, frontal and temporal brain region remains highly active during the arousal of all the 5 emotions (except neutral), whereas only occipital lobe activation is found during neutral (no cognitive task/neutral) condition, as shown in Fig. 5(f).
The second experimental fact is reported in Fig. 6 and Table 2 in terms of overall performance analysis of the two variants of CSP algo- rithms: standard CSP and the proposed phase-sensitive CSP and the other state-of-the-art EEG feature extraction techniques reported in literature till date. The feature-level discrimination ability of the stan- dard and the proposed CSP algorithms are examined by plotting the respective feature values averaged over all the trials for 6 distinct emotion classes as shown in Fig. 6 (a) and (b) respectively. The figure clearly explains that the introducing phase information in formulating CSP gives better discrimination among the 6 different emotional states.
For the proposed FT2FDNN classifier, the values of the three classi- fier parameters: percentage classification accuracy (%CA), True Positive Rate (TPR) and False Positive Rate (FPR) (Mauss & Robinson, 2009) obtained for each feature extraction technique are tabulated in Table 2. The proposed CSP achieves the best accuracy above 87% and higher TPR and lower FPR values indicating the supremacy over the other algorithms.
# 4.1. Offline data analysis
# 4.1. Offline data analysis
# 4.2. Online data analysis
# 4.2. Online data analysis
During offline data analysis, the experiment is carried out over 30 days for each subject. On each day, 5 experimental trials are conducted, where in each trial the subject plays a given android game for 10 min duration and during playing the game his/her facial expressions are captured using a video camera and the corresponding EEG signals are acquired from the subject’s scalp. Therefore, we have altogether (30 × 5) = 150 experimental trials per subject for offline data analysis. The timing diagram of the stimulus presented in each day during offline data analysis is shown in Fig. 3. In each experimental trial, an audio-visual cue is presented to the subject indicating the cognitive task (here, android game playing) needed to be accomplished. At the beginning of the trial, a fixation cross of 2 s duration appears in the screen (see Fig. 3) informing the subject to concentrate on the given task. An audio-visual cue of playing game appears thereafter with duration of 10 min. A rest period of 5 min between consecutive trials is given to the subject. The facial images captured during game playing are then visually investi- gated to differentiate the 6 different emotions: happiness, sadness, sur- prise, disgust, anger and neutral, and are used to train the 3D-CNN. Similarly, the corresponding EEG signals are used to train the GT2FS
The online data analysis includes the performance evaluation of the proposed approach in real-time, thoroughly explained in section 5. Similar to the offline experiment, same procedure is followed for each trial in the online experiment. The only difference is that during each trial of 10 min duration, after every 1 min epoch, the emotional state of the player is recognized by capturing facial images using the video camera and collecting the corresponding EEG signals of the intended 1 min duration. The proposed FT2FDNN, previously trained with offline training instances, is used to determine the overall emotional state of the player which helps understanding the impact (positive/negative) of the game onto the player. The stimulus diagram for online data analysis is depicted in Fig. 7. Data flow diagrams for offline and online data anal-
ysis are shown in Fig. 8.
- 5. Performance evaluation of the proposed FT2FDNN
The performance of the FT2FDNN classifier with the existing ones
has been evaluated in this section.
# classifier.
A.1 Offline Image Data Analysis: Six basic emotional states: happiness, sadness, surprise, disgust, anger and neutral are considered for this study as presented in Fig. 4. A notable change in the area covered by the eye opening, as well as structure of lip contour is present in the figure. The figure has been given as an example to show that how for ‘sad’ emotion the eye-opening region is getting smaller, while the same region for ‘surprise’ emotion is getting much wider. Whereas considering facial
5.1. Parameters of the FT2FDNN model and comparative study with the
existing models
The proposed fused model combines the architectural features of both the GT2FS and 3D-CNN modules and thus the parameters along with their dimensions of individual network module is presented in
Table 3. Here, n is the number of input features of the GT2FS.
expression related to ‘happiness’ emotion, lip contour is completely
Performance evaluation of the proposed FT2FDNN algorithm against
Performance evaluation of the proposed FT2FDNN algorithm against
11
L. Ghosh et al.
Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
the existing state-of-the-art classifiers has been done with respect to the %CA, TPR and FPR values. The parametric values obtained by each of the 6 classifier algorithms are compared in Table 4. It is apparent from the table that FT2FDNN yields best results in all the three phases: training, validation and testing phases for the present problem.
the no. of datasets (here, 2). Here rj i denotes the rank of i-th algorithm for the j-th dataset. The algorithm with smallest rank indicates the best performer.
# 5.2. Convergence Analysis of the training
5.6. Understanding the impact of playing different android games on emotional changes of subject
The experiment deals with investigating the convergence property of the FT2FDNN. To perform the experiment, the training iterations are varied from 0 to 100 and plotted against the respective LF values of the in-sample (training) and out-of-sample (testing) data for the two data- bases, as presented in Fig. 9. The figure clearly explains that the training curve of the proposed FT2FDNN convergences after 50 iterations whereas the testing curve shows significant oscillations at the end for both the datasets due to the over-fitting phenomenon. Therefore, we consider only 50 iterations in experiments to maintain the effectiveness and efficacy of the FT2FDNN.
This experiment is carried out to understand the difference in sub- ject’s emotional state changes for playing different android games. On each day of the experiment, the average values of each emotion for the two different games: Candy Crush Saga and Stickman Archers are plotted using bar charts (Fig. 11). Based on the 30 days’ experiment for all the 35 participants, it can be easily observed that the two games have distinct effects on human emotions. From the graphical representation, it is very much clear that the positive emotions are generated more for the Candy Crush saga and negative emotions are overpowering day-by- day for the Stickman Archers.
# 5.3. Verifications by alterative fusion approcahes
- 6. Discussion
The FT2FDNN is composed of two parts: image processing by 3D- CNN and EEG signal processing by GT2FS. In this experiment, we employ alternative approaches for either or both of these two parts and compare the corresponding fused neural networks with the proposed one. Instead of 3D-CNN, we employ the other well-known image/video data processing algorithms like Long Short-Term Memory (LSTM) network, Recurrent neural network (RNN) in fusion with the proposed GT2FS and compare their performances with the proposed one. On the other hand, the GT2FS plays the role of reducing the data ambiguity (uncertainty) for EEG signal processing. There exists another widely used approach, called Information Theoretic (IT) learning for uncer- tainty handling. Thereby, we examine the performance of a famous IT feature learning network (Torkkola, 2003) in comparison with GT2FS. Apart from this, Interval type 2 fuzzy set is also used to replace the GT2FS. Table 5 includes the results of comparison.
The proposed work is dedicated to recognize 6 different emotional states for android game players. The advantages of the described work can be stated based on the below mentioned parameters.
- a) Robustness analysis: The classification of the emotional states is done using a multi-modal fusion model using GT2FS and 3D-CNN. If any classifier suffers from overfitting and/or underfitting, then that re- flects in the performance of the classification accuracy. But, in the proposed work, 87.58% classification accuracy is obtained which negates the occurrence of overfitting and underfitting and validates
the robustness of the proposed method.
- b) Convenience: The proposed architecture has minimal hardware setup (a 21-channel EEG sensor and a mobile phone’s camera is required). Further, the work is independent of the gender and age of the sub- jects. So, this makes the system very much convenient to deal with the present-day emerging problem of mobile phone addiction, which
many times lead to depression in individuals.
# 5.4. Parametric sensitivity study of the FT2FDNN
The parametric sensitivity of the FT2FDNN has been analyzed by investigating how the hyper-parameter values of the LF: β and γ affect the performance of the FT2FDNN. To carry out the experiment, the parameters are initially set to β, γ = {0.01, 0.1} and are tuned one by one in an iterative manner until one of the two converges and then is fine- tuned using grid search algorithm. The results of the sensitivity anal- ysis of the two parameters with respect to the classification accuracy for the two datasets are given in Fig. 10. It follows from the results that the highest accuracy is obtained at their optimum values: β = 0.5 and γ =
1.4.
- c) Benefits: The main inspiration behind this work is to tackle the constantly increasing problem developing in the young individuals of addiction towards gadgets. If the ‘unhealthy’ games can be iden- tified, who have long term negative impacts on our mind then that
could be beneficial to the whole society.
- d) Productivity: The system proposed can able to correctly identify
emotion classes based on EEG and facial expression data.
- e) Convergence: The computation time of 5 s on Intel Core i5 8th Gen- eration processor @ 1.60 GHz and 4 GB RAM running on Matlab
R2017b software is quite small fin respect to using CNN.
- f) Flexibility: The proposed algorithm is applied for a large number of participants who have different mind sets towards playing android
# games.
# 5.5. Statistical test
- g) Feasibility: The work can be easily applicable to a larger mass of
people, so feasibility is not an issue.
We employ the Friedman 2-way statistical test (Shiota et al., 2011) to validate the comparative performances of the classifier algorithms. Each of the 7 algorithms, enlisted in Table 6, is ranked according to the me- dian of the classifier accuracies obtained for the two datasets. Let the null hypothesis, H0, here be that the medians of classifier accuracies, achieved by the 7 distinct classifiers for both the datasets, are not significantly different. The Friedman’s statistic score, denoted by χ2 F , is
- h) Efficiency: The efficiency of the proposed method has been compared with a wide range of already available algorithms and every time the
work proves its worth.
- i) Diversity: The proposed system has been implemented on a diverse
# group of subjects.
- j) Reliability: The proposed approach is already realized on the real-
# world scenario.
given by
[
(
)2
]
> 12D +1) afin) 724) xl >) 7 (29) izl
where z is the no. of algorithms to be compared (here, 7), D represents
where z is the no. of algorithms to be compared (here, 7), D represents
# 7. Conclusion
- 7. Conclusion
The paper presents a novel method of real-time emotion recognition of android-gamers using the concept of multi-modal fusion. The pro-
posed technique works automatically by fusing the results obtained by
12
L. Ghosh et al.
# Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
EEG classification using GT2FS and facial expression classification by 3D-CNN. To the best of the authors’ knowledge, this is a novel attempt where the brain signals as well as facial expressions are measured simultaneously for understanding how android games are playing a significant role in changing their moods. The work produces significant outcomes in comparison with state-of-the art literatures. Besides this, the proposed method is statistically validated by Friedman statistical
# test with a confidence level of 95%.
The proposed research findings have incredible applications in demonstrating the psychosomatic changes in gamers, who used to spend quite a lot of their times in playing android games. A gamming appli- cation can be rated based on the experimental evaluation such that how it affects the emotional states of a gamer during the execution of the game. If negative emotions (such as sadness, disgust and anger) domi- nate over the positive ones (such as happiness and surprise), then it can be easily concluded that the game can impose adversarial effects on the
- Ghosh, L., Konar, A., Rakshit, P., & Nagar, A. K. (2018). Hemodynamic analysis for cognitive load assessment and classification in motor learning tasks using type-2 fuzzy sets. IEEE Trans. Emerg. Top. Comput. Intell., 3(3), 245–260.
- Ghosh, L., Rakshit, P., & Konar, A. (2019). Working memory modeling using inverse fuzzy relational approach. Applied Soft Computing, 83, 105591.
- Gottman, J. M., & Krokoff, L. J. (1989). Marital interaction and satisfaction: A longitudinal view. Journal of Consulting and Clinical Psychology, 57(1), 47.
- Halder, A., Rakshit, P., Chakraborty, A., Konar, A., & Janarthanan, R. (2011). Emotion recognition from the lip-contour of a subject using artificial bee colony optimization algorithm. In Swarm, evolutionary, and memetic computing (pp. 610–617). Springer.
- Hasan, Y., B`egue, L., Scharkow, M., & Bushman, B. J. (2013). The more you play, the more aggressive you become: A long-term experimental study of cumulative violent video game effects on hostile expectations and aggressive behavior. Journal of Experimental Social Psychology, 49(2), 224–227.
- Homma, I., & Masaoka, Y. (2008). Breathing rhythms and emotions. Experimental Physiology, 93(9), 1011–1021.
- Huang, Y., Yang, J., Liao, P., & Pan, J. (2017). Fusion of facial expressions and EEG for multimodal emotion recognition. Computational Intelligence and Neuroscience, 2017.
- Huang, J., Zhou, W., Zhang, Q., Li, H., & Li, W. (2018). Video-based sign language recognition without temporal segmentation. In Thirty-second AAAI conference on artificial intelligence.
# gamers.
- Kesler, M. L., Andersen, A. H., Smith, C. D., Avison, M. J., Davis, C. E., Kryscio, R. J., & Blonder, L. X. (2001). Neural substrates of facial emotion processing using fMRI. Cognitive Brain Research, 11(2), 213–226.
# Credit author statement
Lidia Ghosh: Methodology, Writing – original draft, Validation, Data curation, Software; Sriparna Saha: Visualization, Writing – original draft, Software, Data curation, Investigation; Amit Konar:
# Conceptualization.
- Kühn, S., Kugler, D. T., Schmalen, K., Weichenberger, M., Witt, C., & Gallinat, J. (2019). Does playing violent video games cause aggression? A longitudinal intervention study. Molecular Psychiatry, 24(8), 1220–1234.
- Laukka, S. J., J¨arvilehto, T., Alexandrov, Y. I., & Lindqvist, J. (1995). Frontal midline theta related to learning in a simulated driving task. Biological Psychology, 40(3), 313–320.
- Lawrence, S., Giles, C. L., Tsoi, A. C., & Back, A. D. (1997). Face recognition: A convolutional neural-network approach. IEEE Transactions on Neural Networks, 8(1), 98–113.
# Acknowledgment
- Levenson, R. W. (2014). The autonomic nervous system and emotion. Emotion Review, 6 (2), 100–112.
The first and third authors thankfully acknowledged the fund pro- vided by Ministry of Human Resource Development for RUSA-II project granted to Jadavpur University, India. The second author is thankful to the University for providing research seed money and UGC Start-up Grant under the scheme of Basic Scientific Research. The work is
# approved by Institutional Ethics Committee.
- Liu, X., Cheung, Y., Li, M., & Liu, H. (2010). A lip contour extraction method using localized active contour model with automatic parameter selection. In 2010 20th international conference on pattern recognition (pp. 4332–4335).
- Maclin, E. L., et al. (2011). Learning to multitask: Effects of video game practice on electrophysiological indices of attention and resource allocation. Psychophysiology, 48(9), 1173–1183.
- Mauss, I. B., & Robinson, M. D. (2009). Measures of emotion: A review. Cognition & Emotion, 23(2), 209–237.
- Mendel, J. M. (2013). General type-2 fuzzy logic systems made simple: A tutorial. IEEE Transactions on Fuzzy Systems, 22(5), 1162–1182.
# References
- Abdi, H., & Williams, L. J. (2010). Principal component analysis. Wiley Interdiscip. Rev. Comput. Stat., 2(4), 433–459.
- Messias, E., Castro, J., Saini, A., Usman, M., & Peeples, D. (2011). Sadness, suicide, and their association with video game and internet overuse among teens: Results from the youth risk behavior survey 2007 and 2009. Suicide and Life-Threatening Behavior, 41(3), 307–315.
- Acharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., & Adeli, H. (2018). Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals. Computers in Biology and Medicine, 100, 270–278.
- Anderson, C. A., et al. (2010). Violent video game effects on aggression, empathy, and prosocial behavior in eastern and western countries: A meta-analytic review. Psychological Bulletin, 136(2), 151.
- Anttonen, J., & Surakka, V. (2005). Emotions and heart rate while sitting on a chair. In Proceedings of the SIGCHI conference on Human factors in computing systems (pp. 491–499).
- Arriaga, P., Monteiro, M. B., & Esteves, F. (2011). Effects of playing violent computer games on emotional desensitization and aggressive behavior 1. Journal of Applied Social Psychology, 41(8), 1900–1925.
- Balters, S., & Steinert, M. (2017). Capturing emotion reactivity through physiology measurement as a foundation for affective engineering in engineering design science and engineering practices. Journal of Intelligent Manufacturing, 28(7), 1585–1607.
- Carnagey, N. L., Anderson, C. A., & Bushman, B. J. (2007). The effect of video game violence on physiological desensitization to real-life violence. Journal of Experimental Social Psychology, 43(3), 489–496.
- Coulson, M. (2004). Attributing emotion to static body postures: Recognition accuracy, confusions, and viewpoint dependence. Journal of Nonverbal Behavior, 28(2), 117–139.
- Cowie, R., & Cornelius, R. R. (2003). Describing the emotional states that are expressed in speech. Speech Communication, 40(1–2), 5–32.
- Delorme, A., & Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of single- trial EEG dynamics including independent component analysis. Journal of Neuroscience Methods, 134(1), 9–21.
- FakhrHosseini, M., Jeon, M., & Bose, R. (2015). Estimation of drivers’ emotional states based on neuroergonmic equipment: An exploratory study using fNIRS. In Adjunct proceedings of the 7th international conference on automotive user interfaces and interactive vehicular applications (pp. 38–43).
- Murugappan, M., Rizon, M., Nagarajan, R., Yaacob, S., Hazry, D., & Zunaidi, I. (2008). Time-frequency analysis of EEG signals for human emotion detection. In 4th Kuala Lumpur international conference on biomedical engineering 2008 (pp. 262–265). Berlin, Heidelberg: Springer.
- Peng, K., Chen, L., Ruan, S., & Kukharev, G. (2005). A robust agorithm for eye detection on gray intensity face without spectacles. Journal of Computer Science and Technology, 5.
- Petrantonakis, P. C., & Hadjileontiadis, L. J. (2010). Emotion recognition from brain signals using hybrid adaptive filtering and higher order crossings analysis. IEEE Trans. Affect. Comput., 1(2), 81–97.
- Pollatos, O., Herbert, B. M., Matthias, E., & Schandry, R. (2007). Heart rate response after emotional picture presentation is modulated by interoceptive awareness. International Journal of Psychophysiology, 63(1), 117–124.
- Prescott, A. T., Sargent, J. D., & Hull, J. G. (2018). Metaanalysis of the relationship between violent video game play and physical aggression over time. Proceedings of the National Academy of Sciences, 115(40), 9882–9888.
- Prucnal, M. A., & Polak, A. G. (2018). Analysis of features extracted from EEG epochs by discrete wavelet decomposition and Hilbert transform for sleep apnea detection. In 2018 40th annual international conference of the IEEE engineering in medicine and biology society (pp. 287–290). EMBC).
- Przybylski, A. K., & Weinstein, N. (2019). “Violent video game engagement is not associated with adolescents’ aggressive behaviour: Evidence from a registered
# report. R. Soc. open Sci., 6(2), 171474.
- Purves, D., Augustine, G., Fitzpatrick, D., William, W. C., Anthony-Samuel, L., & White, L. E. (2012). Neuroscience. Sunderland, MA: Sinauer Associates.
- Rakshit, P., Saha, S., Konar, A., & Saha, S. (2016). A type-2 fuzzy classifier for gesture induced pathological disorder recognition. Fuzzy Sets and Systems, 305. https://doi.
org/10.1016/j.fss.2016.05.001
- Ryan, R. M., Rigby, C. S., & Przybylski, A. (2006). The motivational pull of video games: A self-determination theory approach. Motivation and Emotion, 30(4), 344–360.
- Fredrickson, B. L., Tugade, M. M., Waugh, C. E., & Larkin, G. R. (2003). What good are positive emotions in crisis? A prospective study of resilience and emotions following the terrorist attacks on the United States on september 11th, 2001. Journal of Personality and Social Psychology, 84(2), 365.
- Friesen, E., & Ekman, P. (1978). Facial action coding system: A technique for the measurement of facial movement. Palo Alto, 3.
- Saha, S., & Karia, J. (2019). Analyzing adverse gaming effects on emotions using neural networks based hybrid architecture. In 2019 IEEE 16th India council international conference (INDICON) (pp. 1–4).
- Saha, S., Konar, A., Saha, A., Sadhu, A. K., Banerjee, B., & Nagar, A. K. (2016). EEG based gesture mimicking by an artificial limb using cascade-correlation learning architecture. In Proceedings of the international joint conference on neural networks (Vol. 2016). https://doi.org/10.1109/IJCNN.2016.7727814. Octob.
13
# L. Ghosh et al.
# Computers in Human Behavior 116 (2021) 106640
# L. Ghosh et al.
- Sarlo, M., Palomba, D., Buodo, G., Minghetti, R., & Stegagno, L. (2005). Blood pressure changes highlight gender differences in emotional reactivity to arousing pictures. Biological Psychology, 70(3), 188–196.
- Savran, A., et al. (2006). Emotion detection in the loop from brain signals and facial images. In Proceedings of the eNTERFACE 2006 workshop.
- Schneider, E. F., Lang, A., Shin, M., & Bradley, S. D. (2004). Death with a story: How story impacts emotional, motivational, and physiological responses to first-person shooter video games. Human Communication Research, 30(3), 361–375.
- Shiota, M. N., Neufeld, S. L., Yeung, W. H., Moser, S. E., & Perea, E. F. (2011). Feeling good: Autonomic nervous system responding in five positive emotions. Emotion, 11, 1368-6.
- Sokolov, S., Velchev, Y., Radeva, S., & Radev, D. (2017). “Human emotion estimation from EEG and face using statistical features and SVM. In Proc. Int. Conf. Comput. Sci. Inf. Technol. (pp. 37–47).
- T Dasborough, M., Sinclair, M., Russell-Bennett, R., & Tombs, A. (2008). Measuring emotion: Methodological issues and alternatives.
- Todoran, G., Holonec, R., & Iakab, C. (2008). Discrete hilbert transform. numeric algorithms. Acta Electrotehnica, 49(4), 485–490.
- Torkkola, K. (2003). Feature extraction by non-parametric mutual information maximization. Journal of Machine Learning Research, 3(Mar), 1415–1438.
- Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision (pp. 4489–4497).
- Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063.
- Zheng, W.-L., Dong, B.-N., & Lu, B.-L. (2014). Multimodal emotion recognition using EEG and eye tracking data. In 2014 36th annual international conference of the IEEE engineering in medicine and biology society (pp. 5040–5043).
14
