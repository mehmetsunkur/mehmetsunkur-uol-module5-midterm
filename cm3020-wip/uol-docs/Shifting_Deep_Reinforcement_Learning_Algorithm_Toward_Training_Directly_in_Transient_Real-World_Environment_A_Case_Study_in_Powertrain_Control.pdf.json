[
    {
        "element_id": "70bca2be60c9629ea45dfeaba41a9cfe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.0,
                        92.0
                    ],
                    [
                        115.0,
                        114.2
                    ],
                    [
                        160.1,
                        114.2
                    ],
                    [
                        160.1,
                        92.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83682,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "8198",
        "type": "Header"
    },
    {
        "element_id": "ce722d3d52716b0e0f8471b76979c314",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        618.8,
                        90.4
                    ],
                    [
                        618.8,
                        112.7
                    ],
                    [
                        1432.2,
                        112.7
                    ],
                    [
                        1432.2,
                        90.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83013,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 17, NO. 12, DECEMBER 2021",
        "type": "Header"
    },
    {
        "element_id": "6c71f62b9ff706939f3dee09880134fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1468.0,
                        80.8
                    ],
                    [
                        1468.0,
                        141.8
                    ],
                    [
                        1543.9,
                        141.8
                    ],
                    [
                        1543.9,
                        80.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.647,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "tes",
        "type": "Header"
    },
    {
        "element_id": "3d6285dc8e0dfea8f3e2f5b594da59b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1462.5,
                        82.9
                    ],
                    [
                        1462.5,
                        132.9
                    ],
                    [
                        1540.8,
                        132.9
                    ],
                    [
                        1540.8,
                        82.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-1-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "c56b93fa07f91cdf461c002954bd126f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        133.6,
                        165.9
                    ],
                    [
                        133.6,
                        467.6
                    ],
                    [
                        1514.7,
                        467.6
                    ],
                    [
                        1514.7,
                        165.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62088,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "Shifting Deep Reinforcement Learning Algorithm Toward Training Directly in Transient Real-World Environment: A Case Study in Powertrain Control",
        "type": "NarrativeText"
    },
    {
        "element_id": "94192dace87807c5a399efe27f2f567d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        676.5,
                        484.7
                    ],
                    [
                        676.5,
                        521.3
                    ],
                    [
                        984.7,
                        521.3
                    ],
                    [
                        984.7,
                        484.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71493,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "Bo Hu and Jiaxi Li",
        "type": "NarrativeText"
    },
    {
        "element_id": "ac6ff497c3a0d1e33c70c8d17c879166",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        114.7,
                        605.3
                    ],
                    [
                        114.7,
                        1322.4
                    ],
                    [
                        815.8,
                        1322.4
                    ],
                    [
                        815.8,
                        605.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94461,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "Abstract\u2014Deep reinforcement learning (DRL) excels at playing a wide variety of simulated games and allows for a generic learning process that does not consider a speci\ufb01c knowledge of the task. However, due to the fact that a large prohibitively number of interactions with the environment are required and that the initial policy behavior is almost random, such an algorithm cannot be trained directly in a real-world environment while satisfying given safety con- straints. In this article, a control framework based on DRL that shifts toward training directly in the transient real- world environment is proposed. This research is working on the assumption that some demonstration knowledge that operates under previous controllers and an abstract of the agent environment dynamics are available. By encod- ing this prior knowledge into a sophisticated learning archi- tecture, a warm-starting DRL algorithm with a safe explo- ration guarantee can be anticipated. Taking the boost con- trol problem for a variable geometry turbocharger equipped diesel engine as an example, the proposed algorithm im- proves the initial performance by 74.6% and the learning ef\ufb01ciency by an order of magnitude in contrast to its vanilla counterpart. Compared with other existing DRL-based pow- ertrain control methods, the proposed algorithm can realize the \u201cmodel-free\u201d concept in the strict sense, making it at- tractive for future DRL-based powertrain control algorithms to build on.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e833f000207ed464965b5d594889f7dd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        606.2
                    ],
                    [
                        847.6,
                        660.4
                    ],
                    [
                        1546.2,
                        660.4
                    ],
                    [
                        1546.2,
                        606.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67816,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "Index Terms\u2014Deep reinforcement learning (DRL), demonstration, powertrain control, real-world environment.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0f210fe4fb72875489fd05f977a8cac5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1095.3,
                        717.6
                    ],
                    [
                        1095.3,
                        745.2
                    ],
                    [
                        1300.4,
                        745.2
                    ],
                    [
                        1300.4,
                        717.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76248,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1
        },
        "text": "I. INTRODUCTION",
        "type": "Title"
    },
    {
        "element_id": "a825428d3c5987f84aff4b0a2f0aafe0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        760.1
                    ],
                    [
                        847.6,
                        1456.7
                    ],
                    [
                        1545.6,
                        1456.7
                    ],
                    [
                        1545.6,
                        760.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94557,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "R EINFORCEMENT learning (RL) has long been consid- ered as an approach; animals use it to interact with the surrounding environments to realize optimal behavior based on the reward feedback. When RL was \ufb01rst introduced [1], simpli\ufb01ed tabular solutions were developed that operate with little prior knowledge and, thus, can only be applied to simulated problems in which the state and action spaces are small. Recent years have seen a rise in demand for RL agents capable of performing complex actions in continuous environments [2]. Deep reinforcement learning (DRL) combines deep learning technology and approximate-solution-based RL and it is con- sidered as one hot orientation of today\u2019s machine learning re- search [3]. It appears to offer a viable path for solving many decision-making problems that cannot currently be solved by any other approaches. For instance, DRL excels at solving a wide variety of Atari and board games and some of them can achieve superhuman performance, taking AlphaGo and AlphaGo Zero [4] for example. Due to the fact that DRL allows for a generic learning process that does not consider a speci\ufb01c knowledge of the task, it is rapidly gaining attention and has opened up a new window for many industrial control problems [5].",
        "type": "NarrativeText"
    },
    {
        "element_id": "a996caa3e5f1913f231fb7bd60891e07",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1416.6
                    ],
                    [
                        117.0,
                        1687.8
                    ],
                    [
                        814.4,
                        1687.8
                    ],
                    [
                        814.4,
                        1416.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9545,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Manuscript received November 10, 2020; revised February 5, 2021; accepted February 26, 2021. Date of publication March 3, 2021; date of current version August 20, 2021. This work was supported in part by the National Natural Science Foundation of China under Grant 51905061, in part by the Natural Science Foundation of Chongqing under Grant cstc2019jcyj-msxmX0097, in part by China Postdoctoral Science Foundation under Grant 2020M671842, and in part by the Sci- ence of Technology Research Program of Chongqing Municipal Educa- tion Commission under Grant KJQN201801124. Paper no. TII-20-5161. (Bo Hu and Jiaxi Li contributed equally to this work.) (Corresponding author: Bo Hu.)",
        "type": "NarrativeText"
    },
    {
        "element_id": "9985b159e6abd7d853270b134f43d6cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1690.5
                    ],
                    [
                        117.0,
                        1812.3
                    ],
                    [
                        814.4,
                        1812.3
                    ],
                    [
                        814.4,
                        1690.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94121,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Bo Hu is with the Key Laboratory of Advanced Manufacturing Tech- nology for Automobile Parts, Ministry of Education, Chongqing Univer- sity of Technology, Chongqing 400054, China, and also with Ningbo Yinzhou DLT Technology, Company, Ltd., Ningbo 315000, China (e-mail: b.hu@cqut.edu.cn).",
        "type": "NarrativeText"
    },
    {
        "element_id": "0df1bc7734710f2cc2f11ee6b2676178",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1815.1
                    ],
                    [
                        117.0,
                        1911.9
                    ],
                    [
                        814.4,
                        1911.9
                    ],
                    [
                        814.4,
                        1815.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9382,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Jiaxi Li is with the Key Laboratory of Advanced Manufac- turing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing 400054, China (e-mail: 11607990404@2016.cqut.edu.cn).",
        "type": "NarrativeText"
    },
    {
        "element_id": "083f1e83cb19d53a01dc390aac8a4a0b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1914.7
                    ],
                    [
                        117.0,
                        1961.7
                    ],
                    [
                        814.4,
                        1961.7
                    ],
                    [
                        814.4,
                        1914.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89676,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Color versions of one or more \ufb01gures in this article are available at https://doi.org/10.1109/TII.2021.3063489.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c89cbd8bc8fd21c3597b7c923a1ad26e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1462.3
                    ],
                    [
                        847.6,
                        1988.2
                    ],
                    [
                        1548.4,
                        1988.2
                    ],
                    [
                        1548.4,
                        1462.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94961,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "However, most of the DRL algorithms successfully applied to the \ufb01eld of simulated games currently cannot be directly migrated to tasks operated in the real physical environment [6]. This is mainly because most of the proposed DRL algorithms do not employ prior knowledge to kickstart learning, i.e., the best a DRL controller can initially do is to \u201ctrial and error\u201d uniformly at random and learn from scratch. Therefore, a large prohibitively number of interactions with the environment are required for a DRL-based controller to reach a desirable level of performance. For example, AlphaGo Zero, the evolved version of AlphaGo, re- quires 4.9 million self-matches to reach the master level [4]. This may be acceptable for a simulator but severely pose limitations to many industrial tasks, whose controller has to be trained in the real-world environment. Furthermore, safety is a key challenge for many industrial problems and erroneous behavior in safety critical systems may in\ufb02ict serious consequences in the real",
        "type": "NarrativeText"
    },
    {
        "element_id": "e75ec7223166b194c720b59ee6da8ce9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        136.6,
                        1964.5
                    ],
                    [
                        136.6,
                        1988.0
                    ],
                    [
                        637.1,
                        1988.0
                    ],
                    [
                        637.1,
                        1964.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68268,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Digital Object Identi\ufb01er 10.1109/TII.2021.3063489",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f08b0f50ff781c7da7aac2f73fffc63",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        286.0,
                        2019.9
                    ],
                    [
                        286.0,
                        2068.1
                    ],
                    [
                        1374.2,
                        2068.1
                    ],
                    [
                        1374.2,
                        2019.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83654,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "1551-3203 \u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.",
        "type": "NarrativeText"
    },
    {
        "element_id": "aecf5db116515a7e2f01b48f47d0575b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2122.9
                    ],
                    [
                        119.8,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2122.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76381,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 1,
            "parent_id": "0f210fe4fb72875489fd05f977a8cac5"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3dbb10a206e93697b24a25c2f0d725e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        92.4
                    ],
                    [
                        105.3,
                        112.1
                    ],
                    [
                        1178.3,
                        112.1
                    ],
                    [
                        1178.3,
                        92.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80584,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2
        },
        "text": "HU AND LI: SHIFTING DRL ALGORITHM TOWARD TRAINING DIRECTLY IN TRANSIENT REAL-WORLD ENVIRONMENT",
        "type": "Header"
    },
    {
        "element_id": "24c663a4cb19695b5fea59cae7ddc628",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        180.8
                    ],
                    [
                        105.3,
                        308.1
                    ],
                    [
                        802.7,
                        308.1
                    ],
                    [
                        802.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93985,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "3dbb10a206e93697b24a25c2f0d725e0"
        },
        "text": "physical world. In this context, a complementary mechanism to monitor and interfere with its operation whenever absolutely needed is required in order to get the utmost out of the DRL algorithm while ensuring safety simultaneously [7].",
        "type": "NarrativeText"
    },
    {
        "element_id": "c556c5215c06759111d82c12823c77a9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        349.4
                    ],
                    [
                        105.3,
                        377.3
                    ],
                    [
                        617.7,
                        377.3
                    ],
                    [
                        617.7,
                        349.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78026,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "3dbb10a206e93697b24a25c2f0d725e0"
        },
        "text": "A. Simulation-to-Real Transfer Technique",
        "type": "Title"
    },
    {
        "element_id": "8e92d1f597c74d1831b4e34fb248b279",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.3,
                        396.7
                    ],
                    [
                        103.3,
                        1055.4
                    ],
                    [
                        807.4,
                        1055.4
                    ],
                    [
                        807.4,
                        396.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94573,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "c556c5215c06759111d82c12823c77a9"
        },
        "text": "Although the nature of DRL makes it attractive for a wide range of potential industrial applications, including vehicle powertrain control [8], energy management of hybrid electric vehicle [9], [10], autonomous vehicle [11], intelligent trans- portation system [12], assistive robots [13], unmanned aerial vehicle autonomous target search [14], and industrial internet of things [15], most of the current research articles only focus on the \u201csimulation-to-real\u201d transfer learning technique, that is pretraining the control algorithm of\ufb02ine in a high-\ufb01delity simulation environment and after that transferring the control parameters from the simulator to an online real-world con- troller for \ufb01ne-tuning (some may skip the \ufb01ne-tuning phase and use the control parameters of the learned controller in the simulation directly). However, for many industrial problems, simulation model may be unavailable and the internal repre- sentation (and/or capability) between the simulated and real agent can be very different. In this context, the dif\ufb01culty in building complex industrial systems will cause a shift toward training a DRL-based control strategy directly in the real-world environment.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3004fc41ef4f8a9698fefc744607bb9b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        274.9
                    ],
                    [
                        1533.3,
                        274.9
                    ],
                    [
                        1533.3,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93667,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "c556c5215c06759111d82c12823c77a9"
        },
        "text": "data, the DRL policy may also update toward the ungrounded state-action value (or state value) and, therefore, a reasonably well control behavior cannot be guaranteed.",
        "type": "NarrativeText"
    },
    {
        "element_id": "fbfbd3a1b5fcfecaf67b6c724524beba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        280.5
                    ],
                    [
                        835.9,
                        806.8
                    ],
                    [
                        1536.5,
                        806.8
                    ],
                    [
                        1536.5,
                        280.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95166,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "c556c5215c06759111d82c12823c77a9"
        },
        "text": "Unlike the initialization techniques that directly decide the initial policy behavior, reward shaping is an alternative approach that allows the exploration to be biased toward the states with high potential [21]. In this way, the problem of employing the prior knowledge is transformed into de\ufb01ning the potential function using demonstration data. There are many ways to encode the demonstrated state-action pairs into a meaningful potential function, but only adopting the form as the difference between the new and old state-action potential, the total order over policies can remain unchanged while signi\ufb01cantly facili- tating the learning process [22]. Considering that the potential function can also be used to initialize a DRL policy apart from being integrated to form a shaping function in a normal reward shaping process, if the potential function was formulated using supervised learning or DRL pretraining approach as discussed above, similar issues would arise.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7101c33481dfade922b5c6ed01d0a408",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        862.1
                    ],
                    [
                        835.9,
                        893.1
                    ],
                    [
                        1041.5,
                        893.1
                    ],
                    [
                        1041.5,
                        862.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76701,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "3dbb10a206e93697b24a25c2f0d725e0"
        },
        "text": "C. Contributions",
        "type": "Title"
    },
    {
        "element_id": "71c756f8acb8ca3115e398f16b3cfa64",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        908.4
                    ],
                    [
                        835.9,
                        1105.3
                    ],
                    [
                        1536.4,
                        1105.3
                    ],
                    [
                        1536.4,
                        908.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94987,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "7101c33481dfade922b5c6ed01d0a408"
        },
        "text": "The primary objective of this article is to propose a powertrain control framework based on the DRL that is able to directly train its policy behavior in the transient real-world environment with- out violating safety issues. There are two originally important contributions that clearly distinguish our effort from the other pieces of literature.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f0731396b78ba59c0987d59815ded0c4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1096.7
                    ],
                    [
                        105.3,
                        1124.4
                    ],
                    [
                        647.6,
                        1124.4
                    ],
                    [
                        647.6,
                        1096.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77716,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "3dbb10a206e93697b24a25c2f0d725e0"
        },
        "text": "B. Learning From Demonstration Technique",
        "type": "Title"
    },
    {
        "element_id": "a022d644e3ed26a73a687304d69f2f9f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1144.0
                    ],
                    [
                        105.3,
                        1337.7
                    ],
                    [
                        803.8,
                        1337.7
                    ],
                    [
                        803.8,
                        1144.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95498,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "While high-\ufb01delity simulation models are dif\ufb01cult to build, most of the industrial powertrain control problems have the data of the control system operating under previous controllers that behave suboptimally [16]. In general, most of the current DRL research makes use of the demonstration knowledge from the perspective of either exploitation or exploration.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ec4dd3fee251dd383bcf5a810cb32337",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1343.3
                    ],
                    [
                        105.3,
                        2092.0
                    ],
                    [
                        806.7,
                        2092.0
                    ],
                    [
                        806.7,
                        1343.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94059,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "The most intuitive way to exploit the demonstration data is via DRL policy initialization using supervised learning (some- times termed imitation learning or behavior cloning) with no reward signal that allows the agent to evaluate its behavior. This typically requires a large number of teacher demonstra- tions from which a policy that reproduces and generalizes the demonstration can be derived. However, the prediction errors made in different states could add up; therefore, a mistake made by the controller can easily put it into a state that is far from the demonstration [17]. In addition, the behavior of demonstrations themselves can be imperfect, limiting the quality of the policies derived from them [18]. The other initialization- oriented exploitation approach is by means of DRL pretraining, assuming that the demonstration data were derived from real interactions with the environment [19]. In order to realize a vanilla DRL learning process, the reward function that de\ufb01nes the task problems is incorporated into the demonstration ex- perience. By doing this, the networks of a DRL policy can be updated of\ufb02ine from scratch and an improved initial policy can be expected. Some pieces of literature also suggest that using both demonstrations and real interactions with the sampling ratio automatically tuned by a prioritized replay mechanism facilitates",
        "type": "NarrativeText"
    },
    {
        "element_id": "2941c3748d292c07f438965872801aa5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        2073.9
                    ],
                    [
                        105.3,
                        2101.6
                    ],
                    [
                        802.6,
                        2101.6
                    ],
                    [
                        802.6,
                        2073.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "the learning process [20]. But without suf\ufb01cient demonstration",
        "type": "NarrativeText"
    },
    {
        "element_id": "de303e033ffdd036a1d4dd456f164ed6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        866.0,
                        1110.8
                    ],
                    [
                        866.0,
                        1371.0
                    ],
                    [
                        1547.7,
                        1371.0
                    ],
                    [
                        1547.7,
                        1110.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94145,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "1) First, although the prior knowledge can be either exploited in the of\ufb02ine initialization phase to provide a good \u201ccold- start\u201d performance or merged into the existing reward function to facilitate the online exploration process, to the authors\u2019 best knowledge, there seems to be no literature that systematically make use of the prior knowledge from multiperspective views while absorbing the essence of multidomain knowledge.",
        "type": "ListItem"
    },
    {
        "element_id": "54678ddfc6fbc4cb43c5ff6079f7ae1d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        874.3,
                        1376.5
                    ],
                    [
                        874.3,
                        1894.1
                    ],
                    [
                        1548.5,
                        1894.1
                    ],
                    [
                        1548.5,
                        1376.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9285,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "2) Second, because both the initial control actions and the following \u201ctrial and error\u201d exploration (especially during the beginning of the learning process) are expected to behave reasonably well, not only the learning process of a DRL-based powertrain controller can be accelerated but also more importantly the powertrain controller can be placed directly in a real-world environment with a high probability not to violate the given safety issues. To guarantee that the unsafe actions are not a part of the \ufb01nal policy, a complementary approach that uses a shield policy that overrides the learned policy with a safe backup policy as necessary to ensure safety will be combined with the proposed algorithm. This will provide an attractive op- timization direction for the future DRL-based powertrain control algorithm to build on, although no such research",
        "type": "ListItem"
    },
    {
        "element_id": "e22a77840b661308b4f54844f98d628f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        908.7,
                        1874.6
                    ],
                    [
                        908.7,
                        1902.3
                    ],
                    [
                        1365.2,
                        1902.3
                    ],
                    [
                        1365.2,
                        1874.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "has been found in the pieces of literature.",
        "type": "NarrativeText"
    },
    {
        "element_id": "013df8c4b08b6b16ff0ddb592ae573ad",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1907.9
                    ],
                    [
                        835.9,
                        2101.6
                    ],
                    [
                        1533.3,
                        2101.6
                    ],
                    [
                        1533.3,
                        1907.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94746,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "The rest of the article is organized as follows. In Section II, the control problem formulation and the methodologies of the initialization and reward shaping are detailed. Section III dis- cusses a case study results of the boost control problem for a variable geometry turbocharger (VGT) equipped diesel engine. Section IV concludes the article.",
        "type": "NarrativeText"
    },
    {
        "element_id": "44fe3011645024bee752be61a7ca565a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74815,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2,
            "parent_id": "f0731396b78ba59c0987d59815ded0c4"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4615c106c7e73c5ba104e9176050d5c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1488.4,
                        91.4
                    ],
                    [
                        1488.4,
                        113.4
                    ],
                    [
                        1534.6,
                        113.4
                    ],
                    [
                        1534.6,
                        91.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84395,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 2
        },
        "text": "8199",
        "type": "Header"
    },
    {
        "element_id": "9d875279dd3ae2a181ac8a9343a4f8f6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.9,
                        92.2
                    ],
                    [
                        115.9,
                        113.2
                    ],
                    [
                        160.1,
                        113.2
                    ],
                    [
                        160.1,
                        92.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84279,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "8200",
        "type": "Header"
    },
    {
        "element_id": "1bfdb82ef353bf278e275fbffa3a2f5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        729.2,
                        92.6
                    ],
                    [
                        729.2,
                        111.9
                    ],
                    [
                        1545.1,
                        111.9
                    ],
                    [
                        1545.1,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87987,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 17, NO. 12, DECEMBER 2021",
        "type": "Header"
    },
    {
        "element_id": "1ea7a11c631bd05e25a12a14ffcb00e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        225.4,
                        185.8
                    ],
                    [
                        225.4,
                        609.8
                    ],
                    [
                        705.1,
                        609.8
                    ],
                    [
                        705.1,
                        185.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-3-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "R intercooler Compressor NS \u2014 => gh 6 cylinder diesel engine Jo re ob urbine shaft (e) b \u00a3 VGT ey (0) 5 Zz C Oo > pi aldi D3 8 O sh 8 G dg vE Crankshaft | \u2122 s\u0131z EGR cooler silin",
        "type": "Image"
    },
    {
        "element_id": "5b933841c6e54aff7bc8945cf7bea96d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        915.7,
                        181.0
                    ],
                    [
                        915.7,
                        228.8
                    ],
                    [
                        1476.8,
                        228.8
                    ],
                    [
                        1476.8,
                        181.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.49397,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "TABLE I NETWORK ARCHITECTURE AND HYPERPARAMETERS SETTING",
        "type": "FigureCaption"
    },
    {
        "element_id": "307d9fab8911391f28f7687251f7b143",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        913.3,
                        257.2
                    ],
                    [
                        913.3,
                        677.2
                    ],
                    [
                        1448.2,
                        677.2
                    ],
                    [
                        1448.2,
                        257.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90964,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-3-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "text_as_html": "<table><thead><tr><th>Parameters</th><th>Value</th></tr></thead><tbody><tr><td>Learning rate</td><td>0.0001</td></tr><tr><td>Reward decay</td><td>0.9</td></tr><tr><td>Replay memory size</td><td>10000</td></tr><tr><td>Mini-batch size</td><td>128</td></tr><tr><td>\u20ac-greedy</td><td>0.99</td></tr><tr><td>Number of hidden layer</td><td></td></tr><tr><td>Number of hidden layer neurons</td><td>80</td></tr><tr><td>Number of discrete action</td><td>17</td></tr></tbody></table>"
        },
        "text": "Parameters Value Learning rate 0.0001 Reward decay 0.9 Replay memory size 10000 Mini-batch size 128 \u20ac-greedy 0.99 Number of hidden layer 3 Number of hidden layer neurons 80 Number of discrete action 17",
        "type": "Table"
    },
    {
        "element_id": "9272c056e20783343b4f29eb65ec3e98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        643.6
                    ],
                    [
                        117.0,
                        667.0
                    ],
                    [
                        663.5,
                        667.0
                    ],
                    [
                        663.5,
                        643.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79793,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "Fig. 1. VGT-equipped diesel engine with EGR system.",
        "type": "FigureCaption"
    },
    {
        "element_id": "bb9bc998b4cb7dab0b65465cf57232dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        343.0,
                        748.0
                    ],
                    [
                        343.0,
                        775.7
                    ],
                    [
                        584.9,
                        775.7
                    ],
                    [
                        584.9,
                        748.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77237,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "II. METHODOLOGIES",
        "type": "Title"
    },
    {
        "element_id": "492962ee85f0f74b90076f12ab64c7f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        797.8
                    ],
                    [
                        117.0,
                        825.5
                    ],
                    [
                        513.1,
                        825.5
                    ],
                    [
                        513.1,
                        797.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67614,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "A. Control Problem Formulation",
        "type": "Title"
    },
    {
        "element_id": "77d221b4cac354d725fc8e7c15925102",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        844.8
                    ],
                    [
                        117.0,
                        1305.3
                    ],
                    [
                        814.4,
                        1305.3
                    ],
                    [
                        814.4,
                        844.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95193,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "parent_id": "492962ee85f0f74b90076f12ab64c7f0"
        },
        "text": "Control problems: A learning algorithm speci\ufb01cally targeting the transient boost control of a VGT-equipped diesel engine in the real physical world will be proposed in this article. This control strategy will be implemented on a six-cylinder 3 L VGT-equipped engine (see Fig. 1) and aim to dynamically control the position of the VGT rack in order to track the target boost pressure. Due to the nonlinear characteristics of VGT system and the fact that exhaust gas recirculation (EGR) and VGT systems are strongly interactive, the boost control of the VGT is recognized as a major challenge for diesel engines. In this article, a control-oriented GT-Suite simulation model serves as an engine environment in order to verify the effectiveness of the proposed algorithm, but it can be applied to the real plants without modi\ufb01cation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7fa6a466bcc13a7dcf37fd665fa9484f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1309.7
                    ],
                    [
                        117.0,
                        1537.0
                    ],
                    [
                        815.0,
                        1537.0
                    ],
                    [
                        815.0,
                        1309.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95381,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "parent_id": "492962ee85f0f74b90076f12ab64c7f0"
        },
        "text": "DRL framework: DRL is a \ufb01eld of machine learning focusing on how an agent discovers which control action sequence con- tributes to the maximal expected cumulative return. Considering the control objective in this article and in order to measure the performance of the proposed algorithm, the cumulative return is de\ufb01ned as a tradeoff between the tracking accuracy and stability.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a5e134ceee0ed0c73ca4fbe516f12672",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1542.5
                    ],
                    [
                        117.0,
                        1935.5
                    ],
                    [
                        815.7,
                        1935.5
                    ],
                    [
                        815.7,
                        1542.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95612,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "parent_id": "492962ee85f0f74b90076f12ab64c7f0"
        },
        "text": "This article is based on the assumption that no high-\ufb01delity simulation model of the engine is available but the demonstration knowledge of the control system under previous controllers and an abstract of the agent environment dynamics exist. This prior knowledge will be mined deeper so that a DRL-based VGT boost controller can be placed directly in a real-world environment for policy learning without violating safety issues. Note that this will only be done for a vanilla deep Q-network (DQN) algorithm for the demonstration but can also be extended to other DRL frameworks, such as deep deterministic policy gradient (DDPG) and soft actor critic (SAC), which will be discussed in Section III-D.",
        "type": "NarrativeText"
    },
    {
        "element_id": "46c625600fb939524f19135d9fa95579",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1941.0
                    ],
                    [
                        117.0,
                        2068.3
                    ],
                    [
                        814.4,
                        2068.3
                    ],
                    [
                        814.4,
                        1941.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93651,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "parent_id": "492962ee85f0f74b90076f12ab64c7f0"
        },
        "text": "Fig. 2 shows the learning process of a DQN algorithm. For the proposed control problem, the incremental vane position of the VGT is selected as the control action (a). The four quantities, namely actual boost pressure, target boost pressure,",
        "type": "NarrativeText"
    },
    {
        "element_id": "4d34c2c674d7b884608533f253c76045",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        727.9
                    ],
                    [
                        847.6,
                        1087.7
                    ],
                    [
                        1547.2,
                        1087.7
                    ],
                    [
                        1547.2,
                        727.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95535,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3,
            "parent_id": "492962ee85f0f74b90076f12ab64c7f0"
        },
        "text": "engine speed, and current vane position, are used to form the 4-D state space (s). The reward function (r) has to be carefully designed to accurately represent the task and it is often considered as the most dif\ufb01cult part of an RL algorithm. Nevertheless, most of the powertrain control problems have a relatively clear idea of the reward form. For example, the control objective of this work is to track the target boost pressure under transient driving cycles by regulating the vanes in a QUICK and STABLE manner. Keeping this objective in mind, using Gaussian function as the basic function, the reward is de\ufb01ned as",
        "type": "NarrativeText"
    },
    {
        "element_id": "a84776b72434b997d5c2ff7f60c3cf93",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1087.0,
                        1099.1
                    ],
                    [
                        1087.0,
                        1146.4
                    ],
                    [
                        1545.0,
                        1146.4
                    ],
                    [
                        1545.0,
                        1099.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80415,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "rt = e\u2212 [\u03b1|e(t)|+\u03b2|It |]2 2 (1)",
        "type": "Formula"
    },
    {
        "element_id": "cde6dda3eb1343904c7c4e89f8031b39",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1158.2
                    ],
                    [
                        847.6,
                        1391.7
                    ],
                    [
                        1548.1,
                        1391.7
                    ],
                    [
                        1548.1,
                        1158.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95649,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "where e(t) is the error between the target boost and the current boost, and It is the control action change rate. e(t) is the term that is directly coupled to the control objective and strives to minimize the boost pressure error at every time step. It is designed to decrease the oscillatory behavior in the VGT control signals. The weighting \u03b1 and \u03b2 between the two terms depends on the requirement of the control task.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d460fc785bd56c0d689346d9181b5fcb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1396.7
                    ],
                    [
                        847.6,
                        1856.1
                    ],
                    [
                        1547.7,
                        1856.1
                    ],
                    [
                        1547.7,
                        1396.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9544,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "In order to break the correlations between the consecutive samples, the technique of experience replay is adopted in DON that stores experience tuples (s, a, r,s\u2019) in the replay memory and samples uniformly at random when performing updates. Beyond that, DQN introduces a target Q-network with parameter 0 to calculate the target. It has the same structure as the Q-network with parameter 4 and the initial weights are also the same, except that the Q-network is updated every iteration, while the target Q-network is updated at regular intervals. The loss function between the target and the current Q value can be seen from 2 and it is often optimized using stochastic gradient decent. For comparison purposes, the same network architecture and hyperparameters settings, as given in Table I, are used in the following work:",
        "type": "NarrativeText"
    },
    {
        "element_id": "024ef211b62eb2d92914067345fbfda6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        860.7,
                        1876.6
                    ],
                    [
                        860.7,
                        1968.7
                    ],
                    [
                        1545.0,
                        1968.7
                    ],
                    [
                        1545.0,
                        1876.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84007,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "L(\u03b8) = E r + \u03b3 max ai+1 Q(st+1,at+1,\u03b8\u2212) \u2212 Q(st, at, \u03b8) . (2)",
        "type": "Formula"
    },
    {
        "element_id": "4207f8cb254ebc405b9fe1db77c1ea90",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1973.9
                    ],
                    [
                        847.6,
                        2068.3
                    ],
                    [
                        1545.4,
                        2068.3
                    ],
                    [
                        1545.4,
                        1973.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92691,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "Demonstration: The meaning of demonstration in this work is the mapping of example state to action, which derives from the system operating under an existing \ufb01ne-tuned PID controller that",
        "type": "NarrativeText"
    },
    {
        "element_id": "bd9a88b02027bbf789a023e1eb60913f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.5
                    ],
                    [
                        1530.2,
                        2144.5
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7626,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 3
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f90e14208d1960bbc1455f3a4fb57950",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        92.6
                    ],
                    [
                        105.3,
                        111.9
                    ],
                    [
                        1177.0,
                        111.9
                    ],
                    [
                        1177.0,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84913,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "HU AND LI: SHIFTING DRL ALGORITHM TOWARD TRAINING DIRECTLY IN TRANSIENT REAL-WORLD ENVIRONMENT",
        "type": "Header"
    },
    {
        "element_id": "ab4355834894e4e07f796430572fab82",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        234.4,
                        183.5
                    ],
                    [
                        234.4,
                        803.6
                    ],
                    [
                        1405.3,
                        803.6
                    ],
                    [
                        1405.3,
                        183.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Powertrain I | Control action a, Current state 5, | | from sensing data | During operation \u201cTeommelloge ll \u2014 e-greedy exploration Control | itati ontrol loop & exploitation t Tensori \u2014\u2014 A Update parameters 4 Learning loop (State) a (Action) State action (Next State) Current O value Yem 0(s;,4,,8) 7 Future rewards | 9 ymaxOls,.,4,,,, 0 = Gradient of Net error a \u2014 Target O network \u2014 r (Reward) | | | | transition S' | | | \u2014\u2014 Target O value rt ymaxOls,.,4,.,.0\u201d) dr \u2014 \u2014 e e i ee eC Ce el",
        "type": "Image"
    },
    {
        "element_id": "003f8b3a1fa7fb7e022b7c4b73132b39",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        836.4
                    ],
                    [
                        105.3,
                        859.5
                    ],
                    [
                        595.1,
                        859.5
                    ],
                    [
                        595.1,
                        836.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71816,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Fig. 2. Vanilla DQN implementation mechanism.",
        "type": "ListItem"
    },
    {
        "element_id": "71517dca3dbdad943aa0d42957d3fb17",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        155.4,
                        947.4
                    ],
                    [
                        155.4,
                        1242.4
                    ],
                    [
                        751.7,
                        1242.4
                    ],
                    [
                        751.7,
                        947.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "7? \u2014Demonstration \u00e9 21.6 aa \u015ei VA A 245 a e Bo sms 0 B14 Time (6) 8 \u00a313 \\ B12 > o d14 o 50 100 150 200 Time (s) 250 274",
        "type": "Image"
    },
    {
        "element_id": "ce8d73ecac115cb9095bab314bcdc3f1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        898.5,
                        948.1
                    ],
                    [
                        898.5,
                        1234.1
                    ],
                    [
                        1471.1,
                        1234.1
                    ],
                    [
                        1471.1,
                        948.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-5.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "2 T T B18 2 \u2014Target g2 \u2014 Random 516 3 2 3 514 3 912 \u011fu 0 50 100 150 200 250 274 Time(s)",
        "type": "Image"
    },
    {
        "element_id": "e759dfcae12f2cf0d7dd8cf552086d92",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        100.2,
                        1275.8
                    ],
                    [
                        100.2,
                        1323.3
                    ],
                    [
                        802.6,
                        1323.3
                    ],
                    [
                        802.6,
                        1275.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.917,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Fig. 3. Target boost pressure and the corresponding demonstration policy behavior.",
        "type": "FigureCaption"
    },
    {
        "element_id": "9fa878d30b42c4bc14ec2b2a8bef0879",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.5,
                        1267.9
                    ],
                    [
                        835.5,
                        1314.9
                    ],
                    [
                        1533.3,
                        1314.9
                    ],
                    [
                        1533.3,
                        1267.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90962,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Fig. 4. Simulated boost pressure with random actions during the very \ufb01rst training episode.",
        "type": "FigureCaption"
    },
    {
        "element_id": "6279c5594bb7d14bb576832d934cc09d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1382.0
                    ],
                    [
                        105.3,
                        1575.7
                    ],
                    [
                        805.3,
                        1575.7
                    ],
                    [
                        805.3,
                        1382.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9534,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "behaves suboptimal. Fig. 3 shows the target boost pressure and the corresponding policy behavior of the demonstration under a fraction of US FTP-72 (Federal Test Procedure 72) driving cycle. Although they seem to overlap from the \ufb01gure\u2019s full view, there is a gap that can be further optimized between the demonstration and the optimal from the zoom-in plot.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4dca0263182fea8d6d7559b373909424",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1580.9
                    ],
                    [
                        105.3,
                        2040.7
                    ],
                    [
                        804.4,
                        2040.7
                    ],
                    [
                        804.4,
                        1580.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95325,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Exploration: In the applications of vanilla DQN, one of the random exploration techniques, namely \u03b5-greedy procedure, has been frequently utilized to balance exploitation and exploration. In order to fully explore the state space in the simulated environ- ment, it is a common practice to randomly select actions, i.e., \u03b5 is equal to 0 at the beginning of the training and gradually reduce randomness (while increasing the value of \u03b5 to a large number, for example 0.99) as the agent gains the experience. For industrial applications, this practice of training is strictly not allowed, as random exploration could lead the plant to some uncontrollable or even dangerous states causing irreversible losses. Taking the simulated control problems for example (see from Fig. 4 ), during the very \ufb01rst training episode when the actions were only selected randomly, there would be a large",
        "type": "NarrativeText"
    },
    {
        "element_id": "7b713acb69a982c5d4ea069cddf15c3d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        841.5,
                        1356.8
                    ],
                    [
                        841.5,
                        1663.4
                    ],
                    [
                        1526.9,
                        1663.4
                    ],
                    [
                        1526.9,
                        1356.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-4-6.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Learner Evaluate O network Target Q network Replay memory",
        "type": "Image"
    },
    {
        "element_id": "d4d1d74b0465ec5899afa243ca1aaae7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1695.3
                    ],
                    [
                        835.9,
                        1720.3
                    ],
                    [
                        1211.4,
                        1720.3
                    ],
                    [
                        1211.4,
                        1695.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79668,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Fig. 5. DQN pretraining mechanism.",
        "type": "FigureCaption"
    },
    {
        "element_id": "439a2aaea4617f03fe2c16e8ab3e02b1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        831.8,
                        1780.5
                    ],
                    [
                        831.8,
                        2040.6
                    ],
                    [
                        1538.2,
                        2040.6
                    ],
                    [
                        1538.2,
                        1780.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95777,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "deviation between the target and the actual boost pressure. For some engine operating regions, the engine in real world could be damaged because of the very high mechanical/thermal load. Therefore, a very large value of \u03b5 is set in this article as the exploration is bias toward the mining of the prior knowledge and the purpose of small randomness is only to optimize the policy behavior steadily and compensate the system inconsistency over time (for instance, hardware aging).",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d418bf24e826b69ce2e663415ebce96",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        118.5,
                        2122.7
                    ],
                    [
                        118.5,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2122.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72348,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "220e62df4f7445b140710561037b01a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1488.9,
                        91.9
                    ],
                    [
                        1488.9,
                        113.4
                    ],
                    [
                        1534.0,
                        113.4
                    ],
                    [
                        1534.0,
                        91.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83773,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 4
        },
        "text": "8201",
        "type": "Header"
    },
    {
        "element_id": "3dd9682f514c8d0ed40658f9b5005ca3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.1,
                        91.6
                    ],
                    [
                        116.1,
                        113.7
                    ],
                    [
                        160.1,
                        113.7
                    ],
                    [
                        160.1,
                        91.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80394,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "8202",
        "type": "Header"
    },
    {
        "element_id": "21053ccaa1bb40e4f363b572ad0e9933",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        730.6,
                        92.6
                    ],
                    [
                        730.6,
                        111.9
                    ],
                    [
                        1545.1,
                        111.9
                    ],
                    [
                        1545.1,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88961,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 17, NO. 12, DECEMBER 2021",
        "type": "Header"
    },
    {
        "element_id": "d6569d9e74a0e0842e5550c5bb41e51b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        180.9
                    ],
                    [
                        117.0,
                        208.5
                    ],
                    [
                        517.8,
                        208.5
                    ],
                    [
                        517.8,
                        180.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73388,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5,
            "parent_id": "21053ccaa1bb40e4f363b572ad0e9933"
        },
        "text": "B. Of\ufb02ine Initialization Algorithm",
        "type": "Title"
    },
    {
        "element_id": "ec7fb219129fe95352375f6f437e09ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        228.2
                    ],
                    [
                        117.0,
                        588.0
                    ],
                    [
                        814.4,
                        588.0
                    ],
                    [
                        814.4,
                        228.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95576,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5,
            "parent_id": "d6569d9e74a0e0842e5550c5bb41e51b"
        },
        "text": "In this section, two different initialization techniques, namely supervised learning and DQN pretraining, are introduced in order to exploit the prior demonstration knowledge. There are many variants of supervised learning techniques and most of them work in a similar manner by the minimization of classi- \ufb01cation losses using the same network structure as the DQN policy. Following the literature [23], a score-based multiclass classi\ufb01cation algorithm is adopted in this article [see (3)] and the neural networks are trained using a stochastic gradient descent optimization algorithm to minimize the margin classi\ufb01cation loss:",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca5f0f96c39a4a5f40a520a05485a0da",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        918.5,
                        185.2
                    ],
                    [
                        918.5,
                        545.9
                    ],
                    [
                        1475.2,
                        545.9
                    ],
                    [
                        1475.2,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-7.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Reward: +100 (a) Reward: #1 \u2014\u2014\u2014> \u2014\u2014> (8), \u00a9 pa (b) Reward: +100",
        "type": "Image"
    },
    {
        "element_id": "140fff0293b2b4b934c97c7e1d8c9a27",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        843.8,
                        578.6
                    ],
                    [
                        843.8,
                        601.7
                    ],
                    [
                        1226.8,
                        601.7
                    ],
                    [
                        1226.8,
                        578.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70877,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Fig. 6. Reward shaping mechanism.",
        "type": "FigureCaption"
    },
    {
        "element_id": "084b32a2496b1946950fa9bf5bbc7cd8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        188.3,
                        603.2
                    ],
                    [
                        188.3,
                        656.0
                    ],
                    [
                        814.4,
                        656.0
                    ],
                    [
                        814.4,
                        603.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77171,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "JE (Q) = max a\u2208A [Q (s, a) + l (aE, a)] \u2212 Q (s, aE) (3)",
        "type": "Formula"
    },
    {
        "element_id": "949f3a5e0863412d7c13c63cb04b0eaa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        661.4
                    ],
                    [
                        117.0,
                        695.1
                    ],
                    [
                        814.4,
                        695.1
                    ],
                    [
                        814.4,
                        661.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "where aE is the action, the demonstrator selected in state s and",
        "type": "NarrativeText"
    },
    {
        "element_id": "abd25276a9094a716b8edc33c9e00531",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        669.8
                    ],
                    [
                        117.0,
                        794.7
                    ],
                    [
                        814.4,
                        794.7
                    ],
                    [
                        814.4,
                        669.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93739,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "l(aE, a) is a margin function that is 0 when a = aE and positive, otherwise. This loss forces the values of the other actions to be at least a margin lower than the value of the demonstrator\u2019s actions.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f67d398a20828b1ee261f27ccfd9c648",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        800.2
                    ],
                    [
                        117.0,
                        1126.8
                    ],
                    [
                        814.4,
                        1126.8
                    ],
                    [
                        814.4,
                        800.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95552,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "The other approach to initialize a DQN\u2019s network is via the method of pretraining that uses a similar learning process as a vanilla DQN policy. This requires the demonstration to incorporate a reward function that defines the task problems and assumes that the augmented dataset was derived from the real interactions with the environment (see Fig. 5). By doing so, the actor and the learner process of a vanilla DQN are separated and the actor process is replaced by the augmented demonstration that periodically send the quads of (s,a,r,s') to the replay memory to complete a vanilla DQN training process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0a0188f12eacfc6c2ea7750d8049265d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1132.3
                    ],
                    [
                        117.0,
                        1757.8
                    ],
                    [
                        814.4,
                        1757.8
                    ],
                    [
                        814.4,
                        1132.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94794,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Although both the supervised learning and the DQN pretrain- ing technique can offer a viable solution for the demonstration knowledge transfer during the initialization phase of the training, the problems inherent in the nature of them may prevent them from being widely adopted for powertrain control problems. This is mainly because the demonstration data are generally not perfect for many cases and are only covering a narrow part of the state-action space without realistic values for other unseen conditions. By coupling the theory of supervised learning and DQN pretraining [see (4)], it seems that they can back up and provide each other with ground estimates. For example, when the policy is initialized by pretraining, the network would update toward the maximum value of the next state and this can be grounded by the supervised learning simultaneously. On the contrary, the DQN update can provide the supervised learning process with the ground truth, i.e., reward function and help the Q-network satisfy the Bellman equation. This is required to improve the policy for the following \ufb01ne-tuning learning process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e6b09adf703812ac827c2e82f2261396",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        875.3,
                        671.1
                    ],
                    [
                        875.3,
                        698.7
                    ],
                    [
                        1544.8,
                        698.7
                    ],
                    [
                        1544.8,
                        671.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "The potential function is able to provide the learning controller",
        "type": "NarrativeText"
    },
    {
        "element_id": "69ddc95ef69f8cd775f8cef201bed690",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        693.6
                    ],
                    [
                        847.6,
                        1744.1
                    ],
                    [
                        1545.0,
                        1744.1
                    ],
                    [
                        1545.0,
                        693.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91578,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "with useful gradient information by enriching the base reward signal. It can also be formulated using the of\ufb02ine initialization technique described above. This is because the Q value derived from the initialization training phase is no other than the potential estimate of a state-action pair. Intuitively, this approach will bias the action exploration toward the higher augmented rewards, therefore realizing the demonstration knowledge transfer from a different angle. Nevertheless, it should be noted that modifying the reward function may change the policies that will be learned. Taking the problem in Fig. 6 for example, the agent is learning to walk from the original state SA to the target state ST . The reward for reaching the target state is + 100 and all other nontarget rewards are 0. In the \ufb01rst round of learning, the agent uses random exploration strategy and the probability of being able to reach the target and get the reward is one-fourth. If the state-action space becomes larger, the probability that the agent reaches to the target for the \ufb01rst time will be very low. An intuitive method to solve this reward sparsity problem is to give the agent an intermediate reward, i.e., potential function in addition to the base reward function when the agent takes a step toward the goal. For example, a reward of +1 can be added when the agent reaching a state is closer to the target state. Although it looks effective, the optimal solution for this modi\ufb01ed reward problem is changed to walk back and forth between SA and SB and get a +1 reward constantly, violating the original intention to only facilitate the learning process without changing the solution of the task. If we de\ufb01ne a potential function over the state-action space and take F as the difference between the new and old state-action\u2019s potential (see the following equations), Vecerik et al. [20] and Brys et al. [21] showed that this formulation preserves the total order over policies.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ad2d6b99a02f3c7d37961235988becf5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        282.0,
                        1773.0
                    ],
                    [
                        282.0,
                        1807.9
                    ],
                    [
                        818.5,
                        1807.9
                    ],
                    [
                        818.5,
                        1773.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6634,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "J (Q) = JDQN (Q) + JE (Q) . (4)",
        "type": "Formula"
    },
    {
        "element_id": "3777dde73ac1c9aa46acb8d67fd883c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        920.2,
                        1753.5
                    ],
                    [
                        920.2,
                        1798.3
                    ],
                    [
                        1545.0,
                        1798.3
                    ],
                    [
                        1545.0,
                        1753.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68447,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Rr (s,a,8\u2019) = R(s,a,s') + F(s,a,8') (6)",
        "type": "Formula"
    },
    {
        "element_id": "ead2d280c48ae41261875e9df84f5585",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        920.2,
                        1803.3
                    ],
                    [
                        920.2,
                        1843.5
                    ],
                    [
                        1545.0,
                        1843.5
                    ],
                    [
                        1545.0,
                        1803.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63156,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "F(s,a,s',a') = ymax \u00ae (s\u2019,a\u2019) \u2014 max\u00ae(s,a). (7)",
        "type": "Formula"
    },
    {
        "element_id": "e5cf4d97529d6321789330a44f76b033",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1844.8
                    ],
                    [
                        117.0,
                        1872.4
                    ],
                    [
                        579.0,
                        1872.4
                    ],
                    [
                        579.0,
                        1844.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56213,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "C. Online Reward Shaping Algorithm",
        "type": "Title"
    },
    {
        "element_id": "9eef1fa10a33f4fc5efc62d7067e1a3d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1892.1
                    ],
                    [
                        117.0,
                        2019.4
                    ],
                    [
                        814.4,
                        2019.4
                    ],
                    [
                        814.4,
                        1892.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94159,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5,
            "parent_id": "e5cf4d97529d6321789330a44f76b033"
        },
        "text": "Apart from the practice of incorporating the prior knowledge in the initialization phase, the example demonstration can also be integrated into the standard training process by augmenting the reward expression with the potential function \u03c6.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b0284d3a2f0d233be1b821d280966c81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        240.9,
                        2031.5
                    ],
                    [
                        240.9,
                        2068.9
                    ],
                    [
                        814.4,
                        2068.9
                    ],
                    [
                        814.4,
                        2031.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62241,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Rr (s,a,8!) = R(s,a,8')+6(s,a,8!). (8)",
        "type": "Formula"
    },
    {
        "element_id": "f6e4c0c275a4394a3295deb5191e483c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        845.4,
                        1907.8
                    ],
                    [
                        845.4,
                        2068.4
                    ],
                    [
                        1546.1,
                        2068.4
                    ],
                    [
                        1546.1,
                        1907.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93811,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Based on the discussions above, the prior knowledge can be either exploited in the of\ufb02ine initialization phase to provide a good \u201ccold-start\u201d performance or merged into the existing reward function to facilitate the online exploration process. Meanwhile, the coupling of the supervised learning and the DQN",
        "type": "NarrativeText"
    },
    {
        "element_id": "97356c5b622ebc82bd1a23833e6ffee2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74799,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 5
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ce0fa33b019e599e18d00a3d796a1151",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        92.6
                    ],
                    [
                        105.3,
                        112.0
                    ],
                    [
                        1186.7,
                        112.0
                    ],
                    [
                        1186.7,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83582,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "HU AND LI: SHIFTING DRL ALGORITHM TOWARD TRAINING DIRECTLY IN TRANSIENT REAL-WORLD ENVIRONMENT",
        "type": "Header"
    },
    {
        "element_id": "90989d2ca3a5eca08d766435e366d620",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        208.2,
                        184.7
                    ],
                    [
                        208.2,
                        698.7
                    ],
                    [
                        1431.5,
                        698.7
                    ],
                    [
                        1431.5,
                        184.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-6-8.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "Phase 1: Initialization Phase 2: Fine-Tuning Learner Shield actor Learner Evaluate 0 Gy\u0130M pea Reward 4 A Mode shaping 7 shaping pan \u00a9 Suet network H network (5.5) network Gradient ymax \u00a9(5',a') max \u00ae(s,a) $ War.t. loss ? Gradi . : Copy Sie ola) Evaluate Q Evaluate a Target Q Network Target Q network network Demonstration sas) (sar,",
        "type": "Image"
    },
    {
        "element_id": "e879ef600373fe93a63e52baed58a1fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        732.5
                    ],
                    [
                        105.3,
                        754.6
                    ],
                    [
                        166.8,
                        754.6
                    ],
                    [
                        166.8,
                        732.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "Fig. 7.",
        "type": "Title"
    },
    {
        "element_id": "1afa72b20ae5cf10c04bc090ea41cfda",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        127.1,
                        732.5
                    ],
                    [
                        127.1,
                        755.1
                    ],
                    [
                        1426.3,
                        755.1
                    ],
                    [
                        1426.3,
                        732.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87466,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "e879ef600373fe93a63e52baed58a1fb"
        },
        "text": "Initialization and reward shaping coupling mechanism using the combination of both supervised learning and DQN pretraining.",
        "type": "FigureCaption"
    },
    {
        "element_id": "2a065aa57a0abb8ec0d7703f0874cb2b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        811.2
                    ],
                    [
                        105.3,
                        1004.9
                    ],
                    [
                        804.7,
                        1004.9
                    ],
                    [
                        804.7,
                        811.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95454,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "e879ef600373fe93a63e52baed58a1fb"
        },
        "text": "pretraining technique can effectively utilize the prior demon- stration knowledge. Thus, the next logical step is to make use of the prior knowledge from the perspective of both initialization and reward shaping using the combination of both supervised learning and standard DRL pretraining, but no such research has been found in the pieces of literature.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f051278989d99df254608fff6f24ea6a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.4,
                        970.0
                    ],
                    [
                        103.4,
                        2070.2
                    ],
                    [
                        803.5,
                        2070.2
                    ],
                    [
                        803.5,
                        970.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91058,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "e879ef600373fe93a63e52baed58a1fb"
        },
        "text": "been found in the pieces of literature. To guarantee that the chosen action is safe, a complementary approach that uses a shield policy will be combined with the proposed algorithm [24]. This mechanism overrides the learned policy with a safe backup policy when necessary and will allow the controller\u2019s safety guarantees and optimality to be integrated. As the shield is not the focus of this work and it is only adopted to check whether a given safety constraint is violated and guarantee the safety of the proposed algorithm just in case (which is even not activated for the proposed algorithm in Section III), a brief shield introduction is presented. In general, the shield simply constrains the set of the allowed actions in a way that ensures safety using a planning-ahead strategy. Take the control problem in this article for example, the actual boost pressure of the engine should be constrained below a prescribed level, which is determined by the mechanical and thermal loading of the engine cylinder. This safety speci\ufb01cation can be satis\ufb01ed when a planning-ahead model is given, as the system can be enforced to never cross the state\u2019s safety region by only selecting safe actions. Note that the perfect environment dynamics is not required and the coarse abstraction of the environment dynamics w.r.t the safety speci\ufb01cation is enough to execute the shield. For example, thanks to the intuitive and mature knowledge of the reactive system between the VGT and the diesel engine, it is not dif\ufb01cult to decide at speci\ufb01c state which action is safe or not. Speci\ufb01cally, the shield monitors the actual boost pressure via a pressure sensor and selects safe actions accordingly, i.e., if the actual boost pressure is reaching the safety limit, the aspect ratio of the VGT is controlled smaller and vice-versa. In this article, different transient VGT behaviors at different engine speeds are considered and a conservative abstraction of the environment dynamics is built that guarantees the safety to be obeyed in real physical world. Fig. 7 shows the illustration of",
        "type": "NarrativeText"
    },
    {
        "element_id": "58fdea1350a92f10aaddc8e6521a99d7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        884.6,
                        815.1
                    ],
                    [
                        884.6,
                        1115.1
                    ],
                    [
                        1484.3,
                        1115.1
                    ],
                    [
                        1484.3,
                        815.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-6-9.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "18 FT. \u00e7 Sparen learning ie \u0130m Er p Combined 2 3! 4 Han a 1 o ve 100 ei cad ae",
        "type": "Image"
    },
    {
        "element_id": "e7fa9498edc5b084e18a3b7dd5774983",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1148.9
                    ],
                    [
                        835.9,
                        1196.0
                    ],
                    [
                        1533.3,
                        1196.0
                    ],
                    [
                        1533.3,
                        1148.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91058,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "Fig. 8. Direct demonstration knowledge transfer via supervised learn- ing, DQN pretraining, and combined.",
        "type": "FigureCaption"
    },
    {
        "element_id": "f3edcef96bde25f7e2e5c7dbcf077191",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1263.0
                    ],
                    [
                        835.9,
                        1323.9
                    ],
                    [
                        1533.3,
                        1323.9
                    ],
                    [
                        1533.3,
                        1263.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91509,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "the proposed DQN from the demonstration algorithm, together with the aforementioned shield mechanism.",
        "type": "NarrativeText"
    },
    {
        "element_id": "58f8e4fe4d107fb81627fb4df67423e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        997.8,
                        1371.9
                    ],
                    [
                        997.8,
                        1399.7
                    ],
                    [
                        1371.1,
                        1399.7
                    ],
                    [
                        1371.1,
                        1371.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80246,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "III. RESULTS AND DISCUSSIONS",
        "type": "Title"
    },
    {
        "element_id": "27d202452b23fd42d1d9feb154b94ab6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1419.4
                    ],
                    [
                        835.9,
                        1679.8
                    ],
                    [
                        1535.1,
                        1679.8
                    ],
                    [
                        1535.1,
                        1419.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95713,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "58f8e4fe4d107fb81627fb4df67423e0"
        },
        "text": "The proposed DQN algorithm that makes use of the prior knowledge is validated in this section. In the following, \ufb01rst, the policy behavior directly derived from the demonstration is presented. Then, the comparisons of the following online \ufb01ne-tuning learning process are made from the perspective of utilizing this demonstration via initialization and reward shap- ing. Finally, based on the results above, a suggestion is given on how to exploit the prior demonstration effectively and safely.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b5693ed58cdb4abf6dfa4e93be9b32f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1725.9
                    ],
                    [
                        835.9,
                        1755.8
                    ],
                    [
                        1237.4,
                        1755.8
                    ],
                    [
                        1237.4,
                        1725.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76195,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "A. Of\ufb02ine Initialization Algorithm",
        "type": "Title"
    },
    {
        "element_id": "69f2826d85713a387fa75936bef17514",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        833.5,
                        1774.8
                    ],
                    [
                        833.5,
                        2068.3
                    ],
                    [
                        1535.3,
                        2068.3
                    ],
                    [
                        1535.3,
                        1774.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95327,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "b5693ed58cdb4abf6dfa4e93be9b32f4"
        },
        "text": "In order to show the generalization capability of the knowl- edge transfer from the demonstration, only a small fraction of the imperfect demonstration data corresponding to the \ufb01rst period of boost increase and decrease is utilized (see Fig. 2 for reference). Fig. 8 shows the policy behavior directly derived from this demonstration using the technique of supervised learning, DQN pretraining, and the combined without an online \ufb01ne-tuning learning process. It can be seen that the limited data, which only accounting for 14.5% of the total demonstration already, have the",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a7385790d7fdd45895a4b9b55ddca3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        118.7,
                        2123.6
                    ],
                    [
                        118.7,
                        2144.5
                    ],
                    [
                        1530.2,
                        2144.5
                    ],
                    [
                        1530.2,
                        2123.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76076,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6,
            "parent_id": "b5693ed58cdb4abf6dfa4e93be9b32f4"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "15ab6d793851509975d88e15c630fd81",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1488.8,
                        91.9
                    ],
                    [
                        1488.8,
                        113.3
                    ],
                    [
                        1534.5,
                        113.3
                    ],
                    [
                        1534.5,
                        91.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83593,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 6
        },
        "text": "8203",
        "type": "Header"
    },
    {
        "element_id": "afca9b2d0ca4bef96dc7121d5faf761d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.7,
                        91.8
                    ],
                    [
                        115.7,
                        113.4
                    ],
                    [
                        160.1,
                        113.4
                    ],
                    [
                        160.1,
                        91.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85093,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "8204",
        "type": "Header"
    },
    {
        "element_id": "6ea1af2860bdc9d44e513f522560e2fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        731.3,
                        92.5
                    ],
                    [
                        731.3,
                        111.9
                    ],
                    [
                        1545.1,
                        111.9
                    ],
                    [
                        1545.1,
                        92.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87435,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 17, NO. 12, DECEMBER 2021",
        "type": "Header"
    },
    {
        "element_id": "4676a6d9e0491a6d0a9a676ed5f76c3c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        153.2,
                        186.1
                    ],
                    [
                        153.2,
                        484.8
                    ],
                    [
                        778.2,
                        484.8
                    ],
                    [
                        778.2,
                        186.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-10.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "0 2 2 5 z (4 Supervised learning 2 + DQN pre-train 3 +: Combined 8 5 = 3 o 10000 > 5 10 15 20 Episode",
        "type": "Image"
    },
    {
        "element_id": "9ac423378470c7c4af0ca0007f5e3cc7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        889.3,
                        183.7
                    ],
                    [
                        889.3,
                        482.0
                    ],
                    [
                        1503.3,
                        482.0
                    ],
                    [
                        1503.3,
                        183.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-11.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "\u00a9 ie $ -1000 5 \u201ce- Reward shaping with Initialization 5 20001 \u201cInitialization | 7 \u201cReward shaping without Initialization 2 & -3000 5 E 3 -4000 4 -5000 1 4 6 8 10 12 14 16 18 20 Episode",
        "type": "Image"
    },
    {
        "element_id": "791ed5feb350303d82dab095f717e1e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        518.6
                    ],
                    [
                        117.0,
                        565.6
                    ],
                    [
                        814.3,
                        565.6
                    ],
                    [
                        814.3,
                        518.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9048,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "Fig. 9. Fine-tuning learning curve for supervised learning, DQN pre- training, and combined.",
        "type": "FigureCaption"
    },
    {
        "element_id": "2fb6c9d5ad78a657c1221a709309477d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        515.4
                    ],
                    [
                        847.6,
                        563.1
                    ],
                    [
                        1545.0,
                        563.1
                    ],
                    [
                        1545.0,
                        515.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91173,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "Fig. 11. Learning curve using initialization, reward shaping, and combined.",
        "type": "FigureCaption"
    },
    {
        "element_id": "11d172192438a4947e0f7753c485af6b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        158.8,
                        606.9
                    ],
                    [
                        158.8,
                        906.5
                    ],
                    [
                        772.8,
                        906.5
                    ],
                    [
                        772.8,
                        606.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-12.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "8 -1000 g $ -2000 :- \u00b0 2 -3000 i ~\u00a9 Combined 2 -\u00ab Supervised learning 5 -9-DON pre-train 6 8 10 12 14 16 18 20 Episode",
        "type": "Image"
    },
    {
        "element_id": "9ac6dca27c99b762a4dec783b078991b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        897.7,
                        608.0
                    ],
                    [
                        897.7,
                        901.3
                    ],
                    [
                        1493.7,
                        901.3
                    ],
                    [
                        1493.7,
                        608.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-7-13.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "\u0130\u2014Reward shaping with initialization \u015e 12 li 1.6 \u2014Demonstration | G \u2014_\" 2 Tage GG S167 Li B14 5 3 513 B12 o 0 50 100 150 200 250 274 Time (s)",
        "type": "Image"
    },
    {
        "element_id": "e1f796f82864f4a85a8cc5d1933721be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        940.3
                    ],
                    [
                        117.0,
                        987.3
                    ],
                    [
                        814.4,
                        987.3
                    ],
                    [
                        814.4,
                        940.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91897,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "Fig. 10. Learning curve of reward shaping method using supervised learning, DQN pretraining, and combined techniques.",
        "type": "FigureCaption"
    },
    {
        "element_id": "1ed72c5ee8884513b27a2ed5bde6991f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        934.7
                    ],
                    [
                        847.6,
                        981.8
                    ],
                    [
                        1545.0,
                        981.8
                    ],
                    [
                        1545.0,
                        934.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91128,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "Fig. 12. Final policy behavior between the demonstration and the proposed method.",
        "type": "FigureCaption"
    },
    {
        "element_id": "b13a491e593cd589038ccd32fb72ce5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1046.4
                    ],
                    [
                        117.0,
                        1206.9
                    ],
                    [
                        814.4,
                        1206.9
                    ],
                    [
                        814.4,
                        1046.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95321,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "potential to generalize reasonably well and compared with the results of DQN pretraining, which has a tendency to overshoot, and that of the supervised learning, which undershoots for the most of time, the technique combining both of them performs the best.",
        "type": "NarrativeText"
    },
    {
        "element_id": "31897985cf02459d9574ff984db16202",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1244.2
                    ],
                    [
                        117.0,
                        1308.2
                    ],
                    [
                        759.8,
                        1308.2
                    ],
                    [
                        759.8,
                        1244.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88873,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "B. Of\ufb02ine Initialization Algorithm Followed by Online Learning",
        "type": "NarrativeText"
    },
    {
        "element_id": "be44a2bfa4608b00a0a1f408c68e9ca9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1327.7
                    ],
                    [
                        117.0,
                        1720.7
                    ],
                    [
                        815.8,
                        1720.7
                    ],
                    [
                        815.8,
                        1327.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95667,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "Following the initialization phase, an online learning process is applied using a vanilla DQN exploration algorithm with constant small randomness, i.e., \u03b5 = 0.99. From Fig. 9, it can be seen that, although there is a big difference between the initial policy behaviors (indicated by the cumulated rewards) using the three initialization techniques, they all gradually improve to a similar level during the online learning process while interacting with the environment. Since the combined method surpasses the other two techniques during the entire online learning process, it is not dif\ufb01cult to draw the conclusion that the experience learned from the of\ufb02ine combined technique can also be bene\ufb01cial for the following online learning process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "74b0479b359f1cacf834d416a9cfeffd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1044.4
                    ],
                    [
                        847.6,
                        1404.1
                    ],
                    [
                        1549.3,
                        1404.1
                    ],
                    [
                        1549.3,
                        1044.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95609,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "shows the learning curve of the corresponding reward shaping methods. It can be seen that they all initialize at a poor level and the reward shaping realized using the combined technique performs the best both for the initial behavior and during the following online learning process, indicating its better capability to learn in the real environment. From Figs. 9 and 10, it looks like the result of the supervised learning is better than that of the DQN pretraining. However, it is not always true for different datasets and in fact that is one of the most important reasons why the techniques of both are combined to back up each other in this article.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2824f21ab5794c446afcab0757627246",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1409.7
                    ],
                    [
                        847.6,
                        1769.5
                    ],
                    [
                        1551.4,
                        1769.5
                    ],
                    [
                        1551.4,
                        1409.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95531,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "From the discussion above, it can be seen that the algorithm combining the techniques of both the supervised learning and DQN pretraining is able to have a better utilization of the demon- stration data compared with the technique applied separately, and coupling the initialization and the reward shaping algorithms may provide good initial policy behavior and facilitate the online learning process simultaneously. This is veri\ufb01ed in Fig. 11 in which a better \u201ccold-start\u201d performance and a steadier learning process have been realized using the combined technique for both the initialization and reward shaping methods using the techniques of both supervised learning and DQN pretraining.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8798a6ae9b2c74093c5568fe6deb0223",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1759.4
                    ],
                    [
                        117.0,
                        1788.6
                    ],
                    [
                        577.2,
                        1788.6
                    ],
                    [
                        577.2,
                        1759.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79303,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7
        },
        "text": "C. Online Reward Shaping Algorithm",
        "type": "Title"
    },
    {
        "element_id": "461b2b8570cb8d827d013cdd282c974c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1808.2
                    ],
                    [
                        117.0,
                        2068.7
                    ],
                    [
                        817.4,
                        2068.7
                    ],
                    [
                        817.4,
                        1808.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95524,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7,
            "parent_id": "8798a6ae9b2c74093c5568fe6deb0223"
        },
        "text": "Apart from the practice of exploiting the prior knowledge in the initialization phase of the training process, the exam- ple demonstration can also be integrated into the standard online training process by augmenting the reward expression with some potential function. In order to facilitate the compar- ison between the reward shaping algorithm and the aforemen- tioned of\ufb02ine initialization algorithm, the potential function is also formulated using the three initialization techniques. Fig. 10",
        "type": "NarrativeText"
    },
    {
        "element_id": "dc830c195175604a217da1ba4649e5f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1775.0
                    ],
                    [
                        847.6,
                        2068.3
                    ],
                    [
                        1547.1,
                        2068.3
                    ],
                    [
                        1547.1,
                        1775.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95203,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7,
            "parent_id": "8798a6ae9b2c74093c5568fe6deb0223"
        },
        "text": "The \ufb01nal policy behavior comparison between the demon- stration and the proposed algorithm is made in Fig. 12. Com- pared with the target controller, the proposed algorithm can not only have a better \ufb01nal policy behavior but also feature self-adaptivity, as shown in Fig. 11, making it attractive to real plant control problems whose system consistency may not be strictly guaranteed and whose environment may change over time. Meanwhile, considering safety constraint and training ef\ufb01- ciency, the traditional DRL algorithms often need to be trained in",
        "type": "NarrativeText"
    },
    {
        "element_id": "6036414d82033dd1fa5e97a5f8077b33",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.0,
                        2123.7
                    ],
                    [
                        115.0,
                        2144.6
                    ],
                    [
                        1530.2,
                        2144.6
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74041,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 7,
            "parent_id": "8798a6ae9b2c74093c5568fe6deb0223"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "03865e8d13ca4030f36906954999bb49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        92.2
                    ],
                    [
                        105.3,
                        112.1
                    ],
                    [
                        1174.7,
                        112.1
                    ],
                    [
                        1174.7,
                        92.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81683,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "HU AND LI: SHIFTING DRL ALGORITHM TOWARD TRAINING DIRECTLY IN TRANSIENT REAL-WORLD ENVIRONMENT",
        "type": "Header"
    },
    {
        "element_id": "a5f666c43ba26576dfcd7ff280c18324",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        145.6,
                        185.3
                    ],
                    [
                        145.6,
                        873.7
                    ],
                    [
                        762.6,
                        873.7
                    ],
                    [
                        762.6,
                        185.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-8-14.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "0 0,9000 21-0 gerer ye n , 8 eee \u00a7 -2000 Ay te, 5 FA a an Ee ape vee e 5 \u00c7ayi e ( ssp = = = 2 SITE N \u00a9 -4000 7a a ar | 2 .- / 8 # we \u2014 \u2014 \u2014 \u2014 E-6000- , \u201cDON from demonstration \u2014 ae \u00a9 44 6-DAN from scratch without Shield DON from scratch with Shield 5 10 15 20 25 30 Episode (a) -8000 0 15000 7 \u201c#-DON from demonstration #-DON from scratch without Shield) 5 DON from scratch with Shield 8 10000 2 s w 2 % 5000 2 > 0 0 5 10 15 20 25 30 Episode (0)",
        "type": "Image"
    },
    {
        "element_id": "29892e376c61049733c75b0285a76923",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.8,
                        907.5
                    ],
                    [
                        104.8,
                        979.4
                    ],
                    [
                        802.7,
                        979.4
                    ],
                    [
                        802.7,
                        907.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9401,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "Fig. 13. (a) Learning curve (b) and unsafe state count of the DQN from scratch with and without shield and the proposed DQN from demonstra- tion with shield.",
        "type": "FigureCaption"
    },
    {
        "element_id": "e1ccdfa2d9e0619d7ca8d3011a36824a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.0,
                        1044.4
                    ],
                    [
                        103.0,
                        1437.6
                    ],
                    [
                        804.3,
                        1437.6
                    ],
                    [
                        804.3,
                        1044.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95362,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "a simulation environment and then transferred to a real physical environment. This means that the traditional DRL algorithms generally lack suf\ufb01cient robustness, i.e., once a situation that does not appear in the training environment occurs in the real environment, the strategies that originally performed well in the training environment often cannot be handled correctly. The proposed modi\ufb01ed DRL algorithm, however, has the potential to be applied directly to the real world. As there is no need to explicitly model the plant, the poor robust performance of the traditional DRL algorithms arising from discrepancies between the actual plant and its mathematical model can be greatly improved.",
        "type": "NarrativeText"
    },
    {
        "element_id": "94a067ffa98945f0fb13ef95da922aa9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        540.6
                    ],
                    [
                        1534.9,
                        540.6
                    ],
                    [
                        1534.9,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95646,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "without full understanding of the unsafe behavior, they cannot achieve as large rewards as the vanilla DQN. This is particularly evident for the shielded DQN algorithm, while for the proposed algorithm, the last cumulated reward representing the \ufb01nal pol- icy behavior is only marginally poorer than the vanilla DQN without shield. Considering the proposed algorithm improves the initial performance by 74.6% and the learning ef\ufb01ciency by an order of magnitude while realizing the \u201cmodel-free\u201d concept in the strict sense compared with the vanilla DQN algorithm, it is attractive for many industrial problems whose high-\ufb01delity simulation model is unavailable or too complex to build.",
        "type": "NarrativeText"
    },
    {
        "element_id": "04a1fb13dd4f6a79baf214b9f3dc047a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        590.7
                    ],
                    [
                        835.9,
                        653.8
                    ],
                    [
                        1478.9,
                        653.8
                    ],
                    [
                        1478.9,
                        590.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88966,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "D. Proposed Algorithm Under Actor-Critic (AC) DRL Framework",
        "type": "NarrativeText"
    },
    {
        "element_id": "54efce39808a464384858662c94a5cb9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        673.1
                    ],
                    [
                        835.9,
                        1132.9
                    ],
                    [
                        1535.5,
                        1132.9
                    ],
                    [
                        1535.5,
                        673.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95066,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "Although this article, taking DQN as an example, is based on the value-based DRL framework, similar techniques can also be extended to AC DRL methods, including DDPG and SAC. Speci\ufb01cally, in order to achieve an improved initial policy behav- ior, the prior knowledge can be \ufb01rst incorporated into the of\ufb02ine initialization phase using the proposed algorithm combining the techniques of both supervised learning and DRL pretraining. This is done by formulating the supervised loss in (8) and updating the actor network using the combined policy gradient with the form in (9). After that the state-action value using the initialized AC network can be considered as the potential estimate and the reward shaping algorithm that is similar with the proposed method in this work can be adopted to facilitate an effective exploration by enriching the base reward signal",
        "type": "NarrativeText"
    },
    {
        "element_id": "57b304547cc1d5992ca164922b2ac097",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1029.6,
                        1159.7
                    ],
                    [
                        1029.6,
                        1209.2
                    ],
                    [
                        1538.0,
                        1209.2
                    ],
                    [
                        1538.0,
                        1159.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76671,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "(8) Je =|| de \u2014 (se | 6\") |)?",
        "type": "Formula"
    },
    {
        "element_id": "6b09bc2571e9b73a80a90dbfeedb622a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        995.0,
                        1209.5
                    ],
                    [
                        995.0,
                        1244.5
                    ],
                    [
                        1371.7,
                        1244.5
                    ],
                    [
                        1371.7,
                        1209.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "Vou J\u2019 = Vou \u0130DRL KA: VowJE",
        "type": "Title"
    },
    {
        "element_id": "98db3c89c0f8f38d33a96c80e47147a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1001.7,
                        1218.7
                    ],
                    [
                        1001.7,
                        1250.2
                    ],
                    [
                        1533.3,
                        1250.2
                    ],
                    [
                        1533.3,
                        1218.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.61337,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "(9)",
        "type": "Formula"
    },
    {
        "element_id": "26ff9358a4b6036ce72c7f00237ba5cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1274.6
                    ],
                    [
                        835.9,
                        1310.0
                    ],
                    [
                        1533.2,
                        1310.0
                    ],
                    [
                        1533.2,
                        1274.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "where \u03bc(st | \u03b8\u03bc) and aE represent the actor network\u2019s output",
        "type": "Title"
    },
    {
        "element_id": "499d1f5b7a9e14abd75a299a99d92dcb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1285.6
                    ],
                    [
                        835.9,
                        1442.9
                    ],
                    [
                        1533.4,
                        1442.9
                    ],
                    [
                        1533.4,
                        1285.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94216,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8,
            "parent_id": "26ff9358a4b6036ce72c7f00237ba5cb"
        },
        "text": "where p(s, | 0\u201c) and ag represent the actor network\u2019s output with parameter 4\u201d and the demonstration action, respectively. Von JpRL is the original policy gradient. V9,, Jz is the gradient supervised loss to the actor network weights, 4 is the adjustment factor, and Vg,,.J\u2019 is the combined gradient.",
        "type": "NarrativeText"
    },
    {
        "element_id": "828feacf75ce370ad95e86a563e05cef",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1442.6
                    ],
                    [
                        105.3,
                        2068.3
                    ],
                    [
                        804.0,
                        2068.3
                    ],
                    [
                        804.0,
                        1442.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9443,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8,
            "parent_id": "26ff9358a4b6036ce72c7f00237ba5cb"
        },
        "text": "Finally, the proposed shield mechanism is adopted to guaran- tee the safety constraint to be satis\ufb01ed. Fig. 13 shows the learning curve and unsafe state count of the DQN from scratch with and without shield and the proposed DQN from demonstration with shield. Compared with the vanilla DQN-based powertrain control methods requiring the control algorithm to learn from scratch, thus, cannot be placed directly in the real-world envi- ronment for training, the shielded versions of both the DQN and the proposed algorithm are able to realize safe exploration during the entire learning phase, indicating its potential to be applicable in safe critical training phase. In terms of the accumulated error, the integral absolute error of the proposed algorithm at the very \ufb01rst episode is improved by 74.6% compared with that of the vanilla DQN algorithm. Note that unlike the shielded DQN that guarantees the safety behavior by frequently activating the shield mechanism, the proposed algorithm by its own can constrain its action within the safety speci\ufb01cation and the shield here is only to show whether the safety is violated or not. However, due to the fact that the shielded algorithms can only explore safe actions",
        "type": "NarrativeText"
    },
    {
        "element_id": "91569b1ed98234a40c48f8c3e061193e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1083.8,
                        1495.2
                    ],
                    [
                        1083.8,
                        1522.9
                    ],
                    [
                        1283.7,
                        1522.9
                    ],
                    [
                        1283.7,
                        1495.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83645,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "IV. CONCLUSION",
        "type": "Title"
    },
    {
        "element_id": "f5122915b84742b186c4d199d67490f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        834.7,
                        1542.5
                    ],
                    [
                        834.7,
                        2068.3
                    ],
                    [
                        1535.4,
                        2068.3
                    ],
                    [
                        1535.4,
                        1542.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95148,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8,
            "parent_id": "91569b1ed98234a40c48f8c3e061193e"
        },
        "text": "In this article, a powertrain control framework based on the DQN that was able to directly train its policy behavior in tran- sient real-world environment without violating safety issues was proposed. By incorporating the prior knowledge from previous controllers into both the of\ufb02ine initialization and the following online reward shaping phase while combining the techniques of both the supervised and standard DRL pretraining, taking the boost control problem for a VGT-equipped diesel engine as an example, the proposed algorithm improved the initial performance by 74.6% and the learning ef\ufb01ciency by an order of magnitude while meeting the safety constraint compared with its vanilla DQN-based counterpart. Considering the form of the prior knowledge in this article was easy to obtain for many indus- trial powertrain problems and the proposed algorithm can realize the \u201cmodel-free\u201d concept in the strict sense, it is suggested for future DRL-based powertrain control to build on.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e35396e0bf8d4f6f6ff9993036869a6f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        2123.0
                    ],
                    [
                        117.0,
                        2144.5
                    ],
                    [
                        1530.2,
                        2144.5
                    ],
                    [
                        1530.2,
                        2123.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73588,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8,
            "parent_id": "91569b1ed98234a40c48f8c3e061193e"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bc3d5ddcc931fa9efe5fc52f1079feec",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1488.8,
                        91.7
                    ],
                    [
                        1488.8,
                        113.3
                    ],
                    [
                        1534.4,
                        113.3
                    ],
                    [
                        1534.4,
                        91.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83313,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 8
        },
        "text": "8205",
        "type": "Header"
    },
    {
        "element_id": "aa9a25e4a288d0ca746d779e1802bec9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.0,
                        92.6
                    ],
                    [
                        116.0,
                        113.0
                    ],
                    [
                        160.1,
                        113.0
                    ],
                    [
                        160.1,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84225,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "8206",
        "type": "Header"
    },
    {
        "element_id": "f591bccf008ed300b70778dbfc4b1d3a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        731.3,
                        92.5
                    ],
                    [
                        731.3,
                        111.9
                    ],
                    [
                        1545.1,
                        111.9
                    ],
                    [
                        1545.1,
                        92.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89637,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 17, NO. 12, DECEMBER 2021",
        "type": "Header"
    },
    {
        "element_id": "a435c7a46cbb910070d3c6e5437708aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        384.2,
                        180.6
                    ],
                    [
                        384.2,
                        208.3
                    ],
                    [
                        542.5,
                        208.3
                    ],
                    [
                        542.5,
                        180.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85867,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "f591bccf008ed300b70778dbfc4b1d3a"
        },
        "text": "REFERENCES",
        "type": "Title"
    },
    {
        "element_id": "f2f46ad1f0395292afa04d7d147312a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        229.2
                    ],
                    [
                        128.1,
                        301.5
                    ],
                    [
                        814.4,
                        301.5
                    ],
                    [
                        814.4,
                        229.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93694,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[1] R. S. Sutton and A. G. Barto, \u201cReinforcement learning,\u201d in Reinforcement Learning: An Introduction, J. Peters, Ed., 2nd ed., Cambridge, MA, USA: MIT Press, 2018, pp. 1\u20134.",
        "type": "ListItem"
    },
    {
        "element_id": "3edfff2ddad900ba33de946fca26f8cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        304.0
                    ],
                    [
                        128.1,
                        351.3
                    ],
                    [
                        814.5,
                        351.3
                    ],
                    [
                        814.5,
                        304.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91539,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[2] T. P. Lillicrap et al., \u201cContinuous control with deep reinforcement learn- ing,\u201d in Proc. Int. Conf. Learn. Representation, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "af0f447cb6af075e9e903b60751b5834",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        353.8
                    ],
                    [
                        128.1,
                        401.1
                    ],
                    [
                        816.2,
                        401.1
                    ],
                    [
                        816.2,
                        353.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91563,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[3] V. Mnih et al., \u201cHuman-level control through deep reinforcement learn- ing,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "2ac52d876a5c7e2a1c834d592c93b970",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        185.1
                    ],
                    [
                        847.6,
                        257.1
                    ],
                    [
                        1545.0,
                        257.1
                    ],
                    [
                        1545.0,
                        185.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93597,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[19] V. de la Cruz Gabriel, Y. Du, and M. E. Taylor, \u201cPre-training with non- expert human demonstration for deep reinforcement learning,\u201d Knowl. Eng. Rev., vol. 34, 2019, Art. no. e10.",
        "type": "ListItem"
    },
    {
        "element_id": "21dc622f1408eda532e90a1ab2e14a64",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        259.6
                    ],
                    [
                        847.6,
                        331.8
                    ],
                    [
                        1545.0,
                        331.8
                    ],
                    [
                        1545.0,
                        259.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93599,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[20] M. Vecerik et al., \u201cLeveraging demonstrations for deep reinforce- ment learning on robotics problems with sparse rewards,\u201d 2017, arXiv: 1707.08817.",
        "type": "ListItem"
    },
    {
        "element_id": "55a7246f780e4f104f47ad66c9bcbbff",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        334.6
                    ],
                    [
                        847.6,
                        406.5
                    ],
                    [
                        1545.0,
                        406.5
                    ],
                    [
                        1545.0,
                        334.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93571,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[21] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Nowe, \u201cReinforcement learning from demonstration through shaping,\u201d in Proc. 24th Int. Conf. Artif. Intell., 2015, pp. 3352\u20133358.",
        "type": "ListItem"
    },
    {
        "element_id": "34977b7173bb41046033c9fca4c5b06e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        403.6
                    ],
                    [
                        128.1,
                        451.4
                    ],
                    [
                        821.2,
                        451.4
                    ],
                    [
                        821.2,
                        403.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91341,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[4] D. Silver et al., \u201cMastering the game of Go without human knowledge,\u201d Nature, vol. 550, no. 7676, pp. 354\u2013359, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "6626673514d96d19a5931cef8fe5e29b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        453.7
                    ],
                    [
                        128.1,
                        550.5
                    ],
                    [
                        814.5,
                        550.5
                    ],
                    [
                        814.5,
                        453.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9368,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[5] B. Hu and J. Li, \u201cAn edge computing framework for powertrain con- trol system optimization of intelligent and connected vehicles based on curiosity-driven deep reinforcement learning,\u201d IEEE Trans. Ind. Electron., to be published, doi: 10.1109/TIE.2020.3007100.",
        "type": "ListItem"
    },
    {
        "element_id": "b4e6d603ff49b25e68f8c2c709597095",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        409.3
                    ],
                    [
                        847.6,
                        481.3
                    ],
                    [
                        1545.1,
                        481.3
                    ],
                    [
                        1545.1,
                        409.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92686,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[22] E. Wiewiora, G. Cottrell, and C. Elkan, \u201cPrincipled methods for advising reinforcement learning agents,\u201d in Proc. 20th Int. Conf. Mach. Learn., 2003, pp. 792\u2013799.",
        "type": "ListItem"
    },
    {
        "element_id": "7d8291fcd62a58131cc01fc3b08025e3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        483.8
                    ],
                    [
                        847.6,
                        556.0
                    ],
                    [
                        1545.0,
                        556.0
                    ],
                    [
                        1545.0,
                        483.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93224,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[23] B. Piot et al., \u201cBoosted Bellman residual minimization handling expert demonstrations,\u201d in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases, 2014, pp. 549\u2013564.",
        "type": "ListItem"
    },
    {
        "element_id": "b4d855a4359a75bdf7d892cff5115b3d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        553.3
                    ],
                    [
                        128.1,
                        600.9
                    ],
                    [
                        814.5,
                        600.9
                    ],
                    [
                        814.5,
                        553.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91621,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[6] J. Garc\u00eda and F. Fern\u00e1ndez, \u201cA comprehensive survey on safe reinforce- ment learning,\u201d J. Mach. Learn. Res., vol. 16, no. 1, pp. 1437\u20131480, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "12822d8a304acea04a9741ed2ff2b54b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        603.1
                    ],
                    [
                        128.1,
                        650.2
                    ],
                    [
                        814.4,
                        650.2
                    ],
                    [
                        814.4,
                        603.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91766,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[7] O. Bastani, \u201cSafe reinforcement learning via online shielding,\u201d 2019, arXiv:1905.10691.",
        "type": "ListItem"
    },
    {
        "element_id": "51a9a6c10d9439306e5afa1517467541",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        558.8
                    ],
                    [
                        847.6,
                        630.7
                    ],
                    [
                        1545.0,
                        630.7
                    ],
                    [
                        1545.0,
                        558.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93442,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[24] M. Alshiekh, R. Bloem, R. Ehlers, B. K\u00f6nighofer, S. Niekum, and U. Topcu, \u201cSafe reinforcement learning via shielding,\u201d in Proc. AAAI Conf. Artif. Intell., 2018, pp. 2669\u20132678.",
        "type": "ListItem"
    },
    {
        "element_id": "b98b1ccc5f61dcd14ec27d2e51b53e67",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        652.9
                    ],
                    [
                        128.1,
                        725.0
                    ],
                    [
                        815.7,
                        725.0
                    ],
                    [
                        815.7,
                        652.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9319,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[8] I.-M. Chen, C. Zhao, and C.-Y. Chan, \u201cA deep reinforcement learning- based approach to intelligent powertrain control for automated vehicles,\u201d in Proc. IEEE Intell. Transp. Syst. Conf., 2019, pp. 2620\u20132625.",
        "type": "ListItem"
    },
    {
        "element_id": "24cd05b5c88f27bafe0deb350b793d8b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        727.7
                    ],
                    [
                        128.1,
                        824.5
                    ],
                    [
                        816.0,
                        824.5
                    ],
                    [
                        816.0,
                        727.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93871,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[9] T. Liu, X. Hu, W. Hu, and Y. Zou, \u201cA heuristic planning reinforcement learning-based energy management for power-split plug-in hybrid electric vehicles,\u201d IEEE Trans. Ind. Inform., vol. 15, no. 12, pp. 6436\u20136445, Dec. 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "254ad729b7eb782b07ee648a3ead86f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        827.3
                    ],
                    [
                        117.0,
                        924.7
                    ],
                    [
                        815.9,
                        924.7
                    ],
                    [
                        815.9,
                        827.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93801,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[10] T. Liu, Y. Zou, D. Liu, and F. Sun, \u201cReinforcement learning of adaptive energy management with transition probability for a hybrid electric tracked vehicle,\u201d IEEE Trans. Ind. Electron., vol. 62, no. 12, pp. 7837\u20137846, Dec. 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "f0632d2f4d40466817ed99ed7e8a3319",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        926.9
                    ],
                    [
                        117.0,
                        999.0
                    ],
                    [
                        816.1,
                        999.0
                    ],
                    [
                        816.1,
                        926.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92956,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[11] Q. Zhang, J. Lin, Q. Sha, B. He, and G. Li, \u201cDeep interactive reinforcement learning for path following of autonomous underwater vehicle,\u201d IEEE Access, vol. 8, pp. 24258\u201324268, Jan. 2020.",
        "type": "ListItem"
    },
    {
        "element_id": "75dfde422c74e2f05ec5b4f4116de3f1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1001.4
                    ],
                    [
                        117.0,
                        1073.6
                    ],
                    [
                        816.6,
                        1073.6
                    ],
                    [
                        816.6,
                        1001.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9313,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[12] Q. Qi et al., \u201cKnowledge-driven service of\ufb02oading decision for vehicular edge computing: A deep reinforcement learning approach,\u201d IEEE Trans. Veh. Technol., vol. 68, no. 5, pp. 4192\u20134203, May 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "0b7a1cd9273f7fff35902fa94c43214f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1076.3
                    ],
                    [
                        117.0,
                        1148.3
                    ],
                    [
                        815.9,
                        1148.3
                    ],
                    [
                        815.9,
                        1076.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93255,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9,
            "parent_id": "a435c7a46cbb910070d3c6e5437708aa"
        },
        "text": "[13] H. Shi, L. Shi, M. Xu, and K.-S. Hwang, \u201cEnd-to-end navigation strategy with deep reinforcement learning for mobile robots,\u201d IEEE Trans. Ind. Inform., vol. 16, no. 4, pp. 2393\u20132402, Apr. 2020.",
        "type": "ListItem"
    },
    {
        "element_id": "3b19b0bfed453390ec1918f0a0f81e3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        785.1
                    ],
                    [
                        847.6,
                        1034.1
                    ],
                    [
                        1046.8,
                        1034.1
                    ],
                    [
                        1046.8,
                        785.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-9-15.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "Aw\u2019",
        "type": "Image"
    },
    {
        "element_id": "52145c9d10664c6dcab99814ba83dc0e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1077.5,
                        782.7
                    ],
                    [
                        1077.5,
                        954.3
                    ],
                    [
                        1548.7,
                        954.3
                    ],
                    [
                        1548.7,
                        782.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93231,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "Bo Hu was born in Hefei, Anhui Province, China, in 1989. He received the B.S. degree from the Chongqing University of Technology (CQUT), Chongqing, China, in 2011, and the M.S. and Ph.D. degrees from the University of Bath, Bath, U.K., in 2012 and 2016, respectively, all in automotive engineering.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f69e3dbf7e8a2915c58fa5acf5c0e3dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1076.6,
                        957.0
                    ],
                    [
                        1076.6,
                        1039.4
                    ],
                    [
                        1546.9,
                        1039.4
                    ],
                    [
                        1546.9,
                        957.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8164,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "He is currently an Associate Professor with the Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Ed-",
        "type": "NarrativeText"
    },
    {
        "element_id": "a7e7d9e0cb3579c0f2042002e7cd3deb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1031.8
                    ],
                    [
                        847.6,
                        1104.9
                    ],
                    [
                        1545.0,
                        1104.9
                    ],
                    [
                        1545.0,
                        1031.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8196,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "ucation, CQUT. His current research interests include machine-learning-based control of intelligent and connected ve- hicles and modeling and control of advanced boosted engine systems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "aaef069e290f1fe869d9c874d16d6d32",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1150.8
                    ],
                    [
                        117.0,
                        1223.0
                    ],
                    [
                        819.4,
                        1223.0
                    ],
                    [
                        819.4,
                        1150.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93274,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "[14] C. Wu et al., \u201cUAV autonomous target search based on deep rein- forcement learning in complex disaster scene,\u201d IEEE Access, vol. 7, pp. 117227\u2013117245, Aug. 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "c723e6ef4e2828da43e592b1487ea125",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1225.8
                    ],
                    [
                        117.0,
                        1322.6
                    ],
                    [
                        815.9,
                        1322.6
                    ],
                    [
                        815.9,
                        1225.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93497,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "[15] H. Yang, A. Alphones, W.-D. Zhong, C. Chen, and X. Xie, \u201cLearning- based energy-ef\ufb01cient resource management by heterogeneous RF/VLC for ultra-reliable low-latency industrial IoT networks,\u201d IEEE Trans. Ind. Inform., vol. 16, no. 8, pp. 5565\u20135576, Aug. 2020.",
        "type": "ListItem"
    },
    {
        "element_id": "8333cd470609c9f5069d8771cbd971be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1325.4
                    ],
                    [
                        117.0,
                        1422.6
                    ],
                    [
                        815.7,
                        1422.6
                    ],
                    [
                        815.7,
                        1325.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93617,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "[16] S. Zhang, Z. Sun, C. Li, D. Cabrera, J. Long, and Y. Bai, \u201cDeep hybrid state network with feature reinforcement for intelligent fault diagnosis of delta 3-D printers,\u201d IEEE Trans. Ind. Inform., vol. 16, no. 2, pp. 779\u2013789, Feb. 2020.",
        "type": "ListItem"
    },
    {
        "element_id": "869a68cb19e9ac61e404ee578304a92f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1425.0
                    ],
                    [
                        117.0,
                        1497.0
                    ],
                    [
                        814.4,
                        1497.0
                    ],
                    [
                        814.4,
                        1425.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92681,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "[17] Z. L\u02ddorincz, \u201cA brief overview of imitation learning,\u201d Sep. 19, 2019. [Online]. Available: https://medium.com/@SmartLabAI/a-brief- overview-of-imitation-learning-8a8a75c44a9c",
        "type": "ListItem"
    },
    {
        "element_id": "7ef5eb930e4e1fd302b1d879769c86f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1499.7
                    ],
                    [
                        117.0,
                        1546.8
                    ],
                    [
                        814.4,
                        1546.8
                    ],
                    [
                        814.4,
                        1499.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89869,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "[18] S. Ross and D. Bagnell, \u201cEf\ufb01cient reductions for imitation learning,\u201d in Proc. 13th Int. Conf. Artif. Intell. Statist., 2010, pp. 661\u2013668.",
        "type": "ListItem"
    },
    {
        "element_id": "cc08d71648321d14e92f41ad2986250c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1073.7,
                        1253.1
                    ],
                    [
                        1073.7,
                        1374.9
                    ],
                    [
                        1553.1,
                        1374.9
                    ],
                    [
                        1553.1,
                        1253.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91769,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "Jiaxi Li was born in Xuancheng, Anhui Province, China, in 1997. He received the B.Eng. degree in automotive engineering from the Chongqing University of Technology (CQUT), Chongqing, China, in 2020.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cdde26ca768978bbd0d7fb7e99c71586",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1255.6
                    ],
                    [
                        847.6,
                        1504.5
                    ],
                    [
                        1046.8,
                        1504.5
                    ],
                    [
                        1046.8,
                        1255.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-9-16.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "=",
        "type": "Image"
    },
    {
        "element_id": "5652d86525c88e7f73715b18d2d912cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1078.8,
                        1377.7
                    ],
                    [
                        1078.8,
                        1524.3
                    ],
                    [
                        1554.2,
                        1524.3
                    ],
                    [
                        1554.2,
                        1377.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90468,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "He is currently a Research Assistant with the Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, CQUT. His active research interests include machine learning, autonomous vehicle control, driver behaviors and modeling, control",
        "type": "NarrativeText"
    },
    {
        "element_id": "0d10ab3ba6df7df970cb027805c7adbc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1502.2
                    ],
                    [
                        847.6,
                        1549.5
                    ],
                    [
                        1547.7,
                        1549.5
                    ],
                    [
                        1547.7,
                        1502.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.25209,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "control, driver behaviors and modeling, control topics of battery, optimal control, and multiagent control.",
        "type": "NarrativeText"
    },
    {
        "element_id": "66e1c2707cc76f7cffa2cfabf0b03b26",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.1
                    ],
                    [
                        1530.2,
                        2144.1
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82839,
            "file_directory": "./uol-docs",
            "filename": "Shifting_Deep_Reinforcement_Learning_Algorithm_Toward_Training_Directly_in_Transient_Real-World_Environment_A_Case_Study_in_Powertrain_Control.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T23:27:39",
            "page_number": 9
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 23:20:10 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    }
]