[
    {
        "element_id": "2e684318c3560674de052116efcb19dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.8,
                        92.7
                    ],
                    [
                        104.8,
                        112.0
                    ],
                    [
                        665.4,
                        112.0
                    ],
                    [
                        665.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.848,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "2b7f3515b42c3cf811293d5d055fc8c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        289.1,
                        166.5
                    ],
                    [
                        289.1,
                        233.0
                    ],
                    [
                        1355.9,
                        233.0
                    ],
                    [
                        1355.9,
                        166.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76811,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2e684318c3560674de052116efcb19dc"
        },
        "text": "Deep Learning for Video Game Playing",
        "type": "Title"
    },
    {
        "element_id": "ae31b4058ea64a2ee54a3ab8216aca8c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        324.9,
                        258.4
                    ],
                    [
                        324.9,
                        288.8
                    ],
                    [
                        1315.3,
                        288.8
                    ],
                    [
                        1315.3,
                        258.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7556,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2b7f3515b42c3cf811293d5d055fc8c6"
        },
        "text": "Niels Justesen , Philip Bontrager , Julian Togelius , and Sebastian Risi",
        "type": "NarrativeText"
    },
    {
        "element_id": "78da6f5923c3f18b13008db258567b9d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.2,
                        379.2
                    ],
                    [
                        102.2,
                        597.8
                    ],
                    [
                        805.3,
                        597.8
                    ],
                    [
                        805.3,
                        379.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95482,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2b7f3515b42c3cf811293d5d055fc8c6"
        },
        "text": "Abstract\u2014In this paper, we review recent deep learning advances in the context of how they have been applied to play different types of video games such as \ufb01rst-person shooters, arcade games, and real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.",
        "type": "NarrativeText"
    },
    {
        "element_id": "50d666eca654cf0abdeebe3af4893d91",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        378.4
                    ],
                    [
                        835.9,
                        638.5
                    ],
                    [
                        1535.5,
                        638.5
                    ],
                    [
                        1535.5,
                        378.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95634,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2b7f3515b42c3cf811293d5d055fc8c6"
        },
        "text": "plenty of research on playing games in a believable, entertain- ing, or human-like manner [59]. AI is also used for modeling players\u2019 behavior, experience or preferences [169], or generat- ing game content such as levels, textures, or rules [130]. DL is far from the only AI method used in games. Other prominent methods include Monte Carlo tree search [18] and evolutionary computation [90], [115]. In what follows, it is important to be aware of the limitations of the scope of this paper.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bba94b0a6212791c361a2d28b668a324",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        620.0
                    ],
                    [
                        105.3,
                        672.6
                    ],
                    [
                        802.7,
                        672.6
                    ],
                    [
                        802.7,
                        620.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91204,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2b7f3515b42c3cf811293d5d055fc8c6"
        },
        "text": "Index Terms\u2014Algorithms, learning, machine learning algo- rithms, multilayer neural network, arti\ufb01cial intelligence.",
        "type": "NarrativeText"
    },
    {
        "element_id": "70af90c704132f40631d30ac6b2320d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        350.9,
                        707.1
                    ],
                    [
                        350.9,
                        734.7
                    ],
                    [
                        555.4,
                        734.7
                    ],
                    [
                        555.4,
                        707.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67864,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2e684318c3560674de052116efcb19dc"
        },
        "text": "I. INTRODUCTION",
        "type": "Title"
    },
    {
        "element_id": "012169f1b8d9ea35ed7ff44a5898b88e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.9,
                        750.6
                    ],
                    [
                        102.9,
                        1147.2
                    ],
                    [
                        805.2,
                        1147.2
                    ],
                    [
                        805.2,
                        750.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95393,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "70af90c704132f40631d30ac6b2320d0"
        },
        "text": "A PPLYING arti\ufb01cial intelligence (AI) techniques to games is now an established research \ufb01eld with multiple confer- ences and dedicated journals. In this paper, we review recent advances in deep learning (DL) for video game playing and em- ployed game research platforms while highlighting important open challenges. Our motivation for writing this paper is to re- view the \ufb01eld from the perspective of different types of games, the challenges they pose for DL, and how DL can be used to play these games. A variety of review articles on DL exist [39], [81], [126], as well as surveys on reinforcement learning (RL) [142] and deep RL [87]; here, we focus on these techniques applied to video game playing.",
        "type": "NarrativeText"
    },
    {
        "element_id": "fb667fc3824e3c9ff3d70a4e4cf4e214",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.3,
                        1152.6
                    ],
                    [
                        104.3,
                        1479.2
                    ],
                    [
                        802.9,
                        1479.2
                    ],
                    [
                        802.9,
                        1152.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95619,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "70af90c704132f40631d30ac6b2320d0"
        },
        "text": "In particular, in this paper, we focus on game problems and environments that have been used extensively for DL-based Game AI, such as Atari/ALE, Doom, Minecraft, StarCraft, and car racing. Additionally, we review the existing work and point out important challenges that remain to be solved. We are in- terested in approaches that aim to play a particular video game well (in contrast to board games such as Go, etc.), from pixels or feature vectors, without an existing forward model. Several game genres are analyzed to point out the many and diverse challenges they pose to human and machine players.",
        "type": "NarrativeText"
    },
    {
        "element_id": "82e1a4fc0af6f98082eb05c88e11d19f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.0,
                        1484.7
                    ],
                    [
                        103.0,
                        1613.1
                    ],
                    [
                        802.7,
                        1613.1
                    ],
                    [
                        802.7,
                        1484.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93697,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "70af90c704132f40631d30ac6b2320d0"
        },
        "text": "It is important to note that there are many uses of AI in and for games that are not covered in this paper; Game AI is a large and diverse \ufb01eld [38], [93], [99], [170], [171]. This paper is focused on DL methods for playing video games well, while there is",
        "type": "NarrativeText"
    },
    {
        "element_id": "28b972d0b151c0626addcdd0f974fa97",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.2,
                        1665.9
                    ],
                    [
                        104.2,
                        1787.6
                    ],
                    [
                        802.8,
                        1787.6
                    ],
                    [
                        802.8,
                        1665.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9387,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "70af90c704132f40631d30ac6b2320d0"
        },
        "text": "Manuscript received December 7, 2017; revised April 9, 2018, July 24, 2018, and December 7, 2018; accepted January 7, 2019. Date of publication February 13, 2019; date of current version March 17, 2020. The work of N. Justesen was supported by the Elite Research travel grant from The Danish Ministry for Higher Education and Science. (Corresponding author: Niels Justesen.)",
        "type": "NarrativeText"
    },
    {
        "element_id": "e7f087283395c009a14c88b6339e6497",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        644.1
                    ],
                    [
                        835.9,
                        871.0
                    ],
                    [
                        1536.3,
                        871.0
                    ],
                    [
                        1536.3,
                        644.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9584,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "70af90c704132f40631d30ac6b2320d0"
        },
        "text": "The rest of this paper is structured as follows. Section II gives an overview of different DL methods applied to games, followed by the different research platforms that are currently in use. Section IV reviews the use of DL methods in different video game types. Section V gives a historical overview of the \ufb01eld. We conclude this paper by pointing out important open challenges in Section VI and a conclusion in Section VII.",
        "type": "NarrativeText"
    },
    {
        "element_id": "dfda0014bf83c4ffe5faefe3c68fbaf6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1023.5,
                        923.1
                    ],
                    [
                        1023.5,
                        950.8
                    ],
                    [
                        1346.0,
                        950.8
                    ],
                    [
                        1346.0,
                        923.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82916,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2e684318c3560674de052116efcb19dc"
        },
        "text": "II. DL IN GAMES OVERVIEW",
        "type": "Title"
    },
    {
        "element_id": "3b34d7e8bd7555872bcb17e6cfc998d3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        970.1
                    ],
                    [
                        835.9,
                        1263.5
                    ],
                    [
                        1535.6,
                        1263.5
                    ],
                    [
                        1535.6,
                        970.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95883,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "dfda0014bf83c4ffe5faefe3c68fbaf6"
        },
        "text": "This section gives a brief overview of neural networks and machine learning in the context of games. First, we describe common neural network architectures followed by an overview of the three main categories of machine learning tasks: super- vised learning, unsupervised learning, and RL. Approaches in these categories are typically based on gradient-descent opti- mization. We also highlight evolutionary approaches, as well as a few examples of hybrid approaches that combine several optimization techniques.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c38479274799c79ae350b890678bdb2c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1315.6
                    ],
                    [
                        835.9,
                        1343.3
                    ],
                    [
                        1140.4,
                        1343.3
                    ],
                    [
                        1140.4,
                        1315.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.52043,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "2e684318c3560674de052116efcb19dc"
        },
        "text": "A. Neural Network Models",
        "type": "Title"
    },
    {
        "element_id": "43c9c693aef006378a73d5a9c44cb4f3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1362.7
                    ],
                    [
                        835.9,
                        1788.8
                    ],
                    [
                        1535.7,
                        1788.8
                    ],
                    [
                        1535.7,
                        1362.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95185,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "Arti\ufb01cial neural networks (ANNs) are general-purpose func- tions that are de\ufb01ned by their network structure and the weight of each graph edge. Because of their generality and ability to ap- proximate any continuous real-valued function (given enough parameters), they have been applied to a variety of tasks, in- cluding video game playing. The architectures of these ANNs can roughly be divided into two major categories: feedforward networks and recurrent neural networks (RNNs). Feedforward networks take a single input, for example, a representation of the game state, and outputs probabilities or values for each pos- sible action. Convolutional neural networks (CNNs) consist of trainable \ufb01lters and are suitable for processing image data such as pixels from a video game screen.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8dbaad2eca7f506c6bb48f09781fbf78",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.9,
                        1790.4
                    ],
                    [
                        102.9,
                        1862.3
                    ],
                    [
                        802.8,
                        1862.3
                    ],
                    [
                        802.8,
                        1790.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91344,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "N. Justesen and S. Risi are with the IT University of Copenhagen, 2300 Copenhagen, Denmark (e-mail:, njustesen@gmail.com; sebastian.risi@ gmail.com).",
        "type": "NarrativeText"
    },
    {
        "element_id": "168bd9ebb7311fe66cece3308cf33b9a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1865.1
                    ],
                    [
                        105.3,
                        1912.6
                    ],
                    [
                        804.8,
                        1912.6
                    ],
                    [
                        804.8,
                        1865.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89286,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "P. Bontrager and J. Togelius are with New York University, New York, NY 11201 USA (e-mail:,philipjb@nyu.edu; julian@togelius.com).",
        "type": "NarrativeText"
    },
    {
        "element_id": "05092b20909d49601f7e32c76b1fe801",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1914.9
                    ],
                    [
                        105.3,
                        1962.0
                    ],
                    [
                        802.8,
                        1962.0
                    ],
                    [
                        802.8,
                        1914.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82187,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "Color versions of one or more of the \ufb01gures in this paper are available online at http://ieeexplore.ieee.org.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4879cd285eec1953d851764cd641abb8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        122.6,
                        1964.7
                    ],
                    [
                        122.6,
                        1987.5
                    ],
                    [
                        595.6,
                        1987.5
                    ],
                    [
                        595.6,
                        1964.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66527,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "Digital Object Identi\ufb01er 10.1109/TG.2019.2896986",
        "type": "NarrativeText"
    },
    {
        "element_id": "fcad160a320b78e2c2cb8e5520847ddb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1794.4
                    ],
                    [
                        835.9,
                        1988.1
                    ],
                    [
                        1534.2,
                        1988.1
                    ],
                    [
                        1534.2,
                        1794.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94678,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "RNNs are typically applied to time-series data, in which the output of the network can depend on the network\u2019s activa- tion from previous time steps [82], [165]. The training process is similar to feedforward networks, except that the network\u2019s previous hidden state is fed back into the network together with the next input. This allows the network to become context-aware",
        "type": "NarrativeText"
    },
    {
        "element_id": "f69347b422b7ad92ca7404d426ef4fb9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        323.8,
                        2020.1
                    ],
                    [
                        323.8,
                        2067.8
                    ],
                    [
                        1322.5,
                        2067.8
                    ],
                    [
                        1322.5,
                        2020.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7579,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "2475-1502 \u00a9 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.",
        "type": "NarrativeText"
    },
    {
        "element_id": "50857c3be61ea4a68c91923e1a2227fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.9,
                        2123.7
                    ],
                    [
                        117.9,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67482,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1,
            "parent_id": "c38479274799c79ae350b890678bdb2c"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "290659abcf93027a58f92be10c624de4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1521.3,
                        92.4
                    ],
                    [
                        1521.3,
                        112.9
                    ],
                    [
                        1535.1,
                        112.9
                    ],
                    [
                        1535.1,
                        92.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73101,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 1
        },
        "text": "1",
        "type": "Header"
    },
    {
        "element_id": "dae7cd0446e8346d092079efa1e7203f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.4,
                        92.6
                    ],
                    [
                        115.4,
                        113.3
                    ],
                    [
                        128.2,
                        113.3
                    ],
                    [
                        128.2,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74416,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "2",
        "type": "Header"
    },
    {
        "element_id": "3270a59d4fb1e4a7994ad3d1e2effc9d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        983.7,
                        92.7
                    ],
                    [
                        983.7,
                        112.4
                    ],
                    [
                        1546.4,
                        112.4
                    ],
                    [
                        1546.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77166,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "7b7d82e4f3c39167425005b89d13369a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.8,
                        180.8
                    ],
                    [
                        116.8,
                        341.3
                    ],
                    [
                        818.4,
                        341.3
                    ],
                    [
                        818.4,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94967,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "3270a59d4fb1e4a7994ad3d1e2effc9d"
        },
        "text": "by memorizing the previous activations, which is useful when a single observation from a game does not represent the complete game state. For video game playing, it is common to use a stack of convolutional layers followed by recurrent layers and fully connected feedforward layers.",
        "type": "NarrativeText"
    },
    {
        "element_id": "84f49298aaf8a3c51c731b5ac0ce1e3b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        346.9
                    ],
                    [
                        117.0,
                        573.8
                    ],
                    [
                        819.8,
                        573.8
                    ],
                    [
                        819.8,
                        346.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95374,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "3270a59d4fb1e4a7994ad3d1e2effc9d"
        },
        "text": "The following sections will give a brief overview of differ- ent optimization methods, which are commonly used for learn- ing game-playing behaviors with deep neural networks. These methods search for the optimal set of parameters to solve some problems. Optimization can also be used to \ufb01nd hyperparame- ters, such as network architecture and learning parameters, and is well studied within DL [12], [13].",
        "type": "NarrativeText"
    },
    {
        "element_id": "916cdbc9af07c46d82a33f978e2cbf02",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        894.7,
                        181.1
                    ],
                    [
                        894.7,
                        387.0
                    ],
                    [
                        1492.8,
                        387.0
                    ],
                    [
                        1492.8,
                        181.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89523,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-2-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "reward Environment action > (video game) sampled from x state",
        "type": "Image"
    },
    {
        "element_id": "7ea330ce1c5ad2c22bbcd43d7ad7a0ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        417.3
                    ],
                    [
                        847.6,
                        564.2
                    ],
                    [
                        1545.0,
                        564.2
                    ],
                    [
                        1545.0,
                        417.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94838,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "Fig. 1. RL framework where an agent\u2019s policy \u03c0 is determined by a deep neural network. The state of the environment, or an observation such as screen pixels, is fed as input to the agent\u2019s policy network. An action is sampled from the policy network\u2019s output \u03c0, whereafter it receives a reward and the subsequent game state. The goal is to maximize the cumulated rewards. The RL algorithm updates the policy (network parameters) based on the reward.",
        "type": "FigureCaption"
    },
    {
        "element_id": "97d08d6a9b56a2cad60ee92dfe325684",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        632.1
                    ],
                    [
                        117.0,
                        659.7
                    ],
                    [
                        473.8,
                        659.7
                    ],
                    [
                        473.8,
                        632.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68071,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "B. Optimizing Neural Networks",
        "type": "Title"
    },
    {
        "element_id": "210d07b5b2495756dcc56a486befce92",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.6,
                        679.1
                    ],
                    [
                        115.6,
                        939.2
                    ],
                    [
                        818.7,
                        939.2
                    ],
                    [
                        818.7,
                        679.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95894,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "1) Supervised Learning: In supervised learning, a model is trained from examples. During training, the model is asked to make a decision, for which the correct answer is known. The er- ror, i.e., difference between the provided answer and the ground truth, is used as a loss to update the model. The goal is to achieve a model that can generalize beyond the training data and thus perform well on examples it has never seen before. Large datasets usually improve the model\u2019s ability to generalize.",
        "type": "NarrativeText"
    },
    {
        "element_id": "19f3c4a4c10e18f8ad5309468f8867bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        944.8
                    ],
                    [
                        117.0,
                        1404.2
                    ],
                    [
                        814.4,
                        1404.2
                    ],
                    [
                        814.4,
                        944.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95241,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "In games, these data can come from play traces [16] (i.e., humans playing through the game while being recorded), al- lowing the agent to learn the mapping from the input state to output actions based on what actions the human performed in a given state. If the game is already solved by another algorithm, it can be used to generate training data, which is useful if the \ufb01rst algorithm is too slow to run in real time. While learning to play from existing data allows agents to quickly learn best practices, it is often brittle; the data available can be expensive to produce and may be missing key scenarios the agent should be able to deal with. For gameplay, the algorithm is limited to the strate- gies available in the data and cannot explore new ones itself. Therefore, in games, supervised algorithms are often combined with additional training through RL algorithms [133].",
        "type": "NarrativeText"
    },
    {
        "element_id": "a0993b0b3e80d4441e655796a8730793",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1409.7
                    ],
                    [
                        117.0,
                        1603.4
                    ],
                    [
                        815.4,
                        1603.4
                    ],
                    [
                        815.4,
                        1409.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95734,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "Another application of supervised learning in games is to learn the state transitions of a game. Instead of providing the action for a given state, the neural network can learn to predict the next state for an action\u2013state pair. Thus, the network is essentially learning a model of the game, which can then be used to play the game better or to perform planning [45].",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a78d0a7e1d85381c31efcf6cc785f9e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.6,
                        1609.0
                    ],
                    [
                        116.6,
                        1902.4
                    ],
                    [
                        816.4,
                        1902.4
                    ],
                    [
                        816.4,
                        1609.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9583,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "2) Unsupervised Learning: Instead of learning a mapping between data and its labels, the objective in unsupervised learn- ing is to discover patterns in the data. These algorithms can learn the distribution of features for a dataset, which can be used to cluster similar data, compress data into its essential features, or create new synthetic data that are characteristic of the origi- nal data. For games with sparse rewards (such as Montezuma\u2019s revenge), learning from data in an unsupervised fashion is a potential solution and an important open DL challenge.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f6681744fa7a64ff2f63cc4a81f907e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1907.8
                    ],
                    [
                        117.0,
                        2101.6
                    ],
                    [
                        814.4,
                        2101.6
                    ],
                    [
                        814.4,
                        1907.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94228,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "A prominent unsupervised learning technique in DL is the autoencoder, which is a neural network that attempts to learn the identity function such that the output is identical to the input [80], [117]. The network consists of two parts: an encoder that maps the input x to a low-dimensional hidden vector h, and a decoder that attempts to reconstruct x from h. The main idea is",
        "type": "NarrativeText"
    },
    {
        "element_id": "cf24874e092885057b58ff171ae59d64",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        615.9
                    ],
                    [
                        847.6,
                        843.6
                    ],
                    [
                        1549.7,
                        843.6
                    ],
                    [
                        1549.7,
                        615.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9572,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "that by keeping h small, the network has to learn to compress the data and, therefore, learn a good representation. Researchers are beginning to apply such unsupervised algorithms to games to help distill high-dimensional data to more meaningful lower dimensional data, but this research direction is still in its early stages [45]. For a more detailed overview of supervised and unsupervised learning, see [39] and [126].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b07567f6acc46f47a065cea90eb282e6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        848.7
                    ],
                    [
                        847.6,
                        1075.6
                    ],
                    [
                        1549.7,
                        1075.6
                    ],
                    [
                        1549.7,
                        848.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95628,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "3) RL Approaches: In RL, an agent learns a behavior by interacting with an environment that provides a reward signal back to the agent. A video game can easily be modeled as an environment in an RL setting, wherein players are modeled as agents with a \ufb01nite set of actions that can be taken at each step, and the reward signal can be determined by the game score. Fig. 1 depicts the RL framework.",
        "type": "NarrativeText"
    },
    {
        "element_id": "77762084b0062d49ac4ed61d6632b15e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1081.1
                    ],
                    [
                        847.6,
                        1308.1
                    ],
                    [
                        1549.3,
                        1308.1
                    ],
                    [
                        1549.3,
                        1081.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95267,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "In RL, the agent relies on the reward signal. These signals can occur frequently, such as the change in score within a game, or it can occur infrequently, such as whether an agent has won or lost a game. Video games and RL go well together since most games give rewards for successful strategies. Open-world games do not always have a clear reward model and are thus challenging for RL algorithms.",
        "type": "NarrativeText"
    },
    {
        "element_id": "834da60995270f5428f95373672b2e4f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1313.6
                    ],
                    [
                        847.6,
                        1739.8
                    ],
                    [
                        1550.4,
                        1739.8
                    ],
                    [
                        1550.4,
                        1313.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95163,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2,
            "parent_id": "97d08d6a9b56a2cad60ee92dfe325684"
        },
        "text": "A key challenge in applying RL to games with sparse rewards is to determine how to assign credit to the many previous actions when a reward signal is obtained. The reward R(s) for state s needs to be propagated back to the actions that lead to the re- ward. Historically, there are several different ways this problem is approached, which are described in the following. If an en- vironment can be described as a Markov decision process, then the agent can build a probability tree of future states and their rewards. The probability tree can then be used to calculate the utility of the current state. For an RL agent, this means learn- ing the model P(s'|s,a), where P is the probability of state s\u2019 given state s and action a. With a model P, utilities can be calculated by",
        "type": "NarrativeText"
    },
    {
        "element_id": "216b41a6cb728dfc427a5653100b9cbe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        949.0,
                        1757.9
                    ],
                    [
                        949.0,
                        1825.8
                    ],
                    [
                        1427.6,
                        1825.8
                    ],
                    [
                        1427.6,
                        1757.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8446,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "U(s) = R(s) + ymax >> P(s'|s,a)U(s\u2019)",
        "type": "Formula"
    },
    {
        "element_id": "882ca03bcfa6dccf365aff8d0b1e1576",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1841.1
                    ],
                    [
                        847.6,
                        2101.5
                    ],
                    [
                        1546.0,
                        2101.5
                    ],
                    [
                        1546.0,
                        1841.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95164,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "where \u03b3 is the discount factor for the utility of future states. This algorithm, known as adaptive dynamic programming, can con- verge rather quickly as it directly handles the credit assignment problem [142]. The issue is that it has to build a probability tree over the whole problem space and is, therefore, intractable for large problems. As the games covered in this work are consid- ered \u201clarge problems,\u201d we will not go into further detail on this algorithm.",
        "type": "NarrativeText"
    },
    {
        "element_id": "538a6bbde7db0e5df5b4accf80be29f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.3
                    ],
                    [
                        119.8,
                        2144.8
                    ],
                    [
                        1530.2,
                        2144.8
                    ],
                    [
                        1530.2,
                        2123.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81628,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 2
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2e0c049941b7fa16c0d007ada65b66c0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.8,
                        92.6
                    ],
                    [
                        104.8,
                        112.3
                    ],
                    [
                        664.5,
                        112.3
                    ],
                    [
                        664.5,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.52059,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "NarrativeText"
    },
    {
        "element_id": "93620b41ce01c83aab0c547285ee7ef0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        180.8
                    ],
                    [
                        105.3,
                        374.5
                    ],
                    [
                        802.7,
                        374.5
                    ],
                    [
                        802.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95554,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Another approach to this problem is temporal difference (TD) learning. In TD learning, the agent learns the utilities U directly based off of the observation that the current utility is equal to the current reward plus the utility value of the next state [142]. Instead of learning the state transition model P , it learns to model the utility U for every state. The update equation for U is",
        "type": "NarrativeText"
    },
    {
        "element_id": "b6c2dbe8e3825d9f435a49417a5c6f02",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        204.9,
                        386.7
                    ],
                    [
                        204.9,
                        423.3
                    ],
                    [
                        686.5,
                        423.3
                    ],
                    [
                        686.5,
                        386.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "U(s) = U(s) +a(R(s) + W(s') \u2014 U(s))",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d7b989dd03b3b5806ed8ed9aa6b14565",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        444.6
                    ],
                    [
                        105.3,
                        705.0
                    ],
                    [
                        802.7,
                        705.0
                    ],
                    [
                        802.7,
                        444.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95473,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "where a is the learning rate of the algorithm. The above equa- tion does not take into account how s\u2019 was chosen. If a reward is found at s,, it will only affect U(s,). The next time the agent is at 1-1, then U(s;_1) will be aware of the future reward. This will propagate backward over time. Likewise, less common transi- tions will have less of an impact on utility values. Therefore, U will converge to the same values as are obtained from ADP, albeit slower.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2124aa20b018e645e9b2b56fb96fc35c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        710.6
                    ],
                    [
                        105.3,
                        970.7
                    ],
                    [
                        803.9,
                        970.7
                    ],
                    [
                        803.9,
                        710.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95515,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "There are alternative implementations of TD that learn re- wards for state\u2013action pairs. This allows an agent to choose an action, given the state, with no model of how to transition to future states. For this reason, these approaches are referred to as model-free methods. A popular model-free RL method is Q-learning [162], where the utility of a state is equal to the maximum Q-value for a state. The update equation for Q-learning is",
        "type": "NarrativeText"
    },
    {
        "element_id": "1928d3090df879d7eab61e4e63c82567",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        308.1
                    ],
                    [
                        1533.3,
                        308.1
                    ],
                    [
                        1533.3,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93637,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "is taken, and it is updated to increase the likelihood that the more successful actions are returned in the future. This lends itself well to neural networks, as \u03c0 can be a neural network and \u03b8 the network weights.",
        "type": "NarrativeText"
    },
    {
        "element_id": "669eadc8bcc2a962cfdbb2d612ef998b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        313.7
                    ],
                    [
                        835.9,
                        607.0
                    ],
                    [
                        1536.8,
                        607.0
                    ],
                    [
                        1536.8,
                        313.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9552,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Actor-critic methods combine the policy gradient approach with TD learning, where an actor learns a policy \u03c0\u03b8 (s, a) using the policy gradient algorithm, and the critic learns to approxi- mate R using TD learning [143]. Together, they are an effective approach to iteratively learning a policy. In actor-critic meth- ods, there can either be a single network to predict both \u03c0 and R or two separate networks. For an overview of RL applied to deep neural networks, we suggest the article by Arulkumaran et al. [2].",
        "type": "NarrativeText"
    },
    {
        "element_id": "fca94fcff39a6366cd1782c1b175c11e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        612.6
                    ],
                    [
                        835.9,
                        972.7
                    ],
                    [
                        1538.1,
                        972.7
                    ],
                    [
                        1538.1,
                        612.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95499,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "4) Evolutionary Approaches: The optimization techniques discussed so far rely on gradient descent, based on differenti- ation of a de\ufb01ned error. However, derivative-free optimization methods such as evolutionary algorithms have also been widely used to train neural networks, including, but not limited to, RL tasks. This approach, often referred to as neuroevolution (NE), can optimize a network\u2019s weights, as well as their topol- ogy/architecture. Because of their generality, NE approaches have been applied extensively to different types of video games. For a complete overview of this \ufb01eld, we refer the interested reader to our NE survey paper [115].",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca0ae788a4e916b02a4f1aa717626d59",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        113.2,
                        982.9
                    ],
                    [
                        113.2,
                        1039.2
                    ],
                    [
                        778.1,
                        1039.2
                    ],
                    [
                        778.1,
                        982.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7878,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Olsa) = Olsa) + a(R(s) + ymax Q(s',a\u2019) \u2014 Q(s,4)).",
        "type": "Formula"
    },
    {
        "element_id": "a26c6218d4e13bb309a48a64bd6dc870",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1049.9
                    ],
                    [
                        105.3,
                        1310.1
                    ],
                    [
                        802.7,
                        1310.1
                    ],
                    [
                        802.7,
                        1049.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95486,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "In Q-learning, the future reward is accounted for by selecting the best-known future state\u2013action pair. In a similar algorithm called State\u2013Action\u2013Reward\u2013State\u2013Action (SARSA), Q(s, a) is updated only when the next a has been selected and the next s is known [118]. This action pair is used instead of the maximum Q-value. This makes SARSA an on-policy method in contrast to Q-learning, which is off-policy, because SARSA\u2019s Q-value accounts for the agent\u2019s own policy.",
        "type": "NarrativeText"
    },
    {
        "element_id": "39a8de71849a065224b0d05aafa956de",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1315.6
                    ],
                    [
                        105.3,
                        1476.1
                    ],
                    [
                        803.9,
                        1476.1
                    ],
                    [
                        803.9,
                        1315.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95152,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Q-learning and SARSA can use a neural network as a function approximator for the Q-function. The given Q-update equation can be used to provide the new \u201cexpected\u201d Q-value for a state\u2013 action pair. The network can then be updated as it is in supervised learning.",
        "type": "NarrativeText"
    },
    {
        "element_id": "49f2287f12690805dce2dc04db86fc41",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1475.6
                    ],
                    [
                        105.3,
                        1808.2
                    ],
                    [
                        804.0,
                        1808.2
                    ],
                    [
                        804.0,
                        1475.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95572,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "An agent\u2019s policy 7(s) determines which action to take given astate s. For Q-learning, a simple policy would be to always take the action with the highest Q-value. Yet, early on in training, Q- values are not very accurate, and an agent could get stuck always exploiting a small reward. A learning agent should prioritize exploration of new actions, as well as the exploitation of what it has learned. This problem is known as a multiarmed bandit problem and has been well explored. The e-greedy strategy is a simple approach that selects the (estimated) optimal action with \u20ac probability and otherwise selects a random action.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5b6773a0c872ac3a4b7200ae75ff304e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1813.7
                    ],
                    [
                        105.3,
                        2040.7
                    ],
                    [
                        802.8,
                        2040.7
                    ],
                    [
                        802.8,
                        1813.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95369,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "One approach to RL is to perform gradient descent in the policy\u2019s parameter space. Let 74(s,a) be the probability that action a is taken at state s given parameters 0. The basic policy gradient algorithm from the REINFORCE family of algorithms [164] updates 0 using the gradient Vg >>, 74(s, a) R(s), where R(s) is the discounted cumulative reward obtained from s and forward. In practice, a sample of possible actions from the policy",
        "type": "NarrativeText"
    },
    {
        "element_id": "9ef6e94b47b4148bb0aac35bcdfe7a23",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        977.9
                    ],
                    [
                        835.9,
                        1537.4
                    ],
                    [
                        1535.2,
                        1537.4
                    ],
                    [
                        1535.2,
                        977.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94857,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Compared to gradient-descent-based training methods, NE approaches have the bene\ufb01t of not requiring the network to be differentiable and can be applied to supervised learning, unsu- pervised learning, and RL problems. The ability to evolve the topology, as well as the weights, potentially offers a way of au- tomating the development of neural network architecture, which currently requires considerable domain knowledge. The promise of these techniques is that evolution could \ufb01nd a neural network topology that is better at playing a certain game than existing human-designed architectures. While NE has been tradition- ally applied to problems with lower input dimensionality than typical DL approaches, recently, Salimans et al. [121] showed that evolution strategies (ESs), which rely on parameter explo- ration through stochastic noise instead of calculating gradients, can achieve results competitive to current deep RL approaches for Atari video game playing, given enough computational re- sources.",
        "type": "NarrativeText"
    },
    {
        "element_id": "fa20771623f0b70cdffcf4f20ca3ab09",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1542.4
                    ],
                    [
                        835.9,
                        2035.3
                    ],
                    [
                        1537.7,
                        2035.3
                    ],
                    [
                        1537.7,
                        1542.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95201,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "5) Hybrid Learning Approaches: More recently, researchers have started to investigate hybrid approaches for video game playing, which combine DL methods with other machine learning approaches. Alvernaz and Togelius [1] and Poulsen et al. [113] experimented with combining a deep network trained through gradient descent feeding a condensed feature represen- tation into a network trained through arti\ufb01cial evolution. These hybrids aim to combine the best of both approaches as DL methods are able to learn directly from high-dimensional input, while evolutionary methods do not rely on differentiable archi- tectures and work well in games with sparse rewards. Some results suggest that gradient-free methods seem to be better in the early stages of training to avoid premature convergence, while gradient-based methods may be better in the end when less exploration is needed [139].",
        "type": "NarrativeText"
    },
    {
        "element_id": "ef560c340ec51dd79d31882098db8b1f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.8,
                        2123.7
                    ],
                    [
                        115.8,
                        2144.5
                    ],
                    [
                        1530.2,
                        2144.5
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69157,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3dd13bc30876007f2e395d6649eb0f77",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1522.0,
                        92.7
                    ],
                    [
                        1522.0,
                        113.0
                    ],
                    [
                        1535.3,
                        113.0
                    ],
                    [
                        1535.3,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74935,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 3
        },
        "text": "3",
        "type": "Header"
    },
    {
        "element_id": "23ef898f1755d69d512bcd12014fe82d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.8,
                        92.4
                    ],
                    [
                        115.8,
                        112.6
                    ],
                    [
                        128.4,
                        112.6
                    ],
                    [
                        128.4,
                        92.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73241,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4
        },
        "text": "4",
        "type": "Header"
    },
    {
        "element_id": "9026d1dcaa2dbc70874589b058cd63aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        984.8,
                        92.7
                    ],
                    [
                        984.8,
                        112.3
                    ],
                    [
                        1546.4,
                        112.3
                    ],
                    [
                        1546.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7405,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "b37f4cbdd255c9cbeba8e812149ca37a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        180.8
                    ],
                    [
                        117.0,
                        308.1
                    ],
                    [
                        814.4,
                        308.1
                    ],
                    [
                        814.4,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93705,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "Another hybrid method for board game playing was AlphaGo [133] that relied on deep neural networks and tree search meth- ods to defeat the world champion in Go, and [36] that applies planning on top of a predictive model.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d14ad9b8d1437a785b08d5c59b62fb97",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        313.6
                    ],
                    [
                        117.0,
                        440.9
                    ],
                    [
                        815.2,
                        440.9
                    ],
                    [
                        815.2,
                        313.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93924,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "In general, the hybridization of ontogenetic RL (such as Q- learning) with phylogenetic methods (such as evolutionary algo- rithms) has the potential to be very impactful, as it could enable concurrent learning on different timescales [153].",
        "type": "NarrativeText"
    },
    {
        "element_id": "233e6b8121db0ad0c1a3bb97acad0f6e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        180.8
                    ],
                    [
                        847.6,
                        474.2
                    ],
                    [
                        1549.7,
                        474.2
                    ],
                    [
                        1549.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95483,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "shooters such as Missile Command (Atari Inc., 1980). Another common requirement is navigating mazes or other complex en- vironments, as exempli\ufb01ed clearly by games such as Pac-Man (Namco, 1980) and Boulder Dash (First Star Software, 1984). Some games, such as Montezuma\u2019s Revenge (Parker Brothers, 1984), require long-term planning involving the memorization of temporarily unobservable game states. Some games feature incomplete information and stochasticity; others are completely deterministic and fully observable.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9236226cd32be9495677489068230e10",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        196.7,
                        482.5
                    ],
                    [
                        196.7,
                        510.2
                    ],
                    [
                        736.1,
                        510.2
                    ],
                    [
                        736.1,
                        482.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76559,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "III. GAME GENRES AND RESEARCH PLATFORMS",
        "type": "Title"
    },
    {
        "element_id": "89b6015c40eeb6cd224467c8b5b1fa00",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        529.6
                    ],
                    [
                        117.0,
                        789.7
                    ],
                    [
                        814.5,
                        789.7
                    ],
                    [
                        814.5,
                        529.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95582,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9236226cd32be9495677489068230e10"
        },
        "text": "The fast progression of DL methods is undoubtedly due to the convention of comparing results on publicly available datasets. A similar convention in game AI is to use game environments to compare game playing algorithms, in which methods are ranked based on their ability to score points or win in games. Conferences like the IEEE Conference on Computational Intelligence and Games run popular competitions in a variety of game environments.",
        "type": "NarrativeText"
    },
    {
        "element_id": "611ccecd0456b5ce114b596eac760d61",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.0,
                        795.3
                    ],
                    [
                        116.0,
                        1287.9
                    ],
                    [
                        817.8,
                        1287.9
                    ],
                    [
                        817.8,
                        795.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95183,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9236226cd32be9495677489068230e10"
        },
        "text": "This section describes popular game genres and research plat- forms, used in the literature, that are relevant to DL; some ex- amples are shown in Fig. 2. For each genre, we brie\ufb02y outline what characterizes that genre and describe the challenges faced by algorithms playing games of the genre. The video games that are discussed in this paper have to a large extent supplanted an earlier generation of simpler control problems that long served as the main RL benchmarks but are generally too simple for modern RL methods. In such classic control problems, the in- put is a simple feature vector, describing the position, velocity, and angles. Popular platforms for such problems are rllab [29], which includes classic problems such as pole balancing and the mountain car problem, and MuJoCo (multijoint dynamics with contact), a physics engine for complex control tasks such as the humanoid walking task [152].",
        "type": "NarrativeText"
    },
    {
        "element_id": "2953c1f96f9ff9be4c14a09bfc6d8809",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        479.7
                    ],
                    [
                        847.6,
                        872.7
                    ],
                    [
                        1550.2,
                        872.7
                    ],
                    [
                        1550.2,
                        479.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95463,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9236226cd32be9495677489068230e10"
        },
        "text": "The most notable game platform used for DL methods is the Arcade Learning Environment (ALE) [10]. ALE is built on top of the Atari 2600 emulator Stella and contains more than 50 original Atari 2600 games. The framework extracts the game score, 160 \u00d7 210 screen pixels, and the RAM content that can be used as input for game playing agents. ALE was the main environment explored in the \ufb01rst deep RL papers that used raw pixels as input. By enabling agents to learn from visual input, ALE thus differs from classic control problems in the RL literature, such as the Cart Pole and Mountain Car problems. An overview and discussion of the ALE environment can be found in [91].",
        "type": "NarrativeText"
    },
    {
        "element_id": "df374ab7b0e58fd409c0ecb2033b164c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        878.2
                    ],
                    [
                        847.6,
                        1105.2
                    ],
                    [
                        1548.5,
                        1105.2
                    ],
                    [
                        1548.5,
                        878.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9542,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9236226cd32be9495677489068230e10"
        },
        "text": "Another platform for classic arcade games is the Retro Learn- ing Environment (RLE) that currently contains seven games released for the Super Nintendo Entertainment System (SNES) [15]. Many of these games have 3-D graphics, and the controller allows for over 720 action combinations. SNES games are thus more complex and realistic than Atari 2600 games, but RLE has not been as popular as ALE.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b05c60c118a4934fd2c1e9aa0abf7f49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1110.7
                    ],
                    [
                        847.6,
                        1304.4
                    ],
                    [
                        1548.4,
                        1304.4
                    ],
                    [
                        1548.4,
                        1110.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95119,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9236226cd32be9495677489068230e10"
        },
        "text": "The General Video Game AI (GVG-AI) framework [116] allows for easy creation and modi\ufb01cation of games and levels using the video game description language [122]. This is ideal for testing the generality of agents on multiple games or levels. GVG-AI includes over 100 classic arcade games each with \ufb01ve different levels.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a842f54159eb0bb538ab45f1f71a5e87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        114.7,
                        1329.4
                    ],
                    [
                        114.7,
                        1357.1
                    ],
                    [
                        320.9,
                        1357.1
                    ],
                    [
                        320.9,
                        1329.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73876,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "A. Arcade Games",
        "type": "Title"
    },
    {
        "element_id": "12d4aeb6396cb854d5457ff0a721cacb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.6,
                        1376.5
                    ],
                    [
                        116.6,
                        2035.1
                    ],
                    [
                        816.3,
                        2035.1
                    ],
                    [
                        816.3,
                        1376.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.949,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "a842f54159eb0bb538ab45f1f71a5e87"
        },
        "text": "Classic arcade games, of the type found in the late seventies\u2019 and early eighties\u2019 arcade cabinets, home video game consoles, and home computers, have been commonly used as AI bench- marks within the last decade. Representative platforms for this game type are the Atari 2600, Nintendo NES, Commodore 64, and ZX Spectrum. Most classic arcade games are characterized by movement in a 2-D space (sometimes represented isomet- rically to provide the illusion of 3-D movement), heavy use of graphical logics (where game rules are triggered by the inter- section of sprites or images), continuous-time progression, and either continuous-space or discrete-space movement. The chal- lenges of playing such games vary by game. Most games require fast reactions and precise timing, and a few games, in particu- lar, early sports games such as Track & Field (Konami, 1983) rely almost exclusively on speed and reactions. Many games require prioritization of several co-occurring events, which re- quires some ability to predict the behavior or trajectory of other entities in the game. This challenge is explicit in, e.g., Tapper (Bally Midway, 1983) but also in different ways part of plat- form games such as Super Mario Bros. (Nintendo, 1985) and",
        "type": "NarrativeText"
    },
    {
        "element_id": "42e9b7c3f165a7d8ad80a02b89ded3c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1362.7
                    ],
                    [
                        847.6,
                        1390.4
                    ],
                    [
                        1049.7,
                        1390.4
                    ],
                    [
                        1049.7,
                        1362.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59613,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "9026d1dcaa2dbc70874589b058cd63aa"
        },
        "text": "B. Racing Games",
        "type": "Title"
    },
    {
        "element_id": "412f61cb43f47074262db0adaa0f4762",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1409.7
                    ],
                    [
                        847.6,
                        1869.9
                    ],
                    [
                        1550.9,
                        1869.9
                    ],
                    [
                        1550.9,
                        1409.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95183,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "42e9b7c3f165a7d8ad80a02b89ded3c5"
        },
        "text": "Racing games are games where the player is tasked with con- trolling some kind of vehicle or character so as to reach a goal in the shortest possible time or as to traverse as far as possi- ble along a track in a given time. Usually, the game employs a \ufb01rst-person perspective or a vantage point from just behind the player-controlled vehicle. The vast majority of racing games take a continuous input signal as steering input, similar to a steering wheel. Some games, such as those in the Forza Motor- sport (Microsoft Studios, 2005\u20132016) or Real Racing (Firemint and EA Games, 2009\u20132013) series, allow for complex input in- cluding gear stick, clutch, and handbrake, whereas more arcade- focused games such as those in the Need for Speed (Electronic Arts, 1994\u20132015) series typically have a simpler set of inputs and thus lower branching factor.",
        "type": "NarrativeText"
    },
    {
        "element_id": "994d8bafbf0c19026d3f04367f772ee6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1874.6
                    ],
                    [
                        847.6,
                        2035.2
                    ],
                    [
                        1545.0,
                        2035.2
                    ],
                    [
                        1545.0,
                        1874.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94707,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "42e9b7c3f165a7d8ad80a02b89ded3c5"
        },
        "text": "A challenge that is common in all racing games is that the agent needs to control the position of the vehicle and adjust the acceleration or braking, using \ufb01ne-tuned continuous input, so as to traverse the track as fast as possible. Doing this optimally requires at least short-term planning, one or two turns forward.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3c8e504d68571ff4e668f3c11ce66f2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.3,
                        2123.7
                    ],
                    [
                        119.3,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71465,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 4,
            "parent_id": "42e9b7c3f165a7d8ad80a02b89ded3c5"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "542d7de7525824e42ae35e4ccc347ace",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.8,
                        92.7
                    ],
                    [
                        104.8,
                        112.0
                    ],
                    [
                        663.7,
                        112.0
                    ],
                    [
                        663.7,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81392,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "fbc448eb335e4cc9bbe9130097f985e8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.9,
                        183.6
                    ],
                    [
                        116.9,
                        413.5
                    ],
                    [
                        1511.3,
                        413.5
                    ],
                    [
                        1511.3,
                        183.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92869,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-5-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "a ALE Project Malmo StarCraft: Brood Wa (Minecraft) aran Seo Mar GVG-AI N VizD TORCS (Breakout) (Zelda) vwoom",
        "type": "Image"
    },
    {
        "element_id": "2cd31d2fdedfd779307aa425f3fc773d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        95.2,
                        442.5
                    ],
                    [
                        95.2,
                        464.6
                    ],
                    [
                        1020.4,
                        464.6
                    ],
                    [
                        1020.4,
                        442.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87322,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "Screenshots of selected games and frameworks used as research platforms for research in DL.",
        "type": "FigureCaption"
    },
    {
        "element_id": "73c70b7cbadaac6b0a0c798797b2de4f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        524.5
                    ],
                    [
                        105.3,
                        751.4
                    ],
                    [
                        806.1,
                        751.4
                    ],
                    [
                        806.1,
                        524.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95166,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "If there are resources to be managed in the game, such as fuel, damage, or speed boosts, this requires longer term planning. When other vehicles are present on the track, there is an ad- versarial planning aspect added, in trying to manage or block overtaking; this planning is often done in the presence of hid- den information (position and resources of other vehicles on different parts of the track).",
        "type": "NarrativeText"
    },
    {
        "element_id": "8808e38ec74b9306ab8fad062370bcbc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        757.0
                    ],
                    [
                        105.3,
                        817.8
                    ],
                    [
                        806.5,
                        817.8
                    ],
                    [
                        806.5,
                        757.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92815,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "A popular environment for visual RL with realistic 3-D graph- ics is the open racing car simulator TORCS [168].",
        "type": "NarrativeText"
    },
    {
        "element_id": "4b69faf143699fdb1b6344b3ecc1aaa2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        886.9
                    ],
                    [
                        105.3,
                        914.5
                    ],
                    [
                        472.9,
                        914.5
                    ],
                    [
                        472.9,
                        886.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67792,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "C. First-Person Shooters (FPSs)",
        "type": "Title"
    },
    {
        "element_id": "c995169b9c56abe1cc000dad8cc97686",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.3,
                        933.9
                    ],
                    [
                        103.3,
                        1426.5
                    ],
                    [
                        806.1,
                        1426.5
                    ],
                    [
                        806.1,
                        933.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95099,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "4b69faf143699fdb1b6344b3ecc1aaa2"
        },
        "text": "More advanced game environments have recently emerged for visual RL agents in an FPS. In contrast to classic arcade games such as those in the ALE benchmark, FPSs have 3-D graphics with partially observable states and are thus a more re- alistic environment to study. Usually, the viewpoint is that of the player-controlled character, though some games that are broadly in the FPS categories adopt an over-the-shoulder viewpoint. The design of FPS games is such that part of the challenge is simply fast perception and reaction, in particular, spotting enemies and quickly aiming at them. But there are other cognitive challenges as well, including orientation and movement in a complex 3-D environment, predicting actions and locations of multiple adver- saries, and in some game modes also team-based collaboration. If visual inputs are used, there is the challenge of extracting relevant information from pixels.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7756934250677da9c0f6b47495953767",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1432.0
                    ],
                    [
                        105.3,
                        1592.5
                    ],
                    [
                        806.3,
                        1592.5
                    ],
                    [
                        806.3,
                        1432.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94961,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "4b69faf143699fdb1b6344b3ecc1aaa2"
        },
        "text": "Among FPS platforms are ViZDoom, a framework that allows agents to play the classic FPS Doom (id Software, 1993\u20132017) using the screen buffer as input [73]. DeepMind Lab is a platform for 3-D navigation and puzzle-solving tasks based on the Quake III Arena (id Software, 1999) engine [6].",
        "type": "NarrativeText"
    },
    {
        "element_id": "d269a036ec604b5708aebf276744c5f9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1661.5
                    ],
                    [
                        105.3,
                        1689.2
                    ],
                    [
                        366.9,
                        1689.2
                    ],
                    [
                        366.9,
                        1661.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "D. Open-World Games",
        "type": "Title"
    },
    {
        "element_id": "184ac77ad213f6058ca7bcc068b9809d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        524.5
                    ],
                    [
                        835.9,
                        552.2
                    ],
                    [
                        1243.5,
                        552.2
                    ],
                    [
                        1243.5,
                        524.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.57814,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "E. Real-Time Strategy (RTS) Games",
        "type": "NarrativeText"
    },
    {
        "element_id": "51fe1484d012f13f9dbae40fb879f45a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        571.6
                    ],
                    [
                        835.9,
                        1164.2
                    ],
                    [
                        1536.1,
                        1164.2
                    ],
                    [
                        1536.1,
                        571.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94986,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "Strategy games are games where the player controls multiple characters or units, and the objective of the game is to prevail in some sort of conquest or con\ufb02ict. Usually, but not always, the narrative and graphics re\ufb02ect a military con\ufb02ict, where units may be, e.g., knights, tanks, or battleships. The key challenge in strategy games is to lay out and execute complex plans involving multiple units. This challenge is in general signi\ufb01cantly harder than the planning challenge in classic board games such as Chess mainly because multiple units must be moved at any time and the effective branching factor is typically enormous. The planning horizon can be extremely long, where actions that are taken at the beginning of a game impact the overall strategy. In addition, there is the challenge of predicting the moves of one or several adversaries, who have multiple units themselves. RTS games are strategy games, which do not progress in discrete turns, but where actions can be taken at any point in time. RTS games add the challenge of time prioritization to the already substantial challenges of playing strategy games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2fc825eda97644982acf3b04ff3ae77d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1169.3
                    ],
                    [
                        835.9,
                        1728.8
                    ],
                    [
                        1536.0,
                        1728.8
                    ],
                    [
                        1536.0,
                        1169.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9508,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "The StarCraft (Blizzard Entertainment, 1998\u20132017) series is without a doubt the most studied game in the RTS genre. The Brood War API (BWAPI)1 enables software to communicate with StarCraft while the game runs, e.g., to extract state fea- tures and perform actions. BWAPI has been used extensively in game AI research, but, currently, only a few examples exist where DL has been applied. TorchCraft is a library built on top of BWAPI that connects the scienti\ufb01c computing framework Torch to StarCraft to enable machine learning research for this game [145]. Additionally, DeepMind and Blizzard (the devel- opers of StarCraft) have developed a machine learning API to support research in StarCraft II with features such as simpli\ufb01ed visuals designed for convolutional networks [157]. This API contains several mini-challenges, while it also supports the full 1v1 game setting. \u03bcRTS [104] and ELF [151] are two minimal- istic RTS game engines that implement some of the features that are present in RTS games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca7d223ac8df85f94aa757ef0b3d6fda",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1708.6
                    ],
                    [
                        105.3,
                        2101.6
                    ],
                    [
                        805.6,
                        2101.6
                    ],
                    [
                        805.6,
                        1708.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95577,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "Open-world games such as Minecraft (Mojang, 2011) or the Grand Theft Auto (Rockstar Games, 1997\u20132013) series are char- acterized by very nonlinear gameplay, with a large game world to explore, either no set goals or many goals with unclear inter- nal ordering, and large freedom of action at any given time. Key challenges for agents are exploring the world and setting goals, which are realistic and meaningful. As this is a very complex challenge, most research uses these open environments to ex- plore RL methods that can reuse and transfer learned knowledge to new tasks. Project Malmo is a platform built on top of the open-world game Minecraft, which can be used to de\ufb01ne many diverse and complex problems [65].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b50504849729a2fb2f5ee31b7aaf6357",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1770.4
                    ],
                    [
                        835.9,
                        1798.7
                    ],
                    [
                        1091.8,
                        1798.7
                    ],
                    [
                        1091.8,
                        1770.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69599,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "F. Team Sports Games",
        "type": "NarrativeText"
    },
    {
        "element_id": "f8a6935ac59e17419d41176909633f03",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1818.0
                    ],
                    [
                        835.9,
                        2011.7
                    ],
                    [
                        1534.5,
                        2011.7
                    ],
                    [
                        1534.5,
                        1818.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9543,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "Popular sports games are typically based on team-based sports such as soccer, basketball, and football. These games aim to be as realistic as possible with life-like animations and 3-D graphics. Several soccer-like environments have been used extensively as research platforms, both with physical robots and 2-D/3-D simulations, in the annual Robot World Cup Soccer Games",
        "type": "NarrativeText"
    },
    {
        "element_id": "cd03b6372afed238da5113be4d834294",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.0,
                        2074.5
                    ],
                    [
                        858.0,
                        2100.6
                    ],
                    [
                        1067.7,
                        2100.6
                    ],
                    [
                        1067.7,
                        2074.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5311,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "1http://bwapi.github.io/",
        "type": "NarrativeText"
    },
    {
        "element_id": "3d9c9217a0ff4115a25580f6337cd4be",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.3,
                        2123.3
                    ],
                    [
                        115.3,
                        2144.8
                    ],
                    [
                        1530.2,
                        2144.8
                    ],
                    [
                        1530.2,
                        2123.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78505,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5,
            "parent_id": "d269a036ec604b5708aebf276744c5f9"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3be2f4cc8cfca9f22c5ac96d420fb62e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1522.1,
                        92.7
                    ],
                    [
                        1522.1,
                        112.9
                    ],
                    [
                        1534.6,
                        112.9
                    ],
                    [
                        1534.6,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7237,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 5
        },
        "text": "5",
        "type": "Header"
    },
    {
        "element_id": "9fc8bf6b51f7f134c205298e0bb8e296",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.6,
                        92.7
                    ],
                    [
                        115.6,
                        112.5
                    ],
                    [
                        128.1,
                        112.5
                    ],
                    [
                        128.1,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72444,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6
        },
        "text": "6",
        "type": "Header"
    },
    {
        "element_id": "edea4c265999abe542759d5449b9fe52",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        982.5,
                        92.7
                    ],
                    [
                        982.5,
                        112.3
                    ],
                    [
                        1546.1,
                        112.3
                    ],
                    [
                        1546.1,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77594,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "a1b5da2db02b09fef01b97dd4ead99ad",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        180.8
                    ],
                    [
                        117.0,
                        407.7
                    ],
                    [
                        816.5,
                        407.7
                    ],
                    [
                        816.5,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95568,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "(RoboCup) [3]. Keepaway Soccer is a simplistic soccer-like environment, where one team of agents try to maintain control of the ball while another team tries to gain control of it [138]. A similar environment for multiagent learning is RoboCup 2-D Half-Field-Offense (HFO), where teams of two to three players either take the role as offense or defense on one half of a soccer \ufb01eld [50].",
        "type": "NarrativeText"
    },
    {
        "element_id": "2cfd2ef686329596bef2c3814d058142",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        446.9
                    ],
                    [
                        117.0,
                        474.6
                    ],
                    [
                        407.1,
                        474.6
                    ],
                    [
                        407.1,
                        446.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74788,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "G. Text Adventure Games",
        "type": "Title"
    },
    {
        "element_id": "b271f0c9d858e75d5c6f5a669acfc431",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        494.0
                    ],
                    [
                        117.0,
                        720.9
                    ],
                    [
                        816.1,
                        720.9
                    ],
                    [
                        816.1,
                        494.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95584,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "2cfd2ef686329596bef2c3814d058142"
        },
        "text": "A classic text adventure game is a form of interactive \ufb01ction, where players are given descriptions and instructions in text, rather than graphics, and interact with the storyline through text-based commands [144]. These commands are usually used to query the system about the state, interact with characters in the story, collect and use items, or navigate the space in the \ufb01ctional world.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b65f1447a78594dc4b055991ee632fb7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        726.4
                    ],
                    [
                        117.0,
                        1053.0
                    ],
                    [
                        817.7,
                        1053.0
                    ],
                    [
                        817.7,
                        726.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95583,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "2cfd2ef686329596bef2c3814d058142"
        },
        "text": "These games typically implement one of three text-based in- terfaces: parser-based, choice-based, and hyperlink-based [54]. Choice-based and hyperlink-based interfaces provide the possi- ble actions to the player at a given state as a list, out of context, or as links in the state description. Parser-based interfaces are, on the other hand, open to any input, and the player has to learn what words the game understands. This is interesting for com- puters as it is much more akin to natural language, where you have to know what actions should exist based on your under- standing of language and the given state.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1cab56f2533f07ce68e1caaa9ece0761",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1058.5
                    ],
                    [
                        117.0,
                        1318.8
                    ],
                    [
                        817.9,
                        1318.8
                    ],
                    [
                        817.9,
                        1058.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95736,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "2cfd2ef686329596bef2c3814d058142"
        },
        "text": "Unlike some other game genres, like arcade games, text ad- venture games have not had a standard benchmark of games that everyone can compare against. This makes a lot of results hard to directly compare. A lot of research has focused on games that run on Infocom\u2019s Z-Machine game engine, an engine that can play a lot of the early classic games. Recently, Microsoft has introduced the environment TextWorld to help create a stan- dardized text adventure environment [25].",
        "type": "NarrativeText"
    },
    {
        "element_id": "39d70b42cc9adf24e18db6e8da8bd71a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1357.8
                    ],
                    [
                        117.0,
                        1385.5
                    ],
                    [
                        459.3,
                        1385.5
                    ],
                    [
                        459.3,
                        1357.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5326,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "H. OpenAI Gym and Universe",
        "type": "Title"
    },
    {
        "element_id": "b23cf259ecf96219d230005544bf1ff3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1404.9
                    ],
                    [
                        117.0,
                        1598.6
                    ],
                    [
                        817.2,
                        1598.6
                    ],
                    [
                        817.2,
                        1404.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95771,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "39d70b42cc9adf24e18db6e8da8bd71a"
        },
        "text": "OpenAI Gym is a large platform for comparing RL algorithms with a single interface to a suite of different environments in- cluding ALE, GVG-AI, MuJoCo, Malmo, ViZDoom, and more [17]. OpenAI Universe is an extension to OpenAI Gym that currently interfaces with more than a thousand Flash games and aims to add many modern video games in the future.2",
        "type": "NarrativeText"
    },
    {
        "element_id": "384d974caf6838c80ce40f87fd95b57a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        248.1,
                        1637.7
                    ],
                    [
                        248.1,
                        1665.4
                    ],
                    [
                        685.2,
                        1665.4
                    ],
                    [
                        685.2,
                        1637.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56797,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "IV. DL METHODS FOR GAME PLAYING",
        "type": "Title"
    },
    {
        "element_id": "158465f5ad4ad2205757ffd9faae0ea9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1155.2,
                        180.1
                    ],
                    [
                        1155.2,
                        202.3
                    ],
                    [
                        1237.4,
                        202.3
                    ],
                    [
                        1237.4,
                        180.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "TABLE I",
        "type": "Title"
    },
    {
        "element_id": "280c8f84bb59bfaf1ae1508ae18499a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        896.6,
                        205.0
                    ],
                    [
                        896.6,
                        277.0
                    ],
                    [
                        1496.0,
                        277.0
                    ],
                    [
                        1496.0,
                        205.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.31897,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "HUMAN-NORMALIZED SCORES REPORTED WITH VARIOUS DEEP RL ALGORITHMS IN ALE ON 57 ATARI GAMES USING THE 30 no-ops EVALUATION METRIC",
        "type": "NarrativeText"
    },
    {
        "element_id": "93328c0da138545383883a6d2b3388b8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        856.5,
                        314.3
                    ],
                    [
                        856.5,
                        642.6
                    ],
                    [
                        1512.0,
                        642.6
                    ],
                    [
                        1512.0,
                        314.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93378,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-6-1.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9",
            "text_as_html": "<table><thead><tr><th>DON [161]</th><th>228%</th><th>719%</th><th>2013 [97]</th></tr></thead><tbody><tr><td>Double DON (DDQN) [161] |</td><td>307%</td><td>118%</td><td>2015 [155]</td></tr><tr><td>Dueling DDQN [161]</td><td>373%</td><td>151%</td><td>2015 [161]</td></tr><tr><td>Prior. DDQN [161]</td><td>435%</td><td>124%</td><td>2015 [122]</td></tr><tr><td>Prior. Duel DDQN [161]</td><td>592%</td><td>172%</td><td>2015 [122]</td></tr><tr><td>A3C [63]</td><td>853%</td><td></td><td>2016 [96]</td></tr><tr><td>UNREAL [63]*</td><td>880%</td><td>250%</td><td>2016 [63]</td></tr><tr><td>NoisyNet-DQN [56]</td><td></td><td>118%</td><td>2017 [35]</td></tr><tr><td>Distr. DON (C51) [9]</td><td>701%</td><td>178%</td><td>2017 [9]</td></tr><tr><td>Rainbow [56]</td><td></td><td>223%</td><td>2017 [56]</td></tr><tr><td>IMPALA [30]</td><td>958%</td><td>192%</td><td>2018 [30]</td></tr><tr><td>Ape-X DQN [61]</td><td></td><td>434%</td><td>2018 [61]</td></tr></tbody></table>"
        },
        "text": "Results Mean | Median | Year and orig. paper DON [161] 228% 719% 2013 [97] Double DON (DDQN) [161] | 307% 118% 2015 [155] Dueling DDQN [161] 373% 151% 2015 [161] Prior. DDQN [161] 435% 124% 2015 [122] Prior. Duel DDQN [161] 592% 172% 2015 [122] A3C [63] 853% 2016 [96] UNREAL [63]* 880% 250% 2016 [63] NoisyNet-DQN [56] 118% 2017 [35] Distr. DON (C51) [9] 701% 178% 2017 [9] Rainbow [56] 223% 2017 [56] IMPALA [30] 958% 192% 2018 [30] Ape-X DQN [61] 434% 2018 [61]",
        "type": "Table"
    },
    {
        "element_id": "4903c37660c9de102fcca79b617c2c8e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        655.5
                    ],
                    [
                        847.6,
                        775.0
                    ],
                    [
                        1545.1,
                        775.0
                    ],
                    [
                        1545.1,
                        655.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88762,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "References in the \ufb01rst column refer to the paper that included the results, while the last col- umn references the paper that \ufb01rst introduced the speci\ufb01c technique. Note that the reported scores use various amounts of training time and resources, thus not entirely comparable. Successors typically use more resources and less wall-clock time. *Hyperparameters was tuned for every game leading to higher scores for UNREAL.",
        "type": "FigureCaption"
    },
    {
        "element_id": "630ab4060085d7fb9e374f3873c418c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        811.9
                    ],
                    [
                        847.6,
                        906.0
                    ],
                    [
                        1550.9,
                        906.0
                    ],
                    [
                        1550.9,
                        811.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93316,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "policies directly from raw pixels. This section reviews the main advancements that have been demonstrated in ALE. An overview of these advancements is shown in Table I.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ff98797542d90163c7b0441ae8fe42ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        911.5
                    ],
                    [
                        847.6,
                        1271.4
                    ],
                    [
                        1550.3,
                        1271.4
                    ],
                    [
                        1550.3,
                        911.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95462,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "Deep Q-network (DQN) was the \ufb01rst learning algorithm that showed human expert-level control in ALE [97]. DQN was tested in seven Atari 2600 games and outperformed previous approaches, such as SARSA with feature construction [7] and NE [49], as well as a human expert on three of the games. DQN is based on Q-learning, where a neural network model learns to approximate Q\u03c0 (s, a) that estimates the expected return of taking action a in state s while following a behavior policy \u03bc. A simple network architecture consisting of two convolutional layers followed by a single fully-connected layer was used as a function approximator.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f26ed9435ec2e3e89c761689148f0f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1276.8
                    ],
                    [
                        847.6,
                        1570.2
                    ],
                    [
                        1551.1,
                        1570.2
                    ],
                    [
                        1551.1,
                        1276.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9543,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "A key mechanism in DQN is experience replay [89], where experiences in the form {st , at, rt+1, st+1} are stored in a replay memory and randomly sampled in batches when the network is updated. This enables the algorithm to reuse and learn from past and uncorrelated experiences, which reduces the variance of the updates. DQN was later extended with a separate target Q-network, the parameters of which are held \ufb01xed between individual updates, and was shown to achieve above human expert scores in 29 out of 49 tested games [98].",
        "type": "NarrativeText"
    },
    {
        "element_id": "3c42a9e6cac11299eb7815adbe4cb31d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1575.7
                    ],
                    [
                        847.6,
                        1669.8
                    ],
                    [
                        1549.8,
                        1669.8
                    ],
                    [
                        1549.8,
                        1575.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93566,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "Deep recurrent Q-learning (DRQN) extends the DQN archi- tecture with a recurrent layer before the output and works well for games with partially observable states [51].",
        "type": "NarrativeText"
    },
    {
        "element_id": "8768b4e8fbeb1177b8f667a723abce80",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1684.8
                    ],
                    [
                        117.0,
                        1878.6
                    ],
                    [
                        816.2,
                        1878.6
                    ],
                    [
                        816.2,
                        1684.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95474,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "This section gives an overview of DL techniques used to play video games, divided by game genre. Table II lists DL meth- ods for each game genre and highlights which input features, network architecture, and training methods they rely upon. A typical neural network architecture used in deep RL is shown in Fig. 3.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9a84133f238619781d3baa4a555c0984",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1675.4
                    ],
                    [
                        847.6,
                        1870.6
                    ],
                    [
                        1549.7,
                        1870.6
                    ],
                    [
                        1549.7,
                        1675.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95397,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "158465f5ad4ad2205757ffd9faae0ea9"
        },
        "text": "A distributed version of DQN was shown to outperform a nondistributed version in 41 of the 49 games using the Gorila architecture (general RL architecture) [100]. Gorila parallelizes actors that collect experiences into a distributed replay memory, as well as parallelizing learners that train on samples from the same replay memory.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d3b93d60dc715c546b7d30485c24d661",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1917.7
                    ],
                    [
                        117.0,
                        1945.3
                    ],
                    [
                        321.0,
                        1945.3
                    ],
                    [
                        321.0,
                        1917.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48508,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "edea4c265999abe542759d5449b9fe52"
        },
        "text": "A. Arcade Games",
        "type": "Title"
    },
    {
        "element_id": "eaac5dc3222fbdd7baef9c1b09000108",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1964.7
                    ],
                    [
                        117.0,
                        2025.7
                    ],
                    [
                        814.4,
                        2025.7
                    ],
                    [
                        814.4,
                        1964.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91787,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "d3b93d60dc715c546b7d30485c24d661"
        },
        "text": "The ALE consists of more than 50 Atari games and has been the main testbed for deep RL algorithms that learn control",
        "type": "NarrativeText"
    },
    {
        "element_id": "1649e2d34107139a686028e7fae3c980",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        136.4,
                        2074.5
                    ],
                    [
                        136.4,
                        2100.3
                    ],
                    [
                        401.7,
                        2100.3
                    ],
                    [
                        401.7,
                        2074.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.51682,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "d3b93d60dc715c546b7d30485c24d661"
        },
        "text": "2https://universe.openai.com/",
        "type": "NarrativeText"
    },
    {
        "element_id": "7c65c388813f45bb8a6f1a4bf34da79c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1874.6
                    ],
                    [
                        847.6,
                        2106.4
                    ],
                    [
                        1548.7,
                        2106.4
                    ],
                    [
                        1548.7,
                        1874.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94751,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "d3b93d60dc715c546b7d30485c24d661"
        },
        "text": "One problem with the Q-learning algorithm is that it often overestimates action values because it uses the same value func- tion for action selection and action evaluation. Double DQN, based on double Q-learning [46], reduces the observed overesti- mation by learning two value networks with parameters 0 and 0\u201d that both use the other network for value estimation, such that the target Y\u0131 = As\u0131 + yQ(S:41, max, Q(S)41, 0; 6); 0) [155].",
        "type": "NarrativeText"
    },
    {
        "element_id": "668cd32cba60099cb9714f7702895b46",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73626,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 6,
            "parent_id": "d3b93d60dc715c546b7d30485c24d661"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8aea795dc602167ffc3ecd53a69c4056",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.4,
                        92.7
                    ],
                    [
                        103.4,
                        112.0
                    ],
                    [
                        664.6,
                        112.0
                    ],
                    [
                        664.6,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84031,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "92748e0a3571074c59c3e012d4334646",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        601.9,
                        180.1
                    ],
                    [
                        601.9,
                        227.2
                    ],
                    [
                        1037.9,
                        227.2
                    ],
                    [
                        1037.9,
                        180.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68293,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "TABLE II OVERVIEW OF DL METHODS APPLIED TO GAMES",
        "type": "FigureCaption"
    },
    {
        "element_id": "d522631cbb2ff6faf03bb9b5614cb20a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        198.0,
                        261.0
                    ],
                    [
                        198.0,
                        276.0
                    ],
                    [
                        248.0,
                        276.0
                    ],
                    [
                        248.0,
                        261.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Game",
        "type": "Title"
    },
    {
        "element_id": "025d7405ec6c7a02d08caaf386ae5313",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        316.0,
                        253.0
                    ],
                    [
                        316.0,
                        286.0
                    ],
                    [
                        381.0,
                        286.0
                    ],
                    [
                        381.0,
                        253.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Method",
        "type": "Title"
    },
    {
        "element_id": "2378fde6f131c3fed0be4363843d16a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        762.0,
                        262.0
                    ],
                    [
                        762.0,
                        280.0
                    ],
                    [
                        806.0,
                        280.0
                    ],
                    [
                        806.0,
                        262.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Input",
        "type": "Title"
    },
    {
        "element_id": "48f216a08f868f9d9347475f817779a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        894.0,
                        253.0
                    ],
                    [
                        894.0,
                        286.0
                    ],
                    [
                        1003.0,
                        286.0
                    ],
                    [
                        1003.0,
                        253.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Architecture",
        "type": "Title"
    },
    {
        "element_id": "a7d6eaa7889f8c997dc82329063ecd68",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1117.0,
                        261.0
                    ],
                    [
                        1117.0,
                        280.0
                    ],
                    [
                        1189.0,
                        280.0
                    ],
                    [
                        1189.0,
                        261.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Training",
        "type": "Title"
    },
    {
        "element_id": "2d3ecb4fd0e6d30932dd9a8e96416d73",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1314.0,
                        261.0
                    ],
                    [
                        1314.0,
                        276.0
                    ],
                    [
                        1435.0,
                        276.0
                    ],
                    [
                        1435.0,
                        261.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "Miscellaneous",
        "type": "Title"
    },
    {
        "element_id": "ce9370af04c49755222e41ce3245bc38",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        168.5,
                        291.3
                    ],
                    [
                        168.5,
                        1738.2
                    ],
                    [
                        1469.6,
                        1738.2
                    ],
                    [
                        1469.6,
                        291.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87842,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/table-7-2.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73",
            "text_as_html": "<table><thead><tr><th></th><th rowspan=\"2\">=e &amp;_</th><th>&amp;</th><th></th><th></th><th></th><th></th><th></th><th rowspan=\"2\"></th><th colspan=\"2\"></th><th></th><th rowspan=\"4\"></th><th></th><th rowspan=\"6\">1000000000000000C</th></tr></thead><tbody><tr><td rowspan=\"17\">a2 2 on Elie saz SER gs</td><td>5</td><td>@o</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>(\u00a9)</td><td>ec</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>= 8 e \u00a9 2 \u00a7 Sade o</td><td>(\u00a9)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BODUR paz SLOEZ AAS5SSAL</td><td>o</td><td></td><td></td><td></td><td></td><td></td><td>000000000</td><td>0000006060060060000</td><td>0000000000600</td><td></td><td>0000000006000</td><td>0000000000000</td></tr><tr><td></td><td>o</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EB</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>\u00e7 \u00c7E ASS ESleZ 45595 22585 AR</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>00000</td><td></td><td></td><td></td></tr><tr><td>BRS MEZE ESSEC SQQOSASLZ CLES</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>x 2 = 2 5 BE 2 mesi SZ</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0\u00a90\u00a90066</td><td></td><td></td><td>000000</td><td></td><td></td><td></td></tr><tr><td>pa a 2. 82 Be 88 \u00d6\u00e7. aZa\u00e7 geze 3.387, 2468 a</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>= \u00e2 \u2014 a g\u00f6. \u20ac_22z\u2014-2_29 ogkus \u2014\u0130SGPOBLELESEAZLRESYAR eee Oe Za ee ee G\u00a7 &gt; GUA ec ade-ammREz SSS ES PHSAOLORREESOE SS EY SHSMAO BAZZAZ A CAAZS ER</td><td>0000000c</td><td>\u00a9000000000000000000X 00000000000000000090</td><td>0000000000000000000000000000000000060</td><td>0000000600600000000006000)0(00)(000J00600</td><td>90900000000000000000000000)90)00(000(00000l060/(000000J00J0(00000</td><td>0900000000000000000000000)90)0</td><td>000000 90000000000000000000606)0(00(6600(00060(000(0000060J00J60(00000</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>COOP ACK</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Z.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>=z 5lE</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>e a E iS Oss HARON</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"2\">ZEO) 25)</td><td>Ee ERS ez 266</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EAZLE Seas \u00e7ES\u201cB\u00f6l\u015fE&amp; SrBeesa aRBBol\u011fa 8 else ECR EERAAA</td><td></td><td></td><td></td><td></td><td></td><td>0(000(00000/000/(e60\u00a90066(6069(060600</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"2\">3 a</td><td>zee e Balm</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RSIS SLOP CAC</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">3 A</td><td>Ez</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>8) S3) &lt;a AAA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>\u2014 &amp; &amp; 5 2 3) Aes Zes OSE, eda TERS se LZSOLOS|\u00a7SASS6 BIR</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>o</td></tr><tr><td rowspan=\"3\">- - gq g \u011f d \u011f \u011f +</td><td>5 = oz 5 SAC ZA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>|4 2 asm AAINSR</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&lt; l2S OS</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"4\"></td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>sessiz Hs</td><td>\u00a900(0006006(6060J(660060606060</td><td>OJOOOOOJOOO|OO OOO @eeeeee@eeoooooe#ooeooo0o0c</td><td>00006)</td><td>@e@@ooooeeccec</td><td></td><td></td><td>@ecee@ecee@eocecooopo</td><td>0000(000(00660606(6060000000C</td><td>00(000000</td><td>00000</td><td>200J0900000)0</td><td>000(000066(0</td><td>slooccclooclooc000</td></tr><tr><td>SEBE Belo SZ aZils nO \u015fia 2 2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>5 3 | ege ae alg E\u011fi selE\u011flez srls(oz se</td><td></td><td>O/O</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>2 Balk] (EB) \u00e7\u0131k e</td><td>loo saa maahi</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>\u0130\u015f) (al</td><td>BS) sg E\u015e\u0130 wee sea ylsege</td><td></td><td>O|/O|@</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>o0lolo</td><td></td><td>o</td></tr><tr><td rowspan=\"3\">8 8. 25 \u00a7 &amp; 5</td><td>= E sa e El 23 Fi # 2 8 2 ebhes</td><td></td><td>\u00a9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>O</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&amp; ack em sos EZ ea sa &gt;halMad\u00f6</td><td>90000</td><td>O@</td><td>90006</td><td></td><td></td><td></td><td>peeece</td><td></td><td>900000</td><td>90000</td><td></td><td>0000000</td><td>00000</td></tr></tbody></table>"
        },
        "text": "& $ \u00f6 , \u0130S\u0130. , \u015f \u0130SE SS oes \u011f \u015e > S\u0130SL\u0130 $ e Di S\u0130 SSS SE See SESS SE See FF$ESE ETE OSES SIFFS EE\u015eSE\u011e DON [97], [98] oe\u00b0o @eooo oe@0o0$0 \u00a90000 DRON [51] oe\u00b0o @eeoo oeo0$9 \u00a90000 UCTtoClassification [42] oe\u00b0o \u00a90000 \u00a90000 \u00a90000 Gorila [100] oe\u00b0o @eooo oe@o0$9 0000086 Double DON [155] oe\u00b0o @eooo O 6000 \u00a90000 Prioritized DON [122] oeo\u00b0o eooo O 6000 ooo\u00b0oc Dueling DQN [161] oeo\u00b0o @eooo O 6000 \u00a90000 Bootstrapped DON (106) oe\u00b0o eooo oe@\u00b0o\u00a2 \u00a90000 A3C / A2C [96] oe\u00b0o eeoo o ce 000006 ACER [160] oe\u00b0o @eooo o ce 0000086 Atari Progressive Networks (119) oe\u00b0o @eooo o ce 0000606 2600 UNREAL (63) \u00a90060 @eeoodo o ce ooo e (ALE) Scalable Evolution Strategies [120] nr iel \u00a90000 000 \u00a90006 Distributional DON (C51) [9] o 60 \u00a90000 NX ye \u00a90000 NoisyNet-DON [35] oe\u00b0o @eooo 060 \u00a90000 NoisyNet-A3C (35) > 00 \u00a90000 o ce \u00a9o00006 Rainbow [56] DEK Ere) \u00a90000 ONE ie \u00a90000 ACKTR [166] oe\u00b0o eooo o e o omen ) Deep GA [138] oe\u00b0o \u00a90060 000 o o ce NS-ES / NSR-ES [24] \u00a90060 \u00a90000 o 00 o o ce IMPALA [30] 060 \u00a90000 oo @ o 00 @ TRPO [127] oe\u00b0o .0 00 o oe o o ce PPO [128] DEE ae) @eeoo o ce o oo @ DQID [57] > 00 \u00a90000 e. o \u00a90000 Ape-X DON [61] > 00 @eooo oe@\u00b0o o o ce Ape-X DQ\u00a3D [112] oe\u00b0o @eooo \u00a9. o oo @ Ms. Pac-Man Hybrid Reward Architecture (HRA) [156] @eeo @eooo oO eo O O 0 O Montezuma's o Hierarchical-DON (h-DQN) [77] e. eoood (ONE Wee) O eo o Revenge DQN-CTS / DQN-PixelCNN [8] oe@\u00b0o @eooo \u00a9 @ 0 o e 00 Direct Perception [21] e. \u00a9e00600 eoo o 0 eo Racing Deep DPG (DDPG) [88] oe\u00b0o \u00a90000 omne) o \u00a90000 A3C [96] oe\u00b0o @eooo o ce o oo @ DON [73] > ie \u00a90000 0 eo o \u00a90000 A3C [167] oe\u00b0o eooo 00 @ o o ce Doom DRON [79] oe\u00b0o eeoo Oe o \u00a9 00 DON + SLAM [14] \u00a90060 \u00a90000 @e@eo o ooo Direct Future Prediction (DFP) [28] e oo eo \u00a9 O O00 0 O o 6 @ H-DRIN [149] e. @eooo om eo o \u00a90000 Minecraft RMON / FRMON [102] oe\u00b0o e... 0 6e0 o oe\u00b0o TSCL [92] oe\u00b0o @eeoo O 00 o oo. Zero Order [154] @oo ooo e 000 o oOo \u00a2 IQL [33] o.o @eoo om Ime ooo BiCNet [111] @o @eoo omen ) o \u00a9 00 Sarraf COMA (32) eo eco. 00086 00000 Macro-action SL [68] e o O00 @ @oo (\u00a9) O00 0 Macro-action CNNFQ/PPO [147], [140] ee o ce o ce fo) o\u00b00 @ RoboCup Soccer DDPG + Inverting Gradients [52] @o o ce o ce o oo (HFO) DDPG + Mixing policy targets [53] @o o ce o ce fo) oo. 2D billiard O Object-centric prediction [36] ee @eoo @eoo ooo oO LSTM-DQN [101] oo eoo oe oO \u00a90000 DRRN [54] oo o ce o o \u00a90000 Text adventures frordance Based Action Selection (37) e\u011fe) 0006 060 .0000 games Golovin [75] o\u011fre eoo 00 00000 AE-DQN [173] one) O00 O@ 0 o \u00a90000",
        "type": "Table"
    },
    {
        "element_id": "32c8239dd79cc8e7d2eedc119f664950",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        132.2,
                        1758.1
                    ],
                    [
                        132.2,
                        1802.4
                    ],
                    [
                        1509.5,
                        1802.4
                    ],
                    [
                        1509.5,
                        1758.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82542,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "We refer to features as low-dimensional items and values that describe the state of the game such as health, ammunition, score, objects, etc. MLP refers to a traditional fully connected architecture without convolutional or recurrent layers.",
        "type": "FigureCaption"
    },
    {
        "element_id": "cbabf85605acc1bd59e0abe554e05bff",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1842.2
                    ],
                    [
                        105.3,
                        1969.5
                    ],
                    [
                        802.7,
                        1969.5
                    ],
                    [
                        802.7,
                        1842.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93611,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "Another improvement is prioritized experience replay from which important experiences are sampled more frequently based on the TD error, which was shown to signi\ufb01cantly improve both DQN and double DQN [123].",
        "type": "NarrativeText"
    },
    {
        "element_id": "542ee9628bb31a1f7a0668fe151a1c2d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1837.0
                    ],
                    [
                        835.9,
                        1870.7
                    ],
                    [
                        1533.3,
                        1870.7
                    ],
                    [
                        1533.3,
                        1837.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "value V \u03c0 (s) and the action advantage A\u03c0 (s, a), such that",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4a2bdd6585700cc077579b7d5a0ae7c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1845.2
                    ],
                    [
                        835.9,
                        1970.3
                    ],
                    [
                        1533.2,
                        1970.3
                    ],
                    [
                        1533.2,
                        1845.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93401,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "Q\u03c0 (s, a) = V \u03c0 (s) +A\u03c0 (s, a) [161]. Dueling DQN improves double DQN and can also be combined with prioritized experi- ence replay.",
        "type": "NarrativeText"
    },
    {
        "element_id": "abd0fcf4cbb53ff83b887ebfd5629c4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1975.0
                    ],
                    [
                        105.3,
                        2035.9
                    ],
                    [
                        805.1,
                        2035.9
                    ],
                    [
                        805.1,
                        1975.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91804,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "Dueling DQN uses a network that is split into two streams after the convolutional layers to separately estimate state",
        "type": "NarrativeText"
    },
    {
        "element_id": "acf1f8eeb502e49395f1e8a759f26acf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1975.9
                    ],
                    [
                        835.9,
                        2036.8
                    ],
                    [
                        1533.3,
                        2036.8
                    ],
                    [
                        1533.3,
                        1975.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92693,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "Double DQN and dueling DQN were also tested in the \ufb01ve more complex games in the RLE and achieved a mean score",
        "type": "NarrativeText"
    },
    {
        "element_id": "2416ae14fc3f20db19bf58c265945f4c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.6,
                        2123.7
                    ],
                    [
                        116.6,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76056,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "2d3ecb4fd0e6d30932dd9a8e96416d73"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "41ebfa153bc1060a2339bce103f15ba3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1473.0,
                        377.0
                    ],
                    [
                        1473.0,
                        387.0
                    ],
                    [
                        1485.0,
                        387.0
                    ],
                    [
                        1485.0,
                        377.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7,
            "parent_id": "8aea795dc602167ffc3ecd53a69c4056"
        },
        "text": "SE",
        "type": "Title"
    },
    {
        "element_id": "d31023a3f144dce3276bf4600ba0d317",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1522.2,
                        92.5
                    ],
                    [
                        1522.2,
                        112.5
                    ],
                    [
                        1535.0,
                        112.5
                    ],
                    [
                        1535.0,
                        92.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7432,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 7
        },
        "text": "7",
        "type": "Header"
    },
    {
        "element_id": "c5355033d89ef768ef292dc095a2d298",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.2,
                        92.4
                    ],
                    [
                        115.2,
                        112.9
                    ],
                    [
                        127.6,
                        112.9
                    ],
                    [
                        127.6,
                        92.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7317,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "8",
        "type": "Header"
    },
    {
        "element_id": "994bed809ebe701470136814f3e6abf5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        983.3,
                        92.7
                    ],
                    [
                        983.3,
                        112.1
                    ],
                    [
                        1546.0,
                        112.1
                    ],
                    [
                        1546.0,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81731,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "bbbfc1d095184a186c4cf35587f4eb4c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        157.5,
                        182.1
                    ],
                    [
                        157.5,
                        447.5
                    ],
                    [
                        1518.1,
                        447.5
                    ],
                    [
                        1518.1,
                        182.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92944,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-8-3.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "osu 5 1 | Input | Convolution | Convolution \u201cO Left LSTM O Stay O Right | Convolution [Fully connected | Recurrency | Oupu |",
        "type": "Image"
    },
    {
        "element_id": "64d06fe2f4255e9d1a7eeb7bd5fc6d56",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        478.7
                    ],
                    [
                        117.0,
                        626.2
                    ],
                    [
                        1545.1,
                        626.2
                    ],
                    [
                        1545.1,
                        478.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92536,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "Fig. 3. Example of a typical network architecture used in deep RL for game playing with pixel input. The input usually consists of a preprocessed screen image, or several stacked or concatenated images, which is followed by a couple of convolutional layers (often without pooling), and a few fully connected layers. Recurrent networks have a recurrent layer, usually an LSTM, after the fully connected layers. The output typically consists of one unit for each unique combination of actions in the game, and actor-critic methods also have one for the state value V (s). Examples of this architecture, without a recurrent layer and with some variations, are [9], [24], [30], [35], [56], [96]\u2013[98], [100], [106], [120], [121], [123], [139], [155], [160], [161], and [166], and examples with a recurrent layer are [51], [63], and [96].",
        "type": "FigureCaption"
    },
    {
        "element_id": "ecc655f2f12dabee8bab822d2d5d48d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        663.1
                    ],
                    [
                        117.0,
                        757.2
                    ],
                    [
                        814.3,
                        757.2
                    ],
                    [
                        814.3,
                        663.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92687,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "of around 50% of a human expert [15]. The best result in these experiments was by dueling DQN in the game Mortal Kombat (Midway, 1992) with 128%.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5f6f23a53ca594948b7fdb81e0c1a9f9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        762.8
                    ],
                    [
                        117.0,
                        890.1
                    ],
                    [
                        814.4,
                        890.1
                    ],
                    [
                        814.4,
                        762.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93744,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "Bootstrapped DQN improves exploration by training multiple Q-networks. A randomly sampled network is used during each training episode, and bootstrap masks modulate the gradients to train the networks differently [106].",
        "type": "NarrativeText"
    },
    {
        "element_id": "4fb68a661a962312d433c17b05088b72",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        895.6
                    ],
                    [
                        117.0,
                        1122.5
                    ],
                    [
                        814.5,
                        1122.5
                    ],
                    [
                        814.5,
                        895.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95396,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "Robust policies can be learned with DQN for competitive or cooperative multiplayer games by training one network for each player and play them against each other in the training process [146]. Agents trained in the multiplayer mode perform very well against novel opponents, whereas agents trained against a stationary algorithm fail to generalize their strategies to novel adversaries.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c2118a63eda0198995c7267d31c66923",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1128.1
                    ],
                    [
                        117.0,
                        1521.0
                    ],
                    [
                        817.6,
                        1521.0
                    ],
                    [
                        817.6,
                        1128.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95436,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "Multithreaded asynchronous variants of DQN, SARSA, and actor-critic methods can utilize multiple CPU threads on a sin- gle machine, reducing training roughly linear to the number of parallel threads [96]. These variants do not rely on a re- play memory because the network is updated on uncorrelated experiences from parallel actors, which also helps stabilize on- policy methods. The asynchronous advantage actor-critic (A3C) algorithm is an actor-critic method that uses several parallel agents to collect experiences that all asynchronously update a global actor-critic network. A3C outperformed prioritized du- eling DQN, which was trained for 8 days on a GPU, with just half the training time on a CPU [96].",
        "type": "NarrativeText"
    },
    {
        "element_id": "d3cf497154ea8ddd8dd89aa69128befc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1526.6
                    ],
                    [
                        117.0,
                        1720.3
                    ],
                    [
                        814.4,
                        1720.3
                    ],
                    [
                        814.4,
                        1526.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95146,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "An actor-critic method with experience replay (ACER) imple- ments an ef\ufb01cient trust region policy method that forces updates to not deviate far from a running average of past policies [160]. The performance of the ACER in ALE matches dueling DQN with prioritized experience replay and A3C without experience replay, while it is much more data ef\ufb01cient.",
        "type": "NarrativeText"
    },
    {
        "element_id": "09d3b343642dd2072c5eb846a30d97ba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1725.8
                    ],
                    [
                        117.0,
                        1886.3
                    ],
                    [
                        814.6,
                        1886.3
                    ],
                    [
                        814.6,
                        1725.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9472,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "A3C with progressive neural networks [120] can effectively transfer learning from one game to another. The training is done by instantiating a network for every new task with connections to all the previous learned networks. This gives the new network access to knowledge already learned.",
        "type": "NarrativeText"
    },
    {
        "element_id": "73fbc8f3b03dea84e7f6cae54d6ea97f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1891.9
                    ],
                    [
                        117.0,
                        2052.4
                    ],
                    [
                        818.3,
                        2052.4
                    ],
                    [
                        818.3,
                        1891.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94273,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "The advantage actor-critic (A2C), a synchronous variant of A3C [96], updates the parameters synchronously in batches and has comparable performance while only maintaining one neural network [166]. Actor-critic using Kronecker-factored trust region (ACKTR) extends A2C by approximating the",
        "type": "NarrativeText"
    },
    {
        "element_id": "79f32f2201e3350fb79a31e861574e5e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        663.1
                    ],
                    [
                        847.6,
                        690.8
                    ],
                    [
                        1545.0,
                        690.8
                    ],
                    [
                        1545.0,
                        663.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8
        },
        "text": "natural policy gradient updates for both the actor and the",
        "type": "Title"
    },
    {
        "element_id": "286dd85dc4e5478881a2ad20a14bc0cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        672.3
                    ],
                    [
                        847.6,
                        1421.4
                    ],
                    [
                        1545.2,
                        1421.4
                    ],
                    [
                        1545.2,
                        672.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93996,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8,
            "parent_id": "79f32f2201e3350fb79a31e861574e5e"
        },
        "text": "critic [166]. In Atari, ACKTR has slower updates compared to A2C (at most 25% per time step) but is more sample ef\ufb01cient (e.g., by a factor of 10 in Atlantis) [166]. Trust region policy optimization (TRPO) uses a surrogate objective with theoretical guarantees for monotonic policy improvement, while it practically implements an approximation called trust region [128]. This is done by constraining network updates with a bound on the Kullback\u2013Leibler (KL) divergence between the current and the updated policy. TRPO has robust and data-ef\ufb01cient performance in Atari games, while it has high memory requirements and several restrictions. Proximal policy optimization (PPO) is an improvement on TRPO that uses a similar surrogate objective [129], but instead uses a soft constraint (originally suggested in [128]) by adding the KL divergence as a penalty. Instead of having a \ufb01xed penalty coef\ufb01cient, it uses a clipped surrogate objective that penalizes policy updates outside some speci\ufb01ed interval. PPO was shown to be more sample ef\ufb01cient than A2C and on par with ACER in Atari, while PPO does not rely on replay memory. PPO was also shown to have comparable or better performance than TRPO in continuous control tasks while being simpler and easier to parallelize.",
        "type": "NarrativeText"
    },
    {
        "element_id": "67e14764793041ccf30444b32b5449d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1427.0
                    ],
                    [
                        847.6,
                        1853.2
                    ],
                    [
                        1549.4,
                        1853.2
                    ],
                    [
                        1549.4,
                        1427.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95159,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8,
            "parent_id": "79f32f2201e3350fb79a31e861574e5e"
        },
        "text": "Importance weighted actor\u2013learner architecture (IMPALA) is an actor-critic method, where multiple learners with GPU access share gradients between each other while being synchronously updated from a set of actors [30]. This method can scale to a large number of machines and outperforms A3C. Additionally, IMPALA was trained, with one set of parameters, to play all 57 Atari games in ALE with a mean human-normalized score of 176.9% (median of 59.7%) [30]. Experiences collected by the actors in the IMPALA setup can lack behind the learners\u2019 policy and thus result in off-policy learning. This discrepancy is mit- igated through a V-trace algorithm that weighs the importance of experiences based on the difference between the actor\u2019s and learner\u2019s policies [30].",
        "type": "NarrativeText"
    },
    {
        "element_id": "603cae796ebe6580158784cc9537fd49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1858.7
                    ],
                    [
                        847.6,
                        2052.4
                    ],
                    [
                        1550.1,
                        2052.4
                    ],
                    [
                        1550.1,
                        1858.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95182,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8,
            "parent_id": "79f32f2201e3350fb79a31e861574e5e"
        },
        "text": "The UNsupervised REinforcement and Auxiliary Learning (UNREAL) algorithm is based on A3C but uses a replay memory from which it learns auxiliary tasks and pseudoreward functions concurrently [63]. UNREAL only shows a small improvement over vanilla A3C in ALE, but larger improvements in other domains (see Section IV-D).",
        "type": "NarrativeText"
    },
    {
        "element_id": "e0965dafe5f566de9feb8cb505bdad3b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        118.4,
                        2123.7
                    ],
                    [
                        118.4,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74406,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 8,
            "parent_id": "79f32f2201e3350fb79a31e861574e5e"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5d4814a08d56482668a53f404889c2d7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.7,
                        92.7
                    ],
                    [
                        103.7,
                        112.0
                    ],
                    [
                        664.5,
                        112.0
                    ],
                    [
                        664.5,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74957,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "93e6a76ac77fc557be1dfe5916c7e759",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.5,
                        180.8
                    ],
                    [
                        103.5,
                        440.9
                    ],
                    [
                        802.7,
                        440.9
                    ],
                    [
                        802.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95508,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "Distributional DQN takes a distributional perspective on RL by treating Q(s, a) as an approximate distribution of returns instead of a single approximate expectation for each action [9]. The distribution is divided into a so-called set of atoms, which determines the granularity of the distribution. Their results show that the more \ufb01ne-grained the distributions are, the better are the results, and with 51 atoms (this variant was called C51), it achieved mean scores in ALE almost comparable to UNREAL.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5341e2f3e079141b16d8d0f2f5be0786",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.0,
                        446.5
                    ],
                    [
                        104.0,
                        673.4
                    ],
                    [
                        804.3,
                        673.4
                    ],
                    [
                        804.3,
                        446.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95435,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "In NoisyNets, noise is added to the network parameters, and a unique noise level for each parameter is learned using gradi- ent descent [35]. In contrast to e-greedy exploration, where an agent samples actions either from the policy or from a uniform random distribution, NoisyNets use a noisy version of the pol- icy to ensure exploration, and this was shown to improve DON (NoisyNet-DQN) and A3C (NoisyNet-A3C).",
        "type": "NarrativeText"
    },
    {
        "element_id": "3c0faf4aebec95a6e750bf1c070e94f3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        679.0
                    ],
                    [
                        105.3,
                        806.3
                    ],
                    [
                        802.8,
                        806.3
                    ],
                    [
                        802.8,
                        679.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93867,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "Rainbow combines several DQN enhancements: double DQN, prioritized replay, dueling DQN, distributional DQN, and NoisyNets, and achieved a mean score higher than any of the enhancements individually [56].",
        "type": "NarrativeText"
    },
    {
        "element_id": "bb7dd0db6c18c7ebe7f0bf22327e5912",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        811.8
                    ],
                    [
                        105.3,
                        1437.2
                    ],
                    [
                        805.9,
                        1437.2
                    ],
                    [
                        805.9,
                        811.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94298,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "ESs are black-box optimization algorithms that rely on param- eter exploration through stochastic noise instead of calculating gradients and were found to be highly parallelizable with a lin- ear speedup in training time when more CPUs are used [121]. Seven hundred and twenty CPUs were used for 1 h, whereafter ESs managed to outperform A3C (which ran for four days) in 23 out of 51 games, while ES used three to ten times as much data due to its high parallelization. ESs only ran a single day, and thus, their full potential is currently unknown. Novelty search is a popular algorithm that can overcome environments with deceptive and/or sparse rewards by guiding the search toward novel behaviors [84]. The ES has been extended to use novelty search (NS-ES), which outperforms the ES on several challeng- ing Atari games by de\ufb01ning novel behaviors based on the RAM states [24]. A quality-diversity variant called NSR-ES that uses both novelty and the reward signal reach an even higher per- formance [24]. NS-ES and NSR-ES reached worse results on a few games, possibly where the reward function is not sparse or deceptive.",
        "type": "NarrativeText"
    },
    {
        "element_id": "71b8dcbc8d339a1ed1c5a53eb0cd56d2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        540.7
                    ],
                    [
                        1534.9,
                        540.7
                    ],
                    [
                        1534.9,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9545,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "train one network to mimic a set of policies (e.g., for differ- ent games). These methods can reduce the size of the network and sometimes also improve the performance. A frame predic- tion model can be learned from a dataset generated by a DQN agent using the encoding\u2013transformation\u2013decoding network ar- chitecture; the model can then be used to improve exploration in a retraining phase [103]. Self-supervised tasks, such as re- ward prediction, validation of state\u2013successor pairs, and map- ping states and successor states to actions, can de\ufb01ne auxiliary losses used in pretraining of a policy network, which ultimately can improve learning [132].",
        "type": "NarrativeText"
    },
    {
        "element_id": "d6765572649da82b9d7d2144d86e9691",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        546.2
                    ],
                    [
                        835.9,
                        872.8
                    ],
                    [
                        1535.8,
                        872.8
                    ],
                    [
                        1535.8,
                        546.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95694,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "The training objective provides feedback to the agent, while the performance objective speci\ufb01es the target behavior. Often, a single reward function takes both roles, but, for some games, the performance objective does not guide the training suf\ufb01ciently. The hybrid reward architecture (HRA) splits the reward function into n different reward functions, where each of them is assigned a separate learning agent [156]. The HRA does this by having n output streams in the network, and thus n Q-values, which are combined when actions are selected. The HRA was able to achieve the maximum possible score in less than 3000 episodes.",
        "type": "NarrativeText"
    },
    {
        "element_id": "489b53571255ff34bcecdd52e9fa21cf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        931.0
                    ],
                    [
                        835.9,
                        958.6
                    ],
                    [
                        1123.5,
                        958.6
                    ],
                    [
                        1123.5,
                        931.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64673,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "5d4814a08d56482668a53f404889c2d7"
        },
        "text": "B. Montezuma\u2019s Revenge",
        "type": "Title"
    },
    {
        "element_id": "a65febe271ef468da9d679a0b27b2bed",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        978.0
                    ],
                    [
                        835.9,
                        1438.1
                    ],
                    [
                        1534.1,
                        1438.1
                    ],
                    [
                        1534.1,
                        978.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95132,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "Environments with sparse feedback remain an open challenge for RL. The game Montezuma\u2019s Revenge is a good example of such an environment in ALE and has thus been studied in more detail and used for benchmarking learning methods based on intrinsic motivation and curiosity. The main idea of applying intrinsic motivation is to improve the exploration of the envi- ronment based on some self-rewarding system, which eventually will help the agent to obtain an extrinsic reward. DQN fails to obtain any reward in this game (receiving a score of 0) and Go- rila achieves an average score of just 4.2. A human expert can achieve 4367 points, and it is clear that the methods presented so far are unable to deal with environments with such sparse rewards. A few promising methods aim to overcome these chal- lenges.",
        "type": "NarrativeText"
    },
    {
        "element_id": "037900019d95dcb656412741007112cb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.4,
                        1442.8
                    ],
                    [
                        104.4,
                        1869.0
                    ],
                    [
                        805.8,
                        1869.0
                    ],
                    [
                        805.8,
                        1442.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95199,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "A simple genetic algorithm with a Gaussian noise mutation operator evolves the parameters of a deep neural network (deep GA) and can achieve surprisingly good scores across several Atari games [139]. Deep GA shows comparable results to DQN, A3C, and ES on 13 Atari games using up to thousands of CPUs in parallel. Additionally, random search, given roughly the same amount of computation, was shown to outperform DQN on four out of 13 games and A3C on \ufb01ve games [139]. While there has been concern that evolutionary methods do not scale as well as gradient-descent-based methods, one possibility is separating the feature construction from the policy network; evolutionary algorithms can then create extremely small networks that still play well [26].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b7dcb887219ac22d4b4fea6d217bcfcc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1874.5
                    ],
                    [
                        105.3,
                        2068.8
                    ],
                    [
                        805.4,
                        2068.8
                    ],
                    [
                        805.4,
                        1874.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95463,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "A few supervised learning approaches have been applied to arcade games. In [42], a slow planning agent was applied of- \ufb02ine, using Monte Carlo tree search, to generate data for train- ing a CNN via multinomial classi\ufb01cation. This approach, called UCTtoClassi\ufb01cation, was shown to outperform DQN. Policy distillation [119] or actor-mimic [108] methods can be used to",
        "type": "NarrativeText"
    },
    {
        "element_id": "b7e84f5b392641bb0694a07f199f766c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1442.9
                    ],
                    [
                        835.9,
                        1769.6
                    ],
                    [
                        1537.9,
                        1769.6
                    ],
                    [
                        1537.9,
                        1442.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95652,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "Hierarchical-DQN (h-DQN) [77] operates on two tempo- ral scales, where one Q-value function Q1(s, a; g), the con- troller, learns a policy over actions that satisfy goals chosen by a higher level Q-value function Q2(s, g), the metacontroller, which learns a policy over intrinsic goals (i.e., which goals to select). This method was able to reach an average score of around 400 in Montezuma\u2019s Revenge, where goals were de\ufb01ned as states in which the agent reaches (collides with) a certain type of object. This method, therefore, must rely on some object detection mechanism.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2e3412f56a850b5e8672162529728b83",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1775.0
                    ],
                    [
                        835.9,
                        2068.4
                    ],
                    [
                        1536.7,
                        2068.4
                    ],
                    [
                        1536.7,
                        1775.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95229,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "Pseudocounts have been used to provide intrinsic motiva- tion in the form of exploration bonuses when unexpected pixel con\ufb01gurations are observed and can be derived from context tree switching (CTS) density models [8] or neu- ral density models [107]. Density models assign probabili- ties to images, and a model\u2019s pseudocount of an observed image is the model\u2019s change in prediction compared to be- ing trained one additional time on the same image. Impres- sive results were achieved in Montezuma\u2019s Revenge and other",
        "type": "NarrativeText"
    },
    {
        "element_id": "185efa995f539982b4971ed37443effd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        113.3,
                        2123.7
                    ],
                    [
                        113.3,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73652,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9,
            "parent_id": "489b53571255ff34bcecdd52e9fa21cf"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c3c22b4cf8610b5a3e9061da0e291899",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1522.3,
                        92.4
                    ],
                    [
                        1522.3,
                        112.0
                    ],
                    [
                        1534.8,
                        112.0
                    ],
                    [
                        1534.8,
                        92.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73235,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 9
        },
        "text": "9",
        "type": "Header"
    },
    {
        "element_id": "e15c9be296645bb7bfdda200d05e27a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.9,
                        92.7
                    ],
                    [
                        116.9,
                        112.6
                    ],
                    [
                        138.2,
                        112.6
                    ],
                    [
                        138.2,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76709,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10
        },
        "text": "10",
        "type": "Header"
    },
    {
        "element_id": "95b99e7a63f8aea112030258b0e24b99",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        984.9,
                        92.7
                    ],
                    [
                        984.9,
                        112.3
                    ],
                    [
                        1546.7,
                        112.3
                    ],
                    [
                        1546.7,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74565,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "99361a09b35b70047bc9289f5fcce238",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        180.8
                    ],
                    [
                        117.0,
                        341.3
                    ],
                    [
                        814.6,
                        341.3
                    ],
                    [
                        814.6,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94697,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "hard Atari games by combining DQN with the CTS density model (DQN-CTS) or the PixelCNN density model (DQN- PixelCNN) [8]. Interestingly, the results were less impres- sive when the CTS density model was combined with A3C (A3C-CTS) [8].",
        "type": "NarrativeText"
    },
    {
        "element_id": "87257cab0d90f2ac37d4378f9fceda2f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        346.9
                    ],
                    [
                        117.0,
                        739.9
                    ],
                    [
                        816.4,
                        739.9
                    ],
                    [
                        816.4,
                        346.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95479,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "Ape-X DQN is a distributed DQN architecture similar to Go- rila, as actors are separated from the learner. Ape-X DQN was able to reach state-of-the-art results across the 57 Atari games using 376 cores and one GPU, running at 50K FPS [61]. Deep Q-learning from demonstrations (DQfD) draws samples from an experience replay buffer that is initialized with demonstration data from a human expert and is superior to previous methods on 11 Atari games with sparse rewards [57]. Ape-X DQfD com- bines the distributed architecture from Ape-X, and the learning algorithm from DQfD using expert data and was shown to out- perform all previous methods in ALE, as well as beating level 1 in Montezuma\u2019s Revenge [112].",
        "type": "NarrativeText"
    },
    {
        "element_id": "19a4f50cc8378e75672951aa773f40e2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        180.8
                    ],
                    [
                        847.6,
                        274.9
                    ],
                    [
                        1547.1,
                        274.9
                    ],
                    [
                        1547.1,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93373,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "Deep DPG (DDPG) is a policy gradient method that implements both experience replay and a separate target network and was used to train a CNN end-to-end in TORCS from images [88].",
        "type": "NarrativeText"
    },
    {
        "element_id": "6e9c3d70f5621204d2dfe82c964ea842",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        280.5
                    ],
                    [
                        847.6,
                        474.2
                    ],
                    [
                        1550.2,
                        474.2
                    ],
                    [
                        1550.2,
                        280.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95324,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "The aforementioned A3C methods have also been applied to the racing game TORCS using only pixels as input [96]. In those experiments, rewards were shaped as the agent\u2019s velocity on the track, and after 12 h of training, A3C reached a score between roughly 75% and 90% of a human tester in tracks with and without opponent bots, respectively.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2a8e5b2b4797e25ea1522cb3fd46dee5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        479.7
                    ],
                    [
                        847.6,
                        706.7
                    ],
                    [
                        1550.0,
                        706.7
                    ],
                    [
                        1550.0,
                        479.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95502,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "While most approaches to training deep networks from high- dimensional input in video games are based on gradient descent, a notable exception is an approach by Koutn\u00b4\u0131k et al. [76], where Fourier-type coef\ufb01cients were evolved that encoded a recurrent network with over one million weights. Here, evolution was able to \ufb01nd a high-performing controller for TORCS that only relied on high-dimensional visual input.",
        "type": "NarrativeText"
    },
    {
        "element_id": "05334a04d77364edcc5afe1578e28c31",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        745.4
                    ],
                    [
                        117.0,
                        1071.9
                    ],
                    [
                        817.1,
                        1071.9
                    ],
                    [
                        817.1,
                        745.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95906,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "To improve the performance, Kaplan et al. augmented the agent training with text instructions. An instruction-based RL approach that uses both a CNN for visual input and the RNN for text-based instruction inputs managed to achieve a score of 3500 points. Instructions were linked to positions in rooms, and agents were rewarded when they reached those locations [71], demonstrating a fruitful collaboration between a human and a learning algorithm. Experiments in Montezuma\u2019s Revenge also showed that the network learned to generalize to unseen instructions that were similar to previous instructions.",
        "type": "NarrativeText"
    },
    {
        "element_id": "13488ce34239746f7beb5071c42289fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1077.5
                    ],
                    [
                        117.0,
                        1337.6
                    ],
                    [
                        815.3,
                        1337.6
                    ],
                    [
                        815.3,
                        1077.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95709,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "Similar work demonstrates how an agent can execute text- based commands in a 2-D maze-like environment called XWORLD, such as walking to and picking up objects, after having learned a teacher\u2019s language [172]. An RNN-based lan- guage module is connected to a CNN-based perception module. These two modules were then connected to an action-selection module and a recognition module that learns the teacher\u2019s lan- guage in a question answering process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5ad4f6244cba9ef8b2403ea685337bb2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1395.9
                    ],
                    [
                        117.0,
                        1423.5
                    ],
                    [
                        321.5,
                        1423.5
                    ],
                    [
                        321.5,
                        1395.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70485,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "C. Racing Games",
        "type": "Title"
    },
    {
        "element_id": "178c81f43f0dbaed6df7b6e03a07b0e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1442.9
                    ],
                    [
                        116.4,
                        1869.1
                    ],
                    [
                        817.1,
                        1869.1
                    ],
                    [
                        817.1,
                        1442.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95408,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "5ad4f6244cba9ef8b2403ea685337bb2"
        },
        "text": "There are generally two paradigms for vision-based au- tonomous driving highlighted in [21]: 1) end-to-end systems that learn to map images to actions directly (behavior re\ufb02ex); and 2) systems that parse the sensor data to make informed deci- sions (mediated perception). An approach that falls in between these paradigms is direct perception, where a CNN learns to map from images to meaningful affordance indicators, such as the car angle and distance to lane markings, from which a sim- ple controller can make decisions [21]. Direct perception was trained on recordings of 12 h of human driving in TORCS, and the trained system was able to drive in very diverse environ- ments. Amazingly, the network was also able to generalize to real images.",
        "type": "NarrativeText"
    },
    {
        "element_id": "86a17381046877d0b4ce0533f04335ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1874.6
                    ],
                    [
                        117.0,
                        2068.3
                    ],
                    [
                        816.1,
                        2068.3
                    ],
                    [
                        816.1,
                        1874.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94668,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "5ad4f6244cba9ef8b2403ea685337bb2"
        },
        "text": "End-to-end RL algorithms such as DQN cannot be directly applied to continuous environments such as racing games be- cause the action space must be discrete and with relatively low dimensionality. Instead, policy gradient methods, such as actor- critic [27] and deterministic policy gradient (DPG) [134] can learn policies in high-dimensional and continuous action spaces.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bcdadd73da0b31018e95b30155f327d1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        754.4
                    ],
                    [
                        847.6,
                        782.1
                    ],
                    [
                        1130.6,
                        782.1
                    ],
                    [
                        1130.6,
                        754.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.57663,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "D. First-Person Shooters",
        "type": "Title"
    },
    {
        "element_id": "6d726fc908495f4586f3d6a9f84d89c3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        801.4
                    ],
                    [
                        847.6,
                        1526.5
                    ],
                    [
                        1546.9,
                        1526.5
                    ],
                    [
                        1546.9,
                        801.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94717,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "bcdadd73da0b31018e95b30155f327d1"
        },
        "text": "Kempka et al. [73] demonstrated that a CNN with max- pooling and fully connected layers trained with DQN can achieve human-like behaviors in basic scenarios. In the Visual Doom AI Competition 2016,3 a number of participants submit- ted pretrained neural-network-based agents that competed in a multiplayer deathmatch setting. Both a limited competition, in which bots competed in known levels, and a full competition that included bots competing in unseen levels were held. The winner of the limited track used a CNN trained with A3C using reward shaping and curriculum learning [167]. Reward shaping tackled the problem of sparse and delayed rewards, giving arti\ufb01cial pos- itive rewards for picking up items and negative rewards for using ammunition and losing health. Curriculum learning attempts to speed up learning by training on a set of progressively harder environments [11]. The second-place entry in the limited track used a modi\ufb01ed DRQN network architecture with an additional stream of fully connected layers to learn supervised auxiliary tasks such as enemy detection, with the purpose of speeding up the training of the convolutional layers [79]. Position inference and object mapping from pixels and depth buffers using simul- taneous localization and mapping (SLAM) also improve DQN in Doom [14].",
        "type": "NarrativeText"
    },
    {
        "element_id": "1f43ee8ef882adad9a5edf7c13b4809e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1532.0
                    ],
                    [
                        847.6,
                        1958.2
                    ],
                    [
                        1551.5,
                        1958.2
                    ],
                    [
                        1551.5,
                        1532.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95295,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "bcdadd73da0b31018e95b30155f327d1"
        },
        "text": "The winner of the full deathmatch competition implemented a direct future prediction (DFP) approach that was shown to outperform DQN and A3C [28]. The architecture used in DFP has three streams: one for the screen pixels, one for lower di- mensional measurements describing the agent\u2019s current state, and one for describing the agent\u2019s goal, which is a linear com- bination of prioritized measurements. DFP collects experiences in a memory and is trained with supervised learning techniques to predict the future measurements based on the current state, goal, and selected action. During training, actions are selected that yield the best-predicted outcome, based on the current goal. This method can be trained on various goals and generalizes to unseen goals at test time.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f172ffb602c97695693718bb84f0b41c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        869.7,
                        2041.3
                    ],
                    [
                        869.7,
                        2067.1
                    ],
                    [
                        1325.9,
                        2067.1
                    ],
                    [
                        1325.9,
                        2041.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "95b99e7a63f8aea112030258b0e24b99"
        },
        "text": "3http://vizdoom.cs.put.edu.pl/competition-cig-2016",
        "type": "Title"
    },
    {
        "element_id": "e1feae7af67b779722dfdbb1afa01aa6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        111.2,
                        2123.6
                    ],
                    [
                        111.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74829,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 10,
            "parent_id": "f172ffb602c97695693718bb84f0b41c"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6b467fc131e93adf480d0f9eeed400a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.0,
                        92.7
                    ],
                    [
                        104.0,
                        112.0
                    ],
                    [
                        664.9,
                        112.0
                    ],
                    [
                        664.9,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71053,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "377a910215987195a926ad92df4fb5b1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.7,
                        180.8
                    ],
                    [
                        104.7,
                        342.2
                    ],
                    [
                        802.7,
                        342.2
                    ],
                    [
                        802.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95044,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "Navigation in 3-D environments is one of the important skills required for FPS games and has been studied extensively. A CNN long short-term memory (LSTM) network was trained with A3C extended with additional outputs predicting the pixel depths and loop closure, showing signi\ufb01cant improvements [95].",
        "type": "NarrativeText"
    },
    {
        "element_id": "3e0cae5d4ef97e7184064c36ff888ac9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        341.3
                    ],
                    [
                        1534.1,
                        341.3
                    ],
                    [
                        1534.1,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9464,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "map. Additionally, RTS games have no in-game scoring, and thus, the reward is determined by who wins the game. For these reasons, learning to play RTS games end-to-end may be infea- sible for the foreseeable future, and instead, subproblems have been studied so far.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ab5a836c47162e9f9cfa55408eb7f59f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.8,
                        346.9
                    ],
                    [
                        102.8,
                        540.6
                    ],
                    [
                        804.1,
                        540.6
                    ],
                    [
                        804.1,
                        346.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94903,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "The UNREAL algorithm, based on A3C, implements an aux- iliary task that trains the network to predict the immediate sub- sequent future reward from a sequence of consecutive observa- tions. UNREAL was tested on fruit gathering and exploration tasks in OpenArena and achieved a mean human-normalized score of 87%, where A3C only achieved 53% [63].",
        "type": "NarrativeText"
    },
    {
        "element_id": "ef873e46afaea9a94d9d42116a1568fc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.8,
                        546.1
                    ],
                    [
                        104.8,
                        872.7
                    ],
                    [
                        805.1,
                        872.7
                    ],
                    [
                        805.1,
                        546.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95676,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "The ability to transfer knowledge to new environments can re- duce the learning time and can in some cases be crucial for some challenging tasks. Transfer learning can be achieved by pretrain- ing a network in similar environments with simpler tasks or by using random textures during training [20]. The distill and trans- fer learning (Distral) method trains several worker policies (one for each task) concurrently and shares a distilled policy [149]. The worker policies are regularized to stay close to the shared policy, which will be the centroid of the worker policies. Distral was applied to DeepMind Lab.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0e1936c66b9c0fba7d633e2dd043f389",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        878.2
                    ],
                    [
                        105.3,
                        1038.7
                    ],
                    [
                        804.3,
                        1038.7
                    ],
                    [
                        804.3,
                        878.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94761,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "The intrinsic curiosity module, consisting of several neural networks, computes an intrinsic reward each time step based on the agent\u2019s inability to predict the outcome of taking actions. It was shown to learn to navigate in complex Doom and Super Mario levels only relying on intrinsic rewards [110].",
        "type": "NarrativeText"
    },
    {
        "element_id": "838c4ff6be37eec60f1c364947203802",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        346.7
                    ],
                    [
                        835.9,
                        507.4
                    ],
                    [
                        1535.5,
                        507.4
                    ],
                    [
                        1535.5,
                        346.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9446,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "For the simplistic RTS platform, \u03bcRTS, a CNN was trained as a state evaluator using supervised learning on a generated dataset and used in combination with Monte Carlo tree search [4], [136]. This approach performed signi\ufb01cantly better than previous evaluation methods.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8f0ba584de0ac24c0d117fc10018748d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        513.0
                    ],
                    [
                        835.9,
                        1038.8
                    ],
                    [
                        1536.4,
                        1038.8
                    ],
                    [
                        1536.4,
                        513.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94964,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "StarCraft has been a popular game platform for AI research, but so far only with a few DL approaches. DL methods for StarCraft have focused on micromanagement (unit control) or build-order planning and has ignored other aspects of the game. The problem of delayed rewards in StarCraft can be circum- vented in combat scenarios; here, rewards can be shaped as the difference between damage in\ufb02icted and damage incurred [32], [33], [111], [154]. States and actions are often described lo- cally relative to units, which is extracted from the game engine. If agents are trained individually, it is dif\ufb01cult to know which agents contributed to the global reward [19], a problem known as the multiagent credit assignment problem. One approach is to train a generic network, which controls each unit separately and search in policy space using zero-order optimization based on the reward accrued in each episode [154]. This strategy was able to learn successful policies for armies of up to 15 units.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e0b5e3d06757664c5d6820ebbed2cdc3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.5,
                        1080.3
                    ],
                    [
                        104.5,
                        1108.0
                    ],
                    [
                        365.8,
                        1108.0
                    ],
                    [
                        365.8,
                        1080.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66202,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "E. Open-World Games",
        "type": "Title"
    },
    {
        "element_id": "be71877c21c35714e6e3ed946a87e18a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1127.4
                    ],
                    [
                        105.3,
                        1354.3
                    ],
                    [
                        804.6,
                        1354.3
                    ],
                    [
                        804.6,
                        1127.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95482,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "The hierarchical deep reinforcement learning network (H- DRLN) architecture implements a lifelong learning framework, which is shown to be able to transfer knowledge between sim- ple tasks in Minecraft such as navigation, item collection, and placement tasks [150]. The H-DRLN uses a variation of policy distillation [119] to retain and encapsulate learned knowledge into a single network.",
        "type": "NarrativeText"
    },
    {
        "element_id": "aa47265e58c8e08ab3c39a0abac9fa87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1359.8
                    ],
                    [
                        105.3,
                        1620.0
                    ],
                    [
                        804.5,
                        1620.0
                    ],
                    [
                        804.5,
                        1359.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95225,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "Neural turing machines (NTMs) are fully differentiable neu- ral networks coupled with an external memory resource, which can learn to solve simple algorithmic problems such as copy- ing and sorting [40]. Two memory-based variations, inspired by NTM, called recurrent memory Q-network (RMQN) and feedback recurrent memory Q-network (FRMQN), were able to solve complex navigation tasks that require memory and active perception [102].",
        "type": "NarrativeText"
    },
    {
        "element_id": "a9a4d5fe401545f1bd6412a6bfeff9ca",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1044.3
                    ],
                    [
                        835.9,
                        1404.1
                    ],
                    [
                        1534.5,
                        1404.1
                    ],
                    [
                        1534.5,
                        1044.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95408,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "Independent Q-learning (IQL) simpli\ufb01es the multiagent RL problem by controlling units individually while treating other agents as if they were part of the environment [147]. This en- ables Q-learning to scale well to a large number of agents. However, when combining IQL with recent techniques such as experience replay, agents tend to optimize their policies based on experiences with obsolete policies. This problem is over- come by applying \ufb01ngerprints to experiences and by applying an importance-weighted loss function that naturally decays ob- solete data, which has shown improvements for some small combat scenarios [33].",
        "type": "NarrativeText"
    },
    {
        "element_id": "9cce15b15a8ec2d1025cb23c371cbed9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1409.6
                    ],
                    [
                        835.9,
                        1603.5
                    ],
                    [
                        1534.7,
                        1603.5
                    ],
                    [
                        1534.7,
                        1409.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.955,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "The multiagent bidirectionally-coordinated network (BiC- Net) implements a vectorized actor-critic framework based on a bidirectional RNN, with one dimension for every agent, and outputs a sequence of actions [111]. This network architecture is unique to the other approaches, as it can handle an arbitrary number of units of different types.",
        "type": "NarrativeText"
    },
    {
        "element_id": "91019cdb3eca721d812d944e0bf7be35",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1625.5
                    ],
                    [
                        105.3,
                        1819.2
                    ],
                    [
                        804.7,
                        1819.2
                    ],
                    [
                        804.7,
                        1625.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95482,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "The teacher\u2013student curriculum learning (TSCL) framework incorporates a teacher that prioritizes tasks, wherein the stu- dent\u2019s performance is either increasing (learning) or decreas- ing (forgetting) [92]. TSCL enabled a policy gradient learning method to solve mazes that were otherwise not possible with a uniform sampling of subtasks.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5da454bf651d58cc56d92a57ba457caa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1608.8
                    ],
                    [
                        835.9,
                        1835.8
                    ],
                    [
                        1536.6,
                        1835.8
                    ],
                    [
                        1536.6,
                        1608.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95513,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "e0b5e3d06757664c5d6820ebbed2cdc3"
        },
        "text": "Counterfactual multiagent (COMA) policy gradients is an actor-critic method with a centralized critic and decentralized actors that address the multiagent credit assignment problem with a counterfactual baseline computed by the critic network [32]. COMA achieves state-of-the-art results, for decentralized methods, in small combat scenarios with up to ten units on each side.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cc86907296dfc7ac1a074af95374821b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.1,
                        1860.8
                    ],
                    [
                        105.1,
                        1888.5
                    ],
                    [
                        270.6,
                        1888.5
                    ],
                    [
                        270.6,
                        1860.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77906,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "6b467fc131e93adf480d0f9eeed400a2"
        },
        "text": "F. RTS Games",
        "type": "Title"
    },
    {
        "element_id": "bd46c9a3bde8aeec61001a2b2247b028",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.1,
                        1907.8
                    ],
                    [
                        102.1,
                        2069.4
                    ],
                    [
                        807.0,
                        2069.4
                    ],
                    [
                        807.0,
                        1907.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94496,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "cc86907296dfc7ac1a074af95374821b"
        },
        "text": "The previous sections described methods that learn to play games end-to-end, i.e., a neural network is trained to map states directly to actions. RTS games, however, offer much more com- plex environments, in which players have to control multiple agents simultaneously in real time on a partially observable",
        "type": "NarrativeText"
    },
    {
        "element_id": "97a3f44d2326893b63fb301c727be38a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1841.3
                    ],
                    [
                        835.9,
                        2068.2
                    ],
                    [
                        1534.9,
                        2068.2
                    ],
                    [
                        1534.9,
                        1841.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94908,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "cc86907296dfc7ac1a074af95374821b"
        },
        "text": "DL has also been applied to build-order planning in StarCraft using macro-based supervised learning approach to imitate hu- man strategies [68]. The trained network was integrated as a module used in an existing bot capable of playing the full game with an otherwise hand-crafted behavior. Another macro-based approach, here using RL instead of SL, called convolutional neural-network-\ufb01tted Q-learning (CNNFQ), was trained with",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f5ecb49f144d04d975774de70f7c580",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        112.3,
                        2123.7
                    ],
                    [
                        112.3,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75792,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11,
            "parent_id": "cc86907296dfc7ac1a074af95374821b"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "665af6077227649d770a6e9b6c6cebbf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1513.4,
                        92.7
                    ],
                    [
                        1513.4,
                        113.4
                    ],
                    [
                        1533.9,
                        113.4
                    ],
                    [
                        1533.9,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79909,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 11
        },
        "text": "11",
        "type": "Header"
    },
    {
        "element_id": "0e4cb47673f361139557a22d85b8e2e2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        92.6
                    ],
                    [
                        117.0,
                        113.3
                    ],
                    [
                        137.2,
                        113.3
                    ],
                    [
                        137.2,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78224,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12
        },
        "text": "12",
        "type": "Header"
    },
    {
        "element_id": "771204993d9344d3fd903a8894fbdf11",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        983.9,
                        92.7
                    ],
                    [
                        983.9,
                        112.5
                    ],
                    [
                        1546.0,
                        112.5
                    ],
                    [
                        1546.0,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63347,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "140a602c9eb99f13f5be23b9ee335ab7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        115.8,
                        180.8
                    ],
                    [
                        115.8,
                        607.3
                    ],
                    [
                        817.7,
                        607.3
                    ],
                    [
                        817.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95423,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "double DQN for build-order planning in StarCraft II and was able to win against medium-level scripted bots on small maps [148]. A macro action-based RL method that uses PPO for build-order planning and high-level attack planning was able to outperform the built-in bot in StarCraft II at level 10 [141]. This is particularly impressive as the level-10 bot cheats by having full vision of the map and faster resource harvesting. The re- sults were obtained using 1920 parallel actors on 3840 CPUs across 80 machines and only for one matchup on one map. This system won a few games against platinum-level human players but lost all games against diamond-level players. The authors report that the learned policy \u201clacks strategy diversity in order to consistently beat human players\u201d [141].",
        "type": "NarrativeText"
    },
    {
        "element_id": "12bd26fe3f8ee562e052aa22a6440edf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        180.8
                    ],
                    [
                        847.6,
                        574.3
                    ],
                    [
                        1549.6,
                        574.3
                    ],
                    [
                        1549.6,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95193,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "RL with explicit language understanding is deep reinforcement relevance net (DRRN) [54]. This approach has two networks that learn word embeddings. One embeds the state description, and the other embeds the action description. Relevance between the two embedding vectors is calculated with an interaction function such as the inner product of the vectors or a bilinear operation. The relevance is then used as the Q-value, and the whole process is trained end-to-end with deep Q-learning. This approach allows the network to generalize to phrases not seen during training, which is an improvement for very large text games. The approach was tested on the text games Saving John and Machine of Death, both choice-based games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "130c1a4b7a0f4bd2c73706bd912b70a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        654.1
                    ],
                    [
                        117.0,
                        681.8
                    ],
                    [
                        379.5,
                        681.8
                    ],
                    [
                        379.5,
                        654.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.57809,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "G. Team Sports Games",
        "type": "Title"
    },
    {
        "element_id": "168eb17ff92f7c6b9e81c4685ec128d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        701.2
                    ],
                    [
                        117.0,
                        1094.5
                    ],
                    [
                        816.3,
                        1094.5
                    ],
                    [
                        816.3,
                        701.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95571,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "130c1a4b7a0f4bd2c73706bd912b70a3"
        },
        "text": "DDPG was applied to RoboCup 2-D HFO [51]. The actor network used two output streams: one for the selection of dis- crete action types (dash, turn, tackle, and kick) and one for each action type\u2019s 1\u20132 continuously valued parameters (power and direction). The inverting gradients bounding approach down- scales the gradients as the output approaches its boundaries and inverts the gradients if the parameter exceeds them. This ap- proach outperformed both SARSA and the best agent in the 2012 RoboCup. DDPG was also applied to HFO by mixing on-policy updates with one-step Q-learning updates [53] and outperformed a hand-coded agent with expert knowledge with one player on each team.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3b9010115674ad8dfca0b60e0ad0ed5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.1,
                        1141.2
                    ],
                    [
                        116.1,
                        1168.9
                    ],
                    [
                        328.6,
                        1168.9
                    ],
                    [
                        328.6,
                        1141.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77423,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "H. Physics Games",
        "type": "Title"
    },
    {
        "element_id": "798d96d840c3474a1213379b7fe957d1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1188.3
                    ],
                    [
                        117.0,
                        1415.2
                    ],
                    [
                        815.9,
                        1415.2
                    ],
                    [
                        815.9,
                        1188.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95572,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "3b9010115674ad8dfca0b60e0ad0ed5f"
        },
        "text": "As video games are usually a re\ufb02ection or simpli\ufb01cation of the real world, it can be fruitful to learn an intuition about the physical laws in an environment. A predictive neural network using an object-centered approach (also called \ufb01xations) learned to run simulations of a billiards game after being trained on random interactions [36]. This predictive model could then be used for planning actions in the game.",
        "type": "NarrativeText"
    },
    {
        "element_id": "92dd1f5ad58d216526fe02d7d2fbf35e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1420.7
                    ],
                    [
                        117.0,
                        1647.7
                    ],
                    [
                        814.9,
                        1647.7
                    ],
                    [
                        814.9,
                        1420.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95254,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "3b9010115674ad8dfca0b60e0ad0ed5f"
        },
        "text": "A similar predictive approach was tested in a 3-D game-like environment, using the Unreal Engine, where ResNet-34 [55] (a deep residual network used for image classi\ufb01cation) was extended and trained to predict the visual outcome of blocks that were stacked such that they would usually fall [86]. Resid- ual networks implement shortcut connections that skip layers, which can improve learning in very deep networks.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3d63e96bb001fc713defcc8f41bf3fe3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        579.4
                    ],
                    [
                        847.6,
                        939.1
                    ],
                    [
                        1550.7,
                        939.1
                    ],
                    [
                        1550.7,
                        579.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95533,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "3b9010115674ad8dfca0b60e0ad0ed5f"
        },
        "text": "Taking language modeling further, Fulda et al. explicitly mod- eled language affordances to assist in action selection [37]. A word embedding is \ufb01rst learned from a Wikipedia Corpus via unsupervised learning [94], and this embedding is then used to calculate analogies such as song is to sing as bike is to x, where x can then be calculated in the embedding space [94]. The authors build a dictionary of verbs, noun pairs, and another one of object manipulation pairs. Using the learned affordances, the model can suggest a small set of actions for a state descrip- tion. Policies were learned with Q-learning and tested on 50 Z-machine games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8988978ab819462e2b7080fe8108bef2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        944.7
                    ],
                    [
                        847.6,
                        1238.0
                    ],
                    [
                        1548.6,
                        1238.0
                    ],
                    [
                        1548.6,
                        944.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95472,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "3b9010115674ad8dfca0b60e0ad0ed5f"
        },
        "text": "The Golovin Agent focuses exclusively on language models [75] that are pretrained from a corpus of books in the fantasy genre. Using word embeddings, the agent can replace synonyms with known words. Golovin is built of \ufb01ve command genera- tors: general, movement, battle, gather, and inventory. These are generated by analyzing the state description, using the language models to calculate and sample from a number of features for each command. Golovin uses no RL and scores comparable to the affordance method.",
        "type": "NarrativeText"
    },
    {
        "element_id": "74ca79c4b5cd6518303b5538d6da1456",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1243.5
                    ],
                    [
                        847.6,
                        1636.5
                    ],
                    [
                        1551.3,
                        1636.5
                    ],
                    [
                        1551.3,
                        1243.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9524,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "3b9010115674ad8dfca0b60e0ad0ed5f"
        },
        "text": "Most recently, Zahavy et al. have proposed another DQN method [173]. This method uses a type of attention mecha- nism called action elimination network (AEN). In parser-based games, the action space is very large. The AEN learns, while playing, to predict which actions that will have no effect for a given state description. The AEN is then used to eliminate most of the available actions for a given state and after which the remaining actions are evaluated with the Q-network. The whole process is trained end-to-end and achieves similar performance to DQN with a manually constrained actions space. Despite the progress made for text adventure games, current techniques are still far from matching human performance.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d2b409323fcfd4657f74d7c044cad8bf",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        113.6,
                        1694.7
                    ],
                    [
                        113.6,
                        1722.4
                    ],
                    [
                        396.3,
                        1722.4
                    ],
                    [
                        396.3,
                        1694.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74002,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "I. Text Adventure Games",
        "type": "Title"
    },
    {
        "element_id": "81a7dc46bd901ac3e52c0a0d06d71a5e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1741.8
                    ],
                    [
                        117.0,
                        2001.9
                    ],
                    [
                        815.4,
                        2001.9
                    ],
                    [
                        815.4,
                        1741.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95659,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "d2b409323fcfd4657f74d7c044cad8bf"
        },
        "text": "Text adventure games, in which both states and actions are presented as text only, are a special video game genre. A network architecture called LSTM-DQN [101] was designed speci\ufb01cally to play these games and is implemented using LSTM networks that convert text from the world state into a vector representation, which estimates Q-values for all possible state\u2013action pairs. LSTM-DQN was able to complete between 96% and 100% of the quests on average in two different text adventure games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a6ec9e89b38e7ef13a45514f5693cfba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1642.0
                    ],
                    [
                        847.6,
                        1935.3
                    ],
                    [
                        1549.1,
                        1935.3
                    ],
                    [
                        1549.1,
                        1642.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95501,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "d2b409323fcfd4657f74d7c044cad8bf"
        },
        "text": "Outside of text adventure games, natural language process- ing has been used for other text-based games as well. To fa- cilitate communication, a deep distributed recurrent Q-network (DDRQN) architecture was used to train several agents to learn a communication protocol to solve the multiagent Hats and Switch riddles [34]. One of the novel modi\ufb01cations in DDRQN is that agents use shared network weights that are conditioned on their unique ID, which enables faster learning while retaining diversity between agents.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b6e9fcafb7f0873c02882a15bf204de3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        2007.5
                    ],
                    [
                        117.0,
                        2101.6
                    ],
                    [
                        814.3,
                        2101.6
                    ],
                    [
                        814.3,
                        2007.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89863,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "d2b409323fcfd4657f74d7c044cad8bf"
        },
        "text": "To be able to improve on these results, researchers have moved toward learning language models and word embeddings to augment the neural network. An approach that combines",
        "type": "NarrativeText"
    },
    {
        "element_id": "af6cb361d4d38944d4d489a533656b7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        947.5,
                        1993.6
                    ],
                    [
                        947.5,
                        2021.3
                    ],
                    [
                        1445.4,
                        2021.3
                    ],
                    [
                        1445.4,
                        1993.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62956,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "771204993d9344d3fd903a8894fbdf11"
        },
        "text": "V. HISTORICAL OVERVIEW OF DL IN GAMES",
        "type": "Title"
    },
    {
        "element_id": "0e181baa814672d1769b71b1f8881cbc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        2040.6
                    ],
                    [
                        847.6,
                        2101.5
                    ],
                    [
                        1547.8,
                        2101.5
                    ],
                    [
                        1547.8,
                        2040.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87642,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "af6cb361d4d38944d4d489a533656b7e"
        },
        "text": "The previous section discussed DL methods in games ac- cording to the game type. This section instead looks at the",
        "type": "NarrativeText"
    },
    {
        "element_id": "a8298c5094a7ca438cb0f08ea149a259",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2143.2
                    ],
                    [
                        1530.2,
                        2143.2
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 12,
            "parent_id": "af6cb361d4d38944d4d489a533656b7e"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0e359d5f4ac7150f855963ba1a182aa6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.7,
                        92.7
                    ],
                    [
                        104.7,
                        112.0
                    ],
                    [
                        664.9,
                        112.0
                    ],
                    [
                        664.9,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70438,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "e886c979589d349e1fb366a5f993614c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        180.8
                    ],
                    [
                        105.3,
                        374.5
                    ],
                    [
                        802.7,
                        374.5
                    ],
                    [
                        802.7,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95843,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "development of these methods in terms of how they in\ufb02uenced each other, giving a historical overview of the DL methods that are reviewed in the previous section. Many of these methods are inspired from or directly build upon previous methods, while some are applied to different game genres and others are tai- lored to speci\ufb01c types of games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "57961129e4e926032549ca4f6e1a49d4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        380.1
                    ],
                    [
                        105.3,
                        1038.7
                    ],
                    [
                        806.9,
                        1038.7
                    ],
                    [
                        806.9,
                        380.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94465,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Fig. 4 shows an in\ufb02uence diagram with the reviewed meth- ods and their relations to earlier methods (the current section can be read as a long caption to that \ufb01gure). Each method in the diagram is colored to show the game benchmark. DQN [97] was very in\ufb02uential as an algorithm that uses gradient-based DL for pixel-based video game playing and was originally applied to the Atari benchmark. Note that earlier approaches exist but with less success such as [109], and successful gradient-free methods [115]. Double DQN [155] and dueling DQN [161] are early extensions that use multiple networks to improve estima- tions. DRQN [51] uses an RNN as the Q-network. Prioritized DQN [123] is another early extension, and it adds improved ex- perience replay sampling. Bootstrapped DQN [106] builds off of double DQN with a different improved sampling strategy. Further DQN enhancements used for Atari include: the C51 al- gorithm [9], which is based on DQN but changes the Q function; Noisy-Nets which make the networks stochastic to aid with ex- ploration [35]; DQfD which also learns from examples [57]; and Rainbow, which combines many of these state-of-the-art techniques together [56].",
        "type": "NarrativeText"
    },
    {
        "element_id": "374a1fe4adb05cdafba4f3b4e9c93ca8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1044.3
                    ],
                    [
                        105.3,
                        1404.0
                    ],
                    [
                        804.2,
                        1404.0
                    ],
                    [
                        804.2,
                        1044.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95379,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Gorila was the \ufb01rst asynchronous method based on DQN [100] and was followed by A3C [96], which uses multiple asyn- chronous agents for an actor-critic approach. This was further extended at the end of 2016 with UNREAL [63], which in- corporates work done with auxiliary learning to handle sparse feedback environments. Since then, there has been a lot of ad- ditional extensions on A3C [35], [120], [160], [166]. IMPALA has taken it further with focusing on a single trained agent that can play all of the Atari games [30]. In 2018, the move toward large-scale distributed learning has continued and advanced with Ape-X [61], [112].",
        "type": "NarrativeText"
    },
    {
        "element_id": "4ece86da93275ffe82f846e0cf61b5b3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1409.6
                    ],
                    [
                        105.3,
                        1636.7
                    ],
                    [
                        804.8,
                        1636.7
                    ],
                    [
                        804.8,
                        1409.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95364,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Evolutionary techniques are also seeing a Renaissance for video games. First, Salimans et al. showed that ESs could com- pete with deep RL [121]. Then, two more papers came out of Uber AI: one showing that derivative-free evolutionary algo- rithms can compete with deep RL [139], and an extension to ES [24]. These bene\ufb01t from easy parallelization and possibly have some advantage in exploration.",
        "type": "NarrativeText"
    },
    {
        "element_id": "63058aef70918142a51fa1d61ef0e2a9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1642.0
                    ],
                    [
                        105.3,
                        2068.2
                    ],
                    [
                        805.5,
                        2068.2
                    ],
                    [
                        805.5,
                        1642.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95229,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Another approach used on Atari around the time DQN was introduced is TRPO [77]. This updates a surrogate objective that is updated from the environment. Later, in 2017, PPO was introduced as a more robust and simpler surrogate optimization scheme that also draws from innovations in A3C [129]. Some extensions are speci\ufb01cally for Montezuma\u2019s revenge, which is a game within the ALE benchmark, but it is particularly dif\ufb01cult due to sparse rewards and hidden information. The algorithms that do best on Montezuma do so by extending DQN with intrin- sic motivation [8] and hierarchical learning [77]. Ms. Pac-Man was also singled out from Atari, where the reward function was learned in separate parts to make the agent more robust to new environments [156].",
        "type": "NarrativeText"
    },
    {
        "element_id": "1526194ee7128ba7c0060f3254cf43b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.8
                    ],
                    [
                        835.9,
                        407.8
                    ],
                    [
                        1533.8,
                        407.8
                    ],
                    [
                        1533.8,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95477,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Doom is another benchmark that is new as of 2016. Most of the work for this game has been extending methods designed for Atari to handle richer data. A3C + curriculum learning [167] proposes using curriculum learning with A3C. DRQN + auxil- iary learning [79] extends DRQN by adding additional rewards during training. DQN + SLAM [14] combines techniques for mapping unknown environments with DQN.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7ec966b81d4858fa3be7c8d06ac96fd6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        413.3
                    ],
                    [
                        835.9,
                        773.3
                    ],
                    [
                        1536.1,
                        773.3
                    ],
                    [
                        1536.1,
                        413.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95221,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "DFP [28] is the only approach that is not extending an Atari technique. Like UCT To Classi\ufb01cation [42] for Atari, Object- centric Prediction [36] for Billiard, and Direct Perception [21] for Racing, DFP uses supervised learning to learn about the game. All of these, except UCT To Classi\ufb01cation, learn to di- rectly predict some future state of the game and make a pre- diction from this information. None of these works, all from different years, refer to each other. Besides Direct Perception, the only unique work for racing is DDPG [88], which extends DQN for continuous controls. This technique has been extended for RoboCup Soccer [52] [53].",
        "type": "NarrativeText"
    },
    {
        "element_id": "928b8861b4840e3939d5d1a06b746579",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        778.6
                    ],
                    [
                        835.9,
                        1105.2
                    ],
                    [
                        1536.5,
                        1105.2
                    ],
                    [
                        1536.5,
                        778.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95303,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Work on StarCraft micromanagement (unit control) is based on Q-learning started in late 2016. IQL [33] extends DQN- Prioritized DQN by treating all other agents as part of the environment. COMA [32] extends IQL by calculating coun- terfactual rewards, the marginal contribution each agent added. BiCNet [111] and zero-order optimization [154] are RL based but are not derived from DQN. Another popular approach is hierarchical learning. In 2017, it was tried with replay data [68], and in 2018, state-of-the-art results were achieved by using it with two different RL methods [141], [148].",
        "type": "NarrativeText"
    },
    {
        "element_id": "d2a4ada1e1eccb872134e0399360aa95",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1110.7
                    ],
                    [
                        835.9,
                        1271.2
                    ],
                    [
                        1535.2,
                        1271.2
                    ],
                    [
                        1535.2,
                        1110.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94709,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Some work published in 2016 extends DQN to play Minecraft [150]. At around the same time, techniques were developed to make DQN context-aware and modular to handle the large state space [102]. Recently, curriculum learning has been applied to Minecraft as well [92].",
        "type": "NarrativeText"
    },
    {
        "element_id": "d4ecf81ab6e582412851c36ae9b65af8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1276.8
                    ],
                    [
                        835.9,
                        1536.9
                    ],
                    [
                        1534.0,
                        1536.9
                    ],
                    [
                        1534.0,
                        1276.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9505,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "DQN was applied to text adventure games in 2015 [101]. Soon after, it was modi\ufb01ed to have a language-speci\ufb01c architec- ture and use the state\u2013action pair relevance as the Q-value [54]. Most of the work on these games has been focused on explicit language modeling. Golovin agent and affordance-based action selection both use neural networks to learn language models, which provide the actions for the agents to play [37], [75]. Re- cently, in 2018, DQN was used again paired with an AEN [173].",
        "type": "NarrativeText"
    },
    {
        "element_id": "598e5d9f4dd5e1e949ec63c1bc86a6da",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.7,
                        1542.4
                    ],
                    [
                        835.7,
                        1869.0
                    ],
                    [
                        1535.9,
                        1869.0
                    ],
                    [
                        1535.9,
                        1542.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95264,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "Combining extensions from previous algorithms have proven to be a promising direction for DL applied to video games, with Atari being the most popular benchmark for RL. Another clear trend, which is apparent in Table II, is the focus on paralleliza- tion: distributing the work among multiple CPUs and GPUs. Par- allelization is most common with actor-critic methods, such as A2C and A3C, and evolutionary approaches, such as Deep GA [139] and ESs [24], [121]. Hierarchical RL, intrinsic motivation, and transfer learning are promising new directions to explore to master currently unsolved problems in video game playing.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d59c9ec9ef87d700681cb4c52d6d1d5f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1049.8,
                        1927.2
                    ],
                    [
                        1049.8,
                        1954.9
                    ],
                    [
                        1319.3,
                        1954.9
                    ],
                    [
                        1319.3,
                        1927.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80891,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "0e359d5f4ac7150f855963ba1a182aa6"
        },
        "text": "VI. OPEN CHALLENGES",
        "type": "Title"
    },
    {
        "element_id": "81d898704a848a662b66144f4ab69b31",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1974.2
                    ],
                    [
                        835.9,
                        2068.7
                    ],
                    [
                        1533.3,
                        2068.7
                    ],
                    [
                        1533.3,
                        1974.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93263,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "d59c9ec9ef87d700681cb4c52d6d1d5f"
        },
        "text": "While DL has shown remarkable results in video game play- ing, a multitude of important open challenges remain, which we review here. Indeed, looking back at the current state of research",
        "type": "NarrativeText"
    },
    {
        "element_id": "ad3d248307108ce38115043dbfc80484",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        118.6,
                        2122.7
                    ],
                    [
                        118.6,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2122.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78333,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13,
            "parent_id": "d59c9ec9ef87d700681cb4c52d6d1d5f"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "33eb07c98ea18d6091bde0482764453d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1513.5,
                        92.7
                    ],
                    [
                        1513.5,
                        113.3
                    ],
                    [
                        1534.4,
                        113.3
                    ],
                    [
                        1534.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79151,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 13
        },
        "text": "13",
        "type": "Header"
    },
    {
        "element_id": "8635f136f8b95ea2140f1f55ab6ba032",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.9,
                        92.6
                    ],
                    [
                        116.9,
                        112.6
                    ],
                    [
                        136.7,
                        112.6
                    ],
                    [
                        136.7,
                        92.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76395,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "14",
        "type": "Header"
    },
    {
        "element_id": "9620b326a2be086bdcfb71d22e66dc33",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        983.9,
                        92.7
                    ],
                    [
                        983.9,
                        112.1
                    ],
                    [
                        1545.7,
                        112.1
                    ],
                    [
                        1545.7,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81868,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "dcb6d036a32d1bc06923d14ecb1086a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        175.0,
                        174.2
                    ],
                    [
                        175.0,
                        1672.5
                    ],
                    [
                        1447.6,
                        1672.5
                    ],
                    [
                        1447.6,
                        174.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85756,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "image_path": "/home/msunkur/dev/projects/uol/Module5/midterm/CM3020_Artificial_Intelligence/parta/docs/tmp/tmp_ingest/output/figure-14-4.jpg",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "2018 Golovin 2017 Affordance AE-DON Based Action Selection DDPG + Inverse Gradients DDPG DQN-CTS LSTM-DQN DDPG+ Mixing Policy Targets A3C + Curriculum Learning Scalable Evolutionary Strategies DRQN + Auxiliary Learning",
        "type": "Image"
    },
    {
        "element_id": "fc34f3e23e522247a35605d29bcca43b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        180.0,
                        1669.0
                    ],
                    [
                        180.0,
                        1687.0
                    ],
                    [
                        315.0,
                        1687.0
                    ],
                    [
                        315.0,
                        1669.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "Ms Pac-Man",
        "type": "Title"
    },
    {
        "element_id": "9322cd9fb838d53fedc19f5ca2f95b2e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        351.0,
                        1664.0
                    ],
                    [
                        351.0,
                        1721.0
                    ],
                    [
                        576.0,
                        1721.0
                    ],
                    [
                        576.0,
                        1664.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "Atari + Montezuma\u2019s Revenge",
        "type": "Title"
    },
    {
        "element_id": "6e8caf7ff5296985ac09ad75c1405831",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        597.0,
                        1669.0
                    ],
                    [
                        597.0,
                        1691.0
                    ],
                    [
                        669.0,
                        1691.0
                    ],
                    [
                        669.0,
                        1669.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "Racing",
        "type": "Title"
    },
    {
        "element_id": "69cffcbb27df46d33b6ed47a4cc6af5a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        730.0,
                        1664.0
                    ],
                    [
                        730.0,
                        1696.0
                    ],
                    [
                        940.0,
                        1696.0
                    ],
                    [
                        940.0,
                        1664.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "Doom O Minecraft",
        "type": "Title"
    },
    {
        "element_id": "3cbfc62a1781829a8c32c076e8640c47",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        971.0,
                        1668.0
                    ],
                    [
                        971.0,
                        1687.0
                    ],
                    [
                        1067.0,
                        1687.0
                    ],
                    [
                        1067.0,
                        1668.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "StarCraft",
        "type": "Title"
    },
    {
        "element_id": "c33fb180b3c622191223d3f28d4fa881",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1093.0,
                        1669.0
                    ],
                    [
                        1093.0,
                        1687.0
                    ],
                    [
                        1201.0,
                        1687.0
                    ],
                    [
                        1201.0,
                        1669.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "2D Billiard",
        "type": "Title"
    },
    {
        "element_id": "7812ed6643115abf98250867c576e310",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1220.0,
                        1664.0
                    ],
                    [
                        1220.0,
                        1716.0
                    ],
                    [
                        1330.0,
                        1716.0
                    ],
                    [
                        1330.0,
                        1664.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "Text Adventure",
        "type": "Title"
    },
    {
        "element_id": "7c5f0e74fc492f0db4fcd4ca0bfb83a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1354.0,
                        1668.0
                    ],
                    [
                        1354.0,
                        1691.0
                    ],
                    [
                        1455.0,
                        1691.0
                    ],
                    [
                        1455.0,
                        1668.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "RoboCup",
        "type": "Title"
    },
    {
        "element_id": "61a08c40d000bada2e7a1566b0c9d074",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1753.3
                    ],
                    [
                        116.4,
                        1850.5
                    ],
                    [
                        1545.0,
                        1850.5
                    ],
                    [
                        1545.0,
                        1753.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88342,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14,
            "parent_id": "7c5f0e74fc492f0db4fcd4ca0bfb83a7"
        },
        "text": "Fig. 4. In\ufb02uence diagram of the DL techniques discussed in this paper. Each node is an algorithm while the color represents the game benchmark. The distance from the center represents the date that the original paper was published on arXiv. The arrows represent how techniques are related. Each node points to all other nodes that used or modi\ufb01ed that technique. Arrows pointing to a particular algorithm show which algorithms in\ufb02uenced its design. In\ufb02uences are not transitive: if algorithm a in\ufb02uenced b and b in\ufb02uenced c, a did not necessarily in\ufb02uence c.",
        "type": "FigureCaption"
    },
    {
        "element_id": "446918334d6d2e93538f568e988ede9e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1910.0
                    ],
                    [
                        117.0,
                        2037.3
                    ],
                    [
                        814.4,
                        2037.3
                    ],
                    [
                        814.4,
                        1910.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93224,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14,
            "parent_id": "7c5f0e74fc492f0db4fcd4ca0bfb83a7"
        },
        "text": "from a decade or two in the future, it is likely that we will see the current research as early steps in a broad and important research \ufb01eld. This section is divided into four broad categories (agent model properties, game industry, learning models of games,",
        "type": "NarrativeText"
    },
    {
        "element_id": "86fb9028bec1edeede55514c81206ef5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        830.0,
                        1941.0
                    ],
                    [
                        830.0,
                        1974.0
                    ],
                    [
                        839.0,
                        1974.0
                    ],
                    [
                        839.0,
                        1941.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14
        },
        "text": "o",
        "type": "Title"
    },
    {
        "element_id": "443cae6ba840b5eb7ef09d020aba3c7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1910.0
                    ],
                    [
                        847.6,
                        2037.3
                    ],
                    [
                        1546.5,
                        2037.3
                    ],
                    [
                        1546.5,
                        1910.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93521,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14,
            "parent_id": "86fb9028bec1edeede55514c81206ef5"
        },
        "text": "and computational resources) with different game-playing chal- lenges that remain open for DL techniques. We mention a few potential approaches for some of the challenges, while the best way forward for others is currently not clear.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d610ee7ef422a19a177ea9aa1cf9a91f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80357,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 14,
            "parent_id": "86fb9028bec1edeede55514c81206ef5"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b4cf0207753cf1ccf0b6d18c8fe8cbf6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.3,
                        92.7
                    ],
                    [
                        104.3,
                        112.0
                    ],
                    [
                        665.0,
                        112.0
                    ],
                    [
                        665.0,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71145,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "88ba20ca2ba7a110bd98215da508ead4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        102.3,
                        180.8
                    ],
                    [
                        102.3,
                        208.5
                    ],
                    [
                        409.0,
                        208.5
                    ],
                    [
                        409.0,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75409,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "b4cf0207753cf1ccf0b6d18c8fe8cbf6"
        },
        "text": "A. Agent Model Properties",
        "type": "Title"
    },
    {
        "element_id": "281bed14db0b375ba84cc15d33d2b0f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        230.6
                    ],
                    [
                        105.3,
                        623.6
                    ],
                    [
                        802.7,
                        623.6
                    ],
                    [
                        802.7,
                        230.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95541,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "1) General Video Game Playing: Being able to solve a single problem does not make you intelligent; nobody would say that Deep Blue or AlphaGo [133] possess general intelligence, as they cannot even play Checkers (without retraining), much less make coffee or tie their shoelaces. To learn generally intelligent behavior, you need to train on not just a single task, but many different tasks [83]. Video games have been suggested as ideal environments for learning general intelligence, partly because there are so many video games that share common interface and reward conventions [124]. Yet, the vast majority of work on DL in video games focuses on learning to play a single game or even performing a single task in a single game.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d172af463bafab4acfe755233de3ef19",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        631.9
                    ],
                    [
                        105.3,
                        825.6
                    ],
                    [
                        805.1,
                        825.6
                    ],
                    [
                        805.1,
                        631.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95133,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "While deep RL-based approaches can learn to play a variety of different Atari games, it is still a signi\ufb01cant challenge to develop algorithms that can learn to play any kind of game (e.g., Atari games, DOOM, and StarCraft). Current approaches still require signi\ufb01cant effort to design the network architecture and reward function to a speci\ufb01c type of game.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ea9360f6f7a80d5a5bfe927c28a9ed48",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        833.9
                    ],
                    [
                        105.3,
                        1226.9
                    ],
                    [
                        806.7,
                        1226.9
                    ],
                    [
                        806.7,
                        833.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95357,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "Progress on the problem of playing multiple games includes progressive neural networks [120], which allow new games to be learned (without forgetting previously learned ones) and solved quicker by exploiting previously learned features through lat- eral connections. However, they require a separate network for each task. Elastic weight consolidation [74] can learn multi- ple Atari games sequentially and avoids catastrophic forgetting by protecting weights from being modi\ufb01ed that are important for previously learned games. In PathNet, an evolutionary algo- rithm is used to select which parts of a neural network are used for learning new tasks, demonstrating some transfer learning performance on ALE games [31].",
        "type": "NarrativeText"
    },
    {
        "element_id": "b712d572277485227015299124511e87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        180.9
                    ],
                    [
                        835.9,
                        407.8
                    ],
                    [
                        1535.7,
                        407.8
                    ],
                    [
                        1535.7,
                        180.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95395,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "environment, based on Minecraft, provides an excellent venue for creating tasks with very sparse rewards, where agents need to set their own goals. Derivative-free and gradient-free meth- ods, such as ESs and genetic algorithms, explore the parameter space by sampling locally and are promising for these games, especially when combined with novelty search as in [24] and [139].",
        "type": "NarrativeText"
    },
    {
        "element_id": "38017566baa5a9fc6020ffc7c2629bba",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        413.3
                    ],
                    [
                        835.9,
                        739.9
                    ],
                    [
                        1535.8,
                        739.9
                    ],
                    [
                        1535.8,
                        413.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95704,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "3) Learning With Multiple Agents: Current deep RL ap- proaches are mostly concerned with training a single agent. A few exceptions exist, where multiple agents have to cooperate [32], [33], [85], [111], [154], but it remains an open challenge how these can scale to more agents in various situations. In many current video games such as StarCraft or GTA V, many agents interact with each other and the player. To scale multia- gent learning in video games to the same level of performance as current single agent approaches will likely require new methods that can effectively train multiple agents at the same time.",
        "type": "NarrativeText"
    },
    {
        "element_id": "be64dd44ab8404ccb3cc57bb4f27c6a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        863.6,
                        745.4
                    ],
                    [
                        863.6,
                        773.1
                    ],
                    [
                        1533.3,
                        773.1
                    ],
                    [
                        1533.3,
                        745.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "4) Lifetime Adaptation: While nonplayer characters (NPCs)",
        "type": "ListItem"
    },
    {
        "element_id": "dfea9d2f98eab1b30cff1afae4a7bfbe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        753.1
                    ],
                    [
                        835.9,
                        1271.2
                    ],
                    [
                        1535.5,
                        1271.2
                    ],
                    [
                        1535.5,
                        753.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95002,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "can be trained to play a variety of games well (see Section IV), current machine learning techniques still struggle when it comes to agents that should be able to adapt during their lifetime, i.e., while the game is being played. For example, a human player can quickly change its behavior when realizing that the player is always ambushed at the same position in an FPS map. How- ever, most of the current DL techniques would require expen- sive retraining to adapt to these situations and other unforeseen situations that they have not encountered during training. The amount of data provided by the real-time behavior of a single human are nowhere near that required by common DL methods. This challenge is related to the wider problem of few-shot learn- ing, transfer learning, and general video game playing. Solving it will be important to create more believable and human-like NPCs.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6958470650020149b9b746848cdf07c4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1235.2
                    ],
                    [
                        105.3,
                        1561.8
                    ],
                    [
                        806.8,
                        1561.8
                    ],
                    [
                        806.8,
                        1235.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95689,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "In the future, it will be important to extend these methods to learn to play multiple games, even if those games are very different\u2014most current approaches focus on different (known) games in the ALE framework. One suitable avenue for this kind of research is the new learning track of the GVGAI com- petition [78], [116]. GVGAI has a potentially unlimited set of games, unlike ALE. Recent work in GVGAI showed that model- free deep RL over\ufb01tted not just to the individual game, but even to the individual level; this was countered by continuously gen- erating new levels during training [69].",
        "type": "NarrativeText"
    },
    {
        "element_id": "5f97371db6fd43bad1e261a2adc8c700",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1570.1
                    ],
                    [
                        105.3,
                        1763.8
                    ],
                    [
                        805.6,
                        1763.8
                    ],
                    [
                        805.6,
                        1570.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95383,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "It is possible that signi\ufb01cant advances on the multigame prob- lem will come from outside DL. In particular, the recent tan- gled graph representation, a form of genetic programming, has shown promise in this task [72]. The recent IMPALA algorithm tries to tackle multigame learning through massive scaling, with somewhat promising results [30].",
        "type": "NarrativeText"
    },
    {
        "element_id": "2db341ca3b62456cdf076b12713399bb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1772.1
                    ],
                    [
                        105.3,
                        2066.4
                    ],
                    [
                        804.1,
                        2066.4
                    ],
                    [
                        804.1,
                        1772.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95165,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "2) Overcoming Sparse, Delayed, or Deceptive Rewards: Games such as Montezuma\u2019s Revenge that are characterized by sparse rewards still pose a challenge for most of the deep RL approaches. While recent advances that combine DQN with intrinsic motivation [8] or expert demonstrations [57], [112] can help, games with sparse rewards are still a challenge for current deep RL methods. There is a long history of research in intrinsically motivated RL [22], [125], as well as hierarchical RL, which might be useful here [5], [163]. The Project Malmo",
        "type": "NarrativeText"
    },
    {
        "element_id": "61dc6ddd8404907d9a8f8877c118fa48",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1276.8
                    ],
                    [
                        835.9,
                        1603.4
                    ],
                    [
                        1536.8,
                        1603.4
                    ],
                    [
                        1536.8,
                        1276.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95483,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "5) Human-Like Game Playing: Lifetime learning is just one of the differences that current NPCs lack in comparison to human players. Most approaches are concerned with creating agents that play a particular game as well as possible, often only taking into account the score reached. However, if humans are expected to play against or cooperate with AI-based bots in video games, other factors come into play. Instead of creating a bot that plays perfectly, in this context, it becomes more im- portant that the bot is believable and is fun to play against, with similar idiosyncrasies we expect from a human player.",
        "type": "NarrativeText"
    },
    {
        "element_id": "0194df1c17f4ba9fb04d1c708c873438",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1608.8
                    ],
                    [
                        835.9,
                        1835.8
                    ],
                    [
                        1537.0,
                        1835.8
                    ],
                    [
                        1537.0,
                        1608.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95584,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "Human-like game playing is an active area of research with two different competitions focused on human-like behavior, namely, the 2k BotPrize [58], [59] and the Turing Test track of the Mario AI Championship [131]. Most entries in these com- petitions are based on various nonneural network techniques, while some used evolutionary training of deep neural networks to generate human-like behavior [105], [127].",
        "type": "NarrativeText"
    },
    {
        "element_id": "3f0c8cf9c1a30da2e35502ef6b5e87d4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1841.3
                    ],
                    [
                        835.9,
                        2068.2
                    ],
                    [
                        1534.6,
                        2068.2
                    ],
                    [
                        1534.6,
                        1841.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94913,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "6) Adjustable Performance Levels: Almost all current re- search on DL for game playing aims at creating agents that can play the game as well as possible, maybe even \u201cbeating\u201d it. However, for purposes of game testing, creating tutorials, and demonstrating games\u2014in all those places where it would be important to have human-like game play\u2014it could be impor- tant to be able to create agents with a particular skill level. If",
        "type": "NarrativeText"
    },
    {
        "element_id": "af2ecffb6adf8fd00a1fb1621fa1ce49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        114.8,
                        2123.7
                    ],
                    [
                        114.8,
                        2144.7
                    ],
                    [
                        1530.2,
                        2144.7
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7219,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15,
            "parent_id": "88ba20ca2ba7a110bd98215da508ead4"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "746e21e78d264000af7c9228c28bb420",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1513.9,
                        92.7
                    ],
                    [
                        1513.9,
                        113.3
                    ],
                    [
                        1533.8,
                        113.3
                    ],
                    [
                        1533.8,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79611,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 15
        },
        "text": "15",
        "type": "Header"
    },
    {
        "element_id": "08249b0ed3b29a00ecee3eda7703e460",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.8,
                        92.7
                    ],
                    [
                        116.8,
                        112.7
                    ],
                    [
                        137.4,
                        112.7
                    ],
                    [
                        137.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76043,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16
        },
        "text": "16",
        "type": "Header"
    },
    {
        "element_id": "f52322164f875f88925aba2d247f64b7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        985.8,
                        92.7
                    ],
                    [
                        985.8,
                        112.2
                    ],
                    [
                        1546.2,
                        112.2
                    ],
                    [
                        1546.2,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70537,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "29f390bbb4eb748c34fe94dd0b16e945",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        180.8
                    ],
                    [
                        117.0,
                        540.6
                    ],
                    [
                        816.9,
                        540.6
                    ],
                    [
                        816.9,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95486,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "your agent plays better than any human player, then it is not a good model of what a human would do in the game. At its most basic, this could entail training an agent that plays the game very well and then \ufb01nd a way of decreasing the performance of that agent. However, it would be more useful to be able to ad- just the performance level in a more \ufb01ne-grained way, so as to, for example, separately control the reaction speed or long-term planning ability of an agent. Even more useful would be to be able to ban certain capacities of playstyles of a trained agent, so as to test whether, for example, a given level could be solved without certain actions or tactics.",
        "type": "NarrativeText"
    },
    {
        "element_id": "39893078f50841a1ebb39f885695c74b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        546.1
                    ],
                    [
                        117.0,
                        706.6
                    ],
                    [
                        816.9,
                        706.6
                    ],
                    [
                        816.9,
                        546.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95033,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "One path to realizing this is the concept of procedural per- sonas, where the preferences of an agent are encoded as a set of utility weights [60]. However, this concept has not been im- plemented using DL, and it is still unclear how to realize the planning depth control in this context.",
        "type": "NarrativeText"
    },
    {
        "element_id": "49d75bc88aff561069ce4965b967f452",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        180.8
                    ],
                    [
                        847.6,
                        241.7
                    ],
                    [
                        1545.3,
                        241.7
                    ],
                    [
                        1545.3,
                        180.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90562,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "on training on existing content [140], [158], or for modeling player experience [169].",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca4b5db2bfe6cea1cce699ae29d7b876",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        247.3
                    ],
                    [
                        847.6,
                        441.0
                    ],
                    [
                        1549.2,
                        441.0
                    ],
                    [
                        1549.2,
                        247.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9546,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "Within the game industry, several of the large development and technology companies, including Electronic Arts, Ubisoft, and Unity, have recently started in-house research arms focusing partly on DL. It remains to be seen whether these techniques will also be embraced by the development arms of these companies or their customers.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5424d01ea540df2b7fa3eeec04014428",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        446.5
                    ],
                    [
                        847.6,
                        739.9
                    ],
                    [
                        1551.7,
                        739.9
                    ],
                    [
                        1551.7,
                        446.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95707,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "2) Interactive Tools for Game Development: Related to the previous challenge, there is currently a lack of tools for designers to easily train NPC behaviors. While many open-source tools to training deep networks exist now, most of them require a signi\ufb01cant level of expertise. A tool that allows designers to easily specify desired NPC behaviors (and undesired ones) while assuring a certain level of control over the \ufb01nal trained outcomes would greatly accelerate the uptake of these new methods in the game industry.",
        "type": "NarrativeText"
    },
    {
        "element_id": "c640794a02599c352ea590581bdfa5dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        712.2
                    ],
                    [
                        117.0,
                        1039.2
                    ],
                    [
                        816.4,
                        1039.2
                    ],
                    [
                        816.4,
                        712.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95587,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "7) Dealing With Extremely Large Decision Spaces: Whereas the average branching factor hovers around 30 for Chess and 300 for Go, a game like StarCraft has a branching factor that is orders of magnitudes larger. While recent advances in evolutionary planning have allowed real-time and long-term planning in games with larger branching factors to [66], [67], and [159], how we can scale deep RL to such levels of complex- ity is an important open challenge. Learning heuristics with DL in these games to enhance search algorithms is also a promising direction.",
        "type": "NarrativeText"
    },
    {
        "element_id": "dc17b4c21bced8200d337ea92dbfcd48",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        745.4
                    ],
                    [
                        847.6,
                        1105.2
                    ],
                    [
                        1550.7,
                        1105.2
                    ],
                    [
                        1550.7,
                        745.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95454,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "Learning from human preferences is one promising direction in this area. This approach has been extensively studied in the context of NE [115], and also in the context of video games, allowing nonexpert users to breed behaviors for Super Mario [135]. Recently, a similar preference-based approach was ap- plied to deep RL method [23], allowing agents to learn Atari games based on a combination of human preference learning and deep RL. Recently, the game company King published re- sults using imitation learning to learn policies for playtesting of Candy Crush levels, showing a promising direction for new design tools [41].",
        "type": "NarrativeText"
    },
    {
        "element_id": "c1da7bb830ef0b8c186c29298d6ce9ab",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1097.0
                    ],
                    [
                        117.0,
                        1124.7
                    ],
                    [
                        322.1,
                        1124.7
                    ],
                    [
                        322.1,
                        1097.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58748,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "B. Game Industry",
        "type": "Title"
    },
    {
        "element_id": "e5f5879b17fca7de63717c73ecbe5d2e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1144.0
                    ],
                    [
                        117.0,
                        1769.5
                    ],
                    [
                        818.0,
                        1769.5
                    ],
                    [
                        818.0,
                        1144.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94731,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "c1da7bb830ef0b8c186c29298d6ce9ab"
        },
        "text": "1) Adoption in the Game Industry: Many of the recent ad- vances in DL have been accelerated because of the increased interest by a variety of different companies such as Facebook, Google/Alphabet, Microsoft, and Amazon, which heavily in- vest in its development. However, the game industry has not embraced these advances to the same extent. This sometimes surprises commentators outside of the game industry, as games are seen as making heavy use of AI techniques. However, the type of AI that is most commonly used in the games industry focuses more on hand-authoring of expressive NPC behaviors rather than machine learning. An often-cited reason for the lack of adoption of neural networks (and similar methods) within this industry is that such methods are inherently dif\ufb01cult to control, which could result in unwanted NPC behaviors (e.g., an NPC could decide to kill a key actor that is relevant to the story). Ad- ditionally, training deep network models require a certain level of expertise, and the pool of experts in this area is still limited. It is important to address these challenges to encourage a wide adoption in the game industry.",
        "type": "NarrativeText"
    },
    {
        "element_id": "37443a15b1f7f58960516fa49b1b69b7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1110.7
                    ],
                    [
                        847.6,
                        1604.8
                    ],
                    [
                        1550.7,
                        1604.8
                    ],
                    [
                        1550.7,
                        1110.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9517,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "c1da7bb830ef0b8c186c29298d6ce9ab"
        },
        "text": "3) Creating New Types of Video Games: DL could poten- tially offer a way to create completely new games. Most of today\u2019s game designs stem from a time when no advanced AI methods were available or the hardware too limited to utilize them, meaning that games have been designed to not need AI. Designing new games around AI can help to break out of these limitations. While evolutionary algorithms and NE in particular [115] have allowed the creation of completely new types of games, DL based on gradient descent has not been ex- plored in this context. NE is a core mechanic in games such as NERO [137], Galactic Arms Race [48], Petalz [114], and Evo- Commander [64]. One challenge with gradient-based optimiza- tion is that the structures are often limited to having mathemati- cal smoothness (i.e., differentiability), making it challenging to create interesting and unexpected outputs.",
        "type": "NarrativeText"
    },
    {
        "element_id": "77bec3f0376fd1cf0a2f11ac9e6b81fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1661.6
                    ],
                    [
                        847.6,
                        1689.2
                    ],
                    [
                        1192.1,
                        1689.2
                    ],
                    [
                        1192.1,
                        1661.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59172,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "f52322164f875f88925aba2d247f64b7"
        },
        "text": "C. Learning Models of Games",
        "type": "Title"
    },
    {
        "element_id": "bec6ff07030f2ad679c6620b09697943",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1775.0
                    ],
                    [
                        117.0,
                        2068.5
                    ],
                    [
                        815.7,
                        2068.5
                    ],
                    [
                        815.7,
                        1775.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.954,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "77bec3f0376fd1cf0a2f11ac9e6b81fd"
        },
        "text": "Additionally, while most DL approaches focus exclusively on playing games as well as possible, this goal might not be the most important for the game industry [171]. Here, the level of fun or engagement the player experiences while playing is a crucial component. One use of DL for game playing in the game production process is for game testing, where arti\ufb01cial agents test that levels are solvable or that the dif\ufb01culty is appropriate. DL might see its most prominent use in the games industry not for playing games, but for generating game content [130] based",
        "type": "NarrativeText"
    },
    {
        "element_id": "d66badba5ce16b897c06adf501aa5cd0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1708.6
                    ],
                    [
                        847.6,
                        2002.0
                    ],
                    [
                        1549.4,
                        2002.0
                    ],
                    [
                        1549.4,
                        1708.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95462,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "77bec3f0376fd1cf0a2f11ac9e6b81fd"
        },
        "text": "Much work on DL for game-playing takes a model-free end- to-end learning approach, where a neural network is trained to produce actions given state observations as input. However, it is well known that a good and fast forward model makes game playing much easier, as it makes it possible to use planning methods based on tree search or evolution [171]. Therefore, an important open challenge in this \ufb01eld is to develop methods that can learn a forward model of the game, making it possible to reason about its dynamics.",
        "type": "NarrativeText"
    },
    {
        "element_id": "52b508f6ce90ac6f0d01ec2fa62ef67b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        2007.5
                    ],
                    [
                        847.6,
                        2068.4
                    ],
                    [
                        1545.0,
                        2068.4
                    ],
                    [
                        1545.0,
                        2007.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91457,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "77bec3f0376fd1cf0a2f11ac9e6b81fd"
        },
        "text": "The hope is that approaches that learn the rules of the game can generalize better to different game variations and show more",
        "type": "NarrativeText"
    },
    {
        "element_id": "5c9fbe0870fc89e75cba02a6c6439d47",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.6
                    ],
                    [
                        1530.2,
                        2144.6
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70632,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 16,
            "parent_id": "77bec3f0376fd1cf0a2f11ac9e6b81fd"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "208bac5fea42fcb9c38fecfcb83ef0f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.0,
                        90.9
                    ],
                    [
                        103.0,
                        113.0
                    ],
                    [
                        664.3,
                        113.0
                    ],
                    [
                        664.3,
                        90.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64814,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "4c1ee70041633f300fa2a7ebb688ceb3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.1,
                        180.4
                    ],
                    [
                        103.1,
                        475.0
                    ],
                    [
                        807.2,
                        475.0
                    ],
                    [
                        807.2,
                        180.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95827,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "robust learning. Promising work in this area includes the ap- proach by Guzdial et al. [44] that learns a simple game engine of Super Mario Bros. from gameplay data. Kansky et al. [70] intro- duce the idea of Schema Networks that follow an object-oriented approach and are trained to predict future object attributes and rewards based on the current attributes and actions. A trained schema network thus provides a probabilistic model that can be used for planning and is able to perform zero-shot transfer to variations of Breakout similar to those used in training.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bee2a6b16a89f88760dde63467ca6564",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.0,
                        185.2
                    ],
                    [
                        858.0,
                        257.1
                    ],
                    [
                        1533.3,
                        257.1
                    ],
                    [
                        1533.3,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93278,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "[5] A. G. Barto and S. Mahadevan, \u201cRecent advances in hierarchical reinforcement learning,\u201d Discrete Event Dyn. Syst., vol. 13, no. 4, pp. 341\u2013379, 2003.",
        "type": "ListItem"
    },
    {
        "element_id": "eb8e88087a75f4a0fd7d2c045838688d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.0,
                        259.9
                    ],
                    [
                        858.0,
                        282.3
                    ],
                    [
                        1431.3,
                        282.3
                    ],
                    [
                        1431.3,
                        259.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87163,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "[6] C. Beattie et al., \u201cDeepMind lab,\u201d 2016, arXiv:1612.03801.",
        "type": "ListItem"
    },
    {
        "element_id": "7db7b529bd88b292871ba21f21f6e7fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        856.5,
                        284.8
                    ],
                    [
                        856.5,
                        356.7
                    ],
                    [
                        1539.9,
                        356.7
                    ],
                    [
                        1539.9,
                        284.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92968,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "[7] M. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, \u201cThe arcade learn- ing environment: An evaluation platform for general agents,\u201d in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 4148\u20134152.",
        "type": "ListItem"
    },
    {
        "element_id": "c2ec4f9838239c78421b2fe9de735b97",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.0,
                        359.5
                    ],
                    [
                        858.0,
                        431.4
                    ],
                    [
                        1533.6,
                        431.4
                    ],
                    [
                        1533.6,
                        359.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93202,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "[8] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, \u201cUnifying count-based exploration and intrinsic motivation,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 1471\u20131479.",
        "type": "ListItem"
    },
    {
        "element_id": "d4c191077c3d5f2f9cf3209a13f734fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.0,
                        434.2
                    ],
                    [
                        858.0,
                        506.2
                    ],
                    [
                        1533.3,
                        506.2
                    ],
                    [
                        1533.3,
                        434.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93338,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "[9] M. G. Bellemare, W. Dabney, and R. Munos, \u201cA distributional perspec- tive on reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn., 2017, pp. 449\u2013458.",
        "type": "ListItem"
    },
    {
        "element_id": "1a3723d5b352007f187089a007f4e4c4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        514.3
                    ],
                    [
                        105.3,
                        542.0
                    ],
                    [
                        435.6,
                        542.0
                    ],
                    [
                        435.6,
                        514.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5869,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "D. Computational Resources",
        "type": "Title"
    },
    {
        "element_id": "971cc5597a8de7755338438918dc2f77",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.4,
                        561.4
                    ],
                    [
                        104.4,
                        888.1
                    ],
                    [
                        805.5,
                        888.1
                    ],
                    [
                        805.5,
                        561.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95752,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "With more advanced computational models and a larger num- ber of agents in open worlds, computational speed becomes a concern. Methods that aim to make the networks computation- ally more ef\ufb01cient by either compressing networks [62] or prun- ing networks after training [43], [47] could be useful. Of course, improvements in processing power in general or for neural net- works speci\ufb01cally will also be important. Currently, it is not feasible to train networks in real time to adapt to changes in the game or to \ufb01t players\u2019 playing styles, something which could be useful in the design process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "80528e5bed054c726262b55f414b95ee",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        508.9
                    ],
                    [
                        847.0,
                        580.9
                    ],
                    [
                        1533.3,
                        580.9
                    ],
                    [
                        1533.3,
                        508.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93015,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[10] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, \u201cThe arcade learning environment: An evaluation platform for general agents,\u201d J. Artif. Intell. Res., vol. 47, pp. 253\u2013279, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "0cfcfa6a5a1e76b6c5d4c0b476dae250",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        583.7
                    ],
                    [
                        847.0,
                        631.0
                    ],
                    [
                        1533.4,
                        631.0
                    ],
                    [
                        1533.4,
                        583.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91797,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[11] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum learn- ing,\u201d in Proc. 26th Annu. Int. Conf. Mach. Learn., 2009, pp. 41\u201348.",
        "type": "ListItem"
    },
    {
        "element_id": "f43ed6a6d736982caa236c1659f00b52",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        633.5
                    ],
                    [
                        847.0,
                        730.3
                    ],
                    [
                        1534.9,
                        730.3
                    ],
                    [
                        1534.9,
                        633.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93887,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[12] J. Bergstra, D. Yamins, and D. D. Cox, \u201cMaking a science of model search: Hyperparameter optimization in hundreds of dimensions for vi- sion architectures,\u201d in Proc. 30th Int. Conf. Int. Conf. Mach. Learn., 2013, pp. I-115\u2013I-123.",
        "type": "ListItem"
    },
    {
        "element_id": "405c34033cb168dc1a730dd87c8b274b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        732.9
                    ],
                    [
                        847.0,
                        805.0
                    ],
                    [
                        1533.3,
                        805.0
                    ],
                    [
                        1533.3,
                        732.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93516,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[13] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. K\u00b4egl, \u201cAlgorithms for hyper-parameter optimization,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2011, pp. 2546\u20132554.",
        "type": "ListItem"
    },
    {
        "element_id": "ca6fd37babf7afe0ec5f8ea6a37847d9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        807.8
                    ],
                    [
                        847.0,
                        879.8
                    ],
                    [
                        1535.0,
                        879.8
                    ],
                    [
                        1535.0,
                        807.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93507,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[14] S. Bhatti, A. Desmaison, O. Miksik, N. Nardelli, N. Siddharth, and P. H. Torr, \u201cPlaying doom with SLAM-augmented deep reinforcement learning,\u201d 2016, arXiv:1612.00380.",
        "type": "ListItem"
    },
    {
        "element_id": "3de11c748e030d2af12de5498192b694",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        882.5
                    ],
                    [
                        847.0,
                        929.6
                    ],
                    [
                        1537.8,
                        929.6
                    ],
                    [
                        1537.8,
                        882.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91035,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "1a3723d5b352007f187089a007f4e4c4"
        },
        "text": "[15] N. Bhonker, S. Rozenberg, and I. Hubara, \u201cPlaying SNES in the retro learning environment,\u201d 2017, arXiv:1611.02205.",
        "type": "ListItem"
    },
    {
        "element_id": "101e847e6215dec15799a4d38863e846",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        348.2,
                        928.1
                    ],
                    [
                        348.2,
                        955.8
                    ],
                    [
                        560.7,
                        955.8
                    ],
                    [
                        560.7,
                        928.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84499,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "VII. CONCLUSION",
        "type": "Title"
    },
    {
        "element_id": "ee0fdc46afb4245fac865747936e9f50",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        103.3,
                        975.1
                    ],
                    [
                        103.3,
                        1401.5
                    ],
                    [
                        803.3,
                        1401.5
                    ],
                    [
                        803.3,
                        975.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95604,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "This paper reviewed DL methods applied to game playing in video games of various genres including arcade, racing, FPSs, open-world, RTS, team sports, physics, and text adventure games. Most of the reviewed work is within end-to-end model- free deep RL, where a CNN learns to play directly from raw pixels by interacting with the game. Recent work demonstrates that derivative-free ESs and genetic algorithms are competitive alternatives. Some of the reviewed work apply supervised learn- ing to imitate behaviors from game logs, while others are based on methods that learn a model of the environment. For simple games, such as most arcade games, the reviewed methods can achieve above human-level performance, while there are many open challenges in more complex games.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d7f4dad1b3a4924ad4180975f995e1fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        932.3
                    ],
                    [
                        847.0,
                        1004.3
                    ],
                    [
                        1533.9,
                        1004.3
                    ],
                    [
                        1533.9,
                        932.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9338,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[16] M. Bogdanovic, D. Markovikj, M. Denil, and N. De Freitas, \u201cDeep apprenticeship learning for playing video games,\u201d in Proc. Workshops 29th AAAI Conf. Artif. Intell., Jan. 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "528a9616ba3b85c4b3ac1003ae6ed8f4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1007.1
                    ],
                    [
                        847.0,
                        1029.6
                    ],
                    [
                        1455.8,
                        1029.6
                    ],
                    [
                        1455.8,
                        1007.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8635,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[17] G. Brockman et al., \u201cOpenAI Gym,\u201d 2016, arXiv:1606.01540.",
        "type": "ListItem"
    },
    {
        "element_id": "8eba5ec9ffa091521eaaf8bab82d2928",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1032.0
                    ],
                    [
                        847.0,
                        1079.7
                    ],
                    [
                        1533.3,
                        1079.7
                    ],
                    [
                        1533.3,
                        1032.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91365,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[18] C. B. Browne et al., \u201cA survey of Monte Carlo tree search methods,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 4, no. 1, pp. 1\u201343, Mar. 2012.",
        "type": "ListItem"
    },
    {
        "element_id": "e849be0b44d575ec4aff72303ca26432",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1081.8
                    ],
                    [
                        847.0,
                        1153.7
                    ],
                    [
                        1533.3,
                        1153.7
                    ],
                    [
                        1533.3,
                        1081.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92964,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[19] Y.-H. Chang, T. Ho, and L. P. Kaelbling, \u201cAll learning is local: Multi- agent learning in global reward games,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2003, pp. 807\u2013814.",
        "type": "ListItem"
    },
    {
        "element_id": "89a4a627f1884e18f48cba10c8078009",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1156.5
                    ],
                    [
                        847.0,
                        1228.5
                    ],
                    [
                        1533.6,
                        1228.5
                    ],
                    [
                        1533.6,
                        1156.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93404,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[20] D. S. Chaplot, G. Lample, K. M. Sathyendra, and R. Salakhutdinov, \u201cTransfer deep reinforcement learning in 3D environments: An empirical study,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "8ae66a082d8e156be98faa3faebe1f69",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1231.2
                    ],
                    [
                        847.0,
                        1303.4
                    ],
                    [
                        1535.0,
                        1303.4
                    ],
                    [
                        1535.0,
                        1231.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92958,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[21] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, \u201cDeepDriving: Learning affordance for direct perception in autonomous driving,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 2722\u20132730.",
        "type": "ListItem"
    },
    {
        "element_id": "e2e76095c721e1efc6392cb45d90b097",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1305.9
                    ],
                    [
                        847.0,
                        1377.9
                    ],
                    [
                        1533.9,
                        1377.9
                    ],
                    [
                        1533.9,
                        1305.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93096,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "101e847e6215dec15799a4d38863e846"
        },
        "text": "[22] N. Chentanez, A. G. Barto, and S. P. Singh, \u201cIntrinsically motivated reinforcement learning,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2005, pp. 1281\u20131288.",
        "type": "ListItem"
    },
    {
        "element_id": "0748180b47c76b388e30245e9f978159",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        342.2,
                        1441.5
                    ],
                    [
                        342.2,
                        1469.2
                    ],
                    [
                        565.8,
                        1469.2
                    ],
                    [
                        565.8,
                        1441.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "ACKNOWLEDGMENT",
        "type": "Title"
    },
    {
        "element_id": "bf05bff6ff112f94f7137e7d725f79aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1488.5
                    ],
                    [
                        105.3,
                        1682.2
                    ],
                    [
                        805.8,
                        1682.2
                    ],
                    [
                        805.8,
                        1488.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95524,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "The authors would like to thank the numerous colleagues who took the time to comment on drafts of this paper, including C. Tessler, D. P\u00b4erez-Li\u00b4ebana, E. Caballero, H. Daum\u00b4e, III, J. Busk, K. Arulkumaran, M. Heywood, M. G. Bellemare, M.-P. Huget, M. Preuss, N. de Freitas, N. A. Barriga, O. Delalleau, P. Stone, S. Onta\u02dcn\u00b4on, T. Matiisen, Y. Fu, and Y. Hou.",
        "type": "NarrativeText"
    },
    {
        "element_id": "86063428c864a4af28db447a49ce0b3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1380.7
                    ],
                    [
                        847.0,
                        1452.6
                    ],
                    [
                        1533.3,
                        1452.6
                    ],
                    [
                        1533.3,
                        1380.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93575,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "[23] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, \u201cDeep reinforcement learning from human preferences,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 4302\u20134310.",
        "type": "ListItem"
    },
    {
        "element_id": "790aad3891374a293d22de16ed758271",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1455.4
                    ],
                    [
                        847.0,
                        1552.2
                    ],
                    [
                        1535.1,
                        1552.2
                    ],
                    [
                        1535.1,
                        1455.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93743,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "[24] E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. Stanley, and J. Clune, \u201cImproving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2018, pp. 5032\u20135043.",
        "type": "ListItem"
    },
    {
        "element_id": "fd3f4f8d5107e2d2ac9b4b6bf836b38e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1554.8
                    ],
                    [
                        847.0,
                        1602.2
                    ],
                    [
                        1533.3,
                        1602.2
                    ],
                    [
                        1533.3,
                        1554.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91342,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "[25] M.-A. C\u02c6ot\u00b4e et al., \u201cTextWorld: A learning environment for text-based games,\u201d 2018, arXiv:1806.11532.",
        "type": "ListItem"
    },
    {
        "element_id": "88dde8c15cd50ef32f1f57beda19cc42",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1604.8
                    ],
                    [
                        847.0,
                        1651.9
                    ],
                    [
                        1533.3,
                        1651.9
                    ],
                    [
                        1533.3,
                        1604.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91394,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "[26] G. Cuccu, J. Togelius, and P. Cudre-Mauroux, \u201cPlaying Atari with six neurons,\u201d 2018, arXiv:1806.01363.",
        "type": "ListItem"
    },
    {
        "element_id": "914da1027050ac8e0156b50c089e05d0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1654.7
                    ],
                    [
                        847.0,
                        1726.6
                    ],
                    [
                        1533.3,
                        1726.6
                    ],
                    [
                        1533.3,
                        1654.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93452,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "0748180b47c76b388e30245e9f978159"
        },
        "text": "[27] T. Degris, P. M. Pilarski, and R. S. Sutton, \u201cModel-free reinforcement learning with continuous action in practice,\u201d in Proc. Amer. Control Conf., 2012, pp. 2177\u20132182.",
        "type": "ListItem"
    },
    {
        "element_id": "e83fd6a527c7665a2e1535e817879d12",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        413.5,
                        1722.4
                    ],
                    [
                        413.5,
                        1750.1
                    ],
                    [
                        556.2,
                        1750.1
                    ],
                    [
                        556.2,
                        1722.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "208bac5fea42fcb9c38fecfcb83ef0f0"
        },
        "text": "REFERENCES",
        "type": "Title"
    },
    {
        "element_id": "2279b894e12aab58598b6a556e795ba7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        127.5,
                        1771.0
                    ],
                    [
                        127.5,
                        1843.0
                    ],
                    [
                        802.7,
                        1843.0
                    ],
                    [
                        802.7,
                        1771.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92841,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[1] S. Alvernaz and J. Togelius, \u201cAutoencoder-augmented neuroevolution for visual doom playing,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "14576b3a40d607d6c483e46bad711b63",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        127.5,
                        1845.7
                    ],
                    [
                        127.5,
                        1917.7
                    ],
                    [
                        802.7,
                        1917.7
                    ],
                    [
                        802.7,
                        1845.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92744,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \u201cDeep reinforcement learning: A brief survey,\u201d IEEE Signal Process. Mag., vol. 34, no. 6, pp. 26\u201338, Nov. 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "a9ca0afe3d5f0d4a86593980aa7d0a7d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        127.5,
                        1920.5
                    ],
                    [
                        127.5,
                        1992.4
                    ],
                    [
                        805.0,
                        1992.4
                    ],
                    [
                        805.0,
                        1920.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92796,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[3] M. Asada, M. M. Veloso, M. Tambe, I. Noda, H. Kitano, and G. K. Kraetzschmar, \u201cOverview of RoboCup-98,\u201d AI Mag., vol. 21, no. 1, pp. 9\u201319, 2000.",
        "type": "ListItem"
    },
    {
        "element_id": "86947ee0bfcc2e02dc5fe006d5938318",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        127.5,
                        1995.2
                    ],
                    [
                        127.5,
                        2068.0
                    ],
                    [
                        802.7,
                        2068.0
                    ],
                    [
                        802.7,
                        1995.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92556,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[4] N. A. Barriga, M. Stanescu, and M. Buro, \u201cCombining strategic learning and tactical search in real-time strategy games,\u201d in Proc. 13th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2017, pp. 9\u201315.",
        "type": "ListItem"
    },
    {
        "element_id": "8bc3e2ff75817a1b75885ee03b0c4c41",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1729.4
                    ],
                    [
                        847.0,
                        1826.5
                    ],
                    [
                        1534.1,
                        1826.5
                    ],
                    [
                        1534.1,
                        1729.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9399,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[28] A. Dosovitskiy and V. Koltun, \u201cLearning to act by predicting the fu- ture,\u201d in Proc. Int. Conf. Learn. Represent., 2017, [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.",
        "type": "ListItem"
    },
    {
        "element_id": "9f28f34ca17799ca27e3aff2f8091ab9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1829.0
                    ],
                    [
                        847.0,
                        1901.1
                    ],
                    [
                        1533.4,
                        1901.1
                    ],
                    [
                        1533.4,
                        1829.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93464,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[29] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, \u201cBench- marking deep reinforcement learning for continuous control,\u201d in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1329\u20131338.",
        "type": "ListItem"
    },
    {
        "element_id": "c5a5a3e4453cfba8fe904e626cb4ec46",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1903.7
                    ],
                    [
                        847.0,
                        1975.7
                    ],
                    [
                        1533.3,
                        1975.7
                    ],
                    [
                        1533.3,
                        1903.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93284,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[30] L. Espeholt et al., \u201cIMPALA: Scalable distributed deep-RL with im- portance weighted actor-learner architectures,\u201d inProc. 35th Int. Conf. Mach. Learn., Jul. 10\u201315, 2018, pp. 1407\u20131416.",
        "type": "ListItem"
    },
    {
        "element_id": "e7f6ff7642e36b8f85a883aa81ec7ec2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.0,
                        1981.2
                    ],
                    [
                        847.0,
                        2028.5
                    ],
                    [
                        1537.6,
                        2028.5
                    ],
                    [
                        1537.6,
                        1981.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91141,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "[31] C. Fernando et al., \u201cPathNet: Evolution channels gradient descent in super neural networks,\u201d 2017, arXiv:1701.08734.",
        "type": "ListItem"
    },
    {
        "element_id": "57236298f4501c2aef44ff258efa7779",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2143.9
                    ],
                    [
                        1530.2,
                        2143.9
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.3625,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17,
            "parent_id": "e83fd6a527c7665a2e1535e817879d12"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "ListItem"
    },
    {
        "element_id": "09b4c8e6632b5068b150ead6d995637c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1513.8,
                        92.7
                    ],
                    [
                        1513.8,
                        113.1
                    ],
                    [
                        1534.2,
                        113.1
                    ],
                    [
                        1534.2,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79299,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 17
        },
        "text": "17",
        "type": "Header"
    },
    {
        "element_id": "084eccc562406b89f2546089b6ec4a44",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        92.7
                    ],
                    [
                        117.0,
                        112.8
                    ],
                    [
                        137.0,
                        112.8
                    ],
                    [
                        137.0,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73103,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18
        },
        "text": "18",
        "type": "Header"
    },
    {
        "element_id": "809904bc71620c3907163e4715f85e89",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        986.8,
                        92.7
                    ],
                    [
                        986.8,
                        112.0
                    ],
                    [
                        1552.0,
                        112.0
                    ],
                    [
                        1552.0,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.60624,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "11733aaac9912337b4dd272bc8a58c09",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        185.2
                    ],
                    [
                        128.1,
                        257.1
                    ],
                    [
                        814.4,
                        257.1
                    ],
                    [
                        814.4,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93327,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[32] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, \u201cCounterfactual multi-agent policy gradients,\u201d in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 2974\u20132982.",
        "type": "ListItem"
    },
    {
        "element_id": "b2682773acc646af5d2cd8b9112785f6",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        259.9
                    ],
                    [
                        128.1,
                        331.8
                    ],
                    [
                        814.4,
                        331.8
                    ],
                    [
                        814.4,
                        259.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93702,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[33] J. N. Foerster et al., \u201cStabilising experience replay for deep multi-agent reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn., 2017, pp. 1146\u20131155.",
        "type": "ListItem"
    },
    {
        "element_id": "22a286f081c4ff511e1415f24310d4fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        185.2
                    ],
                    [
                        858.7,
                        232.3
                    ],
                    [
                        1545.0,
                        232.3
                    ],
                    [
                        1545.0,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91748,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[56] M. Hessel et al., \u201cRainbow: Combining improvements in deep rein- forcement learning,\u201d in Proc. AAAI, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "e86d0124d48a2b554ba3c76aec0bec06",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        235.0
                    ],
                    [
                        858.7,
                        282.6
                    ],
                    [
                        1547.5,
                        282.6
                    ],
                    [
                        1547.5,
                        235.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9123,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[57] T. Hester et al., \u201cDeep q-learning from demonstrations,\u201d in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 3223\u20133230.",
        "type": "ListItem"
    },
    {
        "element_id": "9745466699d839754d32bd42616c5ae8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        284.8
                    ],
                    [
                        858.7,
                        331.9
                    ],
                    [
                        1546.2,
                        331.9
                    ],
                    [
                        1546.2,
                        284.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91344,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[58] P. Hingston, \u201cA new design for a turing test for Bots,\u201d in Proc. IEEE Symp. Comput. Intell. Games, 2010, pp. 345\u2013350.",
        "type": "ListItem"
    },
    {
        "element_id": "99f5357eb34902098310923405a31e31",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        334.6
                    ],
                    [
                        128.1,
                        406.5
                    ],
                    [
                        814.5,
                        406.5
                    ],
                    [
                        814.5,
                        334.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93466,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[34] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, \u201cLearn- ing to communicate to solve riddles with deep distributed recurrent q- networks,\u201d 2016, arXiv:1602.02672.",
        "type": "ListItem"
    },
    {
        "element_id": "b35ab31bf3dbc7731806d92bb7084b2c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        409.3
                    ],
                    [
                        128.1,
                        506.2
                    ],
                    [
                        814.4,
                        506.2
                    ],
                    [
                        814.4,
                        409.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93792,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[35] M. Fortunato et al., \u201cNoisy networks for exploration,\u201d in Proc. Int. Conf. Learn. Represent., 2018. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.",
        "type": "ListItem"
    },
    {
        "element_id": "ff47ce885c42e152f66516a59dde06fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        508.9
                    ],
                    [
                        128.1,
                        630.7
                    ],
                    [
                        814.4,
                        630.7
                    ],
                    [
                        814.4,
                        508.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93968,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[36] K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik, \u201cLearn- ing visual predictive models of physics for playing billiards,\u201d in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.",
        "type": "ListItem"
    },
    {
        "element_id": "d5acbdbd464cec1ab2ab926687ee7e76",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        633.5
                    ],
                    [
                        128.1,
                        705.4
                    ],
                    [
                        814.4,
                        705.4
                    ],
                    [
                        814.4,
                        633.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93435,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[37] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, \u201cWhat can you do with a rock? Affordance extraction via word embeddings,\u201d in Proc. 26th Int. Joint Conf. Artif. Intell., 2017, pp. 1039\u20131045.",
        "type": "ListItem"
    },
    {
        "element_id": "425ce223a1433933f95ab50069679746",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        708.2
                    ],
                    [
                        128.1,
                        755.3
                    ],
                    [
                        814.2,
                        755.3
                    ],
                    [
                        814.2,
                        708.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91336,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[38] L. Galway, D. Charles, and M. Black, \u201cMachine learning in digital games: A survey,\u201d Artif. Intell. Rev., vol. 29, no. 2, pp. 123\u2013161, 2008.",
        "type": "ListItem"
    },
    {
        "element_id": "22d3bbcd9bc3120cdbe844bea1aa8f73",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        334.6
                    ],
                    [
                        858.7,
                        381.6
                    ],
                    [
                        1545.0,
                        381.6
                    ],
                    [
                        1545.0,
                        334.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90853,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[59] P. Hingston, Believable Bots: Can Computers Play Like People? New York, NY, USA: Springer, 2012.",
        "type": "ListItem"
    },
    {
        "element_id": "5fff3df9596bbcafb350c65ed661796a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        384.2
                    ],
                    [
                        858.7,
                        481.3
                    ],
                    [
                        1551.4,
                        481.3
                    ],
                    [
                        1551.4,
                        384.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93877,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[60] C. Holmg\u02daard, A. Liapis, J. Togelius, and G. N. Yannakakis, \u201cGen- erative agents for player decision modeling in games,\u201d in Proc. 9th Int. Conf. Found. Digit. Games, 2014. [Online]. Available: http://www.fdg2014.org/papers/fdg2014\\_poster\\_05.pdf",
        "type": "ListItem"
    },
    {
        "element_id": "da64d66c1031b9d30a1ec112fb8cec88",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        856.7,
                        484.0
                    ],
                    [
                        856.7,
                        580.9
                    ],
                    [
                        1548.8,
                        580.9
                    ],
                    [
                        1548.8,
                        484.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93897,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[61] D. Horgan et al., \u201cDistributed prioritized experience replay,\u201d in Proc. Int. Conf. Learn. Represent., 2018. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model",
        "type": "ListItem"
    },
    {
        "element_id": "2b81149bbb5809dcc0e308e6c937cd51",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        583.7
                    ],
                    [
                        858.7,
                        655.6
                    ],
                    [
                        1545.0,
                        655.6
                    ],
                    [
                        1545.0,
                        583.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92599,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[62] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, \u201cSqueezeNet: AlexNet-level accuracy with 50x fewer parame- ters and < 0.5 MB model size,\u201d 2016, arXiv:1602.07360.",
        "type": "ListItem"
    },
    {
        "element_id": "b610088596ad665152aa66e87a85035f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        658.4
                    ],
                    [
                        858.7,
                        755.2
                    ],
                    [
                        1548.4,
                        755.2
                    ],
                    [
                        1548.4,
                        658.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92956,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[63] M. Jaderberg et al., \u201cReinforcement learning with unsupervised auxil- iary tasks,\u201d in Proc. Int. Conf. Learn. Represent., 2017. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.",
        "type": "ListItem"
    },
    {
        "element_id": "da73c87e42774a28fb47229aec5ce53e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        758.0
                    ],
                    [
                        128.1,
                        805.4
                    ],
                    [
                        816.9,
                        805.4
                    ],
                    [
                        816.9,
                        758.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92704,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[39] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "ae071de0159bcea474439667d0715fcc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        807.8
                    ],
                    [
                        128.1,
                        854.9
                    ],
                    [
                        814.5,
                        854.9
                    ],
                    [
                        814.5,
                        807.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92043,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[40] A. Graves, G. Wayne, and I. Danihelka, \u201cNeural turing machines,\u201d 2014, arXiv:1410.5401.",
        "type": "ListItem"
    },
    {
        "element_id": "ffd110a9d1c6fa4cc6845ed3c868ebda",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        857.6
                    ],
                    [
                        128.1,
                        904.9
                    ],
                    [
                        814.4,
                        904.9
                    ],
                    [
                        814.4,
                        857.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.916,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[41] S. Gudmundsson et al., \u201cHuman-like playtesting with deep learning,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2018, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "99409b396012f0cb90ca186b1b6c7770",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        758.0
                    ],
                    [
                        858.7,
                        830.1
                    ],
                    [
                        1545.1,
                        830.1
                    ],
                    [
                        1545.1,
                        758.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9312,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[64] D. Jallov, S. Risi, and J. Togelius, \u201cEvoCommander: A novel game based on evolving and switching between arti\ufb01cial brains,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 9, no. 2, pp. 181\u2013191, Jun. 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "f1866471ff761bea0d6d2ec53a1748d4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        832.7
                    ],
                    [
                        858.7,
                        904.7
                    ],
                    [
                        1545.3,
                        904.7
                    ],
                    [
                        1545.3,
                        832.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92331,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[65] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell, \u201cThe Malmo plat- form for arti\ufb01cial intelligence experimentation,\u201d in Proc. Int. Joint Conf. Artif. Intell., 2016, pp. 4246\u20134247.",
        "type": "ListItem"
    },
    {
        "element_id": "0b75657b8568ae661a10a5f39cf7aca5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        907.4
                    ],
                    [
                        128.1,
                        1004.3
                    ],
                    [
                        814.4,
                        1004.3
                    ],
                    [
                        814.4,
                        907.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93897,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[42] X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang, \u201cDeep learn- ing for real-time Atari game play using of\ufb02ine Monte-Carlo tree search planning,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2014, pp. 3338\u20133346.",
        "type": "ListItem"
    },
    {
        "element_id": "e3586193f5ff7eb7e244e7d73f5bf724",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1007.1
                    ],
                    [
                        128.1,
                        1079.0
                    ],
                    [
                        814.4,
                        1079.0
                    ],
                    [
                        814.4,
                        1007.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93678,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[43] Y. Guo, A. Yao, and Y. Chen, \u201cDynamic network surgery for ef- \ufb01cient DNNs,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 1379\u20131387.",
        "type": "ListItem"
    },
    {
        "element_id": "92a15c5430dd86a1b7a5d83d223b6c86",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1081.8
                    ],
                    [
                        128.1,
                        1129.0
                    ],
                    [
                        814.4,
                        1129.0
                    ],
                    [
                        814.4,
                        1081.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91976,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[44] M. Guzdial, B. Li, and M. O. Riedl, \u201cGame engine learning from video,\u201d in Proc. Int. Joint Conf. Artif. Intell., 2017, pp. 3707\u20133713.",
        "type": "ListItem"
    },
    {
        "element_id": "54d5e775089999925d5b23e661c0f576",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        907.4
                    ],
                    [
                        858.7,
                        979.4
                    ],
                    [
                        1548.6,
                        979.4
                    ],
                    [
                        1548.6,
                        907.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92926,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[66] N. Justesen, T. Mahlmann, and J. Togelius, \u201cOnline evolution for multi- action adversarial games,\u201d in Proc. Eur. Conf. Appl. Evol. Comput., 2016, pp. 590\u2013603.",
        "type": "ListItem"
    },
    {
        "element_id": "d87db21ea24de86739166966742e3361",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        982.1
                    ],
                    [
                        858.7,
                        1054.1
                    ],
                    [
                        1551.8,
                        1054.1
                    ],
                    [
                        1551.8,
                        982.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93643,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[67] N. Justesen and S. Risi, \u201cContinual online evolution for in-game build order adaptation in StarCraft,\u201d in Proc. Genetic Evol. Comput. Conf., 2017, pp. 187\u2013194.",
        "type": "ListItem"
    },
    {
        "element_id": "4b349fd6a17416d3e8ccf69122304742",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        1056.9
                    ],
                    [
                        858.7,
                        1128.8
                    ],
                    [
                        1545.1,
                        1128.8
                    ],
                    [
                        1545.1,
                        1056.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92775,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[68] N. Justesen and S. Risi, \u201cLearning macromanagement in StarCraft from replays using deep learning,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 162\u2013169.",
        "type": "ListItem"
    },
    {
        "element_id": "481177c9eb97d1922eb3a7637c81ed4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1131.6
                    ],
                    [
                        128.1,
                        1253.4
                    ],
                    [
                        814.4,
                        1253.4
                    ],
                    [
                        814.4,
                        1131.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93856,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[45] D. Ha and J. Schmidhuber, \u201cRecurrent world models facilitate policy evolution,\u201d in Advances in Neural Information Processing Systems 31, S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa- Bianchi and R. Garnett, Eds. New York, NY, USA: Curran Associates, 2018, pp. 2450\u20132462.",
        "type": "ListItem"
    },
    {
        "element_id": "5edde58e9a5fc237f3eb51d01f98e572",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1256.1
                    ],
                    [
                        128.1,
                        1303.2
                    ],
                    [
                        814.4,
                        1303.2
                    ],
                    [
                        814.4,
                        1256.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9186,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[46] H. V. Hasselt, \u201cDouble q-learning,\u201d in Proc. Int. Conf. Neural Inf. Pro- cess. Syst., 2010, pp. 2613\u20132621.",
        "type": "ListItem"
    },
    {
        "element_id": "e04c337bd44326710b8cd5f7663db78c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        1131.6
                    ],
                    [
                        858.7,
                        1228.5
                    ],
                    [
                        1545.0,
                        1228.5
                    ],
                    [
                        1545.0,
                        1131.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93772,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[69] N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and S. Risi, \u201cIlluminating generalization in deep reinforcement learning through procedural level generation.\u201d in Proc. NeurIPS Workshop Deep Rein- forcement Learn., 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "98bd17a253ca00a37fd6980d36071ae8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1231.2
                    ],
                    [
                        858.6,
                        1303.2
                    ],
                    [
                        1547.9,
                        1303.2
                    ],
                    [
                        1547.9,
                        1231.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92642,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[70] K. Kansky et al., \u201cSchema networks: Zero-shot transfer with a generative causal model of intuitive physics,\u201d in Proc. Int. Conf. Mach. Learn., 2017, pp. 1809\u20131818.",
        "type": "ListItem"
    },
    {
        "element_id": "07f8a1420f3ffb250a4563adccfd4d86",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1305.9
                    ],
                    [
                        128.1,
                        1377.9
                    ],
                    [
                        814.4,
                        1377.9
                    ],
                    [
                        814.4,
                        1305.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93063,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[47] B. Hassibi et al., \u201cSecond order derivatives for network pruning: Opti- mal brain surgeon,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 1993, pp. 164\u2013164.",
        "type": "ListItem"
    },
    {
        "element_id": "e7484832e56079608032976841670165",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1380.7
                    ],
                    [
                        128.1,
                        1452.6
                    ],
                    [
                        814.4,
                        1452.6
                    ],
                    [
                        814.4,
                        1380.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93769,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[48] E. J. Hastings, R. K. Guha, and K. O. Stanley, \u201cAutomatic content generation in the galactic arms race video game,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 1, no. 4, pp. 245\u2013263, Dec. 2009.",
        "type": "ListItem"
    },
    {
        "element_id": "cb7b775524e087a30c17cc6196b4aea7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1455.4
                    ],
                    [
                        128.1,
                        1527.7
                    ],
                    [
                        814.4,
                        1527.7
                    ],
                    [
                        814.4,
                        1455.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93558,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[49] M. Hausknecht, J. Lehman, R. Miikkulainen, and P. Stone, \u201cA neuroevo- lution approach to general Atari game playing,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 6, no. 4, pp. 355\u2013366, Dec. 2014.",
        "type": "ListItem"
    },
    {
        "element_id": "d8f8ad352f8da72df6a6589f8002068c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1530.1
                    ],
                    [
                        128.1,
                        1627.0
                    ],
                    [
                        814.5,
                        1627.0
                    ],
                    [
                        814.5,
                        1530.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9387,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[50] M. Hausknecht, P. Mupparaju, S. Subramanian, S. Kalyanakrishnan, and P. Stone, \u201cHalf \ufb01eld offense: An environment for multiagent learning and ad hoc teamwork,\u201d in Proc. AAMAS Adaptive Learn. Agents Workshop, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "7f36f0db0eeb702aef35a8b24a133275",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1306.0
                    ],
                    [
                        858.6,
                        1353.6
                    ],
                    [
                        1545.0,
                        1353.6
                    ],
                    [
                        1545.0,
                        1306.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91742,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[71] R. Kaplan, C. Sauer, and A. Sosa, \u201cBeating Atari with natural language guided reinforcement learning,\u201d 2017, arXiv:1704.05539.",
        "type": "ListItem"
    },
    {
        "element_id": "ef44a6ed29c6e38a684bae36cefe5b60",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1355.8
                    ],
                    [
                        858.6,
                        1427.7
                    ],
                    [
                        1550.0,
                        1427.7
                    ],
                    [
                        1550.0,
                        1355.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93024,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[72] S. Kelly and M. I. Heywood, \u201cMulti-task learning in Atari video games with emergent tangled program graphs,\u201d in Proc. Genetic Evol. Comput. Conf., 2017, pp. 195\u2013202.",
        "type": "ListItem"
    },
    {
        "element_id": "4fd36196d9de85517118c95294301acb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1430.3
                    ],
                    [
                        858.6,
                        1502.4
                    ],
                    [
                        1546.2,
                        1502.4
                    ],
                    [
                        1546.2,
                        1430.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92617,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[73] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Ja\u00b4skowski, \u201cViZ- Doom: A doom-based AI research platform for visual reinforcement learning,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "c54efa403a5c84c692932506e1cf7ff5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1505.2
                    ],
                    [
                        858.6,
                        1552.5
                    ],
                    [
                        1545.0,
                        1552.5
                    ],
                    [
                        1545.0,
                        1505.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9095,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[74] J. Kirkpatrick et al., \u201cOvercoming catastrophic forgetting in neural networks,\u201d Proc. Nat. Acad. Sci. USA, vol. 114, pp. 3521\u20133526, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "fe6fdde2ce5173044f017780ae6aa3a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1555.0
                    ],
                    [
                        858.6,
                        1627.0
                    ],
                    [
                        1551.6,
                        1627.0
                    ],
                    [
                        1551.6,
                        1555.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93078,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[75] B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski, \u201cText-based adventures of the Golovin AI agent,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 181\u2013188.",
        "type": "ListItem"
    },
    {
        "element_id": "60ef8545e8ddc08a13a5c167c63f9afa",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1629.7
                    ],
                    [
                        128.1,
                        1701.7
                    ],
                    [
                        814.4,
                        1701.7
                    ],
                    [
                        814.4,
                        1629.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93473,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[51] M. Hausknecht and P. Stone, \u201cDeep recurrent q-learning for partially observable MDPs,\u201d in Proc. AAAI Fall Symp. Sequential Decis. Making Intell. Agents, Nov. 2015, pp. 29\u201337.",
        "type": "ListItem"
    },
    {
        "element_id": "33c5c443a5468a98d96d96eb9ac58af4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1629.5
                    ],
                    [
                        858.6,
                        1701.7
                    ],
                    [
                        1545.0,
                        1701.7
                    ],
                    [
                        1545.0,
                        1629.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92784,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[76] J. Koutn\u00b4\u0131k, G. Cuccu, J. Schmidhuber, and F. Gomez, \u201cEvolving large- scale neural networks for vision-based reinforcement learning,\u201d in Proc. 15th Annu. Conf. Genetic Evol. Comput., 2013, pp. 1061\u20131068.",
        "type": "ListItem"
    },
    {
        "element_id": "f99398a2cb89de238ab33521a0ee97c0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1704.4
                    ],
                    [
                        128.1,
                        1801.3
                    ],
                    [
                        815.8,
                        1801.3
                    ],
                    [
                        815.8,
                        1704.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93845,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[52] M. Hausknecht and P. Stone, \u201cDeep reinforcement learning in parameterized action space,\u201d in Proc. Int. Conf. Learn. Repre- sent., May 2016. [Online]. Available: http://www.cs.utexas.edu/users/ai- lab/?hausknecht:iclr16.",
        "type": "ListItem"
    },
    {
        "element_id": "f205d760af7e39ec941b0c8138721eae",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1704.5
                    ],
                    [
                        858.6,
                        1801.3
                    ],
                    [
                        1546.3,
                        1801.3
                    ],
                    [
                        1546.3,
                        1704.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93126,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[77] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, \u201cHierar- chical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 3675\u20133683.",
        "type": "ListItem"
    },
    {
        "element_id": "6dd19a521e3c864766bba3ac08cf76a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1804.1
                    ],
                    [
                        128.1,
                        1876.0
                    ],
                    [
                        816.2,
                        1876.0
                    ],
                    [
                        816.2,
                        1804.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93428,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[53] M. Hausknecht and P. Stone, \u201cOn-policy vs. off-policy updates for deep reinforcement learning,\u201d in Proc. IJCAI Workshop: Deep Reinforcement Learn.: Frontiers Challenges, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "b2875418178446e4aef4accbaa302f62",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.7,
                        1804.1
                    ],
                    [
                        858.7,
                        1876.0
                    ],
                    [
                        1546.4,
                        1876.0
                    ],
                    [
                        1546.4,
                        1804.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92967,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[78] K. Kunanusont, S. M. Lucas, and D. Perez-Liebana, \u201cGeneral video game AI: Learning from screen capture,\u201d in Proc. IEEE Congr. Evol. Comput., 2017, pp. 2078\u20132085.",
        "type": "ListItem"
    },
    {
        "element_id": "45bc9c14635dead31561c2466394f2a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1878.8
                    ],
                    [
                        128.1,
                        1950.7
                    ],
                    [
                        814.4,
                        1950.7
                    ],
                    [
                        814.4,
                        1878.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93272,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[54] J. He et al., \u201cDeep reinforcement learning with a natural language action space,\u201d in Proc. 54th Annu. Meeting Assoc. Comput. Linguistics, 2016, vol. 1, pp. 1621\u20131630.",
        "type": "ListItem"
    },
    {
        "element_id": "4666942327f86286f951ef389f7bbe7b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        128.1,
                        1953.5
                    ],
                    [
                        128.1,
                        2025.5
                    ],
                    [
                        814.6,
                        2025.5
                    ],
                    [
                        814.6,
                        1953.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93135,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[55] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770\u2013778.",
        "type": "ListItem"
    },
    {
        "element_id": "63107be460b36e10ec9f5da2189e22ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1878.8
                    ],
                    [
                        858.6,
                        1926.1
                    ],
                    [
                        1545.0,
                        1926.1
                    ],
                    [
                        1545.0,
                        1878.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91319,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[79] G. Lample and D. S. Chaplot, \u201cPlaying FPS games with deep reinforce- ment learning,\u201d in Proc. AAAI Conf. Artif. Intell., 2017, pp. 2140\u20132146.",
        "type": "ListItem"
    },
    {
        "element_id": "341b37b43dbd3045afdfdd5341f11b4d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1928.4
                    ],
                    [
                        858.6,
                        1975.8
                    ],
                    [
                        1544.9,
                        1975.8
                    ],
                    [
                        1544.9,
                        1928.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91176,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[80] Y. Le Cun, \u201cMod`eles connexionnistes de l\u2019apprentissage,\u201d Ph.D. disser- tation, Pierre Marie Curie Univ., Paris, France, 1987.",
        "type": "ListItem"
    },
    {
        "element_id": "f4ef98acc8bf895a2c0c737efb15d9b4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        858.6,
                        1978.4
                    ],
                    [
                        858.6,
                        2025.5
                    ],
                    [
                        1549.6,
                        2025.5
                    ],
                    [
                        1549.6,
                        1978.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90717,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "[81] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "3cdf5219dc463d46c726f6680e476854",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.0
                    ],
                    [
                        1530.2,
                        2144.0
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59775,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 18,
            "parent_id": "809904bc71620c3907163e4715f85e89"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5881fde87e79800b59c8efde160a58f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        104.9,
                        92.7
                    ],
                    [
                        104.9,
                        112.0
                    ],
                    [
                        663.4,
                        112.0
                    ],
                    [
                        663.4,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62547,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19
        },
        "text": "JUSTESEN et al.: DEEP LEARNING FOR VIDEO GAME PLAYING",
        "type": "Header"
    },
    {
        "element_id": "074a31f4493b6ffc1335376e96c95f7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        185.2
                    ],
                    [
                        116.4,
                        232.3
                    ],
                    [
                        802.7,
                        232.3
                    ],
                    [
                        802.7,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.921,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[82] Y. LeCun et al., \u201cBackpropagation applied to handwritten zip code recognition,\u201d Neural Comput., vol. 1, no. 4, pp. 541\u2013551, 1989.",
        "type": "ListItem"
    },
    {
        "element_id": "62f37fb2bfb008e5ecc0c4ec5d77fe4d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        235.0
                    ],
                    [
                        116.4,
                        282.7
                    ],
                    [
                        803.6,
                        282.7
                    ],
                    [
                        803.6,
                        235.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9117,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[83] S. Legg and M. Hutter, \u201cUniversal intelligence: A de\ufb01nition of machine intelligence,\u201d Minds Mach., vol. 17, no. 4, pp. 391\u2013444, 2007.",
        "type": "ListItem"
    },
    {
        "element_id": "6ed09d5455f59c9dc41ea9e51d43fdd3",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        284.8
                    ],
                    [
                        116.4,
                        356.7
                    ],
                    [
                        802.7,
                        356.7
                    ],
                    [
                        802.7,
                        284.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92404,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[84] J. Lehman and K. O. Stanley, \u201cExploiting open-endedness to solve prob- lems through the search for novelty,\u201d in Proc. 11th Int. Conf. Artif. Life, 2008, pp. 329\u2013336.",
        "type": "ListItem"
    },
    {
        "element_id": "209fd31ff3a78da62aff69875f86f16f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        359.5
                    ],
                    [
                        116.4,
                        431.4
                    ],
                    [
                        802.7,
                        431.4
                    ],
                    [
                        802.7,
                        359.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93108,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[85] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel, \u201cMulti- agent reinforcement learning in sequential social dilemmas,\u201d in Proc. 16th Conf. Auton. Agents Multiagent Syst., 2017, pp. 464\u2013473.",
        "type": "ListItem"
    },
    {
        "element_id": "f458ccfabed5253c85436d87bf400f87",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        185.2
                    ],
                    [
                        835.9,
                        257.1
                    ],
                    [
                        1533.3,
                        257.1
                    ],
                    [
                        1533.3,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93168,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[109] M. Parker and B. D. Bryant, \u201cNeurovisual control in the Quake II environment,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 4, no. 1, pp. 44\u201354, Mar. 2012.",
        "type": "ListItem"
    },
    {
        "element_id": "1d09a683ef6c06fd3970b34720b36b58",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        259.9
                    ],
                    [
                        835.9,
                        331.8
                    ],
                    [
                        1537.4,
                        331.8
                    ],
                    [
                        1537.4,
                        259.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93661,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[110] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, \u201cCuriosity-driven exploration by self-supervised prediction,\u201d in Proc. Int. Conf. Mach. Learn., 2017, pp. 2778\u20132787.",
        "type": "ListItem"
    },
    {
        "element_id": "3b5c062baec3fbd11f4d245c6116d102",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        334.6
                    ],
                    [
                        835.9,
                        381.6
                    ],
                    [
                        1533.3,
                        381.6
                    ],
                    [
                        1533.3,
                        334.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91943,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[111] P. Peng et al., \u201cMultiagent bidirectionally-coordinated nets for learning to play StarCraft combat games,\u201d 2017, arXiv:1703.10069.",
        "type": "ListItem"
    },
    {
        "element_id": "0179c7752a51af6ee827b594803c30e5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        384.4
                    ],
                    [
                        835.9,
                        431.4
                    ],
                    [
                        1534.3,
                        431.4
                    ],
                    [
                        1534.3,
                        384.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91555,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[112] T. Pohlen et al., \u201cObserve and look further: Achieving consistent per- formance on Atari,\u201d 2018, arXiv:1805.11593.",
        "type": "ListItem"
    },
    {
        "element_id": "2e778e659ecc72fec25d4bb51ae019dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        434.2
                    ],
                    [
                        116.4,
                        481.7
                    ],
                    [
                        802.8,
                        481.7
                    ],
                    [
                        802.8,
                        434.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91258,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[86] A. Lerer, S. Gross, and R. Fergus, \u201cLearning physical intuition of block towers by example,\u201d in Proc. Int. Conf. Mach. Learn., 2016, pp. 430\u2013438.",
        "type": "ListItem"
    },
    {
        "element_id": "511dc56c3d923ece3c9f3766396ab0f1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        484.0
                    ],
                    [
                        116.4,
                        531.1
                    ],
                    [
                        802.7,
                        531.1
                    ],
                    [
                        802.7,
                        484.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92699,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[87] Y. Li, \u201cDeep reinforcement learning: An overview,\u201d 2017, arXiv:1701.07274.",
        "type": "ListItem"
    },
    {
        "element_id": "a650ce376c9cc2065d068eb19881dd33",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        533.9
                    ],
                    [
                        116.4,
                        580.9
                    ],
                    [
                        802.8,
                        580.9
                    ],
                    [
                        802.8,
                        533.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91624,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[88] T. P. Lillicrap et al., \u201cContinuous control with deep reinforcement learn- ing,\u201d in Proc. Int. Conf. Learn. Represent., 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "11dd16a3c1d1444144d8d33b00a0cf68",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        583.7
                    ],
                    [
                        116.4,
                        655.6
                    ],
                    [
                        802.7,
                        655.6
                    ],
                    [
                        802.7,
                        583.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92477,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[89] L.-J. Lin, \u201cReinforcement learning for robots using neural networks,\u201d Ph.D. dissertation, School Comput. Sci., Carnegie Mellon Univ., Pitts- burgh, PA, USA, 1993.",
        "type": "ListItem"
    },
    {
        "element_id": "20571adbde2f7330d68cc5f462d20565",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        658.4
                    ],
                    [
                        116.4,
                        705.4
                    ],
                    [
                        802.7,
                        705.4
                    ],
                    [
                        802.7,
                        658.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91248,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[90] S. M. Lucas and G. Kendall, \u201cEvolutionary computation and games,\u201d IEEE Comput. Intell. Mag., vol. 1, no. 1, pp. 10\u201318, Feb. 2006.",
        "type": "ListItem"
    },
    {
        "element_id": "4836285b2cefa595a9cd660f40614eb8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        708.2
                    ],
                    [
                        116.4,
                        805.0
                    ],
                    [
                        802.8,
                        805.0
                    ],
                    [
                        802.8,
                        708.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93507,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[91] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling, \u201cRevisiting the arcade learning environment: Evaluation protocols and open problems for general agents,\u201d J. Artif. Intell. Res., vol. 61, pp. 523\u2013562, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "ce48ea4b77c3fc4f79dafab38b801563",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        807.8
                    ],
                    [
                        116.4,
                        855.8
                    ],
                    [
                        802.7,
                        855.8
                    ],
                    [
                        802.7,
                        807.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91874,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[92] T. Matiisen, A. Oliver, T. Cohen, and J. Schulman, \u201cTeacher-student curriculum learning,\u201d in Proc. Deep Reinforcement Learn. Symp., 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "3028cf1fc25bf084cc10c9177391bf6a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        434.2
                    ],
                    [
                        835.9,
                        506.2
                    ],
                    [
                        1537.2,
                        506.2
                    ],
                    [
                        1537.2,
                        434.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93178,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[113] A. P. Poulsen, M. Thorhauge, M. Hvilshj, and S. Risi, \u201cDLNE: A hy- bridization of deep learning and neuroevolution for visual control,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 256\u2013263.",
        "type": "ListItem"
    },
    {
        "element_id": "caf58b388ae8fc9bbbe2dcd1a4faae98",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        508.9
                    ],
                    [
                        835.9,
                        605.8
                    ],
                    [
                        1538.9,
                        605.8
                    ],
                    [
                        1538.9,
                        508.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93557,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[114] S. Risi, J. Lehman, D. B. D\u2019Ambrosio, R. Hall, and K. O. Stanley, \u201cCom- bining search-based procedural content generation and social gaming in the Petalz video game,\u201d in Proc. 8th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2012, pp. 63\u201368.",
        "type": "ListItem"
    },
    {
        "element_id": "41f771790c467873597d0096b02d2813",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        608.6
                    ],
                    [
                        835.9,
                        680.5
                    ],
                    [
                        1533.3,
                        680.5
                    ],
                    [
                        1533.3,
                        608.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93125,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[115] S. Risi and J. Togelius, \u201cNeuroevolution in games: State of the art and open challenges,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 9, no. 1, pp. 25\u201341, Mar. 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "3dae0a7cc659d78dca69aea50c06342e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        683.3
                    ],
                    [
                        835.9,
                        755.2
                    ],
                    [
                        1533.3,
                        755.2
                    ],
                    [
                        1533.3,
                        683.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9279,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[116] R. Rodriguez Torrado, P. Bontrager, J. Togelius, J. Liu, and D. Perez- Liebana, \u201cDeep reinforcement learning for general video game AI,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2018, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "9511f807c0ba4d2265fc6885587ba7fb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        758.0
                    ],
                    [
                        835.9,
                        854.9
                    ],
                    [
                        1538.9,
                        854.9
                    ],
                    [
                        1538.9,
                        758.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93818,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[117] D. E. Rumelhart et al., \u201cA general framework for parallel distributed processing,\u201d in Parallel Distributed Processing: Explorations in the Mi- crostructure of Cognition, vol. 1. Cambridge, MA, USA: MIT Press, 1986, pp. 45\u201376.",
        "type": "ListItem"
    },
    {
        "element_id": "ddf812f3684a25ec1addb77142ed5fe0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        857.6
                    ],
                    [
                        116.4,
                        954.5
                    ],
                    [
                        802.7,
                        954.5
                    ],
                    [
                        802.7,
                        857.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93706,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[93] R. Miikkulainen, B. D. Bryant, R. Cornelius, I. V. Karpov, K. O. Stanley, and C. H. Yong, \u201cComputational intelligence in games,\u201d in Computa- tional Intelligence: Principles and Practice. Piscataway, NJ, USA: IEEE Comput. Intell. Soc., 2006, pp. 155\u2013191.",
        "type": "ListItem"
    },
    {
        "element_id": "5bac77e0f1a911c9d9404ebe43c366d9",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        957.2
                    ],
                    [
                        116.4,
                        1004.3
                    ],
                    [
                        802.7,
                        1004.3
                    ],
                    [
                        802.7,
                        957.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91351,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[94] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of word representations in vector space,\u201d 2013, arXiv:1301.3781.",
        "type": "ListItem"
    },
    {
        "element_id": "d3447cc4dd2da8e6461df950d9c09d24",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1007.1
                    ],
                    [
                        116.4,
                        1029.2
                    ],
                    [
                        282.2,
                        1029.2
                    ],
                    [
                        282.2,
                        1007.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "5881fde87e79800b59c8efde160a58f7"
        },
        "text": "[95] P. Mirowski",
        "type": "Title"
    },
    {
        "element_id": "3beca2b575c9c2124109b67befbd30ef",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        132.8,
                        1007.1
                    ],
                    [
                        132.8,
                        1103.9
                    ],
                    [
                        808.4,
                        1103.9
                    ],
                    [
                        808.4,
                        1007.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93761,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "et al., \u201cLearning to navigate in complex environ- ments,\u201d in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model",
        "type": "ListItem"
    },
    {
        "element_id": "71c013245352d9c38c70254a34fae52e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1106.7
                    ],
                    [
                        116.4,
                        1153.7
                    ],
                    [
                        802.7,
                        1153.7
                    ],
                    [
                        802.7,
                        1106.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91982,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[96] V. Mnih et al., \u201cAsynchronous methods for deep reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn., 2016, pp. 1928\u20131937.",
        "type": "ListItem"
    },
    {
        "element_id": "892406ec0ea1b4803e22e49ce192d8f2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        857.6
                    ],
                    [
                        835.9,
                        904.7
                    ],
                    [
                        1533.3,
                        904.7
                    ],
                    [
                        1533.3,
                        857.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91925,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[118] G. A. Rummery and M. Niranjan, On-Line Q-Learning Using Connec- tionist Systems, vol. 37. Cambridge, U.K.: Univ. Cambridge, 1994.",
        "type": "ListItem"
    },
    {
        "element_id": "c897b2948b4bd157cd5cd17b8513980e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        907.4
                    ],
                    [
                        835.9,
                        979.4
                    ],
                    [
                        1534.8,
                        979.4
                    ],
                    [
                        1534.8,
                        907.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93547,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[119] A. A. Rusu et al., \u201cPolicy distillation,\u201d in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Available: https://sites.google.com/site/ representationlearning2014/program-details/publication-model",
        "type": "ListItem"
    },
    {
        "element_id": "0628c77f96c54f5b9bda7b87975faa16",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        982.1
                    ],
                    [
                        835.9,
                        1029.2
                    ],
                    [
                        1533.3,
                        1029.2
                    ],
                    [
                        1533.3,
                        982.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91687,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[120] A. A. Rusu et al., \u201cProgressive neural networks,\u201d 2016, arXiv: 1606.04671.",
        "type": "ListItem"
    },
    {
        "element_id": "d3ec5e3fdc87b299d07c67b9a88558fe",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1032.0
                    ],
                    [
                        835.9,
                        1079.4
                    ],
                    [
                        1533.3,
                        1079.4
                    ],
                    [
                        1533.3,
                        1032.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91758,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[121] T. Salimans, J. Ho, X. Chen, and I. Sutskever, \u201cEvolution strategies as a scalable alternative to reinforcement learning,\u201d 2017, arXiv:1703.03864.",
        "type": "ListItem"
    },
    {
        "element_id": "ea7e2d5637426e557524d938a8c3960f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1081.8
                    ],
                    [
                        835.9,
                        1153.7
                    ],
                    [
                        1535.8,
                        1153.7
                    ],
                    [
                        1535.8,
                        1081.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93219,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[122] T. Schaul, \u201cA video game description language for model-based or in- teractive learning,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2013, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "6b639e8780d27b84437020a14f7223ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1156.5
                    ],
                    [
                        116.4,
                        1203.6
                    ],
                    [
                        802.7,
                        1203.6
                    ],
                    [
                        802.7,
                        1156.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91202,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[97] V. Mnih et al., \u201cPlaying Atari with deep reinforcement learning,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., Deep Learn. Workshop, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "d127e32e44493c72ffd0bb0e5b129636",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1156.5
                    ],
                    [
                        835.9,
                        1203.6
                    ],
                    [
                        1533.3,
                        1203.6
                    ],
                    [
                        1533.3,
                        1156.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91352,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[123] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d in Proc. Int. Conf. Learn. Represent., 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "af4b8b588d54857fd18e41589d0bd54d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1206.3
                    ],
                    [
                        116.4,
                        1253.7
                    ],
                    [
                        802.7,
                        1253.7
                    ],
                    [
                        802.7,
                        1206.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91093,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[98] V. Mnih et al., \u201cHuman-level control through deep reinforcement learn- ing,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "033e346eb67087318b401aa1e239a582",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1206.3
                    ],
                    [
                        835.9,
                        1253.4
                    ],
                    [
                        1533.3,
                        1253.4
                    ],
                    [
                        1533.3,
                        1206.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91404,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[124] T. Schaul, J. Togelius, and J. Schmidhuber, \u201cMeasuring intelligence through games,\u201d 2011, arXiv:1109.1314.",
        "type": "ListItem"
    },
    {
        "element_id": "d5cdf0a94fcb7eb047ebb2e82fec1d90",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.4,
                        1255.9
                    ],
                    [
                        116.4,
                        1328.1
                    ],
                    [
                        802.7,
                        1328.1
                    ],
                    [
                        802.7,
                        1255.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9299,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[99] H. Mu\u02dcnoz-Avila, C. Bauckhage, M. Bida, C. B. Congdon, and G. Kendall, \u201cLearning and game AI,\u201d in Dagstuhl Follow-Ups, vol. 6. Wadern, Ger- many: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "bf97126c9e775c72aeb26cb6f3c9ee36",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1256.1
                    ],
                    [
                        835.9,
                        1328.3
                    ],
                    [
                        1535.3,
                        1328.3
                    ],
                    [
                        1535.3,
                        1256.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93445,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[125] J. Schmidhuber, \u201cFormal theory of creativity, fun, and intrinsic motiva- tion (1990\u20132010),\u201d IEEE Trans. Auton. Mental Develop., vol. 2, no. 3, pp. 230\u2013247, Sep. 2010.",
        "type": "ListItem"
    },
    {
        "element_id": "1cabea41e343d6aa01aadceca4357bcb",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1330.8
                    ],
                    [
                        105.3,
                        1377.9
                    ],
                    [
                        802.7,
                        1377.9
                    ],
                    [
                        802.7,
                        1330.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90527,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[100] A. Nair et al., \u201cMassively parallel methods for deep reinforcement learning,\u201d 2015, arXiv:1507.04296.",
        "type": "ListItem"
    },
    {
        "element_id": "4d004a30bfb90fce7cdee923a212e2bc",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1330.9
                    ],
                    [
                        835.9,
                        1378.3
                    ],
                    [
                        1533.3,
                        1378.3
                    ],
                    [
                        1533.3,
                        1330.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91151,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[126] J. Schmidhuber, \u201cDeep learning in neural networks: An overview,\u201d Neu- ral Netw., vol. 61, pp. 85\u2013117, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "643a64cac0c47e53daf686acbf411016",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1380.7
                    ],
                    [
                        105.3,
                        1452.6
                    ],
                    [
                        802.7,
                        1452.6
                    ],
                    [
                        802.7,
                        1380.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93776,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[101] K. Narasimhan, T. D. Kulkarni, and R. Barzilay, \u201cLanguage understand- ing for textbased games using deep reinforcement learning,\u201d in Proc. Conf. Empirical Methods Natural Lang. Process., 2015, pp. 1\u201311.",
        "type": "ListItem"
    },
    {
        "element_id": "a147d2741b945b0cce3df16608702863",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1380.7
                    ],
                    [
                        835.9,
                        1452.6
                    ],
                    [
                        1533.3,
                        1452.6
                    ],
                    [
                        1533.3,
                        1380.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93535,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[127] J. Schrum, I. V. Karpov, and R. Miikkulainen, \u201cUT 2: Human-like behav- ior via neuroevolution of combat behavior and replay of human traces,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2011, pp. 329\u2013336.",
        "type": "ListItem"
    },
    {
        "element_id": "c593a3818904f36620414e24a8382c94",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1455.4
                    ],
                    [
                        105.3,
                        1527.3
                    ],
                    [
                        802.8,
                        1527.3
                    ],
                    [
                        802.8,
                        1455.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93542,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[102] J. Oh, V. Chockalingam, S. Singh, and H. Lee, \u201cControl of memory, active perception, and action in Minecraft,\u201d in Proc. 33rd Int. Conf. Mach. Learn., 2016, vol. 48, pp. 2790\u20132799.",
        "type": "ListItem"
    },
    {
        "element_id": "0e4dc33f72bb279232423d769e597142",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1455.4
                    ],
                    [
                        835.9,
                        1527.3
                    ],
                    [
                        1535.5,
                        1527.3
                    ],
                    [
                        1535.5,
                        1455.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93625,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[128] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, \u201cTrust region policy optimization,\u201d in Proc. Int. Conf. Mach. Learn., 2015, pp. 1889\u20131897.",
        "type": "ListItem"
    },
    {
        "element_id": "626191ee21b991057140eeba5793e2a7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1530.1
                    ],
                    [
                        105.3,
                        1602.1
                    ],
                    [
                        802.7,
                        1602.1
                    ],
                    [
                        802.7,
                        1530.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9332,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[103] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, \u201cAction-conditional video prediction using deep networks in Atari games,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2015, pp. 2863\u20132871.",
        "type": "ListItem"
    },
    {
        "element_id": "504a4d5eb1ec476f3be9ae81e087c85f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1604.6
                    ],
                    [
                        105.3,
                        1676.8
                    ],
                    [
                        802.7,
                        1676.8
                    ],
                    [
                        802.7,
                        1604.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93332,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[104] S. Ontan\u00b4on, \u201cThe combinatorial multi-armed bandit problem and its application to real-time strategy games,\u201d in Proc. 9th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2013, pp. 58\u201364.",
        "type": "ListItem"
    },
    {
        "element_id": "73834e2198d067b9b79a3ed9456416f8",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1679.6
                    ],
                    [
                        105.3,
                        1751.5
                    ],
                    [
                        804.8,
                        1751.5
                    ],
                    [
                        804.8,
                        1679.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93034,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[105] J. Ortega, N. Shaker, J. Togelius, and G. N. Yannakakis, \u201cImitating human playing styles in Super Mario Bros,\u201d Entertainment Comput., vol. 4, no. 2, pp. 93\u2013104, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "a1292b0157aa03ff2d4c836a00c75857",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1754.3
                    ],
                    [
                        105.3,
                        1826.3
                    ],
                    [
                        802.7,
                        1826.3
                    ],
                    [
                        802.7,
                        1754.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93581,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[106] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy, \u201cDeep exploration via bootstrapped DQN,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 4026\u20134034.",
        "type": "ListItem"
    },
    {
        "element_id": "9e7cb6ab35e0998efff4c2337db05f72",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1530.1
                    ],
                    [
                        835.9,
                        1577.2
                    ],
                    [
                        1533.3,
                        1577.2
                    ],
                    [
                        1533.3,
                        1530.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92114,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[129] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProx- imal policy optimization algorithms,\u201d 2017, arXiv:1707.06347.",
        "type": "ListItem"
    },
    {
        "element_id": "1de7593e779fe1d5e475f6b7d549b46e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1579.9
                    ],
                    [
                        835.9,
                        1627.6
                    ],
                    [
                        1535.7,
                        1627.6
                    ],
                    [
                        1535.7,
                        1579.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91077,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[130] N. Shaker, J. Togelius, and M. J. Nelson, Procedural Content Generation in Games. New York, NY, USA: Springer, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "3c691b61dba494a7469f09f56270dd8f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1629.7
                    ],
                    [
                        835.9,
                        1701.7
                    ],
                    [
                        1533.3,
                        1701.7
                    ],
                    [
                        1533.3,
                        1629.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93042,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[131] N. Shaker et al., \u201cThe turing test track of the 2012 Mario AI Cham- pionship: Entries and evaluation,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2013, pp. 1\u20138.",
        "type": "ListItem"
    },
    {
        "element_id": "7e40a816eab8fbf656da194e1decc650",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1704.5
                    ],
                    [
                        835.9,
                        1776.4
                    ],
                    [
                        1536.7,
                        1776.4
                    ],
                    [
                        1536.7,
                        1704.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93709,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[132] E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell, \u201cLoss is its own reward: Self-supervision for reinforcement learning,\u201d in Proc. Int. Conf. Learn. Represent., 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "dce26e6da3a5781850d9b8446eb97e6d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1779.2
                    ],
                    [
                        835.9,
                        1826.5
                    ],
                    [
                        1533.8,
                        1826.5
                    ],
                    [
                        1533.8,
                        1779.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92008,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[133] D. Silver et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "f71c67cd69236f06502d3403ec99b1a0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1829.0
                    ],
                    [
                        105.3,
                        1901.3
                    ],
                    [
                        802.7,
                        1901.3
                    ],
                    [
                        802.7,
                        1829.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93687,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[107] G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos, \u201cCount- based exploration with neural density models,\u201d in Proc. 34th Int. Conf. Mach. Learn., 2017, pp. 2721\u20132730.",
        "type": "ListItem"
    },
    {
        "element_id": "354e0293dcf10815b4250306cd20813a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1829.0
                    ],
                    [
                        835.9,
                        1900.9
                    ],
                    [
                        1535.9,
                        1900.9
                    ],
                    [
                        1535.9,
                        1829.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93453,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[134] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, \u201cDeterministic policy gradient algorithms,\u201d in Proc. 31st Int. Conf. Mach. Learn., 2014, pp. 387\u2013395.",
        "type": "ListItem"
    },
    {
        "element_id": "82ed2a68594bbfc421754915d587c3f5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        105.3,
                        1903.7
                    ],
                    [
                        105.3,
                        2025.5
                    ],
                    [
                        803.7,
                        2025.5
                    ],
                    [
                        803.7,
                        1903.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93565,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[108] E. Parisotto, J. L. Ba, and R. Salakhutdinov, \u201cActor-mimic: Deep multitask and transfer reinforcement learning,\u201d in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Available: https://sites.google.com/site/representationlearning2014/program- details/publication-model",
        "type": "ListItem"
    },
    {
        "element_id": "e8b081aaf177ad501bbfcdd31c22b920",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1903.7
                    ],
                    [
                        835.9,
                        1975.7
                    ],
                    [
                        1533.3,
                        1975.7
                    ],
                    [
                        1533.3,
                        1903.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92832,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[135] P. D. S\u00f8rensen, J. M. Olsen, and S. Risi, \u201cBreeding a diversity of super Mario behaviors through interactive evolution,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1\u20137.",
        "type": "ListItem"
    },
    {
        "element_id": "b5dfb0ffbcdb1d011634ae391b89dfb2",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        835.9,
                        1978.4
                    ],
                    [
                        835.9,
                        2050.4
                    ],
                    [
                        1537.1,
                        2050.4
                    ],
                    [
                        1537.1,
                        1978.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93235,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "[136] M. Stanescu, N. A. Barriga, A. Hess, and M. Buro, \u201cEvaluating real-time strategy game states using convolutional neural networks,\u201d in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1\u20137.",
        "type": "ListItem"
    },
    {
        "element_id": "c5bc652d84cb1a855e79178f8fcc957c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.4
                    ],
                    [
                        1530.2,
                        2144.4
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48079,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19,
            "parent_id": "d3447cc4dd2da8e6461df950d9c09d24"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "ListItem"
    },
    {
        "element_id": "2c6bb9519b0e1c31cc9b5cb61f914e69",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        1513.9,
                        92.7
                    ],
                    [
                        1513.9,
                        112.6
                    ],
                    [
                        1534.5,
                        112.6
                    ],
                    [
                        1534.5,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78081,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 19
        },
        "text": "19",
        "type": "Header"
    },
    {
        "element_id": "287ec6dd4032caa915f9f65db4ba8a08",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        116.2,
                        92.7
                    ],
                    [
                        116.2,
                        112.5
                    ],
                    [
                        137.6,
                        112.5
                    ],
                    [
                        137.6,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74906,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20
        },
        "text": "20",
        "type": "Header"
    },
    {
        "element_id": "595fda1597fc24ec108d710536d91b6a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        986.3,
                        92.7
                    ],
                    [
                        986.3,
                        112.1
                    ],
                    [
                        1545.8,
                        112.1
                    ],
                    [
                        1545.8,
                        92.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64784,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20
        },
        "text": "IEEE TRANSACTIONS ON GAMES, VOL. 12, NO. 1, MARCH 2020",
        "type": "Header"
    },
    {
        "element_id": "212d670593c6318f4095cb87b70a4566",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        185.2
                    ],
                    [
                        117.0,
                        257.1
                    ],
                    [
                        814.4,
                        257.1
                    ],
                    [
                        814.4,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93287,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[137] K. O. Stanley, B. D. Bryant, and R. Miikkulainen, \u201cReal-time neuroevo- lution in the NERO video game,\u201d IEEE Trans. Evol. Comput., vol. 9, no. 6, pp. 653\u2013668, Dec. 2005.",
        "type": "ListItem"
    },
    {
        "element_id": "1ac63f504f3db87e9ab162e5dbbe2112",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        259.9
                    ],
                    [
                        117.0,
                        306.9
                    ],
                    [
                        814.4,
                        306.9
                    ],
                    [
                        814.4,
                        259.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91723,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[138] P. Stone and R. S. Sutton, \u201cKeepaway soccer: A machine learning test bed,\u201d in Proc. Robot Soccer World Cup, 2001, pp. 214\u2013223.",
        "type": "ListItem"
    },
    {
        "element_id": "9fb6bf7815c6fb4f1d11e2a9f9900b2b",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        309.7
                    ],
                    [
                        117.0,
                        406.5
                    ],
                    [
                        817.2,
                        406.5
                    ],
                    [
                        817.2,
                        309.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93851,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[139] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune, \u201cDeep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning,\u201d in Proc. 11th Annu. Conf. Genetic Evol. Comput., 2017, pp. 145\u2013152.",
        "type": "ListItem"
    },
    {
        "element_id": "7a957a673c674903d42eee1a02538b8c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        185.2
                    ],
                    [
                        847.6,
                        232.4
                    ],
                    [
                        1545.0,
                        232.4
                    ],
                    [
                        1545.0,
                        185.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91884,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[157] O. Vinyals et al., \u201cStarcraft II: A new challenge for reinforcement learning,\u201d 2017, arXiv:1708.04782.",
        "type": "ListItem"
    },
    {
        "element_id": "2d98e8faced1e172d7eb30b8ebd5dd2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        235.0
                    ],
                    [
                        847.6,
                        331.8
                    ],
                    [
                        1545.0,
                        331.8
                    ],
                    [
                        1545.0,
                        235.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93929,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[158] V. Volz, J. Schrum, J. Liu, S. M. Lucas, A. Smith, and S. Risi, \u201cEvolv- ing Mario levels in the latent space of a deep convolutional genera- tive adversarial network,\u201d in Proc. Genetic Evol. Comput. Conf., 2018, pp. 221\u2013228.",
        "type": "ListItem"
    },
    {
        "element_id": "5221d3e9cce4ffe4491f404e7ab83f2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        334.4
                    ],
                    [
                        847.6,
                        407.0
                    ],
                    [
                        1545.0,
                        407.0
                    ],
                    [
                        1545.0,
                        334.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93402,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[159] C. Wang, P. Chen, Y. Li, C. Holmg\u02daard, and J. Togelius, \u201cPortfolio on- line evolution in StarCraft,\u201d in Proc. 12th Artif. Intell. Interact. Digit. Entertainment Conf., 2016, pp. 114\u2013121.",
        "type": "ListItem"
    },
    {
        "element_id": "89af86c14301c6790f6bb9d728053004",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        409.3
                    ],
                    [
                        117.0,
                        481.3
                    ],
                    [
                        814.5,
                        481.3
                    ],
                    [
                        814.5,
                        409.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92996,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[140] A. Summerville et al., \u201cProcedural content generation via machine learning (PCGML),\u201d IEEE Trans. Games, vol. 10, no. 3, pp. 257\u2013270, Sep. 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "96d8304527b95ebacf50f5f77f45d8e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        484.0
                    ],
                    [
                        117.0,
                        531.1
                    ],
                    [
                        814.4,
                        531.1
                    ],
                    [
                        814.4,
                        484.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92061,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[141] P. Sun et al., \u201cTStarBots: Defeating the cheating level builtin AI in StarCraft II in the full game,\u201d 2018, arXiv:1809.07193.",
        "type": "ListItem"
    },
    {
        "element_id": "61f59c3ea58aec413c1e4a5ae1336846",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        409.3
                    ],
                    [
                        847.6,
                        456.4
                    ],
                    [
                        1545.0,
                        456.4
                    ],
                    [
                        1545.0,
                        409.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91671,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[160] Z. Wang et al., \u201cSample ef\ufb01cient actor-critic with experience replay,\u201d 2017, arXiv:1611.01224.",
        "type": "ListItem"
    },
    {
        "element_id": "4ac1aeb01bfc46a5ffd2eaabc3aade85",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        459.1
                    ],
                    [
                        847.6,
                        531.3
                    ],
                    [
                        1545.4,
                        531.3
                    ],
                    [
                        1545.4,
                        459.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93173,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[161] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas, \u201cDueling network architectures for deep reinforcement learning,\u201d in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1995\u20132003.",
        "type": "ListItem"
    },
    {
        "element_id": "02ff60ddea3b07725b8857cd3407d71c",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        533.9
                    ],
                    [
                        117.0,
                        581.1
                    ],
                    [
                        814.4,
                        581.1
                    ],
                    [
                        814.4,
                        533.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91661,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[142] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, vol. 1. Cambridge, MA, USA: MIT Press, 1998.",
        "type": "ListItem"
    },
    {
        "element_id": "6d8c80759660166f81613ca1a7c85a54",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        533.9
                    ],
                    [
                        847.6,
                        580.9
                    ],
                    [
                        1545.0,
                        580.9
                    ],
                    [
                        1545.0,
                        533.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92027,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[162] C. J. Watkins and P. Dayan, \u201cQ-learning,\u201d Mach. Learn., vol. 8, nos. 3/4, pp. 279\u2013292, 1992.",
        "type": "ListItem"
    },
    {
        "element_id": "1c1366b1ed3808105aa33d13515f26a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        583.7
                    ],
                    [
                        117.0,
                        655.6
                    ],
                    [
                        817.2,
                        655.6
                    ],
                    [
                        817.2,
                        583.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93113,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[143] R. S. Sutton et al., \u201cPolicy gradient methods for reinforcement learning with function approximation,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 1999, vol. 99, pp. 1057\u20131063.",
        "type": "ListItem"
    },
    {
        "element_id": "881ee5931d74255c74a9af92b9bba775",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        658.4
                    ],
                    [
                        117.0,
                        705.4
                    ],
                    [
                        814.4,
                        705.4
                    ],
                    [
                        814.4,
                        658.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91772,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[144] P. Sweetser, Emergence in Games. Boston, MA, USA: Cengage Learn- ing, 2008.",
        "type": "ListItem"
    },
    {
        "element_id": "be39a0b044ebe9e87bec194accf5155d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        583.7
                    ],
                    [
                        847.6,
                        630.7
                    ],
                    [
                        1545.0,
                        630.7
                    ],
                    [
                        1545.0,
                        583.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.915,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[163] M. Wiering and J. Schmidhuber, \u201cHQ-learning,\u201d Adaptive Behav., vol. 6, no. 2, pp. 219\u2013246, 1997.",
        "type": "ListItem"
    },
    {
        "element_id": "5d0e4642763626a7e78288f337ad7a49",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        633.5
                    ],
                    [
                        847.6,
                        705.4
                    ],
                    [
                        1545.1,
                        705.4
                    ],
                    [
                        1545.1,
                        633.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92718,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[164] R. J. Williams, \u201cSimple statistical gradient-following algorithms for connectionist reinforcement learning,\u201d Mach. Learn., vol. 8, nos. 3/4, pp. 229\u2013256, 1992.",
        "type": "ListItem"
    },
    {
        "element_id": "e58368d35db4d812cc68c09cc0f88112",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        708.2
                    ],
                    [
                        117.0,
                        755.2
                    ],
                    [
                        814.4,
                        755.2
                    ],
                    [
                        814.4,
                        708.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91635,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[145] G. Synnaeve et al., \u201cTorchCraft: A library for machine learning research on real-time strategy games,\u201d 2016, arXiv:1611.00625.",
        "type": "ListItem"
    },
    {
        "element_id": "56faf6e3620400fd465287d1081e864a",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        758.0
                    ],
                    [
                        117.0,
                        805.6
                    ],
                    [
                        817.3,
                        805.6
                    ],
                    [
                        817.3,
                        758.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92061,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[146] A. Tampuu et al., \u201cMultiagent cooperation and competition with deep re- inforcement learning,\u201d PloS One, vol. 12, no. 4, 2017, Art. no. e0172395.",
        "type": "ListItem"
    },
    {
        "element_id": "ad7199ff8114381e46e31cb0360281ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        807.8
                    ],
                    [
                        117.0,
                        855.9
                    ],
                    [
                        814.5,
                        855.9
                    ],
                    [
                        814.5,
                        807.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91589,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[147] M. Tan, \u201cMulti-agent reinforcement learning: Independent vs. coopera- tive agents,\u201d in Proc. 10th Int. Conf. Mach. Learn., 1993, pp. 330\u2013337.",
        "type": "ListItem"
    },
    {
        "element_id": "bfe52cff0801228277caf6c165534781",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        857.6
                    ],
                    [
                        117.0,
                        929.6
                    ],
                    [
                        814.4,
                        929.6
                    ],
                    [
                        814.4,
                        857.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92614,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[148] Z. Tang, D. Zhao, Y. Zhu, and P. Guo, \u201cReinforcement learning for build- order production in StarCraft II,\u201d in Proc. 8th Int. Conf. Inf. Sci. Technol., 2018, pp. 153\u2013158.",
        "type": "ListItem"
    },
    {
        "element_id": "82620f842b70fa5a9bd68a14ab930166",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        932.3
                    ],
                    [
                        117.0,
                        979.4
                    ],
                    [
                        814.4,
                        979.4
                    ],
                    [
                        814.4,
                        932.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91013,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[149] Y. Teh et al., \u201cDistral: Robust multitask reinforcement learning,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 4497\u20134507.",
        "type": "ListItem"
    },
    {
        "element_id": "0438e46396f013afde17d4910f67bde0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        982.1
                    ],
                    [
                        117.0,
                        1079.0
                    ],
                    [
                        814.4,
                        1079.0
                    ],
                    [
                        814.4,
                        982.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93815,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[150] C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor, \u201cA deep hierarchical approach to lifelong learning in minecraft,\u201d in Proc. 31st AAAI Conf. Artif. Intell., San Francisco, CA, USA, Feb. 4\u20139, 2017, pp. 1553\u20131561.",
        "type": "ListItem"
    },
    {
        "element_id": "53f61dcd2220cdb4c1d28df0036e57ed",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1081.8
                    ],
                    [
                        117.0,
                        1178.6
                    ],
                    [
                        814.4,
                        1178.6
                    ],
                    [
                        814.4,
                        1081.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93362,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[151] Y. Tian, Q. Gong, W. Shang, Y. Wu, and C. L. Zitnick, \u201cELF: An ex- tensive, lightweight and \ufb02exible research platform for real-time strat- egy games,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 2656\u20132666.",
        "type": "ListItem"
    },
    {
        "element_id": "7f22060c7fd65f3ba43957f52d1ba582",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        708.2
                    ],
                    [
                        847.6,
                        780.1
                    ],
                    [
                        1545.0,
                        780.1
                    ],
                    [
                        1545.0,
                        708.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93492,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[165] R. J. Williams and D. Zipser, \u201cA learning algorithm for continually running fully recurrent neural networks,\u201d Neural Comput., vol. 1, no. 2, pp. 270\u2013280, 1989.",
        "type": "ListItem"
    },
    {
        "element_id": "276fa4f9a73cfa9f673a42af1433c307",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        782.9
                    ],
                    [
                        847.6,
                        879.8
                    ],
                    [
                        1545.0,
                        879.8
                    ],
                    [
                        1545.0,
                        782.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93912,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[166] Y. Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba, \u201cScalable trust- region method for deep reinforcement learning using kronecker-factored approximation,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5285\u20135294.",
        "type": "ListItem"
    },
    {
        "element_id": "d92c77c577f9ce4250e01dd2c2fae419",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        882.5
                    ],
                    [
                        847.6,
                        954.5
                    ],
                    [
                        1545.0,
                        954.5
                    ],
                    [
                        1545.0,
                        882.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93296,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[167] Y. Wu and Y. Tian, \u201cTraining agent for \ufb01rst-person shooter game with actor-critic curriculum learning,\u201d in Proc. Int. Conf. Learn. Represent., 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "48ee8b7d231c4c2c5b9d75fe9e9c1f40",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        957.1
                    ],
                    [
                        847.6,
                        1029.2
                    ],
                    [
                        1545.0,
                        1029.2
                    ],
                    [
                        1545.0,
                        957.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93024,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[168] B. Wymann, E. Espi\u00b4e, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner, TORCS, The Open Racing Car Simulator, Software, 2000. [Online]. Available: http://torcs.sourceforge.net",
        "type": "ListItem"
    },
    {
        "element_id": "0933eaa1aca1de2132e7c195c6e136c7",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1031.8
                    ],
                    [
                        847.6,
                        1104.4
                    ],
                    [
                        1546.0,
                        1104.4
                    ],
                    [
                        1546.0,
                        1031.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93127,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[169] G. N. Yannakakis, P. Spronck, D. Loiacono, and E. Andr\u00b4e, \u201cPlayer modeling,\u201d in Dagstuhl Follow-Ups, vol. 6. Wadern, Germany: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.",
        "type": "ListItem"
    },
    {
        "element_id": "94c639383091a3a706169efa64f08aa4",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1106.7
                    ],
                    [
                        847.6,
                        1178.6
                    ],
                    [
                        1545.0,
                        1178.6
                    ],
                    [
                        1545.0,
                        1106.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93379,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[170] G. N. Yannakakis and J. Togelius, \u201cA panorama of arti\ufb01cial and compu- tational intelligence in games,\u201d IEEE Trans. Comput. Intell. AI Games, vol. 7, no. 4, pp. 317\u2013335, Dec. 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "94f4f2944401671bbcdc3dd226bded9e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1181.4
                    ],
                    [
                        117.0,
                        1253.4
                    ],
                    [
                        814.4,
                        1253.4
                    ],
                    [
                        814.4,
                        1181.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92935,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[152] E. Todorov, T. Erez, and Y. Tassa, \u201cMuJoCo: A physics engine for model- based control,\u201d in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2012, pp. 5026\u20135033.",
        "type": "ListItem"
    },
    {
        "element_id": "f4da80a9767a1695b00aa45dffd8ea1e",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1256.1
                    ],
                    [
                        117.0,
                        1328.1
                    ],
                    [
                        814.5,
                        1328.1
                    ],
                    [
                        814.5,
                        1256.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93482,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[153] J. Togelius, T. Schaul, D. Wierstra, C. Igel, F. Gomez, and J. Schmidhu- ber, \u201cOntogenetic and phylogenetic reinforcement learning,\u201d K\u00a8unstliche Intell., vol. 23, no. 3, pp. 30\u201333, 2009.",
        "type": "ListItem"
    },
    {
        "element_id": "6ac3e6ab7e9e3d4fce9b0877999098bd",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1330.8
                    ],
                    [
                        117.0,
                        1402.8
                    ],
                    [
                        814.4,
                        1402.8
                    ],
                    [
                        814.4,
                        1330.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93045,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[154] N. Usunier, G. Synnaeve, Z. Lin, and S. Chintala, \u201cEpisodic exploration for deep deterministic policies: An application to StarCraft microman- agement tasks,\u201d 2016, arXiv:1609.02993.",
        "type": "ListItem"
    },
    {
        "element_id": "41e28f2f50aea7bc53e7a458e7c7b427",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1405.6
                    ],
                    [
                        117.0,
                        1477.5
                    ],
                    [
                        814.4,
                        1477.5
                    ],
                    [
                        814.4,
                        1405.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92859,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[155] H. Van Hasselt, A. Guez, and D. Silver, \u201cDeep reinforcement learning with double q-learning,\u201d in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 2094\u20132100.",
        "type": "ListItem"
    },
    {
        "element_id": "c9e163d89572153399841f19f6dce03d",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1181.4
                    ],
                    [
                        847.6,
                        1228.5
                    ],
                    [
                        1545.0,
                        1228.5
                    ],
                    [
                        1545.0,
                        1181.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91436,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[171] G. N. Yannakakis and J. Togelius, Arti\ufb01cial Intelligence and Games. New York, NY, USA: Springer, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "bbc6336d2bf92d9963177b1f2cbbda3f",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1231.2
                    ],
                    [
                        847.6,
                        1303.2
                    ],
                    [
                        1545.0,
                        1303.2
                    ],
                    [
                        1545.0,
                        1231.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9274,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[172] H. Yu, H. Zhang, and W. Xu, \u201cA deep compositional framework for human-like language acquisition in virtual environment,\u201d 2017, arXiv:1703.09831.",
        "type": "ListItem"
    },
    {
        "element_id": "f566e1e9fe876bc2f3c4ced2406d0a44",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        847.6,
                        1305.9
                    ],
                    [
                        847.6,
                        1452.6
                    ],
                    [
                        1545.0,
                        1452.6
                    ],
                    [
                        1545.0,
                        1305.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93012,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[173] T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor, \u201cLearn what not to learn: Action elimination with deep reinforcement learning,\u201d in Advances in Neural Information Processing Systems 31, S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa- Bianchi and R. Garnett, Eds. New York, NY, USA: Curran Associates, 2018, pp. 3562\u20133573.",
        "type": "ListItem"
    },
    {
        "element_id": "a799634c18d3c21443438221dd2bfac5",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        117.0,
                        1480.3
                    ],
                    [
                        117.0,
                        1552.2
                    ],
                    [
                        814.4,
                        1552.2
                    ],
                    [
                        814.4,
                        1480.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93346,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "[156] H. Van Seijen, R. Laroche, M. Fatemi, and J. Romoff, \u201cHybrid reward architecture for reinforcement learning,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5396\u20135406.",
        "type": "ListItem"
    },
    {
        "element_id": "b8c4daa13c022981ac92afde7a1526e0",
        "metadata": {
            "coordinates": {
                "layout_height": 2200,
                "layout_width": 1650,
                "points": [
                    [
                        119.8,
                        2123.7
                    ],
                    [
                        119.8,
                        2144.1
                    ],
                    [
                        1530.2,
                        2144.1
                    ],
                    [
                        1530.2,
                        2123.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82763,
            "file_directory": "./uol-docs",
            "filename": "Deep_Learning_for_Video_Game_Playing.pdf",
            "languages": [
                "eng",
                "tur"
            ],
            "last_modified": "2024-12-28T22:55:45",
            "page_number": 20,
            "parent_id": "595fda1597fc24ec108d710536d91b6a"
        },
        "text": "Authorized licensed use limited to: University of London: Online Library. Downloaded on December 28,2024 at 22:55:38 UTC from IEEE Xplore. Restrictions apply.",
        "type": "NarrativeText"
    }
]