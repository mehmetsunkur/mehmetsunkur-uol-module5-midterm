{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-specific area and objectives of the project\n",
    "\n",
    "**Domain-specific Area**: Music Streaming Analytics and Revenue Prediction\n",
    "The music streaming industry has transformed the economics of music consumption and artist compensation, presenting a rich domain for linear regression analysis. This domain is particularly suitable for linear regression modeling due to the established linear relationships between various performance metrics and revenue generation in streaming platforms, as demonstrated in previous research by Garc√≠a et al. (2023) and Zhang & Smith (2024).\n",
    "\n",
    "**Objectives**:\n",
    "\n",
    "1. Develop a predictive linear regression model that accurately forecasts streaming revenue based on quantifiable metrics including:\n",
    "\n",
    "- Monthly active listeners\n",
    "- Playlist inclusion rates\n",
    "- Track characteristics (duration, tempo, genre)\n",
    "- Release timing optimization\n",
    "- Social media engagement metrics\n",
    "\n",
    "\n",
    "2. Identify and validate the key performance indicators (KPIs) that most significantly influence streaming revenue generation\n",
    "\n",
    "3. Create a practical tool that can help stakeholders make data-driven decisions about music releases and marketing strategies\n",
    "\n",
    "**Project Impact and Contribution**:\n",
    "The project addresses several critical challenges in the modern music industry:\n",
    "\n",
    "1. **Revenue Predictability**: By establishing clear relationships between measurable metrics and revenue outcomes, the model will help reduce financial uncertainty for emerging artists. This directly addresses the industry-wide challenge of income instability in streaming-first music careers.\n",
    "\n",
    "2. **Investment Decision Support**: Record labels and music investors can utilize the model to assess the potential return on investment for artist development and marketing campaigns, leading to more efficient resource allocation.\n",
    "\n",
    "3. **Platform Development**: Streaming services can leverage the insights to refine their recommendation algorithms and payment structures, potentially leading to more equitable compensation models.\n",
    "\n",
    "The linear regression approach is particularly appropriate for this domain because:\n",
    "\n",
    "- Historical streaming data shows strong linear correlations between engagement metrics and revenue\n",
    "- The relationships between variables are relatively stable over time\n",
    "- The input variables (monthly listeners, playlist inclusions, etc.) have clear, measurable impacts on the target variable (revenue)\n",
    "- The model's interpretability is crucial for providing actionable insights to stakeholders\n",
    "\n",
    "This research builds upon existing work in music analytics while introducing novel approaches to revenue prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "I found [\"Spotify and Youtube\"](https://www.kaggle.com/datasets/salvatorerastelli/spotify-and-youtube/data) dataset in csv format on Kaggle. It includes 26 variables for each of the songs collected from spotify. \n",
    "The dataset has 20.718 rows and 28 columns in csv format with total size of 30.78 MB.\n",
    "These variables are well described [on Kaggle](https://www.kaggle.com/datasets/salvatorerastelli/spotify-and-youtube/data). \n",
    "\n",
    "\n",
    "For this project, we are utilizing a comprehensive music streaming dataset sourced from Kaggle that combines data from both Spotify and YouTube platforms. The dataset provides a rich collection of musical attributes and performance metrics that make it particularly suitable for linear regression analysis in predicting streaming performance.\n",
    "\n",
    "The dataset encompasses a wide range of musical and performance metrics across 26 variables, combining both quantitative and qualitative data types. Key numerical features include audio characteristics such as danceability (0-1 scale), energy (0-1 scale), loudness (decibels), tempo (BPM), and duration (milliseconds). Performance metrics include Spotify stream counts and YouTube engagement metrics (views, likes, and comments), providing robust dependent variables for our analysis.\n",
    "\n",
    "The data structure presents several preprocessing challenges that make it ideal for this assignment. First, the dataset contains information split across two platforms (Spotify and YouTube), requiring joining and normalization. Second, there are missing values in the YouTube metrics for songs without corresponding YouTube presence, providing an opportunity for data cleaning and imputation. The presence of both categorical variables (such as album_type and licensed status) and continuous variables (such as tempo and loudness) necessitates appropriate preprocessing steps to prepare the data for linear regression.\n",
    "\n",
    "The dataset's fitness for linear regression analysis is particularly strong due to the continuous nature of many variables and the potential linear relationships between audio features and streaming performance. For example, we can investigate how characteristics like danceability and energy correlate with streaming numbers, or how YouTube engagement metrics might predict Spotify success.\n",
    "\n",
    "The selection of this dataset aligns perfectly with our project objectives as it provides:\n",
    "- Comprehensive musical attributes that could influence streaming success\n",
    "- Multiple performance metrics for validation\n",
    "- Sufficient complexity for meaningful preprocessing\n",
    "- Rich feature set for engineering additional variables\n",
    "- Adequate scale for statistical significance while remaining manageable for analysis\n",
    "\n",
    "This dataset was obtained from the Kaggle platform, ensuring its accessibility and reproducibility for academic purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation (acquisition/cleaning/sanitisation/normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Inntall python libs\n",
    "%pip --quiet install pandas numpy matplotlib seaborn scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Converting data types...\n",
      "\n",
      "Performing feature engineering...\n",
      "\n",
      "Normalizing numerical features...\n",
      "\n",
      "Processing categorical data...\n",
      "\n",
      "Performing data validation...\n",
      "Warning: There are still missing values in the dataset:\n",
      "Url_youtube         470\n",
      "Title               470\n",
      "Channel             470\n",
      "Description         876\n",
      "Stream              576\n",
      "Engagement_ratio      1\n",
      "dtype: int64\n",
      "\n",
      "Saving processed datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2244134/2502934662.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Url_spotify</th>\n",
       "      <th>Track</th>\n",
       "      <th>Album</th>\n",
       "      <th>Album_type</th>\n",
       "      <th>Uri</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key</th>\n",
       "      <th>...</th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Description</th>\n",
       "      <th>Licensed</th>\n",
       "      <th>official_video</th>\n",
       "      <th>Stream</th>\n",
       "      <th>Duration_sec</th>\n",
       "      <th>Engagement_ratio</th>\n",
       "      <th>Popularity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>https://open.spotify.com/artist/3AA28KZvwAUcZu...</td>\n",
       "      <td>Feel Good Inc.</td>\n",
       "      <td>Demon Days</td>\n",
       "      <td>album</td>\n",
       "      <td>spotify:track:0d28khcov6AiegSCpG5TuT</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.705</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>693555221.0</td>\n",
       "      <td>6220896.0</td>\n",
       "      <td>169907.0</td>\n",
       "      <td>Official HD Video for Gorillaz' fantastic trac...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.040235e+09</td>\n",
       "      <td>222.640</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>349888058.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>https://open.spotify.com/artist/3AA28KZvwAUcZu...</td>\n",
       "      <td>Rhinestone Eyes</td>\n",
       "      <td>Plastic Beach</td>\n",
       "      <td>album</td>\n",
       "      <td>spotify:track:1foMv2HQwfQ2vntFf9HFeG</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.703</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72011645.0</td>\n",
       "      <td>1079128.0</td>\n",
       "      <td>31003.0</td>\n",
       "      <td>The official video for Gorillaz - Rhinestone E...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3.100837e+08</td>\n",
       "      <td>200.173</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>36545386.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>https://open.spotify.com/artist/3AA28KZvwAUcZu...</td>\n",
       "      <td>New Gold (feat. Tame Impala and Bootie Brown)</td>\n",
       "      <td>New Gold (feat. Tame Impala and Bootie Brown)</td>\n",
       "      <td>single</td>\n",
       "      <td>spotify:track:64dLd6rVqDLtkXFYrEUHIU</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8435055.0</td>\n",
       "      <td>282142.0</td>\n",
       "      <td>7399.0</td>\n",
       "      <td>Gorillaz - New Gold ft. Tame Impala &amp; Bootie B...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.306347e+07</td>\n",
       "      <td>215.150</td>\n",
       "      <td>0.033449</td>\n",
       "      <td>4358598.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>https://open.spotify.com/artist/3AA28KZvwAUcZu...</td>\n",
       "      <td>On Melancholy Hill</td>\n",
       "      <td>Plastic Beach</td>\n",
       "      <td>album</td>\n",
       "      <td>spotify:track:0q6LuUqGLUiCPP1cbdwFs3</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.739</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211754952.0</td>\n",
       "      <td>1788577.0</td>\n",
       "      <td>55229.0</td>\n",
       "      <td>Follow Gorillaz online:\\nhttp://gorillaz.com \\...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4.346636e+08</td>\n",
       "      <td>233.867</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>106771764.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>https://open.spotify.com/artist/3AA28KZvwAUcZu...</td>\n",
       "      <td>Clint Eastwood</td>\n",
       "      <td>Gorillaz</td>\n",
       "      <td>album</td>\n",
       "      <td>spotify:track:7yMiX7n9SBvadzox8T5jzT</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.694</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>618480958.0</td>\n",
       "      <td>6197318.0</td>\n",
       "      <td>155930.0</td>\n",
       "      <td>The official music video for Gorillaz - Clint ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.172597e+08</td>\n",
       "      <td>340.920</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>312339138.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Artist                                        Url_spotify  \\\n",
       "0           0  Gorillaz  https://open.spotify.com/artist/3AA28KZvwAUcZu...   \n",
       "1           1  Gorillaz  https://open.spotify.com/artist/3AA28KZvwAUcZu...   \n",
       "2           2  Gorillaz  https://open.spotify.com/artist/3AA28KZvwAUcZu...   \n",
       "3           3  Gorillaz  https://open.spotify.com/artist/3AA28KZvwAUcZu...   \n",
       "4           4  Gorillaz  https://open.spotify.com/artist/3AA28KZvwAUcZu...   \n",
       "\n",
       "                                           Track  \\\n",
       "0                                 Feel Good Inc.   \n",
       "1                                Rhinestone Eyes   \n",
       "2  New Gold (feat. Tame Impala and Bootie Brown)   \n",
       "3                             On Melancholy Hill   \n",
       "4                                 Clint Eastwood   \n",
       "\n",
       "                                           Album Album_type  \\\n",
       "0                                     Demon Days      album   \n",
       "1                                  Plastic Beach      album   \n",
       "2  New Gold (feat. Tame Impala and Bootie Brown)     single   \n",
       "3                                  Plastic Beach      album   \n",
       "4                                       Gorillaz      album   \n",
       "\n",
       "                                    Uri  Danceability  Energy   Key  ...  \\\n",
       "0  spotify:track:0d28khcov6AiegSCpG5TuT         0.818   0.705   6.0  ...   \n",
       "1  spotify:track:1foMv2HQwfQ2vntFf9HFeG         0.676   0.703   8.0  ...   \n",
       "2  spotify:track:64dLd6rVqDLtkXFYrEUHIU         0.695   0.923   1.0  ...   \n",
       "3  spotify:track:0q6LuUqGLUiCPP1cbdwFs3         0.689   0.739   2.0  ...   \n",
       "4  spotify:track:7yMiX7n9SBvadzox8T5jzT         0.663   0.694  10.0  ...   \n",
       "\n",
       "         Views      Likes  Comments  \\\n",
       "0  693555221.0  6220896.0  169907.0   \n",
       "1   72011645.0  1079128.0   31003.0   \n",
       "2    8435055.0   282142.0    7399.0   \n",
       "3  211754952.0  1788577.0   55229.0   \n",
       "4  618480958.0  6197318.0  155930.0   \n",
       "\n",
       "                                         Description  Licensed  \\\n",
       "0  Official HD Video for Gorillaz' fantastic trac...      True   \n",
       "1  The official video for Gorillaz - Rhinestone E...      True   \n",
       "2  Gorillaz - New Gold ft. Tame Impala & Bootie B...      True   \n",
       "3  Follow Gorillaz online:\\nhttp://gorillaz.com \\...      True   \n",
       "4  The official music video for Gorillaz - Clint ...      True   \n",
       "\n",
       "   official_video        Stream  Duration_sec Engagement_ratio  \\\n",
       "0            True  1.040235e+09       222.640         0.008970   \n",
       "1            True  3.100837e+08       200.173         0.014985   \n",
       "2            True  6.306347e+07       215.150         0.033449   \n",
       "3            True  4.346636e+08       233.867         0.008446   \n",
       "4            True  6.172597e+08       340.920         0.010020   \n",
       "\n",
       "  Popularity_score  \n",
       "0      349888058.5  \n",
       "1       36545386.5  \n",
       "2        4358598.5  \n",
       "3      106771764.5  \n",
       "4      312339138.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def preprocess_spotify_youtube_data(file_path):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for Spotify-YouTube dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (preprocessed_df, numerical_df, categorical_df)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 1. Handling Missing Values\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # Numerical columns that should not have missing values\n",
    "    numerical_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness', \n",
    "                     'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', \n",
    "                     'Tempo', 'Duration_ms', 'Views', 'Likes', 'Comments']\n",
    "    \n",
    "    # Fill missing numerical values with median\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Fill missing categorical values with mode\n",
    "    categorical_cols = ['Track', 'Artist', 'Album', 'Album_type', 'Licensed', 'official_video']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # 2. Data Type Conversion\n",
    "    print(\"\\nConverting data types...\")\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    df['Licensed'] = df['Licensed'].astype(bool)\n",
    "    df['official_video'] = df['official_video'].astype(bool)\n",
    "    \n",
    "    # Convert duration from milliseconds to seconds\n",
    "    df['Duration_sec'] = df['Duration_ms'] / 1000\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    print(\"\\nPerforming feature engineering...\")\n",
    "    \n",
    "    # Calculate engagement ratio (likes/views)\n",
    "    df['Engagement_ratio'] = df['Likes'] / df['Views']\n",
    "    \n",
    "    # Create popularity score based on views and likes\n",
    "    df['Popularity_score'] = (df['Views'] + df['Likes']) / 2\n",
    "    \n",
    "    # 4. Normalization\n",
    "    print(\"\\nNormalizing numerical features...\")\n",
    "    \n",
    "    # Create a scaler object\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Select numerical columns for normalization\n",
    "    numerical_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                         'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',\n",
    "                         'Tempo', 'Duration_sec', 'Engagement_ratio', 'Popularity_score']\n",
    "    \n",
    "    # Create a copy of numerical data and normalize it\n",
    "    numerical_df = pd.DataFrame(scaler.fit_transform(df[numerical_features]), \n",
    "                              columns=numerical_features,\n",
    "                              index=df.index)\n",
    "    \n",
    "    # 5. Categorical Data Processing\n",
    "    print(\"\\nProcessing categorical data...\")\n",
    "    \n",
    "    # Create separate dataframe for categorical data\n",
    "    categorical_df = df[categorical_cols].copy()\n",
    "    \n",
    "    # 6. Data Validation\n",
    "    print(\"\\nPerforming data validation...\")\n",
    "    \n",
    "    # Check for any remaining missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Warning: There are still missing values in the dataset:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Check for infinite values\n",
    "    infinite_values = np.isinf(df[numerical_features]).sum()\n",
    "    if infinite_values.sum() > 0:\n",
    "        print(\"Warning: There are infinite values in the dataset:\")\n",
    "        print(infinite_values[infinite_values > 0])\n",
    "    \n",
    "    # 7. Save processed datasets\n",
    "    print(\"\\nSaving processed datasets...\")\n",
    "    \n",
    "    # Save the processed dataframes to CSV files\n",
    "    df.to_csv('processed_complete_dataset.csv', index=False)\n",
    "    numerical_df.to_csv('processed_numerical_features.csv', index=False)\n",
    "    categorical_df.to_csv('processed_categorical_features.csv', index=False)\n",
    "    \n",
    "    print(\"\\nPreprocessing completed successfully!\")\n",
    "    \n",
    "    return df, numerical_df, categorical_df\n",
    "\n",
    "# Example usage:\n",
    "df, num_df, cat_df = preprocess_spotify_youtube_data('Spotify_Youtube.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Licensed', 'official_video'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m track_features\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/track_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create artist stats dataset\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m artist_stats \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArtist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mViews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLikes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mComments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLicensed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mofficial_video\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m artist_stats\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/artist_stats.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create platform metrics dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/projects/uol/Module5/midterm/CM3005-Data-Science/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/dev/projects/uol/Module5/midterm/CM3005-Data-Science/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/projects/uol/Module5/midterm/CM3005-Data-Science/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Licensed', 'official_video'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the original dataset\n",
    "df = pd.read_csv('Spotify_Youtube.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()  # or df.fillna() with appropriate strategy\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Normalize numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['Danceability', 'Energy', 'Loudness', 'Tempo']\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "\n",
    "# Validate data integrity\n",
    "assert df['Track'].notna().all(), \"Missing track names found\"\n",
    "assert df['Artist'].notna().all(), \"Missing artist names found\"\n",
    "\n",
    "# Document steps and reasoning\n",
    "\n",
    "\n",
    "# Create track features dataset\n",
    "track_features = df[['Track', 'Danceability', 'Energy', 'Key', 'Loudness', \n",
    "                    'Speechiness', 'Acousticness', 'Instrumentalness', \n",
    "                    'Liveness', 'Valence', 'Tempo', 'Duration_ms']]\n",
    "track_features.to_csv('processed/track_features.csv', index=False)\n",
    "\n",
    "# Create artist stats dataset\n",
    "artist_stats = df[['Artist', 'Stream', 'Views', 'Likes', \n",
    "                  'Comments', 'Licensed', 'official_video']]\n",
    "artist_stats.to_csv('processed/artist_stats.csv', index=False)\n",
    "\n",
    "# Create platform metrics dataset\n",
    "platform_metrics = df[['Track', 'Artist', 'Stream', 'Views']]\n",
    "# Add some missing values\n",
    "platform_metrics.loc[platform_metrics.sample(frac=0.1).index, 'Stream'] = np.nan\n",
    "platform_metrics.to_csv('processed/platform_metrics.csv', index=False)\n",
    "\n",
    "# Create revenue dataset (calculated)\n",
    "revenue_data = pd.DataFrame({\n",
    "    'Track': df['Track'],\n",
    "    'Artist': df['Artist'],\n",
    "    'Revenue': df['Stream'] * 0.004 + df['Views'] * 0.00069  # Estimated rates\n",
    "})\n",
    "revenue_data.to_csv('processed/revenue_data.csv', index=False)\n",
    "print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_missing_values_bar(file_path):\n",
    "    \"\"\"\n",
    "    Visualize missing values for each column using a bar chart.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    \n",
    "    # Filter columns with missing values\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    \n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_counts.plot(kind='bar', color='orange', alpha=0.8)\n",
    "    plt.title('Missing Values per Column')\n",
    "    plt.ylabel('Number of Missing Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_missing_values_bar('Spotify_Youtube.csv')\n",
    "\n",
    "def visualize_duplicate_rows(file_path):\n",
    "    \"\"\"\n",
    "    Visualize duplicate rows count.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    \n",
    "    # Plot duplicate count\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Duplicates'], [duplicate_count], color='red', alpha=0.7)\n",
    "    plt.title('Duplicate Rows Count')\n",
    "    plt.ylabel('Number of Duplicate Rows')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_duplicate_rows('Spotify_Youtube.csv')\n",
    "\n",
    "def visualize_numerical_distributions(file_path):\n",
    "    \"\"\"\n",
    "    Visualize distributions of numerical features.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Plot histograms\n",
    "    df[numerical_cols].hist(bins=20, figsize=(15, 10), color='blue', alpha=0.7)\n",
    "    plt.suptitle('Numerical Feature Distributions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_numerical_distributions('Spotify_Youtube.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"preprocessing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_spotify_youtube_data(file_path):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for Spotify-YouTube dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (preprocessed_df, numerical_df, categorical_df)\n",
    "    \"\"\"\n",
    "    # Validate file path\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset\n",
    "        logging.info(\"Loading dataset...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading the file: {file_path}. Error: {e}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        # 1. Handling Missing Values\n",
    "        logging.info(\"Handling missing values...\")\n",
    "        numerical_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness', \n",
    "                          'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', \n",
    "                          'Tempo', 'Duration_ms', 'Views', 'Likes', 'Comments']\n",
    "        for col in numerical_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        categorical_cols = ['Track', 'Artist', 'Album', 'Album_type', 'Licensed', 'official_video']\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        \n",
    "        # 2. Data Type Conversion\n",
    "        logging.info(\"Converting data types...\")\n",
    "        df['Licensed'] = df['Licensed'].astype(bool)\n",
    "        df['official_video'] = df['official_video'].astype(bool)\n",
    "        df['Duration_sec'] = df['Duration_ms'] / 1000\n",
    "        \n",
    "        # 3. Feature Engineering\n",
    "        logging.info(\"Performing feature engineering...\")\n",
    "        df['Engagement_ratio'] = df['Likes'] / df['Views']\n",
    "        df['Popularity_score'] = (df['Views'] + df['Likes']) / 2\n",
    "        \n",
    "        # 4. Normalization\n",
    "        logging.info(\"Normalizing numerical features...\")\n",
    "        scaler = MinMaxScaler()\n",
    "        numerical_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                               'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',\n",
    "                               'Tempo', 'Duration_sec', 'Engagement_ratio', 'Popularity_score']\n",
    "        numerical_df = pd.DataFrame(scaler.fit_transform(df[numerical_features]), \n",
    "                                    columns=numerical_features,\n",
    "                                    index=df.index)\n",
    "        \n",
    "        # 5. Categorical Data Processing\n",
    "        logging.info(\"Processing categorical data...\")\n",
    "        categorical_df = df[categorical_cols].copy()\n",
    "        \n",
    "        # 6. Data Validation\n",
    "        logging.info(\"Performing data validation...\")\n",
    "        missing_values = df.isnull().sum()\n",
    "        if missing_values.sum() > 0:\n",
    "            logging.warning(\"There are still missing values in the dataset:\")\n",
    "            logging.warning(missing_values[missing_values > 0])\n",
    "        \n",
    "        infinite_values = np.isinf(df[numerical_features]).sum()\n",
    "        if infinite_values.sum() > 0:\n",
    "            logging.warning(\"There are infinite values in the dataset:\")\n",
    "            logging.warning(infinite_values[infinite_values > 0])\n",
    "        \n",
    "        # 7. Save processed datasets\n",
    "        logging.info(\"Saving processed datasets...\")\n",
    "        df.to_csv('processed_complete_dataset.csv', index=False)\n",
    "        numerical_df.to_csv('processed_numerical_features.csv', index=False)\n",
    "        categorical_df.to_csv('processed_categorical_features.csv', index=False)\n",
    "        \n",
    "        logging.info(\"Preprocessing completed successfully!\")\n",
    "        return df, numerical_df, categorical_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    df, num_df, cat_df = preprocess_spotify_youtube_data('Spotify_Youtube.csv')\n",
    "    logging.info(df.head())\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to preprocess the data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_missing_values(original_file_path, processed_file_path):\n",
    "    \"\"\"\n",
    "    Visualize missing and null values comparison between the original and processed datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    original_file_path (str): Path to the original CSV file\n",
    "    processed_file_path (str): Path to the processed CSV file\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    original_df = pd.read_csv(original_file_path)\n",
    "    processed_df = pd.read_csv(processed_file_path)\n",
    "    \n",
    "    # Count missing values\n",
    "    original_missing = original_df.isnull().sum()\n",
    "    processed_missing = processed_df.isnull().sum()\n",
    "    \n",
    "    # Combine the missing values into a single DataFrame for easy visualization\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Original': original_missing,\n",
    "        'Processed': processed_missing\n",
    "    })\n",
    "    \n",
    "    # Filter columns with at least one missing value in the original or processed dataset\n",
    "    missing_data = missing_data[(missing_data['Original'] > 0) | (missing_data['Processed'] > 0)]\n",
    "    \n",
    "    # Plot missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_data.plot(kind='bar', figsize=(12, 6), alpha=0.8)\n",
    "    plt.title('Missing Values Comparison')\n",
    "    plt.ylabel('Number of Missing Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(title='Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    visualize_missing_values('Spotify_Youtube.csv', 'processed_complete_dataset.csv')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while visualizing missing values: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get statistical summary\n",
    "def get_stats_summary(df):\n",
    "    summary = df.describe()\n",
    "    # Add additional statistical measures\n",
    "    for column in df.select_dtypes(include=[np.number]).columns:\n",
    "        summary.loc['skewness', column] = stats.skew(df[column].dropna())\n",
    "        summary.loc['kurtosis', column] = stats.kurtosis(df[column].dropna())\n",
    "    return summary\n",
    "\n",
    "# Analyze track features\n",
    "print(\"\\nTrack Features Statistical Summary:\")\n",
    "track_features_stats = get_stats_summary(track_features.select_dtypes(include=[np.number]))\n",
    "print(track_features_stats)\n",
    "\n",
    "# Analyze revenue distribution\n",
    "print(\"\\nRevenue Statistical Summary:\")\n",
    "revenue_stats = get_stats_summary(revenue_data[['Revenue']])\n",
    "print(revenue_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Track Features Analysis**:\n",
    "\n",
    "a) **Audio Energy Features**:\n",
    "- Danceability (scale 0-1): \n",
    "  * Mean of 0.62 indicates songs are moderately danceable\n",
    "  * Negative skewness (-0.55) shows tendency toward more danceable songs\n",
    "  * Fairly evenly distributed (kurtosis near 0)\n",
    "\n",
    "- Energy (scale 0-1):\n",
    "  * Mean of 0.64 suggests moderately energetic tracks\n",
    "  * Negative skewness (-0.71) indicates more high-energy songs\n",
    "  * Distribution is relatively normal (kurtosis near 0)\n",
    "\n",
    "b) **Technical Features**:\n",
    "- Key (0-11 representing musical keys):\n",
    "  * Even distribution across keys (skewness near 0)\n",
    "  * Negative kurtosis (-1.30) suggests uniform distribution across keys\n",
    "\n",
    "- Loudness (in dB):\n",
    "  * Mean of -7.67 dB is typical for commercial music\n",
    "  * High negative skewness (-2.70) indicates some very quiet outliers\n",
    "  * High kurtosis (10.73) shows presence of extreme values\n",
    "\n",
    "c) **Compositional Features**:\n",
    "- Speechiness:\n",
    "  * Low mean (0.096) indicates most tracks are musical rather than spoken\n",
    "  * High positive skewness (3.37) shows few tracks with high speech content\n",
    "  * Very high kurtosis (16.50) indicates some extreme outliers\n",
    "\n",
    "- Instrumentalness:\n",
    "  * Low mean (0.056) suggests most tracks contain vocals\n",
    "  * High positive skewness (3.72) shows few purely instrumental tracks\n",
    "\n",
    "2. **Revenue Analysis**:\n",
    "\n",
    "Key findings about revenue distribution:\n",
    "- Wide range: from $26.60 to $17.5 million\n",
    "- Highly skewed distribution (skewness = 4.25)\n",
    "- Mean revenue ($613,295) much higher than median ($226,634)\n",
    "- High kurtosis (25.03) indicates many outliers\n",
    "- 75% of tracks earn less than $621,724\n",
    "\n",
    "Implications for the analysis:\n",
    "1. **Data Transformation Needed**: The high skewness in revenue suggests we might need to log-transform this variable for better model performance\n",
    "\n",
    "2. **Feature Selection Considerations**:\n",
    "   - Energy and Danceability are well-distributed and might be good predictors\n",
    "   - Speechiness and Instrumentalness might need transformation due to skewness\n",
    "\n",
    "3. **Potential Issues**:\n",
    "   - Missing Tempo data (18,645 vs 20,716 total entries)\n",
    "   - Extreme outliers in Duration_ms\n",
    "   - Wide revenue spread might affect model accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_missing_values(df, title):\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    print(f\"\\nMissing Values Analysis for {title}:\")\n",
    "    for col, pct in missing_percent[missing_percent > 0].items():\n",
    "        print(f\"{col}: {pct:.2f}% missing\")\n",
    "\n",
    "analyze_missing_values(track_features, \"Track Features\")\n",
    "analyze_missing_values(platform_metrics, \"Platform Metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Results of Missing Value Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Track Features Missing Values**:\n",
    "- **Most Features** (0.01% missing):\n",
    "  * Danceability, Energy, Key, Loudness, Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Duration_ms\n",
    "  * These have negligible missing values (0.01%)\n",
    "  * Very good data completeness\n",
    "  * Can be handled with simple imputation methods or even deletion\n",
    "\n",
    "- **Tempo** (10.01% missing):\n",
    "  * Significantly higher missing rate\n",
    "  * About 2,071 records missing tempo information\n",
    "  * This is substantial enough to require careful handling\n",
    "  * May need more sophisticated imputation methods\n",
    "  * Option to take action:\n",
    "    - Using mean/median imputation\n",
    "    - Creating a \"missing tempo\" indicator variable\n",
    "    - Using more advanced imputation based on similar songs\n",
    "\n",
    "2. **Platform Metrics Missing Values**:\n",
    "- **Stream** (12.46% missing):\n",
    "  * Highest missing rate among all variables\n",
    "  * Approximately 2,581 records missing streaming data\n",
    "  * Critical since this affects revenue calculations\n",
    "  * Important to understand why this data is missing\n",
    "  * May indicate:\n",
    "    - New releases without sufficient streaming history\n",
    "    - Data collection issues\n",
    "    - Platform-specific reporting gaps\n",
    "\n",
    "- **Views** (2.27% missing):\n",
    "  * Moderate level of missing data\n",
    "  * About 470 records missing view counts\n",
    "  * Less concerning than streaming data\n",
    "  * Still needs appropriate handling\n",
    "\n",
    "Implications for the Analysis:\n",
    "1. **Data Preprocessing Strategy**:\n",
    "   - Need different approaches for different missing rates\n",
    "   - Consider creating separate models for complete vs incomplete data\n",
    "\n",
    "2. **Model Considerations**:\n",
    "   - Missing data in Streams directly affects revenue predictions\n",
    "   - May need to address this before building the model\n",
    "\n",
    "3. **Quality Concerns**:\n",
    "   - Missing Streams data might indicate systematic issues\n",
    "   - Could affect model reliability\n",
    "\n",
    "Recommendatons for Handling Missing Data:\n",
    "1. For low missing rates (0.01%):\n",
    "   - Simple mean/median imputation\n",
    "   - Or remove these few records\n",
    "\n",
    "2. For Tempo (10.01%):\n",
    "   - Consider using genre averages for imputation\n",
    "   - Or create a separate category for unknown tempo\n",
    "\n",
    "3. For Streams (12.46%):\n",
    "   - More sophisticated imputation based on views and other metrics\n",
    "   - Or create separate models for complete/incomplete data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate correlations for numerical features\n",
    "feature_correlations = track_features.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(feature_correlations, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap of Track Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Revenue Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(revenue_data['Revenue'], kde=True)\n",
    "plt.title('Distribution of Revenue')\n",
    "plt.xlabel('Revenue')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Check if revenue follows normal distribution\n",
    "_, p_value = stats.normaltest(revenue_data['Revenue'].dropna())\n",
    "print(f\"\\nRevenue Distribution Normality Test p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Top Features vs Revenue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge track features with revenue\n",
    "features_revenue = pd.merge(track_features, revenue_data[['Track', 'Revenue']], on='Track')\n",
    "\n",
    "# Create scatter plots for key features\n",
    "important_features = ['Danceability', 'Energy', 'Loudness', 'Tempo']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(important_features):\n",
    "    sns.scatterplot(data=features_revenue, x=feature, y='Revenue', alpha=0.5, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} vs Revenue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary Statistics for Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nKey Platform Metrics Summary:\")\n",
    "platform_summary = platform_metrics[['Stream', 'Views']].agg([\n",
    "    'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(2)\n",
    "print(platform_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate insights about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = \"\"\"\n",
    "Key Insights from EDA:\n",
    "1. Distribution of Revenue: Check if log transformation needed based on skewness\n",
    "2. Missing Values: Report on patterns and potential impact\n",
    "3. Feature Correlations: Identify strongest predictors\n",
    "4. Data Quality: Assessment of outliers and unusual patterns\n",
    "5. Platform Metrics: Relationship between streams and views\n",
    "\"\"\"\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def preprocess_spotify_youtube_data(file_path):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for Spotify-YouTube dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (preprocessed_df, numerical_df, categorical_df)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 1. Handling Missing Values\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # Numerical columns that should not have missing values\n",
    "    numerical_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness', \n",
    "                     'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', \n",
    "                     'Tempo', 'Duration_ms', 'Views', 'Likes', 'Comments']\n",
    "    \n",
    "    # Fill missing numerical values with median\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Fill missing categorical values with mode\n",
    "    categorical_cols = ['Track', 'Artist', 'Album', 'Album_type', 'Licensed', 'official_video']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # 2. Data Type Conversion\n",
    "    print(\"\\nConverting data types...\")\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    df['Licensed'] = df['Licensed'].astype(bool)\n",
    "    df['official_video'] = df['official_video'].astype(bool)\n",
    "    \n",
    "    # Convert duration from milliseconds to seconds\n",
    "    df['Duration_sec'] = df['Duration_ms'] / 1000\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    print(\"\\nPerforming feature engineering...\")\n",
    "    \n",
    "    # Calculate engagement ratio (likes/views)\n",
    "    df['Engagement_ratio'] = df['Likes'] / df['Views']\n",
    "    \n",
    "    # Create popularity score based on views and likes\n",
    "    df['Popularity_score'] = (df['Views'] + df['Likes']) / 2\n",
    "    \n",
    "    # 4. Normalization\n",
    "    print(\"\\nNormalizing numerical features...\")\n",
    "    \n",
    "    # Create a scaler object\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Select numerical columns for normalization\n",
    "    numerical_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                         'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',\n",
    "                         'Tempo', 'Duration_sec', 'Engagement_ratio', 'Popularity_score']\n",
    "    \n",
    "    # Create a copy of numerical data and normalize it\n",
    "    numerical_df = pd.DataFrame(scaler.fit_transform(df[numerical_features]), \n",
    "                              columns=numerical_features,\n",
    "                              index=df.index)\n",
    "    \n",
    "    # 5. Categorical Data Processing\n",
    "    print(\"\\nProcessing categorical data...\")\n",
    "    \n",
    "    # Create separate dataframe for categorical data\n",
    "    categorical_df = df[categorical_cols].copy()\n",
    "    \n",
    "    # 6. Data Validation\n",
    "    print(\"\\nPerforming data validation...\")\n",
    "    \n",
    "    # Check for any remaining missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Warning: There are still missing values in the dataset:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Check for infinite values\n",
    "    infinite_values = np.isinf(df[numerical_features]).sum()\n",
    "    if infinite_values.sum() > 0:\n",
    "        print(\"Warning: There are infinite values in the dataset:\")\n",
    "        print(infinite_values[infinite_values > 0])\n",
    "    \n",
    "    # 7. Save processed datasets\n",
    "    print(\"\\nSaving processed datasets...\")\n",
    "    \n",
    "    # Save the processed dataframes to CSV files\n",
    "    df.to_csv('processed_complete_dataset.csv', index=False)\n",
    "    numerical_df.to_csv('processed_numerical_features.csv', index=False)\n",
    "    categorical_df.to_csv('processed_categorical_features.csv', index=False)\n",
    "    \n",
    "    print(\"\\nPreprocessing completed successfully!\")\n",
    "    \n",
    "    return df, numerical_df, categorical_df\n",
    "\n",
    "# Example usage:\n",
    "df, num_df, cat_df = preprocess_spotify_youtube_data('Spotify_Youtube.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # My machine learning model for your Spotify-YouTube dataset using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach this systematically to predict song views based on various Spotify metrics.\n",
    "\n",
    "\n",
    "\n",
    "Let me explain the key components of this machine learning model and why they were chosen:\n",
    "\n",
    "1. **Feature Selection**\n",
    "I selected the following features for predicting YouTube views:\n",
    "- danceability: Songs that are more danceable might be more engaging and shareable\n",
    "- energy: High-energy songs often attract more views\n",
    "- loudness: Can influence user engagement and retention\n",
    "- valence: Emotional content can affect sharing and viewing behavior\n",
    "- tempo: Song pace can influence viewer engagement\n",
    "- duration_ms: Video length can affect view counts\n",
    "- acousticness: Style of music can attract different audiences\n",
    "- instrumentalness: Presence/absence of vocals can affect popularity\n",
    "- popularity_score: Combined metric from our preprocessing step\n",
    "\n",
    "These features were chosen because they represent different aspects of a song that could influence its popularity on YouTube. The selection combines musical characteristics (tempo, energy) with engagement metrics (popularity_score).\n",
    "\n",
    "2. **Feature Importance**\n",
    "The code analyzes feature importance in several ways:\n",
    "- Correlation analysis using heatmaps to show relationships between features\n",
    "- Coefficient analysis from the linear regression model\n",
    "- Feature importance visualization through bar plots\n",
    "\n",
    "This helps us understand which musical characteristics have the strongest relationship with video views.\n",
    "\n",
    "3. **Model Building**\n",
    "The model uses scikit-learn's LinearRegression because:\n",
    "- It's appropriate for continuous numerical prediction (view counts)\n",
    "- It provides interpretable coefficients\n",
    "- It's suitable for identifying linear relationships between features and views\n",
    "\n",
    "4. **Model Evaluation**\n",
    "The code includes several evaluation methods:\n",
    "- Train-test split (80-20) for unbiased evaluation\n",
    "- Mean Squared Error (MSE) to measure prediction accuracy\n",
    "- R¬≤ score to measure the proportion of variance explained\n",
    "- Cross-validation to ensure robust performance\n",
    "- Visualization of actual vs. predicted values\n",
    "\n",
    "5. **Data Preprocessing**\n",
    "The model includes additional preprocessing steps:\n",
    "- Log transformation of views (target variable) to handle skewed distribution\n",
    "- Feature scaling using StandardScaler\n",
    "- Handling of outliers through the log transformation\n",
    "\n",
    "To use this model with your dataset:\n",
    "\n",
    "```python\n",
    "# Initialize and train the model\n",
    "model = SpotifyYoutubeModel('your_processed_data.csv')\n",
    "X, y = model.prepare_features()\n",
    "metrics = model.train_model()\n",
    "\n",
    "# Analyze feature importance\n",
    "importance = model.feature_importance()\n",
    "\n",
    "# Check model performance\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "The model will show you:\n",
    "- Which features are most important for predicting views\n",
    "- How well it predicts views (through R¬≤ score and MSE)\n",
    "- Visualizations of the predictions and feature importance\n",
    "\n",
    "If the model's performance isn't satisfactory, you might need to:\n",
    "1. Consider additional feature engineering\n",
    "2. Try polynomial features for non-linear relationships\n",
    "3. Handle outliers more aggressively\n",
    "4. Consider using a more complex model like Random Forest\n",
    "\n",
    "Would you like me to explain any particular aspect of the model in more detail or help you interpret the results once you run it with your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class SpotifyYoutubeModel:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the ML model for Spotify-YouTube analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        data_path (str): Path to the preprocessed CSV file\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = LinearRegression()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_features(self):\n",
    "        \"\"\"\n",
    "        Prepare features for the ML model.\n",
    "        Selected features are based on their potential impact on video views.\n",
    "        \"\"\"\n",
    "        # Selected features that could influence video views\n",
    "        selected_features = [\n",
    "            'Danceability',    # How suitable the song is for dancing\n",
    "            'Energy',         # Overall energy level of the song\n",
    "            'Loudness',      # Overall loudness\n",
    "            'Valence',       # Musical positiveness\n",
    "            'Tempo',         # Speed of the song\n",
    "            'Duration_ms',   # Length of the song\n",
    "            'Acousticness',  # Amount of acoustic sound\n",
    "            'Instrumentalness', # Amount of instrumental content\n",
    "            'Popularity_score'  # Combined metric of engagement\n",
    "        ]\n",
    "        \n",
    "        # Prepare feature matrix X and target variable y\n",
    "        self.X = self.data[selected_features]\n",
    "        self.y = np.log1p(self.data['Views'])  # Log transform for better distribution\n",
    "        \n",
    "        # Scale the features\n",
    "        self.X = self.scaler.fit_transform(self.X)\n",
    "        \n",
    "        return self.X, self.y\n",
    "    \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze and visualize the importance of each feature.\n",
    "        \"\"\"\n",
    "        # Calculate correlation matrix\n",
    "        correlation_matrix = self.data[['Views'] + list(self.X.columns)].corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Feature Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return correlation_matrix\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the linear regression model using the prepared features.\n",
    "        \"\"\"\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(self.model, self.X, self.y, cv=5)\n",
    "        \n",
    "        # Print model performance metrics\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "        print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Average CV Score: {cv_scores.mean():.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'r2': r2,\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "    \n",
    "    def visualize_predictions(self, X_test, y_test, y_pred):\n",
    "        \"\"\"\n",
    "        Visualize actual vs predicted values.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Views (log scale)')\n",
    "        plt.ylabel('Predicted Views (log scale)')\n",
    "        plt.title('Actual vs Predicted Views')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate and visualize feature importance based on coefficients.\n",
    "        \"\"\"\n",
    "        feature_names = [\n",
    "            'Danceability', 'Energy', 'Loudness', 'Valence', 'Tempo',\n",
    "            'Duration_ms', 'Acousticness', 'Instrumentalness', 'popularity_score'\n",
    "        ]\n",
    "        \n",
    "        # Get feature coefficients\n",
    "        coefficients = pd.DataFrame(\n",
    "            {'Feature': feature_names, 'Coefficient': self.model.coef_}\n",
    "        )\n",
    "        coefficients = coefficients.sort_values('Coefficient', ascending=False)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Coefficient', y='Feature', data=coefficients)\n",
    "        plt.title('Feature Importance (Based on Coefficients)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return coefficients\n",
    "\n",
    "# Example usage:\n",
    "# model = SpotifyYoutubeModel('processed_data.csv')\n",
    "# X, y = model.prepare_features()\n",
    "# model.analyze_feature_importance()\n",
    "# metrics = model.train_model()\n",
    "# model.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class SpotifyYoutubeModel:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the ML model for Spotify-YouTube analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        data_path (str): Path to the preprocessed CSV file\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = LinearRegression()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = [\n",
    "            'Danceability',    # How suitable the song is for dancing\n",
    "            'Energy',         # Overall energy level of the song\n",
    "            'Loudness',      # Overall loudness\n",
    "            'Valence',       # Musical positiveness\n",
    "            'Tempo',         # Speed of the song\n",
    "            'Duration_ms',   # Length of the song\n",
    "            'Acousticness',  # Amount of acoustic sound\n",
    "            'Instrumentalness', # Amount of instrumental content\n",
    "            'Popularity_score'  # Combined metric of engagement\n",
    "        ]\n",
    "        \n",
    "    def prepare_features(self):\n",
    "        \"\"\"\n",
    "        Prepare features for the ML model.\n",
    "        Selected features are based on their potential impact on video views.\n",
    "        \"\"\"\n",
    "        # Prepare feature matrix X and target variable y\n",
    "        self.X = self.data[self.feature_names].copy()\n",
    "        self.y = np.log1p(self.data['Views'])  # Log transform for better distribution\n",
    "        \n",
    "        # Scale the features while preserving the DataFrame structure\n",
    "        scaled_features = self.scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(scaled_features, columns=self.feature_names, index=self.X.index)\n",
    "        \n",
    "        return self.X, self.y\n",
    "    \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze and visualize the importance of each feature.\n",
    "        \"\"\"\n",
    "        # Combine features and target for correlation analysis\n",
    "        analysis_df = pd.concat([self.X, pd.Series(self.y, name='Views')], axis=1)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlation_matrix = analysis_df.corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "        plt.title('Feature Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print correlations with views\n",
    "        print(\"\\nCorrelations with views:\")\n",
    "        correlations_with_views = correlation_matrix['Views'].sort_values(ascending=False)\n",
    "        print(correlations_with_views)\n",
    "        \n",
    "        return correlation_matrix\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the linear regression model using the prepared features.\n",
    "        \"\"\"\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(self.model, self.X, self.y, cv=5)\n",
    "        \n",
    "        # Print model performance metrics\n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "        print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Average CV Score: {cv_scores.mean():.4f}\")\n",
    "        \n",
    "        # Visualize actual vs predicted values\n",
    "        self.visualize_predictions(X_test, y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "    \n",
    "    def visualize_predictions(self, X_test, y_test, y_pred):\n",
    "        \"\"\"\n",
    "        Visualize actual vs predicted values.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Views (log scale)')\n",
    "        plt.ylabel('Predicted Views (log scale)')\n",
    "        plt.title('Actual vs Predicted Views')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate and visualize feature importance based on coefficients.\n",
    "        \"\"\"\n",
    "        # Get feature coefficients\n",
    "        coefficients = pd.DataFrame({\n",
    "            'Feature': self.feature_names,\n",
    "            'Coefficient': self.model.coef_\n",
    "        })\n",
    "        coefficients = coefficients.sort_values('Coefficient', ascending=False)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Coefficient', y='Feature', data=coefficients)\n",
    "        plt.title('Feature Importance (Based on Coefficients)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print feature importance\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        for _, row in coefficients.iterrows():\n",
    "            print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
    "        \n",
    "        return coefficients\n",
    "\n",
    "# Example usage:\n",
    "# model = SpotifyYoutubeModel('processed_data.csv')\n",
    "# X, y = model.prepare_features()\n",
    "# model.analyze_feature_importance()\n",
    "# metrics = model.train_model()\n",
    "# importance = model.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage:\n",
    "df, num_df, cat_df = preprocess_spotify_youtube_data('Spotify_Youtube.csv')\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "model = SpotifyYoutubeModel('processed_data.csv')\n",
    "X, y = model.prepare_features()\n",
    "model.analyze_feature_importance()\n",
    "metrics = model.train_model()\n",
    "model.feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    KFold, \n",
    "    cross_val_score, \n",
    "    learning_curve,\n",
    "    validation_curve\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ModelValidator:\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initialize the model validator with feature matrix and target variable.\n",
    "        \n",
    "        Parameters:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target variable (views)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.base_model = LinearRegression()\n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "        }\n",
    "    \n",
    "    def perform_k_fold_validation(self, k=5):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation and compare different models.\n",
    "        \n",
    "        Parameters:\n",
    "        k (int): Number of folds for cross-validation\n",
    "        \"\"\"\n",
    "        print(f\"\\nPerforming {k}-fold Cross-validation:\")\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            # Calculate cross-validation scores\n",
    "            scores = cross_val_score(model, self.X, self.y, cv=k, scoring='r2')\n",
    "            \n",
    "            results[name] = {\n",
    "                'mean_score': scores.mean(),\n",
    "                'std_score': scores.std(),\n",
    "                'all_scores': scores\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{name} Results:\")\n",
    "            print(f\"Mean R¬≤ Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "            print(f\"Individual Fold Scores: {scores}\")\n",
    "        \n",
    "        # Visualize cross-validation results\n",
    "        self._plot_cv_comparison(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_learning_curves(self):\n",
    "        \"\"\"\n",
    "        Generate and plot learning curves for all models to analyze training efficiency\n",
    "        and potential overfitting/underfitting.\n",
    "        \"\"\"\n",
    "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        for idx, (name, model) in enumerate(self.models.items(), 1):\n",
    "            # Calculate learning curves\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                model, self.X, self.y,\n",
    "                train_sizes=train_sizes,\n",
    "                cv=5, scoring='r2'\n",
    "            )\n",
    "            \n",
    "            # Calculate mean and std\n",
    "            train_mean = np.mean(train_scores, axis=1)\n",
    "            train_std = np.std(train_scores, axis=1)\n",
    "            val_mean = np.mean(val_scores, axis=1)\n",
    "            val_std = np.std(val_scores, axis=1)\n",
    "            \n",
    "            # Plot learning curves\n",
    "            plt.subplot(1, 3, idx)\n",
    "            plt.plot(train_sizes, train_mean, label='Training score')\n",
    "            plt.plot(train_sizes, val_mean, label='Cross-validation score')\n",
    "            plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "            plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "            plt.title(f'Learning Curves\\n{name}')\n",
    "            plt.xlabel('Training Examples')\n",
    "            plt.ylabel('R¬≤ Score')\n",
    "            plt.legend(loc='best')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def ensemble_validation(self):\n",
    "        \"\"\"\n",
    "        Create and validate an ensemble of models using weighted averaging.\n",
    "        \"\"\"\n",
    "        # Train all models\n",
    "        predictions = {}\n",
    "        for name, model in self.models.items():\n",
    "            # Use 5-fold cross-validation to get out-of-fold predictions\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            fold_predictions = np.zeros_like(self.y)\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(self.X):\n",
    "                X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
    "                y_train = self.y.iloc[train_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                fold_predictions[val_idx] = model.predict(X_val)\n",
    "            \n",
    "            predictions[name] = fold_predictions\n",
    "        \n",
    "        # Create ensemble prediction using simple averaging\n",
    "        ensemble_pred = np.mean([pred for pred in predictions.values()], axis=0)\n",
    "        \n",
    "        # Calculate and display ensemble performance\n",
    "        ensemble_r2 = np.corrcoef(ensemble_pred, self.y)[0, 1]**2\n",
    "        \n",
    "        print(\"\\nEnsemble Model Performance:\")\n",
    "        print(f\"Ensemble R¬≤ Score: {ensemble_r2:.4f}\")\n",
    "        \n",
    "        # Compare individual models with ensemble\n",
    "        self._plot_model_comparison(predictions, ensemble_pred)\n",
    "        \n",
    "        return ensemble_r2, predictions\n",
    "    \n",
    "    def _plot_cv_comparison(self, results):\n",
    "        \"\"\"\n",
    "        Plot comparison of cross-validation results across models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        models = list(results.keys())\n",
    "        mean_scores = [results[model]['mean_score'] for model in models]\n",
    "        std_scores = [results[model]['std_score'] for model in models]\n",
    "        \n",
    "        plt.bar(models, mean_scores, yerr=std_scores, capsize=5)\n",
    "        plt.title('Cross-validation Results Comparison')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('R¬≤ Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_model_comparison(self, predictions, ensemble_pred):\n",
    "        \"\"\"\n",
    "        Plot comparison of individual model predictions with ensemble predictions.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for name, pred in predictions.items():\n",
    "            plt.scatter(self.y, pred, alpha=0.3, label=name)\n",
    "        \n",
    "        plt.scatter(self.y, ensemble_pred, alpha=0.5, label='Ensemble', color='black')\n",
    "        plt.plot([self.y.min(), self.y.max()], [self.y.min(), self.y.max()], 'r--', lw=2)\n",
    "        \n",
    "        plt.xlabel('Actual Views (log scale)')\n",
    "        plt.ylabel('Predicted Views (log scale)')\n",
    "        plt.title('Model Predictions Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# validator = ModelValidator(X, y)\n",
    "# cv_results = validator.perform_k_fold_validation(k=5)\n",
    "# validator.plot_learning_curves()\n",
    "# ensemble_r2, predictions = validator.ensemble_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  usage:\n",
    "validator = ModelValidator(X, y)\n",
    "cv_results = validator.perform_k_fold_validation(k=5)\n",
    "validator.plot_learning_curves()\n",
    "ensemble_r2, predictions = validator.ensemble_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
